--- tmp/dl_track_us-0.1.1.tar.gz
+++ tmp/dl_track_us-0.1.2.tar.gz
│   --- dl_track_us-0.1.1.tar
├── +++ dl_track_us-0.1.2.tar
│ ├── file list
│ │ @@ -1,54 +1,59 @@
│ │ --rw-r--r--   0        0        0     8196 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/.DS_Store
│ │ --rw-r--r--   0        0        0       42 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/.gitattributes
│ │ --rw-r--r--   0        0        0      421 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/.pre-commit-config.yaml
│ │ --rw-r--r--   0        0        0      741 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/.readthedocs.yml
│ │ --rw-r--r--   0        0        0     4516 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/README.md
│ │ --rw-r--r--   0        0        0      457 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/environment.yml
│ │ --rw-r--r--   0        0        0      652 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/momentarily_necessary_for_import_structure.txt
│ │ --rw-r--r--   0        0        0      257 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/requirements.txt
│ │ --rw-r--r--   0        0        0       80 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/setup.cfg
│ │ --rw-r--r--   0        0        0      670 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/setup.py
│ │ --rw-r--r--   0        0        0      693 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/.github/workflows/draft-pdf.yml
│ │ --rw-r--r--   0        0        0    62787 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/DLTrack_GUI.py
│ │ --rw-r--r--   0        0        0      168 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/__init__.py
│ │ --rw-r--r--   0        0        0      171 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/__main__.py
│ │ --rw-r--r--   0        0        0   151070 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/home_im.ico
│ │ --rw-r--r--   0        0        0      795 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/gui_helpers/__init__.py
│ │ --rw-r--r--   0        0        0    28677 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/gui_helpers/calculate_architecture.py
│ │ --rw-r--r--   0        0        0    23593 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/gui_helpers/calculate_architecture_video.py
│ │ --rw-r--r--   0        0        0     7013 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/gui_helpers/calibrate.py
│ │ --rw-r--r--   0        0        0     4520 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/gui_helpers/calibrate_video.py
│ │ --rw-r--r--   0        0        0    23502 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/gui_helpers/do_calculations.py
│ │ --rw-r--r--   0        0        0    26815 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/gui_helpers/do_calculations_video.py
│ │ --rw-r--r--   0        0        0   151070 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/gui_helpers/home_im.ico
│ │ --rw-r--r--   0        0        0    27765 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/gui_helpers/image_quality.py
│ │ --rw-r--r--   0        0        0    36049 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/gui_helpers/manual_tracing.py
│ │ --rw-r--r--   0        0        0    27471 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/DL_Track/gui_helpers/model_training.py
│ │ --rw-r--r--   0        0        0    33610 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/Figures/Figure_B-A.png
│ │ --rw-r--r--   0        0        0   494274 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/Figures/Figure_video.png
│ │ --rw-r--r--   0        0        0   134843 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/Figures/home_im.png
│ │ --rw-r--r--   0        0        0   108888 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/Paper/figure1.png
│ │ --rw-r--r--   0        0        0    27712 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/Paper/paper.bib
│ │ --rw-r--r--   0        0        0     5237 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/Paper/paper.md
│ │ --rw-r--r--   0        0        0      658 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/changelog.d/20221107_105509_paul.ritsche_pip_package_pr.rst
│ │ --rw-r--r--   0        0        0      547 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/changelog.d/20221114_112818_paul.ritsche.rst
│ │ --rw-r--r--   0        0        0     8196 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/.DS_Store
│ │ --rw-r--r--   0        0        0      658 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/Makefile
│ │ --rwxr-xr-x   0        0        0      804 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/make.bat
│ │ --rw-r--r--   0        0        0     1948 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/image_labelling/Image_Labeling_DLTrack.ijm
│ │ --rw-r--r--   0        0        0     2041 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/source/DL_Track.gui_helpers.rst
│ │ --rw-r--r--   0        0        0      311 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/source/DL_Track.rst
│ │ --rw-r--r--   0        0        0     1130 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/source/conf.py
│ │ --rw-r--r--   0        0        0     6344 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/source/contribute.rst
│ │ --rw-r--r--   0        0        0     3318 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/source/index.rst
│ │ --rw-r--r--   0        0        0    10243 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/source/installation.rst
│ │ --rw-r--r--   0        0        0      908 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/source/modules.rst
│ │ --rw-r--r--   0        0        0      348 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/source/tests.rst
│ │ --rw-r--r--   0        0        0     2013 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/source/usage.rst
│ │ --rw-r--r--   0        0        0  6265733 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/docs/usage/DL_Track_tutorial.pdf
│ │ --rw-r--r--   0        0        0  1417980 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/tests/DL_Track_tests.pdf
│ │ --rw-r--r--   0        0        0       75 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/.gitignore
│ │ --rw-r--r--   0        0        0    11558 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/LICENSE
│ │ --rw-r--r--   0        0        0     3207 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/README_Pypi.md
│ │ --rw-r--r--   0        0        0     1139 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/pyproject.toml
│ │ --rw-r--r--   0        0        0     4257 2020-02-02 00:00:00.000000 dl_track_us-0.1.1/PKG-INFO
│ │ +-rw-r--r--   0        0        0     8196 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/.DS_Store
│ │ +-rw-r--r--   0        0        0       42 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/.gitattributes
│ │ +-rw-r--r--   0        0        0      421 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/.pre-commit-config.yaml
│ │ +-rw-r--r--   0        0        0      753 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/.readthedocs.yml
│ │ +-rw-r--r--   0        0        0     6515 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/README.md
│ │ +-rw-r--r--   0        0        0      460 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/environment.yml
│ │ +-rw-r--r--   0        0        0      263 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/requirements.txt
│ │ +-rw-r--r--   0        0        0       86 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/setup.cfg
│ │ +-rw-r--r--   0        0        0      772 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/setup.py
│ │ +-rw-r--r--   0        0        0      693 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/.github/workflows/draft-pdf.yml
│ │ +-rw-r--r--   0        0        0       83 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/.vscode/settings.json
│ │ +-rw-r--r--   0        0        0    64168 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/DL_Track_US_GUI.py
│ │ +-rw-r--r--   0        0        0      875 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/DL_Track_US_GUI.spec
│ │ +-rw-r--r--   0        0        0      179 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/__init__.py
│ │ +-rw-r--r--   0        0        0      173 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/__main__.py
│ │ +-rw-r--r--   0        0        0   151070 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/home_im.ico
│ │ +-rw-r--r--   0        0        0      747 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/gui_helpers/__init__.py
│ │ +-rw-r--r--   0        0        0    29061 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/gui_helpers/calculate_architecture.py
│ │ +-rw-r--r--   0        0        0    24000 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/gui_helpers/calculate_architecture_video.py
│ │ +-rw-r--r--   0        0        0     7062 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/gui_helpers/calibrate.py
│ │ +-rw-r--r--   0        0        0     4521 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/gui_helpers/calibrate_video.py
│ │ +-rw-r--r--   0        0        0    23434 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/gui_helpers/do_calculations.py
│ │ +-rw-r--r--   0        0        0    27202 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/gui_helpers/do_calculations_video.py
│ │ +-rw-r--r--   0        0        0   151070 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/gui_helpers/home_im.ico
│ │ +-rw-r--r--   0        0        0    37179 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/gui_helpers/manual_tracing.py
│ │ +-rw-r--r--   0        0        0    27656 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/DL_Track_US/gui_helpers/model_training.py
│ │ +-rw-r--r--   0        0        0    33610 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/Figures/Figure_B-A.png
│ │ +-rw-r--r--   0        0        0   149706 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/Figures/Figure_GUI.png
│ │ +-rw-r--r--   0        0        0   175264 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/Figures/Figure_analysis.png
│ │ +-rw-r--r--   0        0        0   494274 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/Figures/Figure_video.png
│ │ +-rw-r--r--   0        0        0   134843 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/Figures/home_im.png
│ │ +-rw-r--r--   0        0        0   175264 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/Paper/figure1.png
│ │ +-rw-r--r--   0        0        0   149706 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/Paper/figure2.png
│ │ +-rw-r--r--   0        0        0    28350 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/Paper/paper.bib
│ │ +-rw-r--r--   0        0        0     6831 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/Paper/paper.md
│ │ +-rw-r--r--   0        0        0      658 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/changelog.d/20221107_105509_paul.ritsche_pip_package_pr.rst
│ │ +-rw-r--r--   0        0        0      547 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/changelog.d/20221114_112818_paul.ritsche.rst
│ │ +-rw-r--r--   0        0        0      568 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/changelog.d/20230401_211702_paul.ritsche.rst
│ │ +-rw-r--r--   0        0        0     8196 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/.DS_Store
│ │ +-rw-r--r--   0        0        0      658 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/Makefile
│ │ +-rwxr-xr-x   0        0        0      804 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/make.bat
│ │ +-rw-r--r--   0        0        0     1948 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/image_labelling/Image_Labeling_DLTrack.ijm
│ │ +-rw-r--r--   0        0        0     1932 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/source/DL_Track_US.gui_helpers.rst
│ │ +-rw-r--r--   0        0        0      384 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/source/DL_Track_US.rst
│ │ +-rw-r--r--   0        0        0     1134 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/source/conf.py
│ │ +-rw-r--r--   0        0        0     6388 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/source/contribute.rst
│ │ +-rw-r--r--   0        0        0     3354 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/source/index.rst
│ │ +-rw-r--r--   0        0        0    10436 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/source/installation.rst
│ │ +-rw-r--r--   0        0        0      926 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/source/modules.rst
│ │ +-rw-r--r--   0        0        0      257 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/source/requirements.txt
│ │ +-rw-r--r--   0        0        0      367 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/source/tests.rst
│ │ +-rw-r--r--   0        0        0     1567 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/source/usage.rst
│ │ +-rw-r--r--   0        0        0  6265733 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/docs/usage/DL_Track_tutorial.pdf
│ │ +-rw-r--r--   0        0        0  1417980 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/tests/DL_Track_tests.pdf
│ │ +-rw-r--r--   0        0        0       81 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/.gitignore
│ │ +-rw-r--r--   0        0        0    11558 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/LICENSE
│ │ +-rw-r--r--   0        0        0     3274 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/README_Pypi.md
│ │ +-rw-r--r--   0        0        0     1163 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/pyproject.toml
│ │ +-rw-r--r--   0        0        0     3879 2020-02-02 00:00:00.000000 dl_track_us-0.1.2/PKG-INFO
│ │   --- dl_track_us-0.1.1/.DS_Store
│ ├── +++ dl_track_us-0.1.2/.DS_Store
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/.readthedocs.yml
│ ├── +++ dl_track_us-0.1.2/.readthedocs.yml
│ │┄ Files 12% similar despite different names
│ │ @@ -22,8 +22,8 @@
│ │  # If using Sphinx, optionally build your docs in additional formats such as PDF
│ │  # formats:
│ │  #    - pdf
│ │  
│ │  # Optionally declare the Python requirements required to build your docs
│ │  python:
│ │     install:
│ │ -   - requirements: requirements.txt
│ │ +   - requirements: docs/source/requirements.txt
│ │   --- dl_track_us-0.1.1/README.md
│ ├── +++ dl_track_us-0.1.2/PKG-INFO
│ │┄ Files 24% similar despite different names
│ │ @@ -1,47 +1,50 @@
│ │ -# DL_Track
│ │ -
│ │ -[![Documentation Status](https://readthedocs.org/projects/dltrack/badge/?version=latest)](https://dltrack.readthedocs.io/en/latest/?badge=latest)
│ │ -
│ │ -![DL_Track image](./Figures/home_im.png)
│ │ -
│ │ -The DL_Track package provides an easy to use graphical user interface (GUI) for deep learning based analysis of muscle architectural parameters from longitudinal ultrasonography images of human lower limb muscles. Please take a look at our [documentation](https://dltrack.readthedocs.io/en/latest/index.html) for more information.
│ │ -This code is based on a previously published [algorithm](https://github.com/njcronin/DL_Track) and replaces it. We have extended the functionalities of the previously proposed code. The previous code will not be updated and future updates will be included in this repository.
│ │ -
│ │ -## Getting started
│ │ -
│ │ -For detailled information about installaion of the DL_Track python package we refer you to our [documentation](https://dltrack.readthedocs.io/en/latest/installation.html). There you will finde guidelines not only for the installation procedure of DL_Track, but also concerding conda and GPU setup.
│ │ -
│ │ -## Quickstart
│ │ -
│ │ -Once installed, DL_Track can be started from the command prompt with the respective environment activated:
│ │ -
│ │ -``(DL_Track) C:/User/Desktop/ python DL_Track`` 
│ │ -
│ │ -In case you have downloaded the executable, simply double-click the DL_Track icon.
│ │ -
│ │ -Regardless of the used method, the GUI should open. For detailed the desciption of our GUI as well as usage examples, please take a look at the [user instruction](https://github.com/PaulRitsche/DLTrack/docs/usage).
│ │ -
│ │ -## Testing
│ │ -
│ │ -We have not yet integrated unit testing for DL_Track. Nonetheless, we have provided instructions to objectively test whether DL_Track, once installed, is functionable. Do perform the testing procedures yourself, check out the [test instructions](https://github.com/PaulRitsche/DLTrack/tests).
│ │ -
│ │ -## Code documentation 
│ │ -
│ │ -In order to see the detailled scope and description of the modules and functions included in the DL_Track package, you can do so either directly in the code, or in the [Documentation](https://dltrack.readthedocs.io/en/latest/modules.html#documentation) section of our online documentation.
│ │ -
│ │ -## Previous research
│ │ -
│ │ -The previously published [algorithm](https://github.com/njcronin/DL_Track) was developed with the aim to compare the performance of the trained deep learning models with manual analysis of muscle fascicle length, muscle fascicle pennation angle and muscle thickness. The results were presented in a published [preprint](https://arxiv.org/pdf/2009.04790.pdf). The results demonstrated in the article described the DL_Track algorithm to be comparable with manual analysis of muscle fascicle length, muscle fascicle pennation angle and muscle thickness in ultrasonography images as well as videos. The results are briefly illustrated in the figures below.
│ │ -
│ │ -![Bland-altman Plot](./Figures/Figure_B-A.png)
│ │ -
│ │ -Bland-Altman plots of the results obtained with our approach versus the results of manual analyses by the authors (mean of all 3). Results are shown for muscle fascicle length (A), pennation angle (B), and muscle thickness (C). For these plots, only the median fascicle values from the deep learning approach were used, and thickness was computed from the centre of the image. Solid and dotted lines depict bias and 95% limits of agreement, respectively.
│ │ -
│ │ -![Video comparison](./Figures/Figure_video.png)
│ │ -
│ │ -A comparison of fascicle lengths computed using DL_Track with those from [UltraTrack](https://sites.google.com/site/ultratracksoftware/home)(Farris & Lichtwark, 2016, DOI:10.1016/j.cmpb.2016.02.016), a semi-automated method of identifying muscle fascicles. Each row shows trials from a particular task (3 examples per task from different individuals, shown in separate columns). For DL_Track, the length of each individual fascicle detected in every frame is denoted by a gray dot. Solid black lines denote the mean length of all detected fascicles by DL_Track. Red dashed lines show the results of tracking a single fascicle with Ultratrack.
│ │ -
│ │ -
│ │ -## Community guidelines
│ │ -
│ │ -Wheter you want to contribute, report a bug or have troubles with the DL_Track package, take a look at the provided [instructions](https://dltrack.readthedocs.io/en/latest/contribute.html) how to best do so. You can also contact us via email at paul.ritsche@unibas.ch, but we would prefer you to open a discussion as described in the instructions.
│ │ +Metadata-Version: 2.1
│ │ +Name: DL_Track_US
│ │ +Version: 0.1.2
│ │ +Summary: Automatic analysis of logitudinal muscle ultrasonography images
│ │ +Project-URL: Homepage, https://github.com/PaulRitsche/DL_Track_US
│ │ +Project-URL: Bug Tracker, https://github.com/PaulRitsche/DL_Track_US/issues
│ │ +Author-email: Paul Ritsche <paul.ritsche@unibas.ch>, Olivier Seynnes <oliviers@nih.no>, Neil Cronin <neil.j.cronin@jyu.fi>
│ │ +License-File: LICENSE
│ │ +Classifier: License :: OSI Approved :: Apache Software License
│ │ +Classifier: Operating System :: OS Independent
│ │ +Classifier: Programming Language :: Python :: 3.10
│ │ +Requires-Python: >=3.10
│ │ +Description-Content-Type: text/markdown
│ │ +
│ │ +# DL_Track_US
│ │ +
│ │ +[![Documentation Status](https://readthedocs.org/projects/dltrack/badge/?version=latest)](https://dltrack.readthedocs.io/en/latest/?badge=latest)
│ │ +
│ │ +The DL_Track_US package provides an easy to use graphical user interface (GUI) for deep learning based analysis of muscle architectural parameters from longitudinal ultrasonography images of human lower limb muscles. Please take a look at our [documentation](https://dltrack.readthedocs.io/en/latest/index.html) for more information.
│ │ +This code is based on a previously published [algorithm](https://github.com/njcronin/DL_Track) and replaces it. We have extended the functionalities of the previously proposed code. The previous code will not be updated and future updates will be included in this repository.
│ │ +
│ │ +## Getting started
│ │ +
│ │ +For detailled information about installaion of the DL_Track_US python package we refer you to our [documentation](https://dltrack.readthedocs.io/en/latest/installation.html). There you will finde guidelines not only for the installation procedure of DL_Track_US, but also concerning conda and GPU setup.
│ │ +
│ │ +## Quickstart
│ │ +
│ │ +Once installed, DL_Track_US can be started from the command prompt with the respective environment activated:
│ │ +
│ │ +``(DL_Track_US) C:/User/Desktop/ python DL_Track_US`` 
│ │ +
│ │ +In case you have downloaded the executable, simply double-click the DL_Track_US icon.
│ │ +
│ │ +Regardless of the used method, the GUI should open. For detailed the desciption of our GUI as well as usage examples, please take a look at the [user instruction](https://github.com/PaulRitsche/DL_Track_US/tree/main/docs/usage).
│ │ +
│ │ +## Testing
│ │ +
│ │ +We have not yet integrated unit testing for DL_Track_US. Nonetheless, we have provided instructions to objectively test whether DL_Track_US, once installed, is functionable. To perform the testing procedures yourself, check out the [test instructions](https://github.com/PaulRitsche/DL_Track_US/tree/main/tests).
│ │ +
│ │ +## Code documentation 
│ │ +
│ │ +In order to see the detailled scope and description of the modules and functions included in the DL_Track_US package, you can do so either directly in the code, or in the [Documentation](https://dltrack.readthedocs.io/en/latest/modules.html#documentation) section of our online documentation.
│ │ +
│ │ +## Previous research
│ │ +
│ │ +The previously published [algorithm](https://github.com/njcronin/DL_Track) was developed with the aim to compare the performance of the trained deep learning models with manual analysis of muscle fascicle length, muscle fascicle pennation angle and muscle thickness. The results were presented in a published [preprint](https://arxiv.org/pdf/2009.04790.pdf). The results demonstrated in the article described the DL_Track_US algorithm to be comparable with manual analysis of muscle fascicle length, muscle fascicle pennation angle and muscle thickness in ultrasonography images as well as videos.
│ │ +
│ │ +## Community guidelines
│ │ +
│ │ +Wheter you want to contribute, report a bug or have troubles with the DL_Track_US package, take a look at the provided [instructions](https://dltrack.readthedocs.io/en/latest/contribute.html) how to best do so. You can also contact us via email at paul.ritsche@unibas.ch, but we would prefer you to open a discussion as described in the instructions.
│ │   --- dl_track_us-0.1.1/.github/workflows/draft-pdf.yml
│ ├── +++ dl_track_us-0.1.2/.github/workflows/draft-pdf.yml
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/DL_Track/DLTrack_GUI.py
│ ├── +++ dl_track_us-0.1.2/DL_Track_US/DL_Track_US_GUI.py
│ │┄ Files 2% similar despite different names
│ │ @@ -1,87 +1,94 @@
│ │  """
│ │  Description
│ │  -----------
│ │ -This module contains a class with methods to automatically and manually annotate
│ │ -longitudinal ultrasonography images and videos. When the class is initiated,
│ │ -a graphical user interface is opened. This is the main GUI of the DL_Track package.
│ │ +This module contains a class with methods to automatically and manually
│ │ +annotate longitudinal ultrasonography images and videos. When the class
│ │ +is initiated,a graphical user interface is opened.
│ │ +This is the main GUI of the DL_Track package.
│ │  From here, the user is able to navigate all functionalities of the package.
│ │ -These extend the methods in this class. The main functionalities of the GUI contained
│ │ -in this module are automatic and manual evalution of muscle ultrasonography images.
│ │ +These extend the methods in this class. The main functionalities of the GUI
│ │ +contained in this module are automatic and manual evalution of muscle
│ │ +ultrasonography images.
│ │  Inputted images or videos are analyzed and the parameters muscle fascicle
│ │ -length, pennation angle and muscle thickness are returned for each image or video frame.
│ │ +length, pennation angle and muscle thickness are returned for each image
│ │ +or video frame.
│ │  The parameters are analyzed using convolutional neural networks (U-net, VGG16).
│ │ -This module and all submodules contained in the /gui_helpers modality are extensions
│ │ -and improvements of the work presented in Cronin et al. (2020). There, the core functionalities
│ │ -of this code are already outlined and the comparability of the model segmentations to
│ │ -manual analysis (current gold standard) is described. Here, we have improved the code by
│ │ +This module and all submodules contained in the /gui_helpers modality
│ │ +are extensions and improvements of the work presented in Cronin et al. (2020).
│ │ +There, the core functionalities of this code are already outlined and the
│ │ +comparability of the model segmentations to manual analysis (current gold
│ │ +standard) is described. Here, we have improved the code by
│ │  integrating everything into a graphical user interface.
│ │  
│ │  Functions scope
│ │  ---------------
│ │  For scope of the functions see class documentation.
│ │  
│ │  Notes
│ │  -----
│ │  Additional information and usage exaples can be found in the video
│ │  tutorials provided for this package.
│ │  
│ │  References
│ │  ----------
│ │ -VGG16: Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014)
│ │ -U-net: Ronneberger, O., Fischer, P. and Brox, T. "U-Net: Convolutional Networks for Biomedical Image Segmentation." arXiv preprint arXiv:1505.04597 (2015)
│ │ -DL_Track: Cronin, Neil J. and Finni, Taija and Seynnes, Olivier. "Fully automated analysis of muscle architecture from B-mode ultrasound images with deep learning." arXiv preprint arXiv:https://arxiv.org/abs/2009.04790 (2020)
│ │ +[1] VGG16: Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014)
│ │ +[2] U-net: Ronneberger, O., Fischer, P. and Brox, T. "U-Net: Convolutional Networks for Biomedical Image Segmentation." arXiv preprint arXiv:1505.04597 (2015)
│ │ +[3] DL_Track: Cronin, Neil J. and Finni, Taija and Seynnes, Olivier. "Fully automated analysis of muscle architecture from B-mode ultrasound images with deep learning." arXiv preprint arXiv:https://arxiv.org/abs/2009.04790 (2020)
│ │  """
│ │ +import os
│ │  
│ │  import tkinter as tk
│ │  from threading import Lock, Thread
│ │  from tkinter import E, N, S, StringVar, Tk, W, filedialog, ttk
│ │  
│ │ -from DL_Track import gui_helpers
│ │ +from DL_Track_US import gui_helpers
│ │  
│ │  
│ │  class DLTrack:
│ │      """
│ │      Python class to automatically or manually annotate longitudinal muscle
│ │      ultrasonography images/videos of human lower limb muscles.
│ │      An analysis tkinter GUI is opened upon initialization of the class.
│ │      By clicking the buttons, the user can switch between different
│ │ -    analysis modes for image/video analysis and model training. The GUI consists
│ │ -    of the following elements.
│ │ +    analysis modes for image/video analysis and model training.
│ │ +    The GUI consists of the following elements.
│ │      - Input Directory:
│ │      By pressing the "Input" button, the user is asked to select
│ │      an input directory containing all images/videos to be
│ │      analyzed. This can also be entered directly in the entry field.
│ │      - Apo Model Path:
│ │      By pressing the "Apo Model" button, the user is asked to select
│ │      the aponeurosis model used for aponeurosis segmentation. The absolute
│ │      model path can be entered directly in the enty field as well.
│ │      - Fasc Model Path:
│ │      By pressing the "Fasc Model" button, the user is asked to select
│ │      the fascicle model used for aponeurosis segmentation. The absolute
│ │      model path can be entered directly in the enty field as well.
│ │      - Analysis Type:
│ │      The analysis type can be selected. There are four analysis types,
│ │ -    the selection of which will trigger more analysis parameters to be displayed.
│ │ +    the selection of which will trigger more analysis parameters
│ │ +    to be displayed.
│ │      Image (automatic image analysis), Video (automatic video analysis),
│ │      Image Manual (manual image analysis), Video Manual (manual image analysis).
│ │      - Break:
│ │ -    By pressing the "break" button, the user is able to stop the analysis process
│ │ +    By pressing the "break" button, the user can stop the analysis process
│ │      after each finished image or image frame analysis.
│ │      - Run:
│ │      By pressing the "run" button, the user can start the analysis process.
│ │      - Model training:
│ │      By pressing the "train model" button, a new window opens and the
│ │      user can train an own neural network based on existing/own
│ │      training data.
│ │  
│ │      Attributes
│ │      ----------
│ │      self._lock : _thread.lock
│ │ -        Thread object to lock the self._lock variable for access by another thread.
│ │ +        Thread object to lock the self._lock variable for access by another
│ │ +        thread.
│ │      self._is_running : bool, default = False
│ │          Boolen variable determining the active state
│ │          of the GUI. If False, the is not running. If True
│ │          the GUI is running.
│ │      self._should_stop : bool, default = False
│ │          Boolen variable determining the active state
│ │          of the GUI. If False, the is allowed to continue running. If True
│ │ @@ -92,33 +99,33 @@
│ │          tk.Stringvariable containing the path to the input directory.
│ │      self.apo_model : tk.Stringvar
│ │          tk.Stringvariable containing the path to the aponeurosis
│ │          model.
│ │      self.fasc_model : tk.Stringvar
│ │          tk.Stringvariable containing the path to the fascicle
│ │          model.
│ │ -    self.analysis_type : tk.Stringvar
│ │ +    self.analysis_type : {"image", "video", "image_manual", "video_manual"}
│ │          tk.Stringvariable containing the selected analysis type.
│ │          This can be "image", "video", "image_manual", "video_manual".
│ │ -    self.scaling : tk.Stringvar
│ │ +    self.scaling : {"bar", "manual","no scaling"}
│ │          tk.Stringvariable containing the selected scaling type.
│ │          This can be "bar", "manual" or "no scaling".
│ │      self.filetype : tk.Stringvar
│ │          tk.Stringvariabel containing the selected filetype for
│ │          the images to be analyzed. The user can select from the
│ │          dopdown list or enter an own filetype. The formatting
│ │          should be kept constant.
│ │ -    self.spacing : tk.Stringvar
│ │ +    self.spacing : {10, 5, 15, 20}
│ │          tk.Stringvariable containing the selected spacing distance
│ │          used for computation of pixel / cm ratio. This must only be
│ │          specified when the analysis type "bar" or "manual" is selected.
│ │      self.flipflag : tk.Stringvar
│ │          tk.Stringvariable containing the path to the file with the flip
│ │          flags for each image in the input directory.
│ │ -    self.flip : tk.Stringvar
│ │ +    self.flip : {"no_flip", "flip"}
│ │          tk.Stringvariable determining wheter all frames in the video
│ │          file will be flipped during automated analysis of videos. This
│ │          can be "no_flip" or "flip".
│ │      self.video : tk.Stringvar
│ │          tk.Stringvariable containing the absolute path to the video
│ │          file being analyzed during manual video analysis.
│ │      self.apo_threshold : tk.Stringvar
│ │ @@ -126,47 +133,50 @@
│ │          aponeurosis pixels by our neural networks. Must be non-zero and
│ │          non-negative.
│ │      self.fasc_threshold : tk.Stringvar
│ │          tk.Stringvariable containing the threshold applied to predicted
│ │          fascicle pixels by our neural networks. Must be non-zero and
│ │          non-negative.
│ │      self.fasc_cont_threshold : tk.Stringvar
│ │ -        tk.Stringvariable containing the threshold applied to predicted fascicle
│ │ -        segments by our neural networks. Must be non-zero and
│ │ +        tk.Stringvariable containing the threshold applied to predicted
│ │ +        fascicle segments by our neural networks. Must be non-zero and
│ │          non-negative.
│ │      self.min_width : tk.stringvar
│ │ -        tk.Stringvariablecontaining the minimal distance between aponeuroses to be
│ │ -        detected. Must be non-zero and non-negative.
│ │ +        tk.Stringvariablecontaining the minimal distance between aponeuroses
│ │ +        to be detected. Must be non-zero and non-negative.
│ │      self.min_pennation : tk.Stringvar
│ │ -        tk.Stringvariable containing the mininmal (physiological) acceptable pennation
│ │ -        angle occuring in the analyzed image/muscle. Must be non-negative.
│ │ +        tk.Stringvariable containing the mininmal (physiological) acceptable
│ │ +        pennation angle occuring in the analyzed image/muscle.
│ │ +        Must be non-negative.
│ │      self.max_pennation : tk.Stringvariable
│ │ -        tk.Stringvariable containing the maximal (physiological) acceptable pennation
│ │ -        angle occuring in the analyzed image/muscle. Must be non-negative and
│ │ -        larger than min_pennation.
│ │ +        tk.Stringvariable containing the maximal (physiological)
│ │ +        acceptable pennation angle occuring in the analyzed image/muscle.
│ │ +        Must be non-negative and larger than min_pennation.
│ │      self.train_image_dir : tk.Stringvar
│ │ -        tk.Straingvar containing the path to the directory of the training images.
│ │ -        Image must be in RGB format.
│ │ +        tk.Straingvar containing the path to the directory of the training
│ │ +        images. Image must be in RGB format.
│ │      self.mask_path : tk.Stringvar
│ │ -        tk.Stringvariable containing the path to the directory of the mask images.
│ │ -        Masks must be binary.
│ │ +        tk.Stringvariable containing the path to the directory of the mask
│ │ +        images. Masks must be binary.
│ │      self.out_dir : tk.Stringvar
│ │ -        tk.Stringvariable containing the path to the directory where the trained model
│ │ -        should be saved.
│ │ +        tk.Stringvariable containing the path to the directory where the
│ │ +        trained model should be saved.
│ │      self.batch_size : tk.Stringvar
│ │          tk.Stringvariable containing the batch size per iteration through the
│ │          network during model training. Must be non-negative and non-zero.
│ │      self.learning_rate : tk.Stringvariable
│ │          tk.Stringvariable the learning rate used during model training.
│ │          Must be non-negative and non-zero.
│ │      self.epochs : tk.Stringvar
│ │          tk.Straingvariable containing the amount of epochs that the model
│ │ -        is trained befor training is aborted. Must be non-negative and non-zero.
│ │ +        is trained befor training is aborted. Must be non-negative and
│ │ +        non-zero.
│ │      self.loss : tk.Stringvar
│ │ -        tk.Stringvariable containing the loss function that is used during training.
│ │ +        tk.Stringvariable containing the loss function that is used during
│ │ +        training.
│ │  
│ │      Methods
│ │      -------
│ │      get_input_dir
│ │          Instance method to ask the user to select the input directory.
│ │      get_apo_model_path
│ │          Instance method to ask the user to select the apo model path.
│ │ @@ -237,16 +247,18 @@
│ │  
│ │          # set up threading
│ │          self._lock = Lock()
│ │          self._is_running = False
│ │          self._should_stop = False
│ │  
│ │          # set up gui
│ │ -        root.title("DLTrack")
│ │ -        # root.iconbitmap("home_im.ico")
│ │ +        root.title("DL_Track_US")
│ │ +        master_path = os.path.dirname(os.path.abspath(__file__))
│ │ +        iconpath = master_path + "/gui_helpers/home_im.ico"
│ │ +        root.iconbitmap(iconpath)
│ │  
│ │          self.main = ttk.Frame(root, padding="10 10 12 12")
│ │          self.main.grid(column=0, row=0, sticky=(N, S, W, E))
│ │          # Configure resizing of user interface
│ │          self.main.columnconfigure(0, weight=1)
│ │          self.main.columnconfigure(1, weight=1)
│ │          self.main.columnconfigure(2, weight=1)
│ │ @@ -280,36 +292,39 @@
│ │          )
│ │          style.configure(
│ │              "TEntry",
│ │              font=("Lucida Sans", 12),
│ │              background="papaya whip",
│ │              foregrund="black",
│ │          )
│ │ -        style.configure("TCombobox", background="sea green", foreground="black")
│ │ +        style.configure("TCombobox", background="sea green",
│ │ +                        foreground="black")
│ │  
│ │ -        ## Entryboxes
│ │ +        # Entryboxes
│ │          # Input directory
│ │          self.input = StringVar()
│ │          input_entry = ttk.Entry(self.main, width=30, textvariable=self.input)
│ │          input_entry.grid(column=2, row=6, columnspan=3, sticky=(W, E))
│ │          self.input.set("C:/Users/admin/Documents")
│ │  
│ │          # Apo Model path
│ │          self.apo_model = StringVar()
│ │ -        apo_model_entry = ttk.Entry(self.main, width=30, textvariable=self.apo_model)
│ │ +        apo_model_entry = ttk.Entry(self.main, width=30,
│ │ +                                    textvariable=self.apo_model)
│ │          apo_model_entry.grid(column=2, row=7, columnspan=3, sticky=(W, E))
│ │          self.apo_model.set("C:/Users/admin/Documents")
│ │  
│ │          # Fasc Model path
│ │          self.fasc_model = StringVar()
│ │ -        fasc_model_entry = ttk.Entry(self.main, width=30, textvariable=self.fasc_model)
│ │ +        fasc_model_entry = ttk.Entry(self.main, width=30,
│ │ +                                     textvariable=self.fasc_model)
│ │          fasc_model_entry.grid(column=2, row=8, columnspan=3, sticky=(W, E))
│ │          self.fasc_model.set("C:/Users/admin/Documents")
│ │  
│ │ -        ## Radiobuttons
│ │ +        # Radiobuttons
│ │          # Analysis Type
│ │          self.analysis_type = StringVar()
│ │          image = ttk.Radiobutton(
│ │              self.main,
│ │              text="Image",
│ │              variable=self.analysis_type,
│ │              value="image",
│ │ @@ -337,17 +352,18 @@
│ │              text="Video Manual",
│ │              variable=self.analysis_type,
│ │              value="video_manual",
│ │              command=self.video_manual,
│ │          )
│ │          video_manual.grid(column=3, row=11, sticky=(W, E))
│ │  
│ │ -        ## Buttons
│ │ +        # Buttons
│ │          # Input directory
│ │ -        input_button = ttk.Button(self.main, text="Input", command=self.get_input_dir)
│ │ +        input_button = ttk.Button(self.main, text="Input",
│ │ +                                  command=self.get_input_dir)
│ │          input_button.grid(column=5, row=6, sticky=E)
│ │  
│ │          # Apo model path
│ │          apo_model_button = ttk.Button(
│ │              self.main, text="Apo Model", command=self.get_apo_model_path
│ │          )
│ │          apo_model_button.grid(column=5, row=7, sticky=E)
│ │ @@ -355,76 +371,73 @@
│ │          # Fasc model path
│ │          fasc_model_button = ttk.Button(
│ │              self.main, text="Fasc Model", command=self.get_fasc_model_path
│ │          )
│ │          fasc_model_button.grid(column=5, row=8, sticky=E)
│ │  
│ │          # Break button
│ │ -        break_button = ttk.Button(self.main, text="Break", command=self.do_break)
│ │ +        break_button = ttk.Button(self.main, text="Break",
│ │ +                                  command=self.do_break)
│ │          break_button.grid(column=2, row=20, sticky=(W, E))
│ │  
│ │          # Run button
│ │          run_button = ttk.Button(self.main, text="Run", command=self.run_code)
│ │          run_button.grid(column=3, row=20, sticky=(W, E))
│ │  
│ │          # Model training button
│ │          training = ttk.Button(
│ │              self.main, text="Train Model", command=self.train_model_window
│ │          )
│ │          training.grid(column=5, row=20, sticky=E)
│ │  
│ │ -        ## Labels
│ │ +        # Labels
│ │          ttk.Label(self.main, text="Directories", font=("Verdana", 14)).grid(
│ │              column=1, row=5, sticky=(W, E)
│ │          )
│ │          ttk.Label(self.main, text="Input Directory").grid(column=1, row=6)
│ │          ttk.Label(self.main, text="Apo Model Path").grid(column=1, row=7)
│ │          ttk.Label(self.main, text="Fasc Model Path").grid(column=1, row=8)
│ │          ttk.Label(self.main, text="Analysis Type", font=("Verdana", 14)).grid(
│ │              column=1, row=9, sticky=(W, E)
│ │          )
│ │  
│ │          for child in self.main.winfo_children():
│ │              child.grid_configure(padx=5, pady=5)
│ │  
│ │ -    ## Methods used in main GUI window when respective buttons are pressed.
│ │ +    # Methods used in main GUI window when respective buttons are pressed.
│ │  
│ │      # Determine input directory
│ │      def get_input_dir(self):
│ │ -        """
│ │ -        Instance method to ask the user to select the input directory.
│ │ +        """Instance method to ask the user to select the input directory.
│ │          All image files (of the same specified filetype) in
│ │          the input directory are analysed.
│ │          """
│ │          input_dir = filedialog.askdirectory()
│ │          self.input.set(input_dir)
│ │  
│ │      # Get path of aponeurosis model
│ │      def get_apo_model_path(self):
│ │ -        """
│ │ -        Instance method to ask the user to select the apo model path.
│ │ +        """Instance method to ask the user to select the apo model path.
│ │          This must be an absolute path and the model must be a .h5 file.
│ │          """
│ │          apo_model_dir = filedialog.askopenfilename()
│ │          self.apo_model.set(apo_model_dir)
│ │  
│ │      # Get path of fascicle model
│ │      def get_fasc_model_path(self):
│ │ -        """
│ │ -        Instance method to ask the user to select the fascicle model path.
│ │ +        """Instance method to ask the user to select the fascicle model path.
│ │          This must be an absolute path and the model must be a .h5 file.
│ │          """
│ │          fasc_model_dir = filedialog.askopenfilename()
│ │          self.fasc_model.set(fasc_model_dir)
│ │  
│ │      # Analysis type selection
│ │  
│ │      def image_analysis(self):
│ │ -        """
│ │ -        Instance method to display the required parameters
│ │ +        """Instance method to display the required parameters
│ │          that need to be entered by the user when images
│ │          are automatically analyzed.
│ │  
│ │          Several parameters are displayed:
│ │          - Image type:
│ │          The user can enter the type of the image or
│ │          select the examples from the dropdown list.
│ │ @@ -446,15 +459,16 @@
│ │          - Analysis Parameter:
│ │          The user must specify the analysis parameters
│ │          used for computation of fascicle length, muscle
│ │          thickness and pennation angle. The parameters
│ │          will have an influence on computation outcomes.
│ │          """
│ │          # Labels
│ │ -        ttk.Label(self.main, text="Image Properties", font=("Verdana", 14)).grid(
│ │ +        ttk.Label(self.main, text="Image Properties",
│ │ +                  font=("Verdana", 14)).grid(
│ │              column=1, row=9, sticky=(W, E)
│ │          )
│ │          ttk.Label(self.main, text="Image Type").grid(column=1, row=13)
│ │          ttk.Label(self.main, text="Scaling Type").grid(column=1, row=14)
│ │          ttk.Label(self.main, text="Spacing (mm)").grid(column=1, row=15)
│ │          ttk.Label(self.main, text="Flip File Path").grid(column=1, row=16)
│ │  
│ │ @@ -492,24 +506,26 @@
│ │              "/**/*.tif",
│ │              "/**/*.tiff",
│ │              "/**/*.png",
│ │              "/**/*.bmp",
│ │              "/**/*.jpeg",
│ │              "/**/*.jpg",
│ │          )
│ │ -        filetype_entry = ttk.Combobox(self.main, width=10, textvariable=self.filetype)
│ │ +        filetype_entry = ttk.Combobox(self.main, width=10,
│ │ +                                      textvariable=self.filetype)
│ │          filetype_entry["values"] = filetype
│ │          # filetype_entry["state"] = "readonly"
│ │          filetype_entry.grid(column=2, row=13, sticky=(W, E))
│ │          self.filetype.set("/**/*.tiff")
│ │  
│ │          # Spacing
│ │          self.spacing = StringVar()
│ │          spacing = (5, 10, 15, 20)
│ │ -        spacing_entry = ttk.Combobox(self.main, width=10, textvariable=self.spacing)
│ │ +        spacing_entry = ttk.Combobox(self.main, width=10,
│ │ +                                     textvariable=self.spacing)
│ │          spacing_entry["values"] = spacing
│ │          spacing_entry["state"] = "readonly"
│ │          spacing_entry.grid(column=2, row=15, sticky=(W, E))
│ │          self.spacing.set(10)
│ │  
│ │          # Buttons
│ │          # Flipfile model path
│ │ @@ -523,21 +539,21 @@
│ │              self.main, text="Analysis Parameters", command=self.open_window
│ │          )
│ │          analysis.grid(column=5, row=17, sticky=E, pady=5)
│ │  
│ │          # Entry
│ │          # Flip File path
│ │          self.flipflag = StringVar()
│ │ -        flipflag_entry = ttk.Entry(self.main, width=30, textvariable=self.flipflag)
│ │ +        flipflag_entry = ttk.Entry(self.main, width=30,
│ │ +                                   textvariable=self.flipflag)
│ │          flipflag_entry.grid(column=2, row=16, columnspan=2, sticky=(W, E))
│ │          self.flipflag.set("Desktop/DL_Track/FlipFlags.txt")
│ │  
│ │      def video_analysis(self):
│ │ -        """
│ │ -        Instance method to display the required parameters
│ │ +        """Instance method to display the required parameters
│ │          that need to be entered by the user when videos
│ │          are automatically analyzed. Several parameters are
│ │          displayed:
│ │          - Video type:
│ │          The user can enter the type of the image or
│ │          select the examples from the dropdown list.
│ │          The formatting must be kept constant.
│ │ @@ -563,15 +579,16 @@
│ │          thickness and pennation angle. The parameters
│ │          will have an influence on computation outcomes.
│ │          """
│ │          # Reset flipping variable for Video
│ │          self.flip = None
│ │  
│ │          # Labels
│ │ -        ttk.Label(self.main, text="Video Properties ", font=("Verdana", 14)).grid(
│ │ +        ttk.Label(self.main, text="Video Properties ",
│ │ +                  font=("Verdana", 14)).grid(
│ │              column=1, row=9, sticky=(W, E)
│ │          )
│ │          ttk.Label(self.main, text="Video Type").grid(column=1, row=13)
│ │          tk.Label(
│ │              self.main,
│ │              text="Scaling Type",
│ │              font=("Lucida Sans", 13),
│ │ @@ -623,24 +640,26 @@
│ │          flip_n.grid(column=3, row=16, sticky=(W, E))
│ │          self.flip.set("flip")
│ │  
│ │          # Comboboxes
│ │          # Filetype
│ │          self.filetype = StringVar()
│ │          filetype = ("/**/*.avi", "/**/*.mp4")
│ │ -        filetype_entry = ttk.Combobox(self.main, width=10, textvariable=self.filetype)
│ │ +        filetype_entry = ttk.Combobox(self.main, width=10,
│ │ +                                      textvariable=self.filetype)
│ │          filetype_entry["values"] = filetype
│ │          # filetype_entry["state"] = "readonly"
│ │          filetype_entry.grid(column=2, row=13, sticky=(W, E))
│ │          self.filetype.set("/**/*.avi")
│ │  
│ │          # Spacing
│ │          self.spacing = StringVar()
│ │          spacing = (5, 10, 15, 20)
│ │ -        spacing_entry = ttk.Combobox(self.main, width=10, textvariable=self.spacing)
│ │ +        spacing_entry = ttk.Combobox(self.main, width=10,
│ │ +                                     textvariable=self.spacing)
│ │          spacing_entry["values"] = spacing
│ │          spacing_entry["state"] = "readonly"
│ │          spacing_entry.grid(column=2, row=15, sticky=(W, E))
│ │          self.spacing.set(10)
│ │  
│ │          # Step
│ │          self.step = StringVar()
│ │ @@ -654,25 +673,25 @@
│ │          # Analysis parameter button
│ │          analysis = ttk.Button(
│ │              self.main, text="Analysis Parameters", command=self.open_window
│ │          )
│ │          analysis.grid(column=5, row=18, sticky=W, pady=5)
│ │  
│ │      def image_manual(self):
│ │ -        """
│ │ -        Instance method to display the required parameters
│ │ +        """Instance method to display the required parameters
│ │          that need to be entered by the user when images are
│ │          evaluated manually.
│ │          - Image type:
│ │          The user can enter the type of the image or
│ │          select the examples from the dropdown list.
│ │          The formatting must be kept constant.
│ │          """
│ │          # Labels
│ │ -        ttk.Label(self.main, text="Image Properties", font=("Verdana", 14)).grid(
│ │ +        ttk.Label(self.main, text="Image Properties",
│ │ +                  font=("Verdana", 14)).grid(
│ │              column=1, row=9, sticky=(W, E)
│ │          )
│ │          ttk.Label(self.main, text="  Image Type  ").grid(column=1, row=13)
│ │  
│ │          # Remove unnecessary widgets
│ │          ttk.Label(self.main, text="                       ").grid(
│ │              column=3, columnspan=2, row=13, sticky=(W, E)
│ │ @@ -701,31 +720,32 @@
│ │              "/**/*.tiff",
│ │              "/**/*.png",
│ │              "/**/*.bmp",
│ │              "/**/*.jpeg",
│ │              "/**/*.jpg",
│ │              "/**/*.avi",
│ │          )
│ │ -        filetype_entry = ttk.Combobox(self.main, width=10, textvariable=self.filetype)
│ │ +        filetype_entry = ttk.Combobox(self.main, width=10,
│ │ +                                      textvariable=self.filetype)
│ │          filetype_entry["values"] = filetype
│ │          # filetype_entry["state"] = "readonly"
│ │          filetype_entry.grid(column=2, row=13, sticky=(W, E))
│ │          self.filetype.set("/**/*.tif")
│ │  
│ │      def video_manual(self):
│ │ -        """
│ │ -        Instance method to display the required parameters
│ │ +        """Instance method to display the required parameters
│ │          that need to be entered by the user when videos are
│ │          evaluated manually.
│ │          - File path:
│ │                      The user must specify the absolute path to the
│ │                      video file to be analyzed.
│ │          """
│ │          # Labels
│ │ -        ttk.Label(self.main, text="Video Properties ", font=("Verdana", 14)).grid(
│ │ +        ttk.Label(self.main, text="Video Properties ",
│ │ +                  font=("Verdana", 14)).grid(
│ │              column=1, row=9, sticky=(W, E)
│ │          )
│ │          ttk.Label(self.main, text="  File Path  ").grid(column=1, row=13)
│ │  
│ │          # Remove unnecessary widgets
│ │          ttk.Label(self.main, text="                       ").grid(
│ │              column=1, columnspan=5, row=14, sticky=(W, E)
│ │ @@ -747,40 +767,38 @@
│ │          self.video = StringVar()
│ │          video_entry = ttk.Entry(self.main, width=30, textvariable=self.video)
│ │          video_entry.grid(column=2, row=13, columnspan=3, sticky=(W, E))
│ │          self.video.set("C:/Users/admin/Documents/fasc_video.avi")
│ │  
│ │          # Buttons
│ │          # Get video path
│ │ -        vpath = ttk.Button(self.main, text="Video Path", command=self.get_video_path)
│ │ +        vpath = ttk.Button(self.main, text="Video Path",
│ │ +                           command=self.get_video_path)
│ │          vpath.grid(column=5, row=13, sticky=E)
│ │  
│ │      def get_flipfile_path(self):
│ │ -        """
│ │ -        Instance method to ask the user to select the flipfile path.
│ │ +        """Instance method to ask the user to select the flipfile path.
│ │          The flipfile should contain the flags used for flipping each
│ │          image. If 0, the image is not flipped, if 1 the image is
│ │          flipped. This must be an absolute path.
│ │          """
│ │          flipflag_dir = filedialog.askopenfilename()
│ │          self.flipflag.set(flipflag_dir)
│ │  
│ │      def get_video_path(self):
│ │ -        """
│ │ -        Instance method to ask the user to select the video path
│ │ +        """Instance method to ask the user to select the video path
│ │          for manual video analysis.
│ │          This must be an absolute path.
│ │          """
│ │          video_path = filedialog.askopenfilename()
│ │          self.video.set(video_path)
│ │          return video_path
│ │  
│ │      def run_code(self):
│ │ -        """
│ │ -        Instance method to execute the analysis process when the
│ │ +        """Instance method to execute the analysis process when the
│ │          "run" button is pressed.
│ │  
│ │          Which analysis process is executed depends on the user
│ │          selection. By pressing the button, a seperate thread is started
│ │          in which the analysis is run. This allows the user to break any
│ │          analysis process. Moreover, the threading allows interaction
│ │          with the main GUI during ongoing analysis process. This function
│ │ @@ -810,15 +828,16 @@
│ │              self.is_running = True
│ │  
│ │              # Get input dir
│ │              selected_input_dir = self.input.get()
│ │  
│ │              # Make sure some kind of input directory is specified.
│ │              if len(selected_input_dir) < 3:
│ │ -                tk.messagebox.showerror("Information", "Input directory is incorrect.")
│ │ +                tk.messagebox.showerror("Information",
│ │ +                                        "Input directory is incorrect.")
│ │                  self.should_stop = False
│ │                  self.is_running = False
│ │                  self.do_break()
│ │                  return
│ │  
│ │              # Get selected analysis
│ │              selected_analysis = self.analysis_type.get()
│ │ @@ -826,15 +845,16 @@
│ │              # Start thread depending on Analysis type
│ │              if selected_analysis == "image":
│ │  
│ │                  selected_filetype = self.filetype.get()
│ │  
│ │                  # Make sure some kind of filetype is specified.
│ │                  if len(selected_filetype) < 3:
│ │ -                    tk.messagebox.showerror("Information", "Filetype is invalid.")
│ │ +                    tk.messagebox.showerror("Information",
│ │ +                                            "Filetype is invalid.")
│ │                      self.should_stop = False
│ │                      self.is_running = False
│ │                      self.do_break()
│ │                      return
│ │  
│ │                  selected_flipflag_path = self.flipflag.get()
│ │                  selected_apo_model_path = self.apo_model.get()
│ │ @@ -868,25 +888,27 @@
│ │                  )
│ │              elif selected_analysis == "video":
│ │  
│ │                  selected_filetype = self.filetype.get()
│ │  
│ │                  # Make sure some kind of filetype is specified.
│ │                  if len(selected_filetype) < 3:
│ │ -                    tk.messagebox.showerror("Information", "Filetype is invalid.")
│ │ +                    tk.messagebox.showerror("Information",
│ │ +                                            "Filetype is invalid.")
│ │                      self.should_stop = False
│ │                      self.is_running = False
│ │                      self.do_break()
│ │                      return
│ │  
│ │                  selected_step = self.step.get()
│ │  
│ │                  # Make sure some kind of step is specified.
│ │                  if len(selected_step) < 1 or int(selected_step) < 1:
│ │ -                    tk.messagebox.showerror("Information", "Frame Steps is invalid.")
│ │ +                    tk.messagebox.showerror("Information",
│ │ +                                            "Frame Steps is invalid.")
│ │                      self.should_stop = False
│ │                      self.is_running = False
│ │                      self.do_break()
│ │                      return
│ │  
│ │                  selected_flip = self.flip.get()
│ │                  selected_apo_model_path = self.apo_model.get()
│ │ @@ -921,15 +943,16 @@
│ │                  )
│ │              elif selected_analysis == "image_manual":
│ │  
│ │                  selected_filetype = self.filetype.get()
│ │  
│ │                  # Make sure some kind of filetype is specified.
│ │                  if len(selected_filetype) < 3:
│ │ -                    tk.messagebox.showerror("Information", "Filetype is invalid.")
│ │ +                    tk.messagebox.showerror("Information",
│ │ +                                            "Filetype is invalid.")
│ │                      self.should_stop = False
│ │                      self.is_running = False
│ │                      self.do_break()
│ │                      return
│ │  
│ │                  thread = Thread(
│ │                      target=gui_helpers.calculateBatchManual,
│ │ @@ -965,15 +988,16 @@
│ │          # Error handling
│ │          except AttributeError:
│ │              tk.messagebox.showerror(
│ │                  "Information",
│ │                  "Check input parameters."
│ │                  + "\nPotential error sources:"
│ │                  + "\n - Invalid specified directory."
│ │ -                "\n - Analysis Type not set" + "\n - Analysis parameters not set.",
│ │ +                "\n - Analysis Type not set" +
│ │ +                "\n - Analysis parameters not set.",
│ │              )
│ │              self.do_break()
│ │              self.should_stop = False
│ │              self.is_running = False
│ │  
│ │          except FileNotFoundError:
│ │              tk.messagebox.showerror(
│ │ @@ -995,39 +1019,38 @@
│ │              )
│ │              self.do_break()
│ │              self.should_stop = False
│ │              self.is_running = False
│ │  
│ │          except ValueError:
│ │              tk.messagebox.showerror(
│ │ -                "Information", "Analysis parameter entry fields" + " must not be empty."
│ │ +                "Information", "Analysis parameter entry fields" +
│ │ +                " must not be empty."
│ │              )
│ │              self.do_break()
│ │              self.should_stop = False
│ │              self.is_running = False
│ │  
│ │      def do_break(self):
│ │ -        """
│ │ -        Instance method to break the analysis process when the
│ │ +        """Instance method to break the analysis process when the
│ │          button "break" is pressed.
│ │  
│ │          This changes the instance attribute self.should_stop
│ │          to True, given that the analysis is already running.
│ │          The attribute is checked befor every iteration
│ │          of the analysis process.
│ │          """
│ │          if self.is_running:
│ │              self.should_stop = True
│ │  
│ │      # ---------------------------------------------------------------------------------------------------
│ │      # Open new toplevel instance for analysis parameter specification
│ │  
│ │      def open_window(self):
│ │ -        """
│ │ -        Instance method to open new window for analysis parameter input.
│ │ +        """Instance method to open new window for analysis parameter input.
│ │          The window is opened upon pressing of the "analysis parameters"
│ │          button.
│ │  
│ │          Several parameters are displayed.
│ │          - Apo Threshold:
│ │          The user must input the aponeurosis threshold
│ │          either by selecting from the dropdown list or
│ │ @@ -1035,73 +1058,79 @@
│ │          structures will be classified as aponeurosis as the threshold for
│ │          classifying a pixel as aponeurosis is changed. Float, be non-zero and
│ │          non-negative.
│ │          - Fasc Threshold:
│ │          The user must input the fascicle threshold
│ │          either by selecting from the dropdown list or
│ │          entering a value. By varying this threshold, different
│ │ -        structures will be classified as fascicle as the threshold for classifying
│ │ -        a pixel as fascicle is changed.
│ │ +        structures will be classified as fascicle as the threshold for
│ │ +        classifying a pixel as fascicle is changed.
│ │          Float, must be non-negative and non-zero.
│ │          - Fasc Cont Threshold:
│ │          The user must input the fascicle contour threshold
│ │          either by selecting from the dropdown list or
│ │          entering a value. By varying this threshold, different
│ │          structures will be classified as fascicle. By increasing, longer
│ │          fascicle segments will be considered, by lowering shorter segments.
│ │          Integer, must be non-zero and non-negative.
│ │          - Minimal Width:
│ │          The user must input the minimal with either by selecting from the
│ │          dropdown list or entering a value. The aponeuroses must be at least
│ │ -        this distance apart to be detected. The distance is specified in pixels.
│ │ -        Integer, must be non-zero and non-negative.
│ │ +        this distance apart to be detected. The distance is specified in
│ │ +        pixels. Integer, must be non-zero and non-negative.
│ │          - Min Pennation:
│ │ -        The user must enter the minimal pennation angle physiologically acceptable
│ │ +        The user must enter the minimal pennation angle physiologically apt
│ │          occuring in the analyzed image/video. Fascicles with lower pennation
│ │          angles will be excluded. The pennation angle is calculated as the angle
│ │          of insertion between extrapolated fascicle and detected aponeurosis.
│ │          Integer, must be non-negative.
│ │          - Max Pennation:
│ │ -        The user must enter the minimal pennation angle physiologically acceptable
│ │ +        The user must enter the minimal pennation angle physiologically apt
│ │          occuring in the analyzed image/video. Fascicles with lower pennation
│ │          angles will be excluded. The pennation angle is calculated as the angle
│ │          of insertion between extrapolated fascicle and detected aponeurosis.
│ │          Integer, must be non-negative.
│ │  
│ │          The parameters are set upon pressing the "set parameters" button.
│ │          """
│ │          # Create window
│ │          window = tk.Toplevel(bg="DarkSeaGreen3")
│ │          window.title("Analysis Parameter Window")
│ │ -        # window.iconbitmap("home_im.ico")
│ │ +        # Add icon to window
│ │ +        window_path = os.path.dirname(os.path.abspath(__file__))
│ │ +        iconpath = window_path + "/gui_helpers/home_im.ico"
│ │ +        window.iconbitmap(iconpath)
│ │          window.grab_set()
│ │  
│ │          # Labels
│ │ -        ttk.Label(window, text="Analysis Parameters", font=("Verdana", 14)).grid(
│ │ +        ttk.Label(window, text="Analysis Parameters",
│ │ +                  font=("Verdana", 14)).grid(
│ │              column=1, row=11, padx=10
│ │          )
│ │          ttk.Label(window, text="Apo Threshold").grid(column=1, row=12)
│ │          ttk.Label(window, text="Fasc Threshold").grid(column=1, row=13)
│ │          ttk.Label(window, text="Fasc Cont Threshold").grid(column=1, row=14)
│ │          ttk.Label(window, text="Minimal Width").grid(column=1, row=15)
│ │          ttk.Label(window, text="Minimal Pennation").grid(column=1, row=17)
│ │          ttk.Label(window, text="Maximal Pennation").grid(column=1, row=18)
│ │  
│ │          # Apo threshold
│ │          self.apo_threshold = StringVar()
│ │          athresh = (0.1, 0.3, 0.5, 0.7, 0.9)
│ │ -        apo_entry = ttk.Combobox(window, width=10, textvariable=self.apo_threshold)
│ │ +        apo_entry = ttk.Combobox(window, width=10,
│ │ +                                 textvariable=self.apo_threshold)
│ │          apo_entry["values"] = athresh
│ │          apo_entry.grid(column=2, row=12, sticky=(W, E))
│ │          self.apo_threshold.set(0.2)
│ │  
│ │          # Fasc threshold
│ │          self.fasc_threshold = StringVar()
│ │          fthresh = [0.1, 0.3, 0.5]
│ │ -        fasc_entry = ttk.Combobox(window, width=10, textvariable=self.fasc_threshold)
│ │ +        fasc_entry = ttk.Combobox(window, width=10,
│ │ +                                  textvariable=self.fasc_threshold)
│ │          fasc_entry["values"] = fthresh
│ │          fasc_entry.grid(column=2, row=13, sticky=(W, E))
│ │          self.fasc_threshold.set(0.05)
│ │  
│ │          # Fasc cont threshold
│ │          self.fasc_cont_threshold = StringVar()
│ │          fcthresh = (20, 30, 40, 50, 60, 70, 80)
│ │ @@ -1111,15 +1140,16 @@
│ │          fasc_cont_entry["values"] = fcthresh
│ │          fasc_cont_entry.grid(column=2, row=14, sticky=(W, E))
│ │          self.fasc_cont_threshold.set(40)
│ │  
│ │          # Minimal width
│ │          self.min_width = StringVar()
│ │          mwidth = (20, 30, 40, 50, 60, 70, 80, 90, 100)
│ │ -        width_entry = ttk.Combobox(window, width=10, textvariable=self.min_width)
│ │ +        width_entry = ttk.Combobox(window, width=10,
│ │ +                                   textvariable=self.min_width)
│ │          width_entry["values"] = mwidth
│ │          width_entry.grid(column=2, row=15, sticky=(W, E))
│ │          self.min_width.set(60)
│ │  
│ │          # Minimal pennation
│ │          self.min_pennation = StringVar()
│ │          min_pennation_entry = ttk.Entry(
│ │ @@ -1133,27 +1163,27 @@
│ │          max_pennation_entry = ttk.Entry(
│ │              window, width=10, textvariable=self.max_pennation
│ │          )
│ │          max_pennation_entry.grid(column=2, row=18, sticky=(W, E))
│ │          self.max_pennation.set(40)
│ │  
│ │          # Set Params button
│ │ -        set_params = ttk.Button(window, text="Set parameters", command=window.destroy)
│ │ +        set_params = ttk.Button(window, text="Set parameters",
│ │ +                                command=window.destroy)
│ │          set_params.grid(column=1, row=19, sticky=(W, E))
│ │  
│ │          # Add padding
│ │          for child in window.winfo_children():
│ │              child.grid_configure(padx=5, pady=5)
│ │  
│ │      # ---------------------------------------------------------------------------------------------------
│ │      # Open new toplevel instance for model training
│ │  
│ │      def train_model_window(self):
│ │ -        """
│ │ -        Instance method to open new window for model training.
│ │ +        """Instance method to open new window for model training.
│ │          The window is opened upon pressing of the "analysis parameters"
│ │          button.
│ │  
│ │          Several parameters are displayed.
│ │          - Image Directory:
│ │          The user must select or input the image directory. This
│ │          path must to the directory containing the training images.
│ │ @@ -1176,36 +1206,42 @@
│ │          - Learning Rate:
│ │          The user must enter the learning rate used for model training by
│ │          selecting from the dropdown list or entering a value.
│ │          Float, must be non-negative and non-zero.
│ │          - Epochs:
│ │          The user must enter the number of Epochs used during model training by
│ │          selecting from the dropdown list or entering a value.
│ │ -        The total amount of epochs will only be used if early stopping does not happen.
│ │ +        The total amount of epochs will only be used if early stopping
│ │ +        does not happen.
│ │          Integer, must be non-negative and non-zero.
│ │          - Loss Function:
│ │          The user must enter the loss function used for model training by
│ │ -        selecting from the dropdown list. These can be "BCE" (binary
│ │ -        cross-entropy), "Dice" (Dice coefficient) or "FL"(Focal loss).
│ │ +        selecting from the dropdown list. So far, this can be "BCE" only
│ │ +        (binary cross-entropy).
│ │  
│ │ -        Model training is started by pressing the "start training" button. Although
│ │ -        all parameters relevant for model training can be adapted, we advise users with
│ │ -        limited experience to keep the pre-defined settings. These settings are best
│ │ -        practice and devised from the original papers that proposed the models used
│ │ -        here. Singularly the batch size should be adapted to 1 if comupte power is limited
│ │ -        (no GPU or GPU with RAM lower than 8 gigabyte).
│ │ +        Model training is started by pressing the "start training" button.
│ │ +        Although all parameters relevant for model training can be adapted,
│ │ +        we advise users with limited experience to keep the pre-defined
│ │ +        settings. These settings are best practice and devised from the
│ │ +        original papers that proposed the models used here.
│ │ +        Singularly the batch size should be adapted to 1 if comupte power is
│ │ +        limited (no GPU or GPU with RAM lower than 8 gigabyte).
│ │          """
│ │          # Open Window
│ │          window = tk.Toplevel(bg="DarkSeaGreen3")
│ │ -        window.title("Model Training Window")
│ │ -        # window.iconbitmap("home_im.ico")
│ │ +        window.title("DL_Track_US - Model Training")
│ │ +        # Add icon to window
│ │ +        window_path = os.path.dirname(os.path.abspath(__file__))
│ │ +        iconpath = window_path + "/gui_helpers/home_im.ico"
│ │ +        window.iconbitmap(iconpath)
│ │          window.grab_set()
│ │  
│ │          # Labels
│ │ -        ttk.Label(window, text="Training Parameters", font=("Verdana", 14)).grid(
│ │ +        ttk.Label(window, text="Training Parameters",
│ │ +                  font=("Verdana", 14)).grid(
│ │              column=1, row=0, padx=10
│ │          )
│ │          ttk.Label(window, text="Image Directory").grid(column=1, row=2)
│ │          ttk.Label(window, text="Mask Directory").grid(column=1, row=3)
│ │          ttk.Label(window, text="Output Directory").grid(column=1, row=4)
│ │          ttk.Label(window, text="Batch Size").grid(column=1, row=5)
│ │          ttk.Label(window, text="Learning Rate").grid(column=1, row=6)
│ │ @@ -1216,130 +1252,132 @@
│ │          # Train image directory
│ │          self.train_image_dir = StringVar()
│ │          train_image_entry = ttk.Entry(
│ │              window, width=30, textvariable=self.train_image_dir
│ │          )
│ │          train_image_entry.grid(column=2, row=2, columnspan=3, sticky=(W, E))
│ │          self.train_image_dir.set(
│ │ -            "C:/Users/admin/Documents/DL_Track/Train_Data_DL_Track/apo_test"
│ │ +            "C:/Users/"
│ │          )
│ │  
│ │          # Mask directory
│ │          self.mask_dir = StringVar()
│ │          mask_entry = ttk.Entry(window, width=30, textvariable=self.mask_dir)
│ │          mask_entry.grid(column=2, row=3, columnspan=3, sticky=(W, E))
│ │          self.mask_dir.set(
│ │ -            "C:/Users/admin/Documents/DL_Track/Train_Data_DL_Track/apo_mask_test"
│ │ +            "C:/Users/"
│ │          )
│ │  
│ │          # Output path
│ │          self.out_dir = StringVar()
│ │          out_entry = ttk.Entry(window, width=30, textvariable=self.out_dir)
│ │          out_entry.grid(column=2, row=4, columnspan=3, sticky=(W, E))
│ │ -        self.out_dir.set("C:/Users/admin/Documents")
│ │ +        self.out_dir.set("C:/Users/")
│ │  
│ │          # Buttons
│ │          # Train image button
│ │ -        train_img_button = ttk.Button(window, text="Images", command=self.get_train_dir)
│ │ +        train_img_button = ttk.Button(window, text="Images",
│ │ +                                      command=self.get_train_dir)
│ │          train_img_button.grid(column=5, row=2, sticky=E)
│ │  
│ │          # Mask button
│ │ -        mask_button = ttk.Button(window, text="Masks", command=self.get_mask_dir)
│ │ +        mask_button = ttk.Button(window, text="Masks",
│ │ +                                 command=self.get_mask_dir)
│ │          mask_button.grid(column=5, row=3, sticky=E)
│ │  
│ │          # Input directory
│ │ -        out_button = ttk.Button(window, text="Output", command=self.get_output_dir)
│ │ +        out_button = ttk.Button(window, text="Output",
│ │ +                                command=self.get_output_dir)
│ │          out_button.grid(column=5, row=4, sticky=E)
│ │  
│ │          # Model train button
│ │          model_button = ttk.Button(
│ │              window, text="Start Training", command=self.train_model
│ │          )
│ │          model_button.grid(column=5, row=10, sticky=E)
│ │  
│ │          # Comboboxes
│ │          # Batch size
│ │          self.batch_size = StringVar()
│ │          size = ("1", "2", "3", "4", "5", "6")
│ │ -        size_entry = ttk.Combobox(window, width=10, textvariable=self.batch_size)
│ │ +        size_entry = ttk.Combobox(window, width=10,
│ │ +                                  textvariable=self.batch_size)
│ │          size_entry["values"] = size
│ │          size_entry.grid(column=2, row=5, sticky=(W, E))
│ │          self.batch_size.set("1")
│ │  
│ │          # Learning rate
│ │          self.learn_rate = StringVar()
│ │          learn = ("0.005", "0.001", "0.0005", "0.0001", "0.00005", "0.00001")
│ │ -        learn_entry = ttk.Combobox(window, width=10, textvariable=self.learn_rate)
│ │ +        learn_entry = ttk.Combobox(window, width=10,
│ │ +                                   textvariable=self.learn_rate)
│ │          learn_entry["values"] = learn
│ │          learn_entry.grid(column=2, row=6, sticky=(W, E))
│ │          self.learn_rate.set("0.00001")
│ │  
│ │          # Number of training epochs
│ │          self.epochs = StringVar()
│ │          epoch = ("30", "40", "50", "60", "70", "80")
│ │          epoch_entry = ttk.Combobox(window, width=10, textvariable=self.epochs)
│ │          epoch_entry["values"] = epoch
│ │          epoch_entry.grid(column=2, row=7, sticky=(W, E))
│ │          self.epochs.set("3")
│ │  
│ │          # Loss function
│ │          self.loss_function = StringVar()
│ │ -        loss = ("BCE", "Dice", "FL")
│ │ -        loss_entry = ttk.Combobox(window, width=10, textvariable=self.loss_function)
│ │ +        loss = ("BCE")
│ │ +        loss_entry = ttk.Combobox(window, width=10,
│ │ +                                  textvariable=self.loss_function)
│ │          loss_entry["values"] = loss
│ │          loss_entry["state"] = "readonly"
│ │          loss_entry.grid(column=2, row=8, sticky=(W, E))
│ │          self.loss_function.set("BCE")
│ │  
│ │          # Add padding
│ │          for child in window.winfo_children():
│ │              child.grid_configure(padx=5, pady=5)
│ │  
│ │ -    ## Methods used for model training
│ │ +    # Methods used for model training
│ │  
│ │      def get_train_dir(self):
│ │ -        """
│ │ -        Instance method to ask the user to select the training image
│ │ +        """Instance method to ask the user to select the training image
│ │          directory path. All image files (of the same specified filetype) in
│ │          the directory are analysed. This must be an absolute path.
│ │          """
│ │          train_image_dir = filedialog.askdirectory()
│ │          self.train_image_dir.set(train_image_dir)
│ │  
│ │      def get_mask_dir(self):
│ │ -        """
│ │ -        Instance method to ask the user to select the training mask
│ │ +        """Instance method to ask the user to select the training mask
│ │          directory path. All mask files (of the same specified filetype) in
│ │          the directory are analysed.The mask files and the corresponding
│ │          image must have the exact same name. This must be an absolute path.
│ │          """
│ │          mask_dir = filedialog.askdirectory()
│ │          self.mask_dir.set(mask_dir)
│ │  
│ │      def get_output_dir(self):
│ │ -        """
│ │ -        Instance method to ask the user to select the output
│ │ +        """Instance method to ask the user to select the output
│ │          directory path. Here, all file created during model
│ │          training (model file, weight file, graphs) are saved.
│ │          This must be an absolute path.
│ │          """
│ │          out_dir = filedialog.askdirectory()
│ │          self.out_dir.set(out_dir)
│ │  
│ │      def train_model(self):
│ │ -        """
│ │ -        Instance method to execute the model training when the
│ │ +        """Instance method to execute the model training when the
│ │          "start training" button is pressed.
│ │  
│ │          By pressing the button, a seperate thread is started
│ │          in which the model training is run. This allows the user to break any
│ │          training process at certain stages. When the analysis can be
│ │          interrupted, a tk.messagebox opens asking the user to either
│ │ -        continue or terminate the analysis. Moreover, the threading allows interaction
│ │ -        with the GUI during ongoing analysis process.
│ │ +        continue or terminate the analysis. Moreover, the threading allows
│ │ +        interaction with the GUI during ongoing analysis process.
│ │          """
│ │          try:
│ │              # See if GUI is already running
│ │              if self.is_running:
│ │                  # don't run again if it is already running
│ │                  return
│ │              self.is_running = True
│ │ @@ -1351,15 +1389,16 @@
│ │  
│ │              # Make sure some kind of filetype is specified.
│ │              if (
│ │                  len(selected_images) < 3
│ │                  or len(selected_masks) < 3
│ │                  or len(selected_outpath) < 3
│ │              ):
│ │ -                tk.messagebox.showerror("Information", "Specified directories invalid.")
│ │ +                tk.messagebox.showerror("Information",
│ │ +                                        "Specified directories invalid.")
│ │                  self.should_stop = False
│ │                  self.is_running = False
│ │                  self.do_break()
│ │                  return
│ │  
│ │              selected_batch_size = int(self.batch_size.get())
│ │              selected_learning_rate = float(self.learn_rate.get())
│ │ @@ -1382,27 +1421,27 @@
│ │              )
│ │  
│ │              thread.start()
│ │  
│ │          # Error handling
│ │          except ValueError:
│ │              tk.messagebox.showerror(
│ │ -                "Information", "Analysis parameter entry fields" + " must not be empty."
│ │ +                "Information", "Analysis parameter entry fields" +
│ │ +                " must not be empty."
│ │              )
│ │              self.do_break()
│ │              self.should_stop = False
│ │              self.is_running = False
│ │  
│ │      # ---------------------------------------------------------------------------------------------------
│ │      # Methods and properties required for threading
│ │  
│ │      @property
│ │      def should_stop(self) -> bool:
│ │ -        """
│ │ -        Instance method to define the should_stop
│ │ +        """Instance method to define the should_stop
│ │          property getter method. By defining this as a property,
│ │          should_stop is treated like a public attribute even
│ │          though it is private.
│ │  
│ │          This is used to stop the analysis process running
│ │          in a seperate thread.
│ │  
│ │ @@ -1417,16 +1456,15 @@
│ │          # Get private variable _should_stop
│ │          should_stop = self._should_stop
│ │          self._lock.release()
│ │          return should_stop
│ │  
│ │      @property
│ │      def is_running(self) -> bool:
│ │ -        """
│ │ -        Instance method to define the is_running
│ │ +        """Instance method to define the is_running
│ │          property getter method. By defining this as a property,
│ │          is_running is treated like a public attribute even
│ │          though it is private.
│ │  
│ │          This is used to stop the analysis process running
│ │          in a seperate thread.
│ │  
│ │ @@ -1440,29 +1478,27 @@
│ │          self._lock.acquire()
│ │          is_running = self._is_running
│ │          self._lock.release()
│ │          return is_running
│ │  
│ │      @should_stop.setter
│ │      def should_stop(self, flag: bool):
│ │ -        """
│ │ -        Instance method to define the should_stop
│ │ +        """Instance method to define the should_stop
│ │          property setter method. The setter method is used
│ │          to set the self._should_stop attribute as if it was
│ │          a public attribute. The argument "flag" is thereby
│ │          validated to ensure proper input (boolean)
│ │          """
│ │          self._lock.acquire()
│ │          self._should_stop = flag
│ │          self._lock.release()
│ │  
│ │      @is_running.setter
│ │      def is_running(self, flag: bool):
│ │ -        """
│ │ -        Instance method to define the is_running
│ │ +        """Instance method to define the is_running
│ │          property setter method. The setter method is used
│ │          to set the self._is_running attribute as if it was
│ │          a public attribute. The argument "flag" is thereby
│ │          validated to ensure proper input (boolean)
│ │          """
│ │          self._lock.acquire()
│ │          self._is_running = flag
│ │ @@ -1470,33 +1506,33 @@
│ │  
│ │  
│ │  # ---------------------------------------------------------------------------------------------------
│ │  # Function required to run the GUI frm the prompt
│ │  
│ │  
│ │  def runMain() -> None:
│ │ -    """
│ │ -    Function that enables usage of the gui from command promt
│ │ +    """Function that enables usage of the gui from command promt
│ │      as pip package.
│ │  
│ │      Notes
│ │      -----
│ │ -    The GUI can be executed by typin 'python -m DLTrack_GUI.py' in the command
│ │ -    subsequtently to installing the pip package´and activating the
│ │ +    The GUI can be executed by typing 'python -m DL_Track_US_GUI.py' in a
│ │ +    terminal subsequtently to installing the pip package and activating the
│ │      respective library.
│ │  
│ │      It is not necessary to download any files from the repository when the pip
│ │      package is installed.
│ │  
│ │      For documentation of DL_Track see top of this module.
│ │      """
│ │      root = Tk()
│ │      DLTrack(root)
│ │      root.mainloop()
│ │  
│ │  
│ │ -# This statement is required to execute the GUI by typing 'python DLTrack_GUI.py' in the prompt
│ │ +# This statement is required to execute the GUI by typing
│ │ +# 'python DL_Track_US_GUI.py' in the prompt
│ │  # when navigated to the folder containing the file and all dependencies.
│ │  if __name__ == "__main__":
│ │      root = Tk()
│ │      DLTrack(root)
│ │      root.mainloop()
│ │   --- dl_track_us-0.1.1/DL_Track/home_im.ico
│ ├── +++ dl_track_us-0.1.2/DL_Track_US/home_im.ico
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/DL_Track/gui_helpers/__init__.py
│ ├── +++ dl_track_us-0.1.2/DL_Track_US/gui_helpers/__init__.py
│ │┄ Files 27% similar despite different names
│ │ @@ -2,22 +2,20 @@
│ │      "calculate_architecture",
│ │      "calibrate",
│ │      "do_calculations",
│ │      "calibrate_video",
│ │      "do_calculations_video",
│ │      "calculate_architecture_video",
│ │      "manual_tracing",
│ │ -    "image_quality",
│ │      "model_training",
│ │  ]
│ │  
│ │  __author__ = "Paul Ritsche, Olivier Seynnes, Neil Cronin"
│ │  
│ │ -from DL_Track.gui_helpers.calculate_architecture import *
│ │ -from DL_Track.gui_helpers.calculate_architecture_video import *
│ │ -from DL_Track.gui_helpers.calibrate import *
│ │ -from DL_Track.gui_helpers.calibrate_video import *
│ │ -from DL_Track.gui_helpers.do_calculations import *
│ │ -from DL_Track.gui_helpers.do_calculations_video import *
│ │ -from DL_Track.gui_helpers.image_quality import *
│ │ -from DL_Track.gui_helpers.manual_tracing import *
│ │ -from DL_Track.gui_helpers.model_training import *
│ │ +from DL_Track_US.gui_helpers.calculate_architecture import *
│ │ +from DL_Track_US.gui_helpers.calculate_architecture_video import *
│ │ +from DL_Track_US.gui_helpers.calibrate import *
│ │ +from DL_Track_US.gui_helpers.calibrate_video import *
│ │ +from DL_Track_US.gui_helpers.do_calculations import *
│ │ +from DL_Track_US.gui_helpers.do_calculations_video import *
│ │ +from DL_Track_US.gui_helpers.manual_tracing import *
│ │ +from DL_Track_US.gui_helpers.model_training import *
│ │   --- dl_track_us-0.1.1/DL_Track/gui_helpers/calculate_architecture.py
│ ├── +++ dl_track_us-0.1.2/DL_Track_US/gui_helpers/calculate_architecture.py
│ │┄ Files 6% similar despite different names
│ │ @@ -23,19 +23,20 @@
│ │       Function to import an image.
│ │  importFlipFlagsList
│ │       Function to retrieve flip values from a .txt file.
│ │  compileSaveResults
│ │       Function to save the analysis results to a .xlsx file.
│ │  IoU
│ │      Function to compute the intersection over union score (IoU),
│ │ -    a measure of prediction accuracy. This is sometimes also called Jaccard score.
│ │ +    a measure of prediction accuracy. This is sometimes also called Jaccard
│ │ +    score.
│ │  calculateBatch
│ │ -    Function to calculate muscle architecture in longitudinal ultrasonography images
│ │ -    of human lower limb muscles. The values computed are fascicle length (FL),
│ │ -    pennation angle (PA), and muscle thickness (MT).
│ │ +    Function to calculate muscle architecture in longitudinal ultrasonography
│ │ +    images of human lower limb muscles. The values computed are fascicle
│ │ +    length (FL), pennation angle (PA), and muscle thickness (MT).
│ │  calculateBatchManual
│ │      Function used for manual calculation of fascicle length, muscle thickness
│ │      and pennation angles in longitudinal ultrasonography images of human lower
│ │      limb muscles.
│ │  
│ │  Notes
│ │  -----
│ │ @@ -55,40 +56,39 @@
│ │  import pandas as pd
│ │  from keras import backend as K
│ │  from keras.models import load_model
│ │  from matplotlib.backends.backend_pdf import PdfPages
│ │  from skimage.transform import resize
│ │  from tensorflow.keras.utils import img_to_array
│ │  
│ │ -from DL_Track.gui_helpers.calibrate import (
│ │ +from DL_Track_US.gui_helpers.calibrate import (
│ │      calibrateDistanceManually,
│ │      calibrateDistanceStatic,
│ │  )
│ │ -from DL_Track.gui_helpers.do_calculations import doCalculations
│ │ -from DL_Track.gui_helpers.manual_tracing import ManualAnalysis
│ │ +from DL_Track_US.gui_helpers.do_calculations import doCalculations
│ │ +from DL_Track_US.gui_helpers.manual_tracing import ManualAnalysis
│ │  
│ │  plt.style.use("ggplot")
│ │  plt.switch_backend("agg")
│ │  
│ │  
│ │  def importAndReshapeImage(path_to_image: str, flip: int):
│ │ -    """
│ │ -    Function to import and reshape an image. Moreover, based upon
│ │ +    """Function to import and reshape an image. Moreover, based upon
│ │      user specification the image might be flipped.
│ │  
│ │      Usually flipping is only required when the imported image is of
│ │      a specific structure that is incompatible with the trained models
│ │      provided here
│ │  
│ │      Parameters
│ │      ----------
│ │      path_to_image : str
│ │           String variable containing the imagepath. This should be an
│ │           absolute path.
│ │ -    flip : int
│ │ +    flip : {0, 1}
│ │          Integer value defining wheter an image should be flipped.
│ │          This can be 0 (image is not flipped) or 1 (image is flipped).
│ │  
│ │      Returns
│ │      -------
│ │      img : np.ndarray
│ │          The loaded images is converted to a np.nadarray. This is done
│ │ @@ -136,58 +136,57 @@
│ │      img = resize(img, (1, 512, 512, 3), mode="constant", preserve_range=True)
│ │      img = img / 255.0
│ │  
│ │      return img, img_copy, non_flipped_img, height, width, filename
│ │  
│ │  
│ │  def importImageManual(path_to_image: str, flip: int):
│ │ -    """
│ │ -    Function to import an image.
│ │ +    """Function to import an image.
│ │  
│ │      This function is used when manual analysis of the
│ │      image is selected in the GUI. For manual analysis,
│ │      it is not necessary to resize, reshape and normalize the image.
│ │      The image may be flipped.
│ │  
│ │      Parameters
│ │      ----------
│ │      path_to_image : str
│ │           String variable containing the imagepath. This should be an
│ │           absolute path.
│ │ -    flip : int
│ │ +    flip : {0, 1}
│ │          Integer value defining wheter an image should be flipped.
│ │          This can be 0 (image is not flipped) or 1 (image is flipped).
│ │  
│ │      Returns
│ │      -------
│ │      img : np.ndarray
│ │          The loaded images as a np.nadarray in grayscale.
│ │      filename : str
│ │          String value containing the name of the input image, not the
│ │          entire path.
│ │  
│ │      Examples
│ │      --------
│ │      >>> importImageManual(path_to_image="C:/Desktop/Test/Img1.tif", flip=0)
│ │ -    [[[28 26 25] [28 26 25] [28 26 25] ... [[ 0  0  0] [ 0  0  0] [ 0  0  0]]], Img1.tif
│ │ +    [[[28 26 25] [28 26 25] [28 26 25] ... [[ 0  0  0] [ 0  0  0] [ 0  0  0]]],
│ │ +    Img1.tif
│ │      """
│ │      # Load image
│ │      filename = os.path.splitext(os.path.basename(path_to_image))[0]
│ │      img = cv2.imread(path_to_image, 0)
│ │  
│ │      # Flip image if required
│ │      if flip == "flip":
│ │          img = cv2.flip(img, 1)
│ │  
│ │      return img, filename
│ │  
│ │  
│ │  def getFlipFlagsList(flip_flag_path: str) -> list:
│ │ -    """
│ │ -    Function to retrieve flip values from a .txt file.
│ │ +    """Function to retrieve flip values from a .txt file.
│ │  
│ │      The flip flags decide wether an image should be flipped or not.
│ │      The flags can be 0 (image not flipped) or 1 (image is flipped).
│ │      The flags must be specified in the .txt file and can either be
│ │      on a seperate line for each image or on a seperate line for each folder.
│ │      The amount of flip flags must equal the amount of images analyzed.
│ │  
│ │ @@ -217,16 +216,15 @@
│ │              if digit.isdigit():
│ │                  flip_flags.append(digit)
│ │  
│ │      return flip_flags
│ │  
│ │  
│ │  def compileSaveResults(rootpath: str, dataframe: pd.DataFrame) -> None:
│ │ -    """
│ │ -    Function to save the analysis results to a .xlsx file.
│ │ +    """Function to save the analysis results to a .xlsx file.
│ │  
│ │      A pd.DataFrame object must be inputted. The results
│ │      inculded in the dataframe are saved to an .xlsx file.
│ │      The .xlsx file is saved to the specified rootpath.
│ │  
│ │      Parameters
│ │      ----------
│ │ @@ -244,31 +242,32 @@
│ │                                   ['PA', 17], ...])
│ │      """
│ │      # Make filepath
│ │      excelpath = rootpath + "/Results.xlsx"
│ │  
│ │      # Check if file already existing and write .xlsx
│ │      if os.path.exists(excelpath):
│ │ -        with pd.ExcelWriter(excelpath, mode="a", if_sheet_exists="replace") as writer:
│ │ +        with pd.ExcelWriter(excelpath,
│ │ +                            mode="a", if_sheet_exists="replace") as writer:
│ │              data = dataframe
│ │              data.to_excel(writer, sheet_name="Results")
│ │      else:
│ │          with pd.ExcelWriter(excelpath, mode="w") as writer:
│ │              data = dataframe
│ │              data.to_excel(writer, sheet_name="Results")
│ │  
│ │  
│ │  def IoU(y_true, y_pred, smooth: int = 1) -> float:
│ │ -    """
│ │ -    Function to compute the intersection of union score (IoU),
│ │ -    a measure of prediction accuracy. This is sometimes also called Jaccard score.
│ │ +    """Function to compute the intersection of union score (IoU),
│ │ +    a measure of prediction accuracy. This is sometimes also called Jaccard
│ │ +    score.
│ │  
│ │      The IoU can be used as a loss metric during binary segmentation when
│ │ -    convolutional neural networks are applied. The IoU is calculated for both, the
│ │ -    training and validation set.
│ │ +    convolutional neural networks are applied. The IoU is calculated for both,
│ │ +    the training and validation set.
│ │  
│ │      Parameters
│ │      ----------
│ │      y_true : tf.Tensor
│ │          True positive image segmentation label predefined by the user.
│ │          This is the mask that is provided prior to model training.
│ │      y_pred : tf.Tensor
│ │ @@ -286,16 +285,18 @@
│ │      -----
│ │      The IoU is usually calculated as IoU = intersection / union.
│ │      The intersection is calculated as the overlap of y_true and
│ │      y_pred, whereas the union is the sum of y_true and y_pred.
│ │  
│ │      Examples
│ │      --------
│ │ -    >>> IoU(y_true=Tensor("IteratorGetNext:1", shape=(1, 512, 512, 1), dtype=float32),
│ │ -             y_pred=Tensor("VGG16_U-Net/conv2d_8/Sigmoid:0", shape=(1, 512, 512, 1), dtype=float32),
│ │ +    >>> IoU(y_true=Tensor("IteratorGetNext:1", shape=(1, 512, 512, 1),
│ │ +             dtype=float32),
│ │ +             y_pred=Tensor("VGG16_U-Net/conv2d_8/Sigmoid:0",
│ │ +             shape=(1, 512, 512, 1), dtype=float32),
│ │               smooth=1)
│ │      Tensor("truediv:0", shape=(1, 512, 512), dtype=float32)
│ │      """
│ │      # Calculate Intersection
│ │      intersect = K.sum(K.abs(y_true * y_pred), axis=-1)
│ │      # Calculate Union
│ │      union = K.sum(y_true, -1) + K.sum(y_pred, -1) - intersect
│ │ @@ -317,87 +318,92 @@
│ │      fasc_threshold: float,
│ │      fasc_cont_thresh: int,
│ │      min_width: int,
│ │      min_pennation: int,
│ │      max_pennation: int,
│ │      gui,
│ │  ) -> None:
│ │ -    """
│ │ -    Function to calculate muscle architecture in longitudinal ultrasonography images
│ │ -    of human lower limb muscles. The values computed are fascicle length (FL),
│ │ -    pennation angle (PA), and muscle thickness (MT).
│ │ -
│ │ -    The scope of this function is limited. Images of the vastus lateralis, tibialis anterior
│ │ -    soleus and gastrocnemius  muscles can be analyzed. This is due to the limited amount of
│ │ -    training data for our convolutional neural networks. This functions makes extensive use
│ │ +    """Function to calculate muscle architecture in longitudinal
│ │ +    ultrasonography images of human lower limb muscles. The values
│ │ +    computed are fascicle length (FL), pennation angle (PA),
│ │ +    and muscle thickness (MT).
│ │ +
│ │ +    The scope of this function is limited. Images of the vastus lateralis,
│ │ +    tibialis anterior soleus and gastrocnemius  muscles can be analyzed.
│ │ +    This is due to the limited amount of training data for our convolutional
│ │ +    neural networks. This functions makes extensive use
│ │      of several other functions and was designed to be executed from a GUI.
│ │  
│ │      Parameters
│ │      ----------
│ │      rootpath : str
│ │          String variable containing the path to the folder where all images
│ │          to be analyzed are saved.
│ │      apo_modelpath : str
│ │ -        String variable containing the absolute path to the aponeurosis neural network.
│ │ +        String variable containing the absolute path to the aponeurosis
│ │ +        neural network.
│ │      fasc_modelpath : str
│ │ -        String variable containing the absolute path to the fascicle neural network.
│ │ +        String variable containing the absolute path to the fascicle
│ │ +        neural network.
│ │      flip_flag_path : str
│ │          String variabel containing the absolute path to the flip flag
│ │ -        .txt file containing the flip flags. Flipping is necessary as the models were trained
│ │ -        on images of with specific fascicle orientation.
│ │ +        .txt file containing the flip flags. Flipping is necessary as the
│ │ +        models were trained on images of with specific fascicle orientation.
│ │      filetype : str
│ │          String variable containg the respective type of the images.
│ │          This is needed to select only the relevant image files
│ │          in the root directory.
│ │ -    scaling : str
│ │ +    scaling : {"bar", "manual", "No scaling"}
│ │          String variabel determining the image scaling method.
│ │          There are three types of scaling available:
│ │          - scaling = "manual" (user must scale images manually)
│ │          - sclaing = "bar" (image are scaled automatically. This is done by
│ │            detecting scaling bars on the right side of the image.)
│ │          - scaling = "No scaling" (image is not scaled.)
│ │          Scaling is necessary to compute measurements in centimeter,
│ │          if "no scaling" is chosen, the results are in pixel units.
│ │ -    spacing : int
│ │ +    spacing : {10, 5, 15, 20}
│ │          Distance (in milimeter) between two scaling bars in the image.
│ │          This is needed to compute the pixel/cm ratio and therefore report
│ │          the results in centimeter rather than pixel units.
│ │      apo_threshold : float
│ │ -        Float variable containing the threshold applied to predicted aponeurosis
│ │ -        pixels by our neural networks. By varying this threshold, different
│ │ -        structures will be classified as aponeurosis as the threshold for classifying
│ │ +        Float variable containing the threshold applied to predicted
│ │ +        aponeurosis pixels by our neural networks. By varying this
│ │ +        threshold, different structures will be classified as
│ │ +        aponeurosis as the threshold for classifying
│ │          a pixel as aponeurosis is changed. Must be non-zero and
│ │          non-negative.
│ │      fasc_threshold : float
│ │          Float variable containing the threshold applied to predicted fascicle
│ │          pixels by our neural networks. By varying this threshold, different
│ │ -        structures will be classified as fascicle as the threshold for classifying
│ │ -        a pixel as fascicle is changed.
│ │ +        structures will be classified as fascicle as the threshold for
│ │ +        classifying a pixel as fascicle is changed.
│ │      fasc_cont_threshold : float
│ │          Float variable containing the threshold applied to predicted fascicle
│ │          segments by our neural networks. By varying this threshold, different
│ │ -        structures will be classified as fascicle. By increasing, longer fascicle segments
│ │ -        will be considered, by lowering shorter segments. Must be non-zero and
│ │ -        non-negative.
│ │ +        structures will be classified as fascicle. By increasing, longer
│ │ +        fascicle segments will be considered, by lowering shorter segments.
│ │ +        Must be non-zero and non-negative.
│ │      min_width : int
│ │ -        Integer variable containing the minimal distance between aponeuroses to be
│ │ -        detected. The aponeuroses must be at least this distance apart to be
│ │ -        detected. The distance is specified in pixels. Must be non-zero and non-negative.
│ │ +        Integer variable containing the minimal distance between aponeuroses
│ │ +        to be detected. The aponeuroses must be at least this distance apart
│ │ +        to be detected. The distance is specified in pixels.
│ │ +        Must be non-zero and non-negative.
│ │      min_pennation : int
│ │ -        Integer variable containing the mininmal (physiological) acceptable pennation
│ │ -        angle occuring in the analyzed image/muscle. Fascicles with lower pennation
│ │ -        angles will be excluded. The pennation angle is calculated as the amgle
│ │ -        of insertion between extrapolated fascicle and detected aponeurosis. Must
│ │ -        be non-negative.
│ │ -    min_pennation : int
│ │ -        Integer variable containing the maximal (physiological) acceptable pennation
│ │ -        angle occuring in the analyzed image/muscle. Fascicles with higher pennation
│ │ -        angles will be excluded. The pennation angle is calculated as the amgle
│ │ -        of insertion between extrapolated fascicle and detected aponeurosis. Must
│ │ -        be non-negative and larger than min_pennation.
│ │ +        Integer variable containing the mininmal (physiological) acceptable
│ │ +        pennation angle occuring in the analyzed image/muscle. Fascicles
│ │ +        with lower pennation angles will be excluded.
│ │ +        The pennation angle is calculated as the amgle of insertion between
│ │ +        extrapolated fascicle and detected aponeurosis. Must be non-negative.
│ │ +    max_pennation : int
│ │ +        Integer variable containing the maximal (physiological) acceptable
│ │ +        pennation angle occuring in the analyzed image/muscle. Fascicles
│ │ +        with higher pennation angles will be excluded.
│ │ +        The pennation angle is calculated as the amgle of insertion between
│ │ +        extrapolated fascicle and detected aponeurosis. Must be non-negative.
│ │      gui : tk.TK
│ │          A tkinter.TK class instance that represents a GUI. By passing this
│ │          argument, interaction with the GUI is possible i.e., stopping
│ │          the calculation process after each image.
│ │  
│ │      See Also
│ │      --------
│ │ @@ -406,39 +412,41 @@
│ │  
│ │      Notes
│ │      -----
│ │      For specific explanations of the included functions see the respective
│ │      function docstrings in this module. To see an examplenary PDF output
│ │      and .xlsx file take at look at the examples provided in the "examples"
│ │      directory.
│ │ -    This function is called by the GUI. Note that the functioned was specifically
│ │ -    designed to be called from the GUI. Thus, tk.messagebox will pop up when errors are
│ │ -    raised even if the GUI is not started.
│ │ +    This function is called by the GUI. Note that the functioned was
│ │ +    specifically designed to be called from the GUI. Thus, tk.messagebox
│ │ +    will pop up when errors are raised even if the GUI is not started.
│ │  
│ │      Examples
│ │      --------
│ │      >>> calculateBatch(rootpath="C:/Users/admin/Dokuments/images",
│ │                         apo_modelpath="C:/Users/admin/Dokuments/models/apo_model.h5",
│ │                         fasc_modelpath="C:/Users/admin/Dokuments/models/apo_model.h5",
│ │                         flip_flag_path="C:/Users/admin/Dokuments/flip_flags.txt",
│ │ -                       filetype="/**/*.tif, scaline="bar", spacing=10, apo_threshold=0.1,
│ │ +                       filetype="/**/*.tif, scaline="bar", spacing=10,
│ │ +                       apo_threshold=0.1,
│ │                         fasc_threshold=0.05, fasc_cont_thres=40, curvature=3,
│ │                         min_pennation=10, max_pennation=35,
│ │ -                       gui=<__main__.DLTrack object at 0x000002BFA7528190>)
│ │ +                       gui=<__main__.DL_Track_US object at 0x000002BFA7528190>)
│ │      """
│ │      # Get list of files
│ │      list_of_files = glob.glob(rootpath + file_type, recursive=True)
│ │  
│ │      try:
│ │          # Load models
│ │          model_apo = load_model(apo_modelpath, custom_objects={"IoU": IoU})
│ │          model_fasc = load_model(fasc_modelpath, custom_objects={"IoU": IoU})
│ │  
│ │      except OSError:
│ │ -        tk.messagebox.showerror("Information", "Apo/Fasc model path is incorrect.")
│ │ +        tk.messagebox.showerror("Information",
│ │ +                                "Apo/Fasc model path is incorrect.")
│ │          gui.should_stop = False
│ │          gui.is_running = False
│ │          gui.do_break()
│ │          return
│ │  
│ │      # Check validity of flipflag path and rais execption
│ │      try:
│ │ @@ -519,15 +527,16 @@
│ │                      if scaling == "Bar":
│ │                          calibrate_fn = calibrateDistanceStatic
│ │                          # Find length of the scaling line
│ │                          calib_dist, scale_statement = calibrate_fn(
│ │                              nonflipped_img, spacing
│ │                          )
│ │  
│ │ -                        # Append warning to failed images when no error was found
│ │ +                        # Append warning to failed images when no error was
│ │ +                        # found
│ │                          if calib_dist is None:
│ │                              fail = f"Scalingbars not found in {imagepath}"
│ │                              failed_files.append(fail)
│ │                              warnings.warn("Image fails with StaticScalingError")
│ │                              continue
│ │  
│ │                      # Manual scaling
│ │ @@ -553,16 +562,16 @@
│ │                          filename,
│ │                          model_apo,
│ │                          model_fasc,
│ │                          scale_statement,
│ │                          dic,
│ │                      )
│ │  
│ │ -                    # Append warning to failes files when no aponeurosis was found and
│ │ -                    # and continue analysis
│ │ +                    # Append warning to failes files when no aponeurosis was
│ │ +                    # found and continue analysis
│ │                      if fasc_l is None:
│ │                          fail = f"No two aponeuroses found in {imagepath}"
│ │                          failed_files.append(fail)
│ │                          continue
│ │  
│ │                      # Define output dataframe
│ │                      dataframe2 = pd.DataFrame(
│ │ @@ -590,21 +599,23 @@
│ │                      count += 1
│ │  
│ │                  # Get time of total analysis
│ │                  duration = time.time() - start_time
│ │                  print(f"duration total analysis: {duration}")
│ │  
│ │              except FileNotFoundError:
│ │ -                tk.messagebox.showerror("Information", "Input directory is incorrect.")
│ │ +                tk.messagebox.showerror("Information",
│ │ +                                        "Input directory is incorrect.")
│ │                  gui.should_stop = False
│ │                  gui.is_running = False
│ │                  gui.do_break()
│ │                  return
│ │  
│ │ -            # Subsequent to analysis of all images, results are saved and the GUI is stopped
│ │ +            # Subsequent to analysis of all images, results are saved and
│ │ +            # the GUI is stopped
│ │              finally:
│ │  
│ │                  # Save predicted area results
│ │                  compileSaveResults(rootpath, dataframe)
│ │  
│ │                  # Write failed images in file
│ │                  if len(failed_files) >= 1:
│ │ @@ -627,64 +638,69 @@
│ │              gui.do_break()
│ │              gui.should_stop = False
│ │              gui.is_running = False
│ │  
│ │          # Filpflage != number of images
│ │          else:
│ │              tk.messagebox.showerror(
│ │ -                "Information", "Number of flipflags must match number of images."
│ │ +                "Information",
│ │ +                "Number of flipflags must match number of images."
│ │              )
│ │              gui.should_stop = False
│ │              gui.is_running = False
│ │  
│ │  
│ │  def calculateBatchManual(rootpath: str, filetype: str, gui):
│ │ -    """
│ │ -    Function used for manual calculation of fascicle length, muscle thickness
│ │ -    and pennation angles in longitudinal ultrasonography images of human lower
│ │ -    limb muscles.
│ │ -
│ │ -    This function is not restricted to any specific muscles. However, its use is
│ │ -    restricted to a specific method for assessing muscle thickness fascicle
│ │ -    length and pennation angles.
│ │ +    """Function used for manual calculation of fascicle length,
│ │ +    muscle thickness and pennation angles in longitudinal
│ │ +    ultrasonography images of human lower limb muscles.
│ │ +
│ │ +    This function is not restricted to any specific muscles. However,
│ │ +    its use is restricted to a specific method for assessing muscle
│ │ +    thickness fascicle length and pennation angles.
│ │  
│ │      - Muscle thickness:
│ │                         Exactly one segment reaching from the superficial to the
│ │ -                       deep aponeuroses of the muscle must be drawn. If multiple
│ │ -                       measurement are drawn, these are averaged. Drawing can
│ │ -                       be started by clickling the left mouse button and keeping
│ │ -                       it pressed until it is not further required to draw the line
│ │ -                       (i.e., the other aponeurosis border is reached). Only the
│ │ -                       respective y-coordinates of the points where the cursor
│ │ -                       was clicked and released are considered for calculation of
│ │ -                       muscle thickness.
│ │ +                       deep aponeuroses of the muscle must be drawn.
│ │ +                       If multiple measurement are drawn, these are averaged.
│ │ +                       Drawing can be started by clickling the left mouse
│ │ +                       button and keeping it pressed until it is not further
│ │ +                       required to draw the line (i.e., the other aponeurosis
│ │ +                       border is reached). Only the respective y-coordinates
│ │ +                       of the points where the cursor was clicked and released
│ │ +                       are considered for calculation of muscle thickness.
│ │      - Fascicle length:
│ │ -                      Exactly three segments along the fascicleof the muscle must
│ │ -                      be drawn. If multiple fascicle are drawn, their lengths are
│ │ -                      averaged. Drawing can be started by clickling the left mouse
│ │ -                      button and keeping it pressed until one segment is finished
│ │ -                      (mostly where fascicle curvature occurs the other aponeurosis
│ │ -                      border is reached). Using the euclidean distance, the total
│ │ -                      fascicle length is computed as a sum of the segments.
│ │ +                      Exactly three segments along the fascicleof the muscle
│ │ +                      must be drawn. If multiple fascicle are drawn, their
│ │ +                      lengths are averaged. Drawing can be started by clickling
│ │ +                      the left mouse button and keeping it pressed until one
│ │ +                      segment is finished (mostly where fascicle curvature
│ │ +                      occurs the other aponeurosis border is reached). Using
│ │ +                      the euclidean distance, the total fascicle length is
│ │ +                      computed as a sum of the segments.
│ │      - Pennation angle:
│ │ -                      Exactly two segments, one along the fascicle orientation, the
│ │ -                      other along the aponeurosis orientation must be drawn. The line
│ │ -                      along the aponeurosis must be started where the line along the
│ │ -                      fascicle ends. If multiple angle are drawn, they are averaged.
│ │ -                      Drawing can be started by clickling the left mouse button and keeping
│ │ -                      it pressed until it is not further required to draw the line
│ │ -                      (i.e., the aponeurosis border is reached by the fascicle). The
│ │ -                      angle is calculated using the arc-tan function.
│ │ -    In order to scale the image, it is required to draw a line of length 10 milimeter
│ │ -    somewhere in the image. The line can be drawn in the same fashion as for example
│ │ -    the muscle thickness. Here however, the euclidean distance is used to calculate
│ │ -    the pixel / centimeter ratio.
│ │ -    We also provide the functionality to extent the muscle aponeuroses to more easily
│ │ -    extrapolate fascicles. The lines can be drawn in the same fashion as for example
│ │ -    the muscle thickness.
│ │ +                      Exactly two segments, one along the fascicle
│ │ +                      orientation, the other along the aponeurosis orientation
│ │ +                      must be drawn. The line along the aponeurosis must be
│ │ +                      started where the line along the fascicle ends. If
│ │ +                      multiple angle are drawn, they are averaged. Drawing can
│ │ +                      be started by clickling the left mouse button and keeping
│ │ +                      it pressed until it is not further required to draw the
│ │ +                      line (i.e., the aponeurosis border is reached by the
│ │ +                      fascicle). The angle is calculated using the arc-tan
│ │ +                      function.
│ │ +    In order to scale the frame, it is required to draw a line of length 10
│ │ +    milimeter somewhere in the image. The line can be drawn in the same
│ │ +    fashion as for example the muscle thickness. Here however, the euclidean
│ │ +    distance is used to calculate the pixel / centimeter ratio. This has to
│ │ +    be done for every frame.
│ │ +
│ │ +    We also provide the functionality to extent the muscle aponeuroses to more
│ │ +    easily extrapolate fascicles. The lines can be drawn in the same fashion as
│ │ +    for example the muscle thickness.
│ │  
│ │      Parameters
│ │      ----------
│ │      rootpath : str
│ │          String variable containing the path to the folder where all images
│ │          to be analyzed are saved.
│ │      gui : tk.TK
│ │ @@ -709,15 +725,16 @@
│ │  
│ │      try:
│ │          analysis = ManualAnalysis(list_of_files, rootpath)
│ │          analysis.calculateBatchManual()
│ │  
│ │      except IndexError:
│ │          tk.messagebox.showerror(
│ │ -            "Information", "No image files founds." + "\nEnter correct file type"
│ │ +            "Information", "No image files founds." +
│ │ +            "\nEnter correct file type"
│ │          )
│ │          gui.do_break()
│ │          gui.should_stop = False
│ │          gui.is_running = False
│ │  
│ │      finally:
│ │          # clean up
│ │   --- dl_track_us-0.1.1/DL_Track/gui_helpers/calculate_architecture_video.py
│ ├── +++ dl_track_us-0.1.2/DL_Track_US/gui_helpers/calculate_architecture_video.py
│ │┄ Files 2% similar despite different names
│ │ @@ -23,17 +23,17 @@
│ │  importImageManual
│ │      Function to import an image.
│ │  importFlipFlagsList
│ │      Function to retrieve flip values from a .txt file.
│ │  compileSaveResults
│ │      Function to save the analysis results to a .xlsx file.
│ │  calculateBatch
│ │ -    Function to calculate muscle architecture in longitudinal ultrasonography images
│ │ -    of human lower limb muscles. The values computed are fascicle length (FL),
│ │ -    pennation angle (PA), and muscle thickness (MT).
│ │ +    Function to calculate muscle architecture in longitudinal ultrasonography
│ │ +    images of human lower limb muscles. The values computed are fascicle length
│ │ +    (FL), pennation angle (PA), and muscle thickness (MT).
│ │  calculateBatchManual
│ │      Function used for manual calculation of fascicle length, muscle thickness
│ │      and pennation angles in longitudinal ultrasonography images of human lower
│ │      limb muscles.
│ │  
│ │  Notes
│ │  -----
│ │ @@ -53,25 +53,24 @@
│ │  
│ │  import cv2
│ │  import matplotlib.pyplot as plt
│ │  import numpy as np
│ │  import pandas as pd
│ │  from pandas import ExcelWriter
│ │  
│ │ -from DL_Track.gui_helpers.calibrate_video import calibrateDistanceManually
│ │ -from DL_Track.gui_helpers.do_calculations_video import doCalculationsVideo
│ │ -from DL_Track.gui_helpers.manual_tracing import ManualAnalysis
│ │ +from DL_Track_US.gui_helpers.calibrate_video import calibrateDistanceManually
│ │ +from DL_Track_US.gui_helpers.do_calculations_video import doCalculationsVideo
│ │ +from DL_Track_US.gui_helpers.manual_tracing import ManualAnalysis
│ │  
│ │  plt.style.use("ggplot")
│ │  plt.switch_backend("agg")
│ │  
│ │  
│ │  def importVideo(vpath: str):
│ │ -    """
│ │ -    Function to import a video. Video file types should be common
│ │ +    """Function to import a video. Video file types should be common
│ │      ones like .avi or .mp4.
│ │  
│ │      Parameters
│ │      ----------
│ │      vpath : str
│ │          String variable containing the video. This should be an
│ │          absolute path.
│ │ @@ -105,23 +104,23 @@
│ │      vid_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
│ │      vid_fps = int(cap.get(cv2.CAP_PROP_FPS))
│ │      vid_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
│ │      vid_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
│ │      filename = os.path.splitext(os.path.basename(vpath))[0]
│ │      outpath = str(vpath[0:-4] + "_proc" + ".avi")
│ │      vid_out = cv2.VideoWriter(
│ │ -        outpath, cv2.VideoWriter_fourcc(*"MPEG"), vid_fps, (vid_width, vid_height)
│ │ +        outpath, cv2.VideoWriter_fourcc(*"MPEG"), vid_fps,
│ │ +        (vid_width, vid_height)
│ │      )
│ │  
│ │      return cap, vid_len, filename, vid_out
│ │  
│ │  
│ │  def importVideoManual(vpath: str):
│ │ -    """
│ │ -    Function to import a video. Video file types should be common
│ │ +    """Function to import a video. Video file types should be common
│ │      ones like .avi or .mp4. This function is used for manual
│ │      analysis of videos.
│ │  
│ │      Here, no processed video is saved subsequent to analysis.
│ │  
│ │      Parameters
│ │      ----------
│ │ @@ -164,16 +163,15 @@
│ │      filename: str,
│ │      fasc_l_all: list,
│ │      pennation_all: list,
│ │      x_lows_all: list,
│ │      x_highs_all: list,
│ │      thickness_all: list,
│ │  ):
│ │ -    """
│ │ -    Function to save the analysis results to a .xlsx file.
│ │ +    """Function to save the analysis results to a .xlsx file.
│ │  
│ │      A list of each variable to be saved must be inputted. The inputs are
│ │      inculded in a dataframe and saved to an .xlsx file.
│ │      The .xlsx file is saved to the specified rootpath containing
│ │      each analyzed frame. Estimates or fascicle length, pennation angle,
│ │      muscle thickness and intersections of fascicles with aponeuroses
│ │      are saved.
│ │ @@ -202,37 +200,44 @@
│ │          of a single frame that was analyzed.
│ │      thickness_all : list
│ │          List variable containing all muscle thickness estimates from
│ │          a single frame that was analyzed.
│ │  
│ │      Examples
│ │      --------
│ │ -    >>> exportToExcel(path = "C:/Users/admin/Dokuments/videos", filename="video1.avi",
│ │ -                             fasc_l_all=[7.8,, 6.4, 9.1], pennation_all=[20, 21.1, 24],
│ │ -                             x_lows_all=[749, 51, 39], x_highs_all=[54, 739, 811],
│ │ +    >>> exportToExcel(path = "C:/Users/admin/Dokuments/videos",
│ │ +                             filename="video1.avi",
│ │ +                             fasc_l_all=[7.8,, 6.4, 9.1],
│ │ +                             pennation_all=[20, 21.1, 24],
│ │ +                             x_lows_all=[749, 51, 39],
│ │ +                             x_highs_all=[54, 739, 811],
│ │                               thickness_all=[1.85])
│ │      """
│ │      # Create empty arrays
│ │ -    fl = np.zeros([len(fasc_l_all), len(max(fasc_l_all, key=lambda x: len(x)))])
│ │ -    pe = np.zeros([len(pennation_all), len(max(pennation_all, key=lambda x: len(x)))])
│ │ -    xl = np.zeros([len(x_lows_all), len(max(x_lows_all, key=lambda x: len(x)))])
│ │ -    xh = np.zeros([len(x_highs_all), len(max(x_highs_all, key=lambda x: len(x)))])
│ │ +    fl = np.zeros([len(fasc_l_all),
│ │ +                   len(max(fasc_l_all, key=lambda x: len(x)))])
│ │ +    pe = np.zeros([len(pennation_all),
│ │ +                   len(max(pennation_all, key=lambda x: len(x)))])
│ │ +    xl = np.zeros([len(x_lows_all),
│ │ +                   len(max(x_lows_all, key=lambda x: len(x)))])
│ │ +    xh = np.zeros([len(x_highs_all),
│ │ +                   len(max(x_highs_all, key=lambda x: len(x)))])
│ │  
│ │      # Add respective values to the respecive array
│ │      for i, j in enumerate(fasc_l_all):
│ │ -        fl[i][0 : len(j)] = j  # fascicle length
│ │ +        fl[i][0: len(j)] = j  # fascicle length
│ │      fl[fl == 0] = np.nan
│ │      for i, j in enumerate(pennation_all):
│ │ -        pe[i][0 : len(j)] = j  # pennation angle
│ │ +        pe[i][0: len(j)] = j  # pennation angle
│ │      pe[pe == 0] = np.nan
│ │      for i, j in enumerate(x_lows_all):
│ │ -        xl[i][0 : len(j)] = j  # lower intersection
│ │ +        xl[i][0: len(j)] = j  # lower intersection
│ │      xl[xl == 0] = np.nan
│ │      for i, j in enumerate(x_highs_all):
│ │ -        xh[i][0 : len(j)] = j  # upper intersection
│ │ +        xh[i][0: len(j)] = j  # upper intersection
│ │      xh[xh == 0] = np.nan
│ │  
│ │      # Create dataframes with values
│ │      df1 = pd.DataFrame(data=fl)
│ │      df2 = pd.DataFrame(data=pe)
│ │      df3 = pd.DataFrame(data=xl)
│ │      df4 = pd.DataFrame(data=xh)
│ │ @@ -265,127 +270,138 @@
│ │      fasc_threshold: float,
│ │      fasc_cont_thresh: int,
│ │      min_width: int,
│ │      min_pennation: int,
│ │      max_pennation: int,
│ │      gui,
│ │  ):
│ │ -    """
│ │ -    Function to calculate muscle architecture in longitudinal ultrasonography videos
│ │ -    of human lower limb muscles. The values computed are fascicle length (FL),
│ │ -    pennation angle (PA), and muscle thickness (MT).
│ │ -
│ │ -    The scope of this function is limited. videos of the vastus lateralis, tibialis anterior
│ │ -    soleus and gastrocnemius  muscles can be analyzed. This is due to the limited amount of
│ │ -    training data for our convolutional neural networks. This functions makes extensive use
│ │ +    """Function to calculate muscle architecture in longitudinal
│ │ +    ultrasonography videos of human lower limb muscles. The values
│ │ +    computed are fascicle length (FL), pennation angle (PA),
│ │ +    and muscle thickness (MT).
│ │ +
│ │ +    The scope of this function is limited. videos of the vastus lateralis,
│ │ +    tibialis anterior soleus and gastrocnemius  muscles can be analyzed.
│ │ +    This is due to the limited amount of training data for our convolutional
│ │ +    neural networks. This functions makes extensive use
│ │      of several other functions and was designed to be executed from a GUI.
│ │  
│ │      Parameters
│ │      ----------
│ │      rootpath : str
│ │          String variable containing the path to the folder where all videos
│ │          to be analyzed are saved.
│ │      apo_modelpath : str
│ │ -        String variable containing the absolute path to the aponeurosis neural network.
│ │ +        String variable containing the absolute path to the aponeurosis
│ │ +        neural network.
│ │      fasc_modelpath : str
│ │ -        String variable containing the absolute path to the fascicle neural network.
│ │ +        String variable containing the absolute path to the fascicle
│ │ +        neural network.
│ │      flip : str
│ │ -        String variable determining wheter all frames of a video are flipped vetically.
│ │ -        Flipping is necessary as the models were trained in images of with specific
│ │ -        fascicle orientation.
│ │ +        String variable determining wheter all frames of a video are
│ │ +        flipped vetically.
│ │ +        Flipping is necessary as the models were trained in images of
│ │ +        with specific fascicle orientation.
│ │      filetype : str
│ │          String variable containg the respective type of the videos.
│ │          This is needed to select only the relevant video files
│ │          in the root directory.
│ │      scaling : str
│ │          String variable determining the image scaling method.
│ │          There are three types of scaling available:
│ │ -        - scaling = "manual" (user must scale the video manually. This only needs to be
│ │ -                    done in the first frame.)
│ │ +        - scaling = "manual" (user must scale the video manually.
│ │ +          This only needs to be done in the first frame.)
│ │            detecting scaling bars on the right side of the image.)
│ │          - scaling = "No scaling" (video frames are not scaled.)
│ │          Scaling is necessary to compute measurements in centimeter,
│ │          if "no scaling" is chosen, the results are in pixel units.
│ │      spacing : int
│ │ -        Integer variable containing the distance (in milimeter) between two scaling bars in the image.
│ │ +        Integer variable containing the distance (in milimeter) between
│ │ +        two scaling bars in the image.
│ │          This is needed to compute the pixel/cm ratio and therefore report
│ │          the results in centimeter rather than pixel units.
│ │      step : int
│ │          Integer variable containing the step for the range of video frames.
│ │          If step != 1, frames are skipped according to the size of step.
│ │          This might decrease processing time but also accuracy.
│ │      apo_threshold : float
│ │ -        Float variable containing the threshold applied to predicted aponeurosis
│ │ -        pixels by our neural networks. By varying this threshold, different
│ │ -        structures will be classified as aponeurosis as the threshold for classifying
│ │ -        a pixel as aponeurosis is changed. Must be non-zero and
│ │ -        non-negative.
│ │ +        Float variable containing the threshold applied to predicted
│ │ +        aponeurosis pixels by our neural networks. By varying this
│ │ +        threshold, different structures will be classified as aponeurosis
│ │ +        as the threshold for classifying a pixel as aponeurosis is changed.
│ │ +        Must be non-zero and non-negative.
│ │      fasc_threshold : float
│ │          Float variable containing the threshold applied to predicted fascicle
│ │          pixels by our neural networks. By varying this threshold, different
│ │ -        structures will be classified as fascicle as the threshold for classifying
│ │ -        a pixel as fascicle is changed. Must be non-zero and non-negative.
│ │ +        structures will be classified as fascicle as the threshold for
│ │ +        classifying a pixel as fascicle is changed. Must be non-zero and
│ │ +        non-negative.
│ │      fasc_cont_threshold : float
│ │          Float variable containing the threshold applied to predicted fascicle
│ │          segments by our neural networks. By varying this threshold, different
│ │ -        structures will be classified as fascicle. By increasing, longer fascicle segments
│ │ -        will be considered, by lowering shorter segments. Must be non-zero and
│ │ -        non-negative.
│ │ +        structures will be classified as fascicle. By increasing, longer
│ │ +        fascicle segments will be considered, by lowering shorter segments.
│ │ +        Must be non-zero and non-negative.
│ │      min_width : int
│ │ -        Integer variable containing the minimal distance between aponeuroses to be
│ │ -        detected. The aponeuroses must be at least this distance apart to be
│ │ -        detected. The distance is specified in pixels. Must be non-zero and non-negative.
│ │ +        Integer variable containing the minimal distance between aponeuroses
│ │ +        to be detected. The aponeuroses must be at least this distance apart
│ │ +        to be detected. The distance is specified in pixels. Must be non-zero
│ │ +        and non-negative.
│ │      min_pennation : int
│ │ -        Integer variable containing the mininmal (physiological) acceptable pennation
│ │ -        angle occuring in the analyzed image/muscle. Fascicles with lower pennation
│ │ -        angles will be excluded. The pennation angle is calculated as the angle
│ │ -        of insertion between extrapolated fascicle and detected aponeurosis. Must
│ │ -        be non-negative.
│ │ +        Integer variable containing the mininmal (physiological) acceptable
│ │ +        pennation angle occuring in the analyzed image/muscle. Fascicles with
│ │ +        lower pennation angles will be excluded. The pennation angle is
│ │ +        calculated as the angle of insertion between extrapolated fascicle
│ │ +        and detected aponeurosis. Must be non-negative.
│ │      max_pennation : int
│ │ -        Integer variable containing the maximal (physiological) acceptable pennation
│ │ -        angle occuring in the analyzed image/muscle. Fascicles with higher pennation
│ │ -        angles will be excluded. The pennation angle is calculated as the angle
│ │ -        of insertion between extrapolated fascicle and detected aponeurosis. Must
│ │ -        be non-negative and larger than min_pennation.
│ │ +        Integer variable containing the maximal (physiological) acceptable
│ │ +        pennation angle occuring in the analyzed image/muscle. Fascicles with
│ │ +        higher pennation angles will be excluded. The pennation angle is
│ │ +        calculated as the angle of insertion between extrapolated fascicle and
│ │ +        detected aponeurosis. Must be non-negative and larger than
│ │ +        min_pennation.
│ │      gui : tk.TK
│ │          A tkinter.TK class instance that represents a GUI. By passing this
│ │          argument, interaction with the GUI is possible i.e., stopping
│ │          the calculation process after each image.
│ │  
│ │      See Also
│ │      --------
│ │ -    do_calculations_video.py for exact description of muscle architecture parameter
│ │ -    calculation.
│ │ +    do_calculations_video.py for exact description of muscle architecture
│ │ +    parameter calculation.
│ │  
│ │      Notes
│ │      -----
│ │      For specific explanations of the included functions see the respective
│ │      function docstrings in this module. To see an examplenary video output
│ │      and .xlsx file take at look at the examples provided in the "examples"
│ │      directory.
│ │ -    This function is called by the GUI. Note that the functioned was specifically
│ │ -    designed to be called from the GUI. Thus, tk.messagebox will pop up when errors are
│ │ -    raised even if the GUI is not started.
│ │ +    This function is called by the GUI. Note that the functioned was
│ │ +    specifically designed to be called from the GUI. Thus, tk.messagebox
│ │ +    will pop up when errors are raised even if the GUI is not started.
│ │  
│ │      Examples
│ │      --------
│ │      >>> calculateBatch(rootpath="C:/Users/admin/Dokuments/images",
│ │                         apo_modelpath="C:/Users/admin/Dokuments/models/apo_model.h5",
│ │                         fasc_modelpath="C:/Users/admin/Dokuments/models/apo_model.h5",
│ │ -                       flip="Flip", filetype="/**/*.avi, scaline="manual", spacing=10,
│ │ -                       apo_threshold=0.1, fasc_threshold=0.05, fasc_cont_thres=40,
│ │ +                       flip="Flip", filetype="/**/*.avi, scaline="manual",
│ │ +                       spacing=10,
│ │ +                       apo_threshold=0.1, fasc_threshold=0.05,
│ │ +                       fasc_cont_thres=40,
│ │                         curvature=3, min_pennation=10, max_pennation=35,
│ │                         gui=<__main__.DLTrack object at 0x000002BFA7528190>)
│ │      """
│ │      list_of_files = glob.glob(rootpath + filetype, recursive=True)
│ │  
│ │      if len(list_of_files) == 0:
│ │          tk.messagebox.showerror(
│ │              "Information",
│ │ -            "No video files found." + "\nCheck specified video type or input directory",
│ │ +            "No video files found." +
│ │ +            "\nCheck specified video type or input directory",
│ │          )
│ │          gui.do_break()
│ │          gui.should_stop = False
│ │          gui.is_running = False
│ │  
│ │      dic = {
│ │          "apo_treshold": apo_treshold,
│ │ @@ -453,15 +469,16 @@
│ │              )
│ │  
│ │      except ValueError:
│ │          pass
│ │  
│ │      except IndexError:
│ │          tk.messagebox.showerror(
│ │ -            "Information", "No Aponeurosis detected. Change aponeurosis threshold."
│ │ +            "Information",
│ │ +            "No Aponeurosis detected. Change aponeurosis threshold."
│ │          )
│ │          gui.should_stop = False
│ │          gui.is_running = False
│ │          gui.do_break()
│ │  
│ │      # except:
│ │      #     tk.messagebox.showerror("Information", "Enter correct video type.")
│ │ @@ -472,57 +489,63 @@
│ │      finally:
│ │          # clean up
│ │          gui.should_stop = False
│ │          gui.is_running = False
│ │  
│ │  
│ │  def calculateArchitectureVideoManual(videopath: str, gui):
│ │ -    """
│ │ -    Function used for manual calculation of fascicle length, muscle thickness
│ │ -    and pennation angles in longitudinal ultrasonography videos of human lower
│ │ -    limb muscles.
│ │ -
│ │ -    This function is not restricted to any specific muscles. However, its use is
│ │ -    restricted to a specific method for assessing muscle thickness fascicle
│ │ -    length and pennation angles. Moreover, each video frame is analyzed seperately.
│ │ +    """Function used for manual calculation of fascicle length, muscle
│ │ +    thickness and pennation angles in longitudinal ultrasonography videos
│ │ +    of human lower limb muscles.
│ │ +
│ │ +    This function is not restricted to any specific muscles. However, its use
│ │ +    is restricted to a specific method for assessing muscle thickness fascicle
│ │ +    length and pennation angles. Moreover, each video frame is analyzed
│ │ +    seperately.
│ │  
│ │      - Muscle thickness:
│ │                         Exactly one segment reaching from the superficial to the
│ │ -                       deep aponeuroses of the muscle must be drawn. If multiple
│ │ -                       measurement are drawn, these are averaged. Drawing can
│ │ -                       be started by clickling the left mouse button and keeping
│ │ -                       it pressed until it is not further required to draw the line
│ │ -                       (i.e., the other aponeurosis border is reached). Only the
│ │ -                       respective y-coordinates of the points where the cursor
│ │ -                       was clicked and released are considered for calculation of
│ │ -                       muscle thickness.
│ │ +                       deep aponeuroses of the muscle must be drawn.
│ │ +                       If multiple measurement are drawn, these are averaged.
│ │ +                       Drawing can be started by clickling the left mouse
│ │ +                       button and keeping it pressed until it is not further
│ │ +                       required to draw the line (i.e., the other aponeurosis
│ │ +                       border is reached). Only the respective y-coordinates
│ │ +                       of the points where the cursor was clicked and released
│ │ +                       are considered for calculation of muscle thickness.
│ │      - Fascicle length:
│ │ -                      Exactly three segments along the fascicleof the muscle must
│ │ -                      be drawn. If multiple fascicle are drawn, their lengths are
│ │ -                      averaged. Drawing can be started by clickling the left mouse
│ │ -                      button and keeping it pressed until one segment is finished
│ │ -                      (mostly where fascicle curvature occurs the other aponeurosis
│ │ -                      border is reached). Using the euclidean distance, the total
│ │ -                      fascicle length is computed as a sum of the segments.
│ │ +                      Exactly three segments along the fascicleof the muscle
│ │ +                      must be drawn. If multiple fascicle are drawn, their
│ │ +                      lengths are averaged. Drawing can be started by clickling
│ │ +                      the left mouse button and keeping it pressed until one
│ │ +                      segment is finished (mostly where fascicle curvature
│ │ +                      occurs the other aponeurosis border is reached). Using
│ │ +                      the euclidean distance, the total fascicle length is
│ │ +                      computed as a sum of the segments.
│ │      - Pennation angle:
│ │ -                      Exactly two segments, one along the fascicle orientation, the
│ │ -                      other along the aponeurosis orientation must be drawn. The line
│ │ -                      along the aponeurosis must be started where the line along the
│ │ -                      fascicle ends. If multiple angle are drawn, they are averaged.
│ │ -                      Drawing can be started by clickling the left mouse button and keeping
│ │ -                      it pressed until it is not further required to draw the line
│ │ -                      (i.e., the aponeurosis border is reached by the fascicle). The
│ │ -                      angle is calculated using the arc-tan function.
│ │ -    In order to scale the frame, it is required to draw a line of length 10 milimeter
│ │ -    somewhere in the image. The line can be drawn in the same fashion as for example
│ │ -    the muscle thickness. Here however, the euclidean distance is used to calculate
│ │ -    the pixel / centimeter ratio. This has to be done for every frame.
│ │ -    We also provide the functionality to extent the muscle aponeuroses to more easily
│ │ -    extrapolate fascicles. The lines can be drawn in the same fashion as for example
│ │ -    the muscle thickness.
│ │ +                      Exactly two segments, one along the fascicle
│ │ +                      orientation, the other along the aponeurosis orientation
│ │ +                      must be drawn. The line along the aponeurosis must be
│ │ +                      started where the line along the fascicle ends. If
│ │ +                      multiple angle are drawn, they are averaged. Drawing can
│ │ +                      be started by clickling the left mouse button and keeping
│ │ +                      it pressed until it is not further required to draw the
│ │ +                      line (i.e., the aponeurosis border is reached by the
│ │ +                      fascicle). The angle is calculated using the arc-tan
│ │ +                      function.
│ │ +    In order to scale the frame, it is required to draw a line of length 10
│ │ +    milimeter somewhere in the image. The line can be drawn in the same
│ │ +    fashion as for example the muscle thickness. Here however, the euclidean
│ │ +    distance is used to calculate the pixel / centimeter ratio. This has to
│ │ +    be done for every frame.
│ │ +
│ │ +    We also provide the functionality to extent the muscle aponeuroses to more
│ │ +    easily extrapolate fascicles. The lines can be drawn in the same fashion as
│ │ +    for example the muscle thickness.
│ │ +
│ │  
│ │      Parameters
│ │      ----------
│ │      videopath : str
│ │          String variable containing the absolute path to the video
│ │          to be analyzed.
│ │      gui : tk.TK
│ │ @@ -572,15 +595,16 @@
│ │          list_of_frames = glob.glob(frame_cap + "/**/*.tif", recursive=True)
│ │  
│ │          # annotate thickness, fasicles and angles
│ │          man_analysis = ManualAnalysis(list_of_frames, file_path)
│ │          man_analysis.calculateBatchManual()
│ │  
│ │      except IndexError:
│ │ -        tk.messagebox.showerror("Information", "Make sure to select a video file.")
│ │ +        tk.messagebox.showerror("Information",
│ │ +                                "Make sure to select a video file.")
│ │          gui.should_stop = False
│ │          gui.is_running = False
│ │          gui.do_break()
│ │  
│ │      except FileNotFoundError:
│ │          tk.messagebox.showerror(
│ │              "Information", "Make sure the video file path is correct."
│ │   --- dl_track_us-0.1.1/DL_Track/gui_helpers/calibrate.py
│ ├── +++ dl_track_us-0.1.2/DL_Track_US/gui_helpers/calibrate.py
│ │┄ Files 2% similar despite different names
│ │ @@ -2,15 +2,16 @@
│ │  Description
│ │  -----------
│ │  This module contains functions to automatically or manually
│ │  scale images.
│ │  The scope of the automatic method is limited to scaling bars being
│ │  present in the right side of the image. The scope of the manual method
│ │  is not limited to specific scaling types in images. However, the distance
│ │ -between two selected points in the image required for the scaling must be known.
│ │ +between two selected points in the image required for the scaling must be
│ │ +known.
│ │  
│ │  Functions scope
│ │  ---------------
│ │  mclick
│ │      Instance method to detect mouse click coordinates in image.
│ │  calibrateDistanceManually
│ │      Function to manually calibrate an image to convert measurements
│ │ @@ -27,16 +28,15 @@
│ │  import numpy as np
│ │  
│ │  # global variable to store mouse clicks
│ │  mlocs = []
│ │  
│ │  
│ │  def mclick(event, x_val, y_val, flags, param):
│ │ -    """
│ │ -    Instance method to detect mouse click coordinates in image.
│ │ +    """Instance method to detect mouse click coordinates in image.
│ │  
│ │      This instance is used when the image to be analyzed should be
│ │      cropped. Upon clicking the mouse button, the coordinates
│ │      of the cursor position are stored in the instance attribute
│ │      self.mlocs.
│ │  
│ │      Parameters
│ │ @@ -61,16 +61,15 @@
│ │      # if the left mouse button was clicked, record the (x, y) coordinates
│ │      if event == cv2.EVENT_LBUTTONDOWN:
│ │          mlocs.append(y_val)
│ │          mlocs.append(x_val)
│ │  
│ │  
│ │  def calibrateDistanceManually(img: np.ndarray, spacing: int):
│ │ -    """
│ │ -    Function to manually calibrate an image to convert measurements
│ │ +    """Function to manually calibrate an image to convert measurements
│ │      in pixel units to centimeters.
│ │  
│ │      The function calculates the distance in pixel units between two
│ │      points on the input image. The points are determined by clicks of
│ │      the user. The distance (in milimeters) is determined by the value
│ │      contained in the spacing variable. Then the ratio of pixel / centimeter
│ │      is calculated. To get the distance, the euclidean distance between the
│ │ @@ -78,15 +77,15 @@
│ │  
│ │      Parameters
│ │      ----------
│ │      img : np.ndarray
│ │          Input image to be analysed as a numpy array. The image must
│ │          be loaded prior to calibration, specifying a path
│ │          is not valid.
│ │ -    spacing : int
│ │ +    spacing : {10, 5, 15, 20}
│ │          Integer variable containing the known distance in milimeter
│ │          between the two placed points by the user. This can be 5, 10,
│ │          15 or 20 milimeter.
│ │  
│ │      Returns
│ │      -------
│ │          calib_dist : int
│ │ @@ -138,16 +137,15 @@
│ │      # print(str(spacing) + ' mm corresponds to ' + str(calib_dist) + ' pixels')
│ │      scale_statement = "10 mm corresponds to " + str(calib_dist) + " pixels"
│ │  
│ │      return calib_dist, scale_statement
│ │  
│ │  
│ │  def calibrateDistanceStatic(img: np.ndarray, spacing: str):
│ │ -    """
│ │ -    Function to calibrate an image to convert measurements
│ │ +    """Function to calibrate an image to convert measurements
│ │      in pixel units to centimeter.
│ │  
│ │      The function calculates the distance in pixel units between two
│ │      scaling bars on the input image. The bars should be positioned on the
│ │      right side of image. The distance (in milimeter) between two bars must
│ │      be specified by the spacing variable. It is the known distance between two
│ │      bars in milimeter. Then the ratio of pixel / centimeter is calculated.
│ │ @@ -156,15 +154,15 @@
│ │  
│ │      Parameters
│ │      ----------
│ │      img : np.ndarray
│ │          Input image to be analysed as a numpy array. The image must
│ │          be loaded prior to calibration, specifying a path
│ │          is not valid.
│ │ -    spacing : int
│ │ +    spacing : {10, 5, 15, 20}
│ │          Integer variable containing the known distance in milimeter
│ │          between the two scaling bars. This can be 5, 10,
│ │          15 or 20 milimeter.
│ │  
│ │      Returns
│ │      -------
│ │      calib_dist : int
│ │ @@ -180,26 +178,28 @@
│ │      99, 5 mm corresponds to 99 pixels
│ │      """
│ │      try:
│ │          # crop right border of image
│ │          height = img.shape[0]
│ │          width = img.shape[1]
│ │          imgscale = img[
│ │ -            int(height * 0.4) : (height), (width - int(width * 0.15)) : width
│ │ +            int(height * 0.4): (height), (width - int(width * 0.15)): width
│ │          ]
│ │  
│ │          # search for rows with white pixels, calculate median of distance
│ │ -        calib_dist = np.max(np.diff(np.argwhere(imgscale.max(axis=1) > 150), axis=0))
│ │ +        calib_dist = np.max(np.diff(np.argwhere(imgscale.max(axis=1) > 150),
│ │ +                                    axis=0))
│ │  
│ │          # return none if distance too small
│ │          if int(calib_dist) < 1:
│ │              return None, None
│ │  
│ │          # create scale statement
│ │          scale_statement = f"{spacing} mm corresponds to {calib_dist} pixels"
│ │  
│ │          return calib_dist, scale_statement
│ │  
│ │ -    # Handle error occuring when no bright pixels detected on right side of image
│ │ +    # Handle error occuring when no bright pixels detected on right side
│ │ +    # of image
│ │      except ValueError:
│ │  
│ │          return None, None
│ │   --- dl_track_us-0.1.1/DL_Track/gui_helpers/calibrate_video.py
│ ├── +++ dl_track_us-0.1.2/DL_Track_US/gui_helpers/calibrate_video.py
│ │┄ Files 1% similar despite different names
│ │ @@ -27,27 +27,26 @@
│ │  import numpy as np
│ │  
│ │  # global variable to store mouse clicks
│ │  mlocs = []
│ │  
│ │  
│ │  def mclick(event, x_val, y_val, flags, param):
│ │ -    """
│ │ -    Instance method to detect mouse click coordinates in image.
│ │ +    """Instance method to detect mouse click coordinates in image.
│ │  
│ │      This instance is used when the image to be analyzed should be
│ │      cropped. Upon clicking the mouse button, the coordinates
│ │      of the cursor position are stored in the instance attribute
│ │      self.mlocs.
│ │  
│ │      Parameters
│ │      ----------
│ │      event
│ │          Event flag specified as Cv2 mouse event left mouse button down.
│ │ -    x_val
│ │ +    x_val 
│ │          Value of x-coordinate of mouse event to be recorded.
│ │      y_val
│ │          Value of y-coordinate of mouse event to be recorded.
│ │      flags
│ │          Specific condition whenever a mouse event occurs. This
│ │          is not used here but needs to be specified as input
│ │          parameter.
│ │ @@ -59,31 +58,30 @@
│ │      global mlocs
│ │      # if the left mouse button was clicked, record the (x, y) coordinates
│ │      if event == cv2.EVENT_LBUTTONDOWN:
│ │          mlocs.append(y_val)
│ │  
│ │  
│ │  def calibrateDistanceManually(cap, spacing: int):
│ │ -    """
│ │ -    Function to manually calibrate an image to convert measurements
│ │ +    """Function to manually calibrate an image to convert measurements
│ │      in pixel units to centimeters.
│ │  
│ │      The function calculates the distance in pixel units between two
│ │      points on the input image. The points are determined by clicks of
│ │      the user. The distance (in milimeter) is determined by the value
│ │      contained in the spacing variable. Then the ratio of pixel / centimeter
│ │      is calculated. To get the distance, the euclidean distance between the
│ │      two points is calculated.
│ │  
│ │      Parameters
│ │      ----------
│ │      cap : cv2.VideoCapture
│ │          Object that contains the video in a np.ndarrray format.
│ │          In this way, seperate frames can be accessed.
│ │ -    spacing : int
│ │ +    spacing : {10, 5, 15, 20}
│ │          Integer variable containing the known distance in milimeter
│ │          between the two placed points by the user. This can be 5, 10,
│ │          15 or 20 milimeter.
│ │  
│ │      Returns
│ │      -------
│ │      calib_dist : int
│ │   --- dl_track_us-0.1.1/DL_Track/gui_helpers/do_calculations.py
│ ├── +++ dl_track_us-0.1.2/DL_Track_US/gui_helpers/do_calculations.py
│ │┄ Files 1% similar despite different names
│ │ @@ -1,12 +1,12 @@
│ │  """
│ │  Description
│ │  -----------
│ │  This module contains functions to calculate muscle architectural
│ │ -parameters based on binary segmentations by convolutional neural networks (CNNs).
│ │ +parameters based on binary segmentations by convolutional neural networks.
│ │  The parameters include muscle thickness, pennation angle and fascicle length.
│ │  First, input images are segmented by the CNNs. Then the predicted aponeuroses
│ │  and fascicle fragments are thresholded and filtered. Fascicle fragments
│ │  and aponeuroses are extrapolated and the intersections determined.
│ │  This module is specifically designed for single image analysis.
│ │  The architectural parameters are calculated and the results are plotted.
│ │  
│ │ @@ -28,25 +28,23 @@
│ │  """
│ │  import math
│ │  
│ │  import cv2
│ │  import matplotlib.pyplot as plt
│ │  import numpy as np
│ │  import tensorflow as tf
│ │ -from keras import backend as K
│ │  from scipy.signal import savgol_filter
│ │  from skimage.morphology import skeletonize
│ │  from skimage.transform import resize
│ │  
│ │  plt.style.use("ggplot")
│ │  
│ │  
│ │  def sortContours(cnts: list):
│ │ -    """
│ │ -    Function to sort detected contours from proximal to distal.
│ │ +    """Function to sort detected contours from proximal to distal.
│ │  
│ │      The input contours belond to the aponeuroses and are sorted
│ │      based on their coordinates, from smallest to largest.
│ │      Moreover, for each detected contour a bounding box is built.
│ │      The bounding boxes are sorted as well. They are however not
│ │      needed for further analyses.
│ │  
│ │ @@ -61,44 +59,45 @@
│ │      cnts : tuple
│ │          Tuple containing arrays of sorted contours.
│ │      bounding_boxes : tuple
│ │          Tuple containing tuples with sorted bounding boxes.
│ │  
│ │      Examples
│ │      --------
│ │ -    >>> sortContours(cnts=[array([[[928, 247]], ... [[929, 247]]], dtype=int32),
│ │ +    >>> sortContours(cnts=[array([[[928, 247]], ... [[929, 247]]],
│ │ +    dtype=int32),
│ │      ((array([[[228,  97]], ... [[229,  97]]], dtype=int32),
│ │      (array([[[228,  97]], ... [[229,  97]]], dtype=int32),
│ │      (array([[[928, 247]], ... [[929, 247]]], dtype=int32)),
│ │      ((201, 97, 747, 29), (201, 247, 750, 96))
│ │      """
│ │      # initialize the reverse flag and sort index
│ │      i = 1
│ │      # construct the list of bounding boxes and sort them from top to bottom
│ │      bounding_boxes = [cv2.boundingRect(c) for c in cnts]
│ │      (cnts, bounding_boxes) = zip(
│ │ -        *sorted(zip(cnts, bounding_boxes), key=lambda b: b[1][i], reverse=False)
│ │ +        *sorted(zip(cnts, bounding_boxes), key=lambda b: b[1][i],
│ │ +                reverse=False)
│ │      )
│ │  
│ │      return (cnts, bounding_boxes)
│ │  
│ │  
│ │  def contourEdge(edge: str, contour: list) -> np.ndarray:
│ │ -    """
│ │ -    Function to find only the coordinates representing one edge
│ │ +    """Function to find only the coordinates representing one edge
│ │      of a contour.
│ │  
│ │      Either the upper or lower edge of the detected contours is
│ │      calculated. From the contour detected lower in the image,
│ │      the upper edge is searched. From the contour detected
│ │      higher in the image, the lower edge is searched.
│ │  
│ │      Parameters
│ │      ----------
│ │ -    edge : str
│ │ +    edge : {"T", "B"}
│ │          String variable defining the type of edge that is
│ │          searched. The variable can be either "T" (top) or
│ │          "B" (bottom).
│ │      contour : list
│ │          List variable containing sorted contours.
│ │  
│ │      Returns
│ │ @@ -109,15 +108,16 @@
│ │      y : np.ndarray
│ │          Array variable containing all y-coordinated from the
│ │          detected contour.
│ │  
│ │      Examples
│ │      --------
│ │      >>> contourEdge(edge="T", contour=[[[195 104]] ... [[196 104]]])
│ │ -    [196 197 198 199 200 ... 952 953 954 955 956 957], [120 120 120 120 120 ... 125 125 125 125 125 125]
│ │ +    [196 197 198 199 200 ... 952 953 954 955 956 957],
│ │ +    [120 120 120 120 120 ... 125 125 125 125 125 125]
│ │      """
│ │      # Turn tuple into list
│ │      pts = list(contour)
│ │      # sort conntours
│ │      ptsT = sorted(pts, key=lambda k: [k[0][0], k[0][1]])
│ │  
│ │      # Get x and y coordinates from contour
│ │ @@ -129,15 +129,15 @@
│ │      # Get rid of doubles
│ │      un = np.unique(allx)
│ │  
│ │      # Filter x and y coordinates from cont according to selected edge
│ │      leng = len(un) - 1
│ │      x = []
│ │      y = []
│ │ -    for each in range(5, leng - 5):  # Ignore 1st and last 5 points to avoid any curves
│ │ +    for each in range(5, leng - 5):  # Ignore 1st and last 5 points
│ │          indices = [i for i, x in enumerate(allx) if x == un[each]]
│ │          if edge == "T":
│ │              loc = indices[0]
│ │          else:
│ │              loc = indices[-1]
│ │          x.append(ptsT[loc][0, 0])
│ │          y.append(ptsT[loc][0, 1])
│ │ @@ -154,16 +154,15 @@
│ │      spacing: int,
│ │      filename: str,
│ │      model_apo,
│ │      model_fasc,
│ │      scale_statement: str,
│ │      dictionary: dict,
│ │  ):
│ │ -    """
│ │ -    Function to compute muscle architectural parameters based on
│ │ +    """Function to compute muscle architectural parameters based on
│ │      convolutional neural network segmentation in images.
│ │  
│ │      Firstly, images are segmented by the network. Then, predictions
│ │      are thresholded and filtered. The aponeuroses edges are computed and
│ │      the fascicle length and pennation angle calculated. This is done
│ │      by extrapolating fascicle segments above a threshold length. Then
│ │      the intersection between aponeurosis edge and fascicle structures are
│ │ @@ -185,26 +184,28 @@
│ │      w : int
│ │          Integer variable containing the width of the input image (img).
│ │      calib_dist : int
│ │          Integer variable containing the distance between the two
│ │          specified point in pixel units. This value was either computed
│ │          automatically or manually. Must be non-negative. If "None", the
│ │          values are outputted in pixel units.
│ │ -    spacing : int
│ │ +    spacing : {10, 5, 15, 20}
│ │          Integer variable containing the known distance in milimeter
│ │          between the two placed points by the user or the scaling bars
│ │          present in the image. This can be 5, 10, 15 or 20 milimeter.
│ │          Must be non-negative and non-zero.
│ │      filename : str
│ │          String value containing the name of the input image, not the
│ │          entire path.
│ │      apo_modelpath : str
│ │ -        String variable containing the absolute path to the aponeurosis neural network.
│ │ +        String variable containing the absolute path to the aponeurosis
│ │ +        neural network.
│ │      fasc_modelpath : str
│ │ -        String variable containing the absolute path to the fascicle neural network.
│ │ +        String variable containing the absolute path to the fascicle
│ │ +        neural network.
│ │      scale_statement : str
│ │          String variable containing a statement how many milimeter
│ │          correspond to how many pixels. If calib_dist is "None", scale statement
│ │          will also be "None"
│ │      dictionary : dict
│ │          Dictionary variable containing analysis parameters.
│ │          These include must include apo_threshold, fasc_threshold,
│ │ @@ -279,24 +280,24 @@
│ │      max_pennation = int(dic["max_pennation"])
│ │      min_pennation = int(dic["min_pennation"])
│ │      apo_threshold = float(dic["apo_treshold"])
│ │      fasc_threshold = float(dic["fasc_threshold"])
│ │  
│ │      # load the aponeurosis model
│ │      pred_apo = model_apo.predict(img)
│ │ -    pred_apo_t = (pred_apo > apo_threshold).astype(np.uint8)  # SET APO THRESHOLD
│ │ +    pred_apo_t = (pred_apo > apo_threshold).astype(np.uint8)  # SET APO THS
│ │      pred_apo = resize(pred_apo, (1, h, w, 1))
│ │      pred_apo = np.reshape(pred_apo, (h, w))
│ │      pred_apo_t = resize(pred_apo_t, (1, h, w, 1))
│ │      pred_apo_t = np.reshape(pred_apo_t, (h, w))
│ │      tf.keras.backend.clear_session()
│ │  
│ │      # load the fascicle model
│ │      pred_fasc = model_fasc.predict(img)
│ │ -    pred_fasc_t = (pred_fasc > fasc_threshold).astype(np.uint8)  # SET FASC THRESHOLD
│ │ +    pred_fasc_t = (pred_fasc > fasc_threshold).astype(np.uint8)  # SET FASC THS
│ │      pred_fasc = resize(pred_fasc, (1, h, w, 1))
│ │      pred_fasc = np.reshape(pred_fasc, (h, w))
│ │      pred_fasc_t = resize(pred_fasc_t, (1, h, w, 1))
│ │      pred_fasc_t = np.reshape(pred_fasc_t, (h, w))
│ │      tf.keras.backend.clear_session()
│ │  
│ │      xs = []
│ │ @@ -393,31 +394,33 @@
│ │          upp_x, upp_y = contourEdge("B", contoursE[0])
│ │  
│ │          if contoursE[1][0, 0, 1] > contoursE[0][0, 0, 1] + min_width:
│ │              low_x, low_y = contourEdge("T", contoursE[1])
│ │          else:
│ │              low_x, low_y = contourEdge("T", contoursE[2])
│ │  
│ │ -        upp_y_new = savgol_filter(upp_y, 81, 2)  # window size 51, polynomial order 3
│ │ +        upp_y_new = savgol_filter(upp_y, 81, 2)  # window size 51, polynomial 3
│ │          low_y_new = savgol_filter(low_y, 81, 2)
│ │  
│ │ -        # Make a binary mask to only include fascicles within the region between the 2 aponeuroses
│ │ +        # Make a binary mask to only include fascicles within the region
│ │ +        # between the 2 aponeuroses
│ │          ex_mask = np.zeros(thresh.shape, np.uint8)
│ │          ex_1 = 0
│ │          ex_2 = np.minimum(len(low_x), len(upp_x))
│ │  
│ │          for ii in range(ex_1, ex_2):
│ │              ymin = int(np.floor(upp_y_new[ii]))
│ │              ymax = int(np.ceil(low_y_new[ii]))
│ │  
│ │              ex_mask[:ymin, ii] = 0
│ │              ex_mask[ymax:, ii] = 0
│ │              ex_mask[ymin:ymax, ii] = 255
│ │  
│ │ -        # Calculate slope of central portion of each aponeurosis & use this to compute muscle thickness
│ │ +        # Calculate slope of central portion of each aponeurosis & use this to
│ │ +        # compute muscle thickness
│ │          Alist = list(set(upp_x).intersection(low_x))
│ │          Alist = sorted(Alist)
│ │          Alen = len(
│ │              list(set(upp_x).intersection(low_x))
│ │          )  # How many values overlap between x-axes
│ │          A1 = int(Alist[0] + (0.33 * Alen))
│ │          A2 = int(Alist[0] + (0.66 * Alen))
│ │ @@ -430,26 +433,27 @@
│ │              upp_ind -= 1
│ │  
│ │          for val in range(A1, A2):
│ │              if val >= len(low_x):
│ │                  continue
│ │              else:
│ │                  dist = math.dist(
│ │ -                    (upp_x[upp_ind], upp_y_new[upp_ind]), (low_x[val], low_y_new[val])
│ │ +                    (upp_x[upp_ind], upp_y_new[upp_ind]), (low_x[val],
│ │ +                                                           low_y_new[val])
│ │                  )
│ │                  if dist < mindist:
│ │                      mindist = dist
│ │  
│ │          # Compute functions to approximate the shape of the aponeuroses
│ │          zUA = np.polyfit(upp_x, upp_y_new, 2)
│ │          g = np.poly1d(zUA)
│ │          zLA = np.polyfit(low_x, low_y_new, 2)
│ │          h = np.poly1d(zLA)
│ │  
│ │ -        mid = (low_x[-1] - low_x[0]) / 2 + low_x[0]  # Find middle of the aponeurosis
│ │ +        mid = (low_x[-1] - low_x[0]) / 2 + low_x[0]  # Find middle
│ │          x1 = np.linspace(
│ │              low_x[0] - 700, low_x[-1] + 700, 10000
│ │          )  # Extrapolate polynomial fits to either side of the mid-point
│ │          y_UA = g(x1)
│ │          y_LA = h(x1)
│ │  
│ │          new_X_UA = np.linspace(
│ │ @@ -505,17 +509,17 @@
│ │              # Find intersection between each fascicle and the aponeuroses.
│ │              diffU = newY - new_Y_UA  # Find intersections
│ │              locU = np.where(diffU == min(diffU, key=abs))[0]
│ │              diffL = newY - new_Y_LA
│ │              locL = np.where(diffL == min(diffL, key=abs))[0]
│ │  
│ │              coordsX = newX[
│ │ -                int(locL) : int(locU)
│ │ +                int(locL): int(locU)
│ │              ]  # Get coordinates of fascicle between the two aponeuroses
│ │ -            coordsY = newY[int(locL) : int(locU)]
│ │ +            coordsY = newY[int(locL): int(locU)]
│ │  
│ │              # Get angle of aponeurosis in region close to fascicle intersection
│ │              if locL >= 4950:
│ │                  Apoangle = int(
│ │                      np.arctan(
│ │                          (new_Y_LA[locL - 50] - new_Y_LA[locL - 50])
│ │                          / (new_X_LA[locL] - new_X_LA[locL - 50])
│ │ @@ -530,17 +534,16 @@
│ │                          / (new_X_LA[locL + 50] - new_X_LA[locL])
│ │                      )
│ │                      * 180
│ │                      / np.pi
│ │                  )  # Angle relative to horizontal
│ │              Apoangle = 90.0 + abs(Apoangle)
│ │  
│ │ -            # Don't include fascicles that are completely outside of the field of view or
│ │ +            # Don't include fascicles that are completely outside of the FoV
│ │              # those that don't pass through central 1/3 of the image
│ │ -            #     if np.sum(coordsX) > 0 and coordsX[-1] > 0 and coordsX[0] < np.maximum(upp_x[-1],low_x[-1]) and coordsX[-1] - coordsX[0] < w and Apoangle != float('nan'):
│ │              if (
│ │                  np.sum(coordsX) > 0
│ │                  and coordsX[-1] > 0
│ │                  and coordsX[0] < np.maximum(upp_x[-1], low_x[-1])
│ │                  and Apoangle != float("nan")
│ │              ):
│ │                  FascAng = (
│ │ @@ -554,40 +557,44 @@
│ │                      )
│ │                      * -1
│ │                  )
│ │                  ActualAng = Apoangle - FascAng
│ │  
│ │                  if (
│ │                      ActualAng <= max_pennation and ActualAng >= min_pennation
│ │ -                ):  # Don't include 'fascicles' beyond a range of pennation angles
│ │ +                ):  # Don't include 'fascicles' beyond a range of PA
│ │                      length1 = np.sqrt(
│ │ -                        (newX[locU] - newX[locL]) ** 2 + (y_UA[locU] - y_LA[locL]) ** 2
│ │ +                        (newX[locU] - newX[locL]) ** 2 +
│ │ +                        (y_UA[locU] - y_LA[locL]) ** 2
│ │                      )
│ │                      fasc_l.append(length1[0])  # Calculate fascicle length
│ │                      pennation.append(Apoangle - FascAng)
│ │                      x_low1.append(coordsX[0].astype("int32"))
│ │                      x_high1.append(coordsX[-1].astype("int32"))
│ │                      coords = np.array(
│ │ -                        list(zip(coordsX.astype("int32"), coordsY.astype("int32")))
│ │ +                        list(zip(coordsX.astype("int32"),
│ │ +                                 coordsY.astype("int32")))
│ │                      )
│ │                      plt.plot(coordsX, coordsY, ":w", linewidth=6)
│ │          # cv2.polylines(imgT, [coords], False, (20, 15, 200), 3)
│ │  
│ │          # DISPLAY THE RESULTS
│ │          plt.imshow(img_copy, cmap="gray")
│ │ -        plt.title(f"Image ID: {filename}" + f"\n{scale_statement}", fontsize=25)
│ │ +        plt.title(f"Image ID: {filename}" + f"\n{scale_statement}",
│ │ +                  fontsize=25)
│ │          plt.plot(
│ │              low_x, low_y_new, marker="p", color="w", linewidth=10
│ │          )  # Plot the aponeuroses
│ │          plt.plot(upp_x, upp_y_new, marker="p", color="w", linewidth=10)
│ │  
│ │          xplot = 125
│ │          yplot = 700
│ │  
│ │ -        # Store the results for each frame and normalise using scale factor (if calibration was done above)
│ │ +        # Store the results for each frame and normalise using scale factor
│ │ +        # (if calibration was done above)
│ │          try:
│ │              midthick = mindist[0]  # Muscle thickness
│ │          except:
│ │              midthick = mindist
│ │  
│ │          if calib_dist:
│ │              fasc_l = fasc_l / (calib_dist / int(spacing))
│ │ @@ -599,15 +606,16 @@
│ │              ("Fascicle length: " + str("%.2f" % np.median(fasc_l)) + " mm"),
│ │              fontsize=15,
│ │              color="white",
│ │          )
│ │          plt.text(
│ │              xplot,
│ │              yplot + 50,
│ │ -            ("Pennation angle: " + str("%.1f" % np.median(pennation)) + " deg"),
│ │ +            ("Pennation angle: " +
│ │ +             str("%.1f" % np.median(pennation)) + " deg"),
│ │              fontsize=15,
│ │              color="white",
│ │          )
│ │          plt.text(
│ │              xplot,
│ │              yplot + 100,
│ │              ("Thickness at centre: " + str("%.1f" % midthick) + " mm"),
│ │   --- dl_track_us-0.1.1/DL_Track/gui_helpers/do_calculations_video.py
│ ├── +++ dl_track_us-0.1.2/DL_Track_US/gui_helpers/do_calculations_video.py
│ │┄ Files 2% similar despite different names
│ │ @@ -1,12 +1,12 @@
│ │  """
│ │  Description
│ │  -----------
│ │  This module contains functions to caculate muscle architectural
│ │ -parameters based on binary segmentations by convolutional neural networks (CNNs).
│ │ +parameters based on binary segmentations by convolutional neural networks.
│ │  The parameters include muscle thickness, pennation angle and fascicle length.
│ │  First, input images are segmented by the CNNs. Then the predicted aponeuroses
│ │  and fascicle fragments are thresholded and filtered. Fascicle fragments
│ │  and aponeuroses are extrapolated and the intersections determined.
│ │  This module is specifically designed for video analysis and is predisposed
│ │  for execution from a tk.TK GUI instance.
│ │  The architectural parameters are calculated. The results are plotted and
│ │ @@ -39,16 +39,16 @@
│ │  import numpy as np
│ │  from keras.models import load_model
│ │  from scipy.signal import savgol_filter
│ │  from skimage.morphology import skeletonize
│ │  from skimage.transform import resize
│ │  from tensorflow.keras.utils import img_to_array
│ │  
│ │ -from DL_Track.gui_helpers.calculate_architecture import IoU
│ │ -from DL_Track.gui_helpers.do_calculations import contourEdge, sortContours
│ │ +from DL_Track_US.gui_helpers.calculate_architecture import IoU
│ │ +from DL_Track_US.gui_helpers.do_calculations import contourEdge, sortContours
│ │  
│ │  plt.style.use("ggplot")
│ │  
│ │  
│ │  def doCalculationsVideo(
│ │      vid_len: int,
│ │      cap,
│ │ @@ -83,26 +83,29 @@
│ │          Object that contains the video in a np.ndarrray format.
│ │          In this way, seperate frames can be accessed.
│ │      vid_out : cv2.VideoWriter
│ │          Object that is stored in the vpath folder.
│ │          Contains the analyzed video frames and is titled "..._proc.avi"
│ │          The name can be changed but must be different than the input
│ │          video.
│ │ -    flip : str
│ │ +    flip : {"no_flip", "flip"}
│ │          String variable defining wheter an image should be flipped.
│ │ -        This can be "no_flip" (video is not flipped) or "flipe"
│ │ +        This can be "no_flip" (video is not flipped) or "flip"
│ │          (video is flipped).
│ │      apo_modelpath : str
│ │ -        String variable containing the absolute path to the aponeurosis neural network.
│ │ +        String variable containing the absolute path to the aponeurosis
│ │ +        neural network.
│ │      fasc_modelpath : str
│ │ -        String variable containing the absolute path to the fascicle neural network.
│ │ +        String variable containing the absolute path to the fascicle
│ │ +        neural network.
│ │      calib_dist : int
│ │          Integer variable containing the distance between the two
│ │          specified point in pixel units. The points must be 10mm
│ │ -        apart. Must be non-negative. If "None", the values are outputted in pixel units.
│ │ +        apart. Must be non-negative. If "None", the values are outputted in
│ │ +        pixel units.
│ │      dic : dict
│ │          Dictionary variable containing analysis parameters.
│ │          These include must include apo_threshold, fasc_threshold,
│ │          fasc_cont_threshold, min_width, max_pennation,
│ │          min_pennation.
│ │      step : int
│ │          Integer variable containing the step for the range of video frames.
│ │ @@ -143,30 +146,32 @@
│ │      --------
│ │      >>> doCalculations(vid_len=933, cap=< cv2.VideoCapture 000002BFAD0560B0>,
│ │                          vid_out=< cv2.VideoWriter 000002BFACEC0130>,
│ │                          flip="no_flip",
│ │                          apo_modelpath="C:/Users/admin/Documents/DL_Track/Models_DL_Track/Final_models/model-VGG16-fasc-BCE-512.h5",
│ │                          fasc_modelpath="C:/Users/admin/Documents/DL_Track/Models_DL_Track/Final_models/model-apo-VGG-BCE-512.h5",
│ │                          calib_dist=98,
│ │ -                        dic={'apo_treshold': '0.2', 'fasc_threshold': '0.05', 'fasc_cont_thresh': '40', 'min_width': '60', 'min_pennation': '10', 'max_pennation': '40'},
│ │ -                        gui=<__main__.DLTrack object at 0x000002BFA7528190>)
│ │ +                        dic={'apo_treshold': '0.2', 'fasc_threshold': '0.05',
│ │ +                        'fasc_cont_thresh': '40', 'min_width': '60',
│ │ +                        'min_pennation': '10', 'max_pennation': '40'},
│ │ +                        gui=<__main__.DL_Track_US object at 0x000002BFA7528190>)
│ │      [array([60.5451731 , 58.86892027, 64.16011534, 55.46192704, 63.40711356]), ..., array([64.90849385, 60.31621836])]
│ │      [[19.124207107383114, 19.409753216521565, 18.05706763600641, 20.54453899050867, 17.808652286488794], ..., [17.26241882195032, 16.284803480359543]]
│ │      [[148, 5, 111, 28, -164], [356, 15, 105, -296], [357, 44, -254], [182, 41, -233], [40, 167, 42, -170], [369, 145, 57, -139], [376, 431, 32], [350, 0]]
│ │      [[725, 568, 725, 556, 444], [926, 572, 516, 508], [971, 565, 502], [739, 578, 474], [554, 766, 603, 475], [1049, 755, 567, 430], [954, 934, 568], [968, 574]]
│ │      [23.484416057267826, 22.465452189555794, 21.646971767045816, 21.602856412413924, 21.501286239714894, 21.331137350026623, 21.02446763240188, 21.250352548097883]
│ │      """
│ │      try:
│ │  
│ │          # Check if analysis parameters are postive
│ │          for _, value in dic.items():
│ │              if float(value) <= 0:
│ │                  tk.messagebox.showerror(
│ │                      "Information",
│ │ -                    "Analysis paremters must be non-zero" + " and non-negative",
│ │ +                    "Analysis paremters must be non-zero" + " and non-negative"
│ │                  )
│ │                  gui.should_stop = False
│ │                  gui.is_running = False
│ │                  gui.do_break()
│ │                  return
│ │  
│ │          # Get variables from dictionary
│ │ @@ -202,15 +207,16 @@
│ │              if flip == "flip":
│ │                  img = np.fliplr(img)
│ │              img_orig = img  # Make a copy
│ │              # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
│ │              h = img.shape[0]
│ │              w = img.shape[1]
│ │              img = np.reshape(img, [-1, h, w, 3])
│ │ -            img = resize(img, (1, 512, 512, 3), mode="constant", preserve_range=True)
│ │ +            img = resize(img, (1, 512, 512, 3), mode="constant",
│ │ +                         preserve_range=True)
│ │              img = img / 255.0
│ │  
│ │              # Predict aponeurosis
│ │              pred_apo = model_apo.predict(img)
│ │              # Employ threshold for segmentation
│ │              pred_apo_t = (pred_apo > apo_threshold).astype(np.uint8)
│ │  
│ │ @@ -227,15 +233,15 @@
│ │              pred_apo_t = resize(pred_apo_t, (1, h, w, 1))
│ │              pred_apo_t = np.reshape(pred_apo_t, (h, w))
│ │              pred_fasc = resize(pred_fasc, (1, h, w, 1))
│ │              pred_fasc = np.reshape(pred_fasc, (h, w))
│ │              pred_fasc_t = resize(pred_fasc_t, (1, h, w, 1))
│ │              pred_fasc_t = np.reshape(pred_fasc_t, (h, w))
│ │  
│ │ -            ## Aponuerosis calculation PArt
│ │ +            # Aponuerosis calculation PArt
│ │  
│ │              # Compute the contours to identify aponeuroses
│ │              _, thresh = cv2.threshold(pred_apo_t, 0, 255, cv2.THRESH_BINARY)
│ │              thresh = thresh.astype("uint8")
│ │              # Find contours in thresholded image
│ │              contours, hierarchy = cv2.findContours(
│ │                  thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE
│ │ @@ -278,15 +284,16 @@
│ │              for countU in range(0, len(contours_re2) - 1):
│ │                  if (
│ │                      xs1[countU + 1] > xs2[countU]
│ │                  ):  # Check if x of contour2 is higher than x of contour 1
│ │                      y1 = ys2[countU]
│ │                      y2 = ys1[countU + 1]
│ │                      if y1 - 10 <= y2 <= y1 + 10:
│ │ -                        m = np.vstack((contours_re2[countU], contours_re2[countU + 1]))
│ │ +                        m = np.vstack((contours_re2[countU],
│ │ +                                       contours_re2[countU + 1]))
│ │                          cv2.drawContours(maskT, [m], 0, 255, -1)
│ │                  countU += 1
│ │  
│ │              # Make binary
│ │              maskT[maskT > 0] = 1
│ │              # Skeletonize o detect edges
│ │              skeleton = skeletonize(maskT).astype(np.uint8)
│ │ @@ -310,38 +317,40 @@
│ │  
│ │              # Sort contours again from top to bottom
│ │              contoursE, _ = sortContours(contoursE)
│ │  
│ │              # Continue only when 2 or more aponeuroses were detected
│ │              if len(contoursE) >= 2:
│ │  
│ │ -                # Get the x,y coordinates of the upper/lower edge of the 2 aponeuroses
│ │ +                # Get the x,y coordinates of the upper/lower edge of the 2
│ │ +                # aponeuroses
│ │                  upp_x, upp_y = contourEdge("B", contoursE[0])
│ │                  if contoursE[1][0, 0, 1] > (contoursE[0][0, 0, 1] + min_width):
│ │                      low_x, low_y = contourEdge("T", contoursE[1])
│ │                  else:
│ │                      low_x, low_y = contourEdge("T", contoursE[2])
│ │  
│ │                  # Filter data one-dimensionally to extend the data
│ │ -                upp_y_new = savgol_filter(upp_y, 81, 2)  # window size, polynomial order
│ │ +                upp_y_new = savgol_filter(upp_y, 81, 2)
│ │                  low_y_new = savgol_filter(low_y, 81, 2)
│ │  
│ │                  # Make a binary mask
│ │                  ex_mask = np.zeros(thresh.shape, np.uint8)
│ │                  ex_1 = 0
│ │                  ex_2 = np.minimum(len(low_x), len(upp_x))
│ │                  for ii in range(ex_1, ex_2):
│ │                      ymin = int(np.floor(upp_y_new[ii]))
│ │                      ymax = int(np.ceil(low_y_new[ii]))
│ │  
│ │                      ex_mask[:ymin, ii] = 0
│ │                      ex_mask[ymax:, ii] = 0
│ │                      ex_mask[ymin:ymax, ii] = 255
│ │  
│ │ -                # Calculate slope of central portion of each aponeurosis & use this to compute muscle thickness
│ │ +                # Calculate slope of central portion of each aponeurosis
│ │ +                # & use this to compute muscle thickness
│ │                  Alist = list(set(upp_x).intersection(low_x))
│ │                  Alist = sorted(Alist)
│ │                  Alen = len(
│ │                      list(set(upp_x).intersection(low_x))
│ │                  )  # How many values overlap between x-axes
│ │                  A1 = int(Alist[0] + (0.33 * Alen))
│ │                  A2 = int(Alist[0] + (0.66 * Alen))
│ │ @@ -373,31 +382,32 @@
│ │                  h = np.poly1d(zLA)
│ │  
│ │                  mid = (low_x[-1] - low_x[0]) / 2 + low_x[
│ │                      0
│ │                  ]  # Find middle of the aponeurosis
│ │                  x1 = np.linspace(
│ │                      low_x[0] - 700, low_x[-1] + 700, 10000
│ │ -                )  # Extrapolate polynomial fits to either side of the mid-point
│ │ +                )  # Extrapolate polynomial fits to either side
│ │                  y_UA = g(x1)
│ │                  y_LA = h(x1)
│ │  
│ │                  new_X_UA = np.linspace(
│ │                      mid - 700, mid + 700, 5000
│ │                  )  # Extrapolate x,y data using f function
│ │                  new_Y_UA = g(new_X_UA)
│ │                  new_X_LA = np.linspace(
│ │                      mid - 700, mid + 700, 5000
│ │                  )  # Extrapolate x,y data using f function
│ │                  new_Y_LA = h(new_X_LA)
│ │  
│ │ -                ## Fascicle calculation part
│ │ +                # Fascicle calculation part
│ │  
│ │                  # Compute contours to identify fascicles / fascicle orientation
│ │ -                _, threshF = cv2.threshold(pred_fasc_t, 0, 255, cv2.THRESH_BINARY)
│ │ +                _, threshF = cv2.threshold(pred_fasc_t, 0, 255,
│ │ +                                           cv2.THRESH_BINARY)
│ │                  threshF = threshF.astype("uint8")
│ │                  contoursF, hierarchy = cv2.findContours(
│ │                      threshF, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
│ │                  )
│ │  
│ │                  # Remove any contours that are very small
│ │                  maskF = np.zeros(threshF.shape, np.uint8)
│ │ @@ -434,22 +444,23 @@
│ │                      z = np.polyfit(np.array(x), np.array(y), 1)
│ │                      f = np.poly1d(z)
│ │                      newX = np.linspace(
│ │                          -400, w + 400, 5000
│ │                      )  # Extrapolate x,y data using f function
│ │                      newY = f(newX)
│ │  
│ │ -                    # Find intersection between each fascicle and the aponeuroses.
│ │ +                    # Find intersection between each fascicle and the
│ │ +                    # aponeuroses.
│ │                      diffU = newY - new_Y_UA  # Find intersections
│ │                      locU = np.where(diffU == min(diffU, key=abs))[0]
│ │                      diffL = newY - new_Y_LA
│ │                      locL = np.where(diffL == min(diffL, key=abs))[0]
│ │  
│ │ -                    coordsX = newX[int(locL) : int(locU)]
│ │ -                    coordsY = newY[int(locL) : int(locU)]
│ │ +                    coordsX = newX[int(locL): int(locU)]
│ │ +                    coordsY = newY[int(locL): int(locU)]
│ │  
│ │                      if locL >= 4950:
│ │                          Apoangle = int(
│ │                              np.arctan(
│ │                                  (new_Y_LA[locL - 50] - new_Y_LA[locL - 50])
│ │                                  / (new_X_LA[locL] - new_X_LA[locL - 50])
│ │                              )
│ │ @@ -463,16 +474,17 @@
│ │                                  / (new_X_LA[locL + 50] - new_X_LA[locL])
│ │                              )
│ │                              * 180
│ │                              / np.pi
│ │                          )  # Angle relative to horizontal
│ │                      Apoangle = 90 + abs(Apoangle)
│ │  
│ │ -                    # Don't include fascicles that are completely outside of the field of view or
│ │ -                    # those that don't pass through central 1/3 of the image
│ │ +                    # Don't include fascicles that are completely outside of
│ │ +                    # the field of view or those that don't pass through
│ │ +                    # central 1/3 of the image
│ │                      if (
│ │                          np.sum(coordsX) > 0
│ │                          and coordsX[-1] > 0
│ │                          and coordsX[0] < np.maximum(upp_x[-1], low_x[-1])
│ │                          and Apoangle != float("nan")
│ │                      ):
│ │                          FascAng = (
│ │ @@ -486,33 +498,36 @@
│ │                              )
│ │                              * -1
│ │                          )
│ │                          ActualAng = Apoangle - FascAng
│ │  
│ │                          if (
│ │                              ActualAng <= max_pennation and ActualAng >= min_pennation
│ │ -                        ):  # Don't include 'fascicles' beyond a range of pennation angles
│ │ +                        ):  # Don't include 'fascicles' beyond a range of PA
│ │                              length1 = np.sqrt(
│ │                                  (newX[locU] - newX[locL]) ** 2
│ │                                  + (y_UA[locU] - y_LA[locL]) ** 2
│ │                              )
│ │ -                            fasc_l.append(length1[0])  # Calculate fascicle length
│ │ +                            fasc_l.append(length1[0])  # Calculate FL
│ │                              pennation.append(Apoangle - FascAng)
│ │                              x_low1.append(coordsX[0].astype("int32"))
│ │                              x_high1.append(coordsX[-1].astype("int32"))
│ │                              coords = np.array(
│ │                                  list(
│ │                                      zip(
│ │ -                                        coordsX.astype("int32"), coordsY.astype("int32")
│ │ +                                        coordsX.astype("int32"),
│ │ +                                        coordsY.astype("int32")
│ │                                      )
│ │                                  )
│ │                              )
│ │ -                            cv2.polylines(imgT, [coords], False, (20, 15, 200), 3)
│ │ +                            cv2.polylines(imgT, [coords], False, (20, 15, 200),
│ │ +                                          3)
│ │  
│ │ -                # Store the results for each frame and normalise using scale factor (if calibration was done above)
│ │ +                # Store the results for each frame and normalise using scale
│ │ +                # factor (if calibration was done above)
│ │                  try:
│ │                      midthick = mindist[0]  # Muscle thickness
│ │                  except:
│ │                      midthick = mindist
│ │  
│ │                  if calib_dist:
│ │                      fasc_l = fasc_l / (calib_dist / 10)
│ │ @@ -572,15 +587,16 @@
│ │                      (125, 380),
│ │                      cv2.FONT_HERSHEY_DUPLEX,
│ │                      1,
│ │                      (249, 249, 249),
│ │                  )
│ │                  cv2.putText(
│ │                      comb,
│ │ -                    ("Thickness at centre: " + str("%.1f" % thickness_all[-1]) + " mm"),
│ │ +                    ("Thickness at centre: " + str("%.1f" % thickness_all[-1]) +
│ │ +                     " mm"),
│ │                      (125, 440),
│ │                      cv2.FONT_HERSHEY_DUPLEX,
│ │                      1,
│ │                      (249, 249, 249),
│ │                  )
│ │              else:
│ │                  cv2.putText(
│ │ @@ -592,15 +608,16 @@
│ │                      (125, 380),
│ │                      cv2.FONT_HERSHEY_DUPLEX,
│ │                      1,
│ │                      (249, 249, 249),
│ │                  )
│ │                  cv2.putText(
│ │                      comb,
│ │ -                    ("Thickness at centre: " + str("%.1f" % thickness_all[-1]) + " px"),
│ │ +                    ("Thickness at centre: " + str("%.1f" % thickness_all[-1])
│ │ +                     + " px"),
│ │                      (125, 440),
│ │                      cv2.FONT_HERSHEY_DUPLEX,
│ │                      1,
│ │                      (249, 249, 249),
│ │                  )
│ │  
│ │              # Check platform for imshow
│ │ @@ -620,15 +637,16 @@
│ │          vid_out.release()
│ │          cv2.destroyAllWindows()
│ │  
│ │          return fasc_l_all, pennation_all, x_lows_all, x_highs_all, thickness_all
│ │  
│ │      # Check if model path is correct
│ │      except OSError:
│ │ -        tk.messagebox.showerror("Information", "Apo/Fasc model path is incorrect.")
│ │ +        tk.messagebox.showerror("Information",
│ │ +                                "Apo/Fasc model path is incorrect.")
│ │          gui.should_stop = False
│ │          gui.is_running = False
│ │          gui.do_break()
│ │          return
│ │  
│ │      finally:
│ │          # clean up
│ │   --- dl_track_us-0.1.1/DL_Track/gui_helpers/home_im.ico
│ ├── +++ dl_track_us-0.1.2/DL_Track_US/gui_helpers/home_im.ico
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/DL_Track/gui_helpers/image_quality.py
│ ├── +++ dl_track_us-0.1.2/DL_Track_US/gui_helpers/model_training.py
│ │┄ Files 27% similar despite different names
│ │ @@ -1,712 +1,727 @@
│ │  """
│ │  Description
│ │  -----------
│ │ -This module contains a class to augment image brightness, contrast and noise.
│ │ -Moreover, using the class, the quality of images can be evaluated. The quality score
│ │ -is calculated based on mean pixel intensity, standard deviation or pixel intensity,
│ │ -noise estimation, blur estimation and signal-to-noise ratio.
│ │ -These parameters were selected because we believe that they are most
│ │ -relevant for the evaluation of ultrasound images.
│ │ -The module was specifically designed to be executed from a GUI, but each
│ │ -function can be used separately.
│ │ -When used from the GUI, only the image evaluation funtions are used. A .xlsx file
│ │ -containing the parameters will be saved to a specified directory. This is the
│ │ -directory where all images to be evaluated should be.
│ │ +This module contains functions to train a VGG16 encoder U-net decoder CNN.
│ │ +The module was specifically designed to be executed from a GUI.
│ │ +When used from the GUI, the module saves the trained model and weights to
│ │ +a given directory. The user needs to provide paths to the image and label/
│ │ +mask directories. Instructions for correct image labelling can be found
│ │ +in the Labelling directory.
│ │ +
│ │  
│ │  Functions scope
│ │  ---------------
│ │ -For scope of the functions see class documentation.
│ │ +conv_block
│ │ +    Function to build a convolutional block for the U-net decoder path of
│ │ +    the network.
│ │ +    The block is built using several keras.layers functionalities.
│ │ +decoder_block
│ │ +    Function to build a decoder block for the U-net decoder path of
│ │ +    the network.
│ │ +    The block is built using several keras.layers functionalities.
│ │ +build_vgg16_model
│ │ +    Function that builds a convolutional network consisting of an VGG16
│ │ +    encoder path and a U-net decoder path.
│ │ +IoU
│ │ +    Function to compute the intersection over union score (IoU),
│ │ +    a measure of prediction accuracy. This is sometimes also called
│ │ +    Jaccard score.
│ │ +dice_score
│ │ +    Function to compute the Dice score, a measure of prediction accuracy.
│ │ +focal_loss
│ │ +     Function to compute the focal loss, a measure of prediction accuracy.
│ │ +load_images
│ │ +    Function to load images and manually labeled masks from a specified
│ │ +    directory.
│ │ +train_model
│ │ +    Function to train a convolutional neural network with VGG16 encoder and
│ │ +    U-net decoder. All the steps necessary to properly train a neural
│ │ +    network are included in this function.
│ │  
│ │  Notes
│ │  -----
│ │  Additional information and usage examples can be found at the respective
│ │ -functions docstrings.
│ │ +functions documentations.
│ │  """
│ │ -
│ │ -import glob
│ │  import os
│ │ -import time
│ │ +import tkinter as tk
│ │  
│ │ +import matplotlib.pyplot as plt
│ │  import numpy as np
│ │ -import pandas as pd
│ │ -from scipy import stats
│ │ -from sewar.full_ref import ssim
│ │ -from skimage import restoration, util
│ │ +from keras import backend as K
│ │ +from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
│ │ +from keras.layers import Activation, BatchNormalization, Input
│ │ +from keras.models import Model
│ │ +from keras.optimizers import Adam
│ │ +from skimage.transform import resize
│ │ +from sklearn.model_selection import train_test_split
│ │ +from tensorflow.keras.applications import VGG16
│ │ +from tensorflow.keras.layers import (
│ │ +    Concatenate,
│ │ +    Conv2D,
│ │ +    Conv2DTranspose,
│ │ +)
│ │ +from tensorflow.keras.utils import img_to_array, load_img
│ │ +from tqdm import tqdm
│ │ +
│ │ +
│ │ +def conv_block(inputs, num_filters: int):
│ │ +    """Function to build a convolutional block for the U-net decoder path of
│ │ +    the network to be build.
│ │ +    The block is built using several keras.layers functionalities.
│ │ +
│ │ +    Here, we decided to use 'padding = same' and and a convolutional
│ │ +    kernel of 3.
│ │ +    This is adaptable in the code but will influence the model outcome.
│ │ +    The convolutional block consists of two convolutional layers. Each creates
│ │ +    a convolution kernel that is convolved with the layer input to produce a
│ │ +    tensor of outputs.
│ │  
│ │ +    Parameters
│ │ +    ----------
│ │ +    inputs : KerasTensor
│ │ +        Concattenated Tensorflow.Keras Tensor outputted from previous layer.
│ │ +        The Tensor can be altered by adapting, i.e. the filter numbers but
│ │ +        this will change the model training output.
│ │ +        The input is then convolved using the built kernel.
│ │ +    num_filters : int
│ │ +        Integer variable determining the number of filters used during model
│ │ +        training.
│ │ +        Here, we started with 'num_filers = 512'.
│ │ +        The filter number is halfed each layer.
│ │ +        The number of filters can be adapted in the code.
│ │ +        Must be non-negative and non-zero.
│ │ +
│ │ +    Returns
│ │ +    -------
│ │ +    x : KerasTensor
│ │ +        Tensorflow.Keras Tensor used during model Training.
│ │ +        The Tensor can be altered by adapting the input paramenters to the
│ │ +        function or the upsampling but this will change the model training.
│ │ +        The number of filters is halfed.
│ │  
│ │ -class ImageQuali:
│ │ +    Example
│ │ +    -------
│ │ +    >>> conv_block(inputs=KerasTensor(type_spec=TensorSpec(shape=(None, 256, 256, 128),
│ │ +                   dtype=tf.float32, name=None),
│ │ +                   num_filters=128)
│ │ +    KerasTensor(type_spec=TensorSpec(shape=(None, 256, 256, 64),
│ │ +    dtype=tf.float32, name=None)
│ │      """
│ │ -    Python class to adapt and assess the quality of images.
│ │ -    The functionalities offered by this class can adapt the
│ │ -    brightness, contrast and noise of images. Image quality can
│ │ -    also be assessed. All instance methods can be used
│ │ -    separately but the main instance is 'adaptImages' where images can be
│ │ -    augmented and their quality assessed.
│ │ +    # Define Conv2D layer witch Batchnor and Activation relu
│ │ +    x = Conv2D(filters=num_filters, kernel_size=3, padding="same")(inputs)
│ │ +    x = BatchNormalization()(x)
│ │ +    x = Activation("relu")(x)
│ │ +    # Define second Conv2D layer witch Batchnor and Activation relu
│ │ +    x = Conv2D(filters=num_filters, kernel_size=3, padding="same")(inputs)
│ │ +    x = BatchNormalization()(x)
│ │ +    x = Activation("relu")(x)
│ │ +
│ │ +    return x
│ │ +
│ │ +
│ │ +def decoder_block(inputs, skip_features, num_filters):
│ │ +    """Function to build a decoder block for the U-net decoder path
│ │ +    of the network to be build.
│ │ +    The block is build using several keras.layers functionalities.
│ │ +
│ │ +    The block is built by applying a deconvolution (Keras.Conv2DTranspose)
│ │ +    to upsample to input by a factor of 2. A concatenation with the skipped
│ │ +    features from the mirrored vgg16 convolutional layer follows.
│ │ +    Subsequently a convolutional block (see conv_block
│ │ +    function) is applied to convolve the input with the built kernel.
│ │  
│ │ -    Attributes
│ │ +    Parameters
│ │      ----------
│ │ -    self.root : str
│ │ -        Path to root directory containing the images to be augmented/
│ │ -        analyzed.
│ │ -    self.out : str
│ │ -        Path to output directory where augmented images are saved.
│ │ -        Note that the quality assessment .xlsx file is always saved
│ │ -        to the root path.
│ │ -    self.mlocs : list
│ │ -        List variable to store coordinates of mouse clicks. This is
│ │ -        only used when images are cropped.
│ │ +    inputs : KerasTensor
│ │ +        Concattenated Tensorflow.Keras Tensor outputted from previous layer.
│ │ +        The Tensor can be altered by adapting, i.e. the filter numbers but
│ │ +        this will change the model training output.
│ │ +    skip_features : Keras Tensor
│ │ +        Skip connections to the encoder path of the vgg16 encoder.
│ │ +    num_filters : int
│ │ +        Integer variable determining the number of filters used during
│ │ +        model training.
│ │ +        Here, we started with 'num_filers = 512'.
│ │ +        The filter number is halfed each layer. The number of filters can
│ │ +        be adapted in the code. Must be non-neagtive and non-zero.
│ │  
│ │ -    Methods
│ │ +    Returns
│ │      -------
│ │ -    __init__
│ │ -        Instance method to initialize class
│ │ -    mclick
│ │ -         Instance method to detect mouse click coorindates in image.
│ │ -    getBoundingBox
│ │ -        Instance method to get the user selected bounding box and
│ │ -        crop the image
│ │ -    saveResults
│ │ -        Instance method to save image quality assessment results to
│ │ -        excel sheet
│ │ -    getSignalToNoise
│ │ -        Instance method to calculate the signel to noise ratio of a given image.
│ │ -    calculateNoise
│ │ -        Instance method to assess image quality/noise in several measures
│ │ -    makeImageContrast
│ │ -        Instance method to change image contrast/brightness
│ │ -    makeImageNoise
│ │ -        Instance method to add speckle noise to image
│ │ -    makeImageBlur
│ │ -        Instance method to blur the input image
│ │ -    adaptImages
│ │ -        Instance method to apply quality changes and assess qualiy
│ │ +    x : KerasTensor
│ │ +        Tensorflow.Keras Tensor used during model Training. The tensor is
│ │ +        upsampled using Keras.Conv2DTranspose with a kernel of (2,2),
│ │ +        'stride=2' and 'padding=same'.
│ │ +        The upsampling increases image size by a factor of 2.
│ │ +        The number of filters is halfed.
│ │ +        The Tensor can be altered by adapting the input paramenters to the
│ │ +        function or the upsampling but this will change the model training.
│ │ +
│ │ +    Example
│ │ +    -------
│ │ +    >>> decoder_block(inputs=KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 512),
│ │ +                      skip_features=KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 512),
│ │ +                      dtype=tf.float32, name=None)),
│ │ +                      num_filters=256)
│ │ +    KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128, 256),
│ │ +    dtype=tf.float32, name=None)
│ │ +    """
│ │ +    # Define a whole decoder block
│ │ +    x = Conv2DTranspose(
│ │ +        filters=num_filters, kernel_size=(2, 2), strides=2, padding="same"
│ │ +    )(inputs)
│ │ +    x = Concatenate()([x, skip_features])
│ │ +    # Use pre-specified convolutional block
│ │ +    x = conv_block(inputs=x, num_filters=num_filters)
│ │ +
│ │ +    return x
│ │ +
│ │ +
│ │ +def build_vgg16_unet(input_shape: tuple):
│ │ +    """Function that builds a convolutional network consisting of a VGG16
│ │ +    encoder path and a U-net decoder path.
│ │ +
│ │ +    The model is built using several Tensorflow.Keras functions.
│ │ +    First, the whole VGG16 model is imported and built using pretrained
│ │ +    imagenet weights and the input shape.
│ │ +    Then, the encoder layers are pulled from the model incldung the bridge.
│ │ +    Subsequently the decoder path from the U-net is built.
│ │ +    Lastly, a 1x1 convolution is applied with sigmoid activation
│ │ +    to perform binary segmentation on the input.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    input_shape : tuple
│ │ +        Tuple describing the input shape. Must be of shape (...,...,...).
│ │ +        Here we used (512,512,3) as input shape. The image size (512,512,)
│ │ +        can be easily adapted. The channel numer (,,3) is given by the
│ │ +        model and the pretrained weights. We advide the user not to change
│ │ +        the image size segmentation results were best with the predefined
│ │ +        size.
│ │ +
│ │ +    Returns
│ │ +    -------
│ │ +    model
│ │ +        The built VGG16 encoder U-net decoder convolutional network
│ │ +        for binary segmentation on the input.
│ │ +        The model can subsequently be used for training.
│ │  
│ │      Notes
│ │      -----
│ │ -    This class solely consists of instance methods.
│ │ -    Each method can be used on its own with the respective input parameters.
│ │ -    For more information on the instance methods check the respective
│ │ -    docstrings.
│ │ -
│ │ -    Some of the instance methods are not used when the class is initialized.
│ │ -    These include the following ones: mclick, getBoundingBox, makeImageContrast,
│ │ -    makeImageBlur, makeImageNoise.
│ │ -    Moreover, the self.out instance attribute is initialized to be the same
│ │ -    directory as the self.input attribute.
│ │ +    See our paper () and references for more detailed model description
│ │ +
│ │ +    References
│ │ +    ----------
│ │ +    [1] VGG16: Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014)
│ │ +    [2] U-net: Ronneberger, O., Fischer, P. and Brox, T. "U-Net: Convolutional Networks for Biomedical Image Segmentation." arXiv preprint arXiv:1505.04597 (2015)
│ │      """
│ │ +    # Get input shape
│ │ +    _inputs = Input(input_shape)
│ │  
│ │ -    def __init__(self, rootpath: str, outpath: str):
│ │ -        """
│ │ -        Instance method to initialize the ImageQuali class.
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        rootpath : str
│ │ -            Path to root directory containing images to be
│ │ -            augmented/analyzed.
│ │ -        outpath : str
│ │ -            Path to output directory where images/results
│ │ -            are saved.
│ │ -        mlocs : list
│ │ -            List variable to store detected mouse click
│ │ -            coordinates for bounding box selection.
│ │ -        """
│ │ -        self.root = rootpath
│ │ -        self.out = outpath
│ │ -        self.mlocs = []
│ │ -
│ │ -    def mclick(self, event, x_val, y_val, flags, param):
│ │ -        """
│ │ -        Instance method to detect mouse click coorindates in image.
│ │ -
│ │ -        This instance is used when the image to be analyzed should be
│ │ -        cropped.  Upon klicking of the mouse button, the coordinates
│ │ -        of the cursor position are stored in the instance attribute
│ │ -        self.mlocs.
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        event
│ │ -            Event flag specified as Cv2 mouse event left mousebutton down.
│ │ -        x_val
│ │ -            Value of x-coordinate of mouse event to be recorded
│ │ -        y_val
│ │ -            Value of y-coordinate of mouse event to be recorded
│ │ -        flags
│ │ -            Specific condition whenever a mouse event occurs. This
│ │ -            is not used here but needs to be specified as input
│ │ -            parameter.
│ │ -        param
│ │ -            User input data. This is not required here but needs to
│ │ -            be specified as input parameter.
│ │ -        """
│ │ -        # if the left mouse button was clicked, record the (x, y) coordinates
│ │ -        if event == cv2.EVENT_LBUTTONDOWN:
│ │ -            self.mlocs.append(y_val)
│ │ -            self.mlocs.append(x_val)
│ │ -
│ │ -    def getBoundingBox(self, img: np.ndarray) -> np.ndarray:
│ │ -        """
│ │ -        Instance method to get the selected bounding box and
│ │ -        crop the image.
│ │ -
│ │ -        The image will be cropped based on the coordinates stored
│ │ -        in self.mlocs and the bounding box will always be a
│ │ -        rectangle. See Notes for information on exact placement of mouse clicks.
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        img : np.ndarray
│ │ -            Grayscale image to be analysed as a numpy array. The image must
│ │ -            be loaded prior to bounding box selection, specifying a path
│ │ -            is not valid.
│ │ -
│ │ -        Returns
│ │ -        -------
│ │ -        crop_img : np.ndarray
│ │ -            The cropped grayscale image based on the bounding box as a numpy array.
│ │ -            The size of the image is not adapted and is similar to the size
│ │ -            of the selected bounding box.
│ │ -
│ │ -        Notes
│ │ -        -----
│ │ -        To function properly, the user must select exactly four point
│ │ -        on the image around which the bounding box built:
│ │ -        - The first point must be the top left
│ │ -        - The second point top right
│ │ -        - The third point bottom left
│ │ -        - The fourth bottom right
│ │ -        The cropped image might look weird when other orders are used.
│ │ -
│ │ -        Examples
│ │ -        --------
│ │ -        >>> getBoundingBox(img=([[[[0.22414216 0.19730392 0.22414216] ... [0.2509804  0.2509804  0.2509804 ]]]))
│ │ -        ([[[[0.22414216 0.19730392 0.22414216] ... [0.2509804  0.2509804  0.2509804 ]]])
│ │ -        """
│ │ -        # display the image and wait for a keypress
│ │ -        cv2.imshow("image", img)
│ │ -        cv2.setMouseCallback("image", self.mclick)
│ │ -        key = cv2.waitKey(0)
│ │ -
│ │ -        # if the 'q' key is pressed, break from the loop
│ │ -        if key == ord("q"):
│ │ -            cv2.destroyAllWindows()
│ │ -
│ │ -        # Crop image according to selection
│ │ -        crop_img = img[
│ │ -            self.mlocs[1] : self.mlocs[1] + self.mlocs[4],
│ │ -            self.mlocs[0] : self.mlocs[0] + self.mlocs[3],
│ │ -        ]
│ │ -
│ │ -        # Reset coordinates
│ │ -        self.mlocs = []
│ │ -
│ │ -        return crop_img
│ │ -
│ │ -    def saveResults(self, path: str, dataframe: pd.DataFrame):
│ │ -        """
│ │ -        Instance method to save the quality assessment results.
│ │ -
│ │ -        A pd.DataFrame object must be inputted. The results
│ │ -        inculded in the dataframe are saved to an .xlsx file. Depending
│ │ -        on the form of class initialization, the .xlsx file is
│ │ -        either saved to self.root (GUI) or self.out (command prompt).
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        path : str
│ │ -            String variable containing the path to where the .xlsx file
│ │ -            should be saved.
│ │ -        dataframe : pd.DataFrame
│ │ -            Pandas dataframe variable containing the image analysis results
│ │ -            for every image anlyzed.
│ │ -
│ │ -        Notes
│ │ -        -----
│ │ -        An .xlsx file is saved to a specified location
│ │ -        containing all analysis results.
│ │ -
│ │ -        Examples
│ │ -        --------
│ │ -        >>> saveResults(img_path = "C:/Users/admin/Dokuments/images",
│ │ -                        dataframe = [['File',"image1"],['mean',12],
│ │ -                                     ['Sigma',0.56]])
│ │ -        """
│ │ -        # Define path
│ │ -        excelpath = path + "/Quality.xlsx"
│ │ -
│ │ -        # Check if directory is existent
│ │ -        if os.path.exists(excelpath):
│ │ -            with pd.ExcelWriter(excelpath, mode="a") as writer:
│ │ -                data = dataframe
│ │ -                data.to_excel(writer, sheet_name="Results")
│ │ -        else:
│ │ -            with pd.ExcelWriter(excelpath, mode="w") as writer:
│ │ -                data = dataframe
│ │ -                data.to_excel(writer, sheet_name="Results")
│ │ -
│ │ -    def getSignalToNoise(self, img: np.ndarray, axis: int = None, ddof: int = 0):
│ │ -        """
│ │ -        Instance method to calculate the signal to noise ratio (SNR) of a given image.
│ │ -
│ │ -        Here we calculate the SNR using the mean of the image brightness and the
│ │ -        standard deviation.
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        img : np.ndarray
│ │ -            Grayscale image to be analysed as an numpy array. The image must
│ │ -            be loaded prior to bounding box selection, specifying a path
│ │ -            is not valid.
│ │ -        axis : int, default = None
│ │ -            Axis or axes along which the standard deviation is computed.
│ │ -            The default is to compute the standard deviation of the flattened array.
│ │ -        ddof : int, default = 0
│ │ -            Means Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
│ │ -            where N represents the number of elements. By default ddof is zero.
│ │ -
│ │ -        Returns
│ │ -        -------
│ │ -        sig_noi_rat : float
│ │ -            Float variabel containing the SNR for the input image.
│ │ -
│ │ -        Examples
│ │ -        --------
│ │ -        >>> getSignalToNoise(img=([[[[0.22414216 0.19730392 0.22414216] ... [0.2509804  0.2509804  0.2509804 ]]]))
│ │ -        0.317
│ │ -        """
│ │ -        img = np.asanyarray(img)
│ │ -        # Calculate image mean
│ │ -        me = img.mean(axis)
│ │ -        # Caclulate img sd
│ │ -        sd = img.std(axis=axis, ddof=ddof)
│ │ -        # Caclulate SNR
│ │ -        sig_noi_rat = np.where(sd == 0, 0, me / sd)
│ │ -
│ │ -        return sig_noi_rat
│ │ -
│ │ -    def getSSIM(self, comp_imgs_path: str, filetype: str = "/**/*.tif") -> None:
│ │ -        """
│ │ -        Instance method to calculate the structural similarity index (SSIM)
│ │ -        between two images.
│ │ -
│ │ -        The method requires two directories as input. Both directories are required
│ │ -        to contain images of a given filetype that are compared. Each image in one
│ │ -        folder is compared to each image in the other folder. The results are saved
│ │ -        in the root directory that was specifed upon class initialization.
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        comp_ims_path : str
│ │ -            String variable containing the path to where one set of images should be
│ │ -            located. The other set of images provided for the comparison should be
│ │ -            located in root.
│ │ -        filetype : str, default = "/**/*.tif"
│ │ -            String variable containg the respective type of the images.
│ │ -            Both image sets must be of the same type.
│ │ -
│ │ -        Examples
│ │ -        --------
│ │ -         >>> getSSIM(comp_img_path = "C:/Users/admin/Dokuments/images")
│ │ -        """
│ │ -        start = time.time()
│ │ -        # Get all image files to be adapted
│ │ -        list_of_files_comp = glob.glob(comp_imgs_path + filetype, recursive=True)
│ │ -        list_of_files = glob.glob(self.root + filetype, recursive=True)
│ │ -
│ │ -        # Create DF for export
│ │ -        dataframe = pd.DataFrame(columns=["File", "SSIM"])
│ │ -
│ │ -        # Loop through all images to calculate ssim
│ │ -        for comp in list_of_files_comp:
│ │ -
│ │ -            img_comp = cv2.imread(comp, 0)
│ │ -            img_comp = cv2.resize(img_comp, (512, 512))
│ │ -            filename_comp = os.path.splitext(os.path.basename(comp))[0]
│ │ -
│ │ -            # Crop image to relevant region
│ │ -            img_comp = self.getBoundingBox(img=img_comp)
│ │ -            img_comp = cv2.resize(img_comp, (512, 512))
│ │ -
│ │ -            # Loop trough all images to be compared to calculate ssim for each
│ │ -            for image in list_of_files:
│ │ -
│ │ -                # load and resize image
│ │ -                img = cv2.imread(image, 0)
│ │ -                filename = os.path.splitext(os.path.basename(image))[0]
│ │ -
│ │ -                # Crop image to relevant region
│ │ -                img = self.getBoundingBox(img=img)
│ │ -                img = cv2.resize(img, (512, 512))
│ │ -
│ │ -                # Calculate ssim
│ │ -                ssim_score, _ = ssim(img, img_comp)
│ │ -
│ │ -                dataframe = dataframe.append(
│ │ -                    {"File": filename + "_vs_" + filename_comp, "SSIM": ssim_score},
│ │ -                    ignore_index=True,
│ │ -                )
│ │ +    # Load vgg16 model
│ │ +    vgg16 = VGG16(include_top=False, weights="imagenet", input_tensor=_inputs)
│ │  
│ │ -                print(f"Image {filename} compared to Image {filename_comp}")
│ │ +    # Get encoder part
│ │ +    # skip connections
│ │ +    s1 = vgg16.get_layer("block1_conv2").output  # 256
│ │ +    s2 = vgg16.get_layer("block2_conv2").output  # 128
│ │ +    s3 = vgg16.get_layer("block3_conv3").output  # 64
│ │ +    s4 = vgg16.get_layer("block4_conv3").output  # 32
│ │ +
│ │ +    # Get bottleneck/bridge part
│ │ +    b1 = vgg16.get_layer("block5_conv3").output  # 16
│ │ +
│ │ +    # Get decoder part
│ │ +    d1 = decoder_block(inputs=b1, skip_features=s4, num_filters=512)
│ │ +    d2 = decoder_block(inputs=d1, skip_features=s3, num_filters=256)
│ │ +    d3 = decoder_block(inputs=d2, skip_features=s2, num_filters=128)
│ │ +    d4 = decoder_block(inputs=d3, skip_features=s1, num_filters=64)
│ │ +
│ │ +    # Model outputs
│ │ +    _outputs = Conv2D(
│ │ +        filters=1, kernel_size=(1, 1), padding="same", activation="sigmoid"
│ │ +    )(d4)
│ │ +    model = Model(_inputs, _outputs, name="VGG16_U-Net")
│ │ +
│ │ +    return model
│ │ +
│ │ +
│ │ +def IoU(y_true, y_pred, smooth: int = 1) -> float:
│ │ +    """Function to compute the intersection over union score (IoU),
│ │ +    a measure of prediction accuracy. This is sometimes also called
│ │ +    Jaccard score.
│ │ +
│ │ +    The IoU can be used as a loss metric during binary segmentation when
│ │ +    convolutional neural networks are applied. The IoU is calculated for
│ │ +    both the training and validation set.
│ │  
│ │ -        # Calculate mean
│ │ -        dataframe = dataframe.append(
│ │ -            {"MeanSSIM": np.mean(dataframe["SSIM"])}, ignore_index=True
│ │ -        )
│ │ -        # Define excelpath
│ │ +    Parameters
│ │ +    ----------
│ │ +    y_true : tf.Tensor
│ │ +        True positive image segmentation label predefined by the user.
│ │ +        This is the mask that is provided prior to model training.
│ │ +    y_pred : tf.Tensor
│ │ +        Predicted image segmentation by the network.
│ │ +    smooth : int, default = 1
│ │ +        Smoothing operator applied during final calculation of
│ │ +        IoU. Must be non-negative and non-zero.
│ │  
│ │ -        self.saveResults(path=self.root, dataframe=dataframe)
│ │ +    Returns
│ │ +    -------
│ │ +    iou : tf.Tensor
│ │ +        IoU representation in the same shape as y_true, y_pred.
│ │  
│ │ -    def calculateNoise(self, img: np.ndarray, crop: int = 0) -> float:
│ │ -        """
│ │ -        Instance method to calculate image quality/comparability parameters.
│ │ -
│ │ -        The quality parameters we use are mean pixel values (mittel),
│ │ -        standard deviation of pixel values (std_dev), an image noise estimate (sigma),
│ │ -        an image blur estimate (blur). We decided to use these parameters
│ │ -        because we deemed them to be most valid and relevant for ultrasound images.
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        img : np.ndarray
│ │ -            Grayscale image to be analysed as an numpy array. The image must
│ │ -            be loaded prior to bounding box selection, specifying a path
│ │ -            is not valid.
│ │ -        crop : int, default = 0
│ │ -            Integer variable determining if image is cropped.
│ │ -            If crop == 1, user must select bounding box. Must be either 1 or 0.
│ │ -
│ │ -        Returns
│ │ -        -------
│ │ -        mittel : float
│ │ -            Float value containing average of pixel values. This values lies
│ │ -            somewhere between 0 and 255. It is calculated either on all image
│ │ -            pixels or the pixels in the bounding box if crop == 1.
│ │ -        std_dev : float
│ │ -            Float value containing average of pixel values. This value
│ │ -            can be understood as a proxy for the dynamic range, as the "variablity"
│ │ -            of pixel values is captured here. It is calculated either on all image
│ │ -            pixels or the pixels in the bounding box if crop == 1.
│ │ -        sigma : float
│ │ -            Float value containing the noise estimation in the input image. This
│ │ -            value is calculated using the skimage.restoration.estimate_sigma
│ │ -            function. It is calculated either on all image pixels or the pixels
│ │ -            in the bounding box if crop == 1.
│ │ -        blur : float
│ │ -            Float value containing the estimate of image blur. The blur estimate
│ │ -            is calculated using a Laplacian filter (usually used for edge filtering)
│ │ -            and the resulting variance of the filter output. It is calculated either
│ │ -            on all image pixels or the pixels in the bounding box if crop == 1.
│ │ -
│ │ -        Examples
│ │ -        --------
│ │ -        >>> calculateNoise(img=([[[[0.22414216 0.19730392 0.22414216] ... [0.2509804  0.2509804  0.2509804 ]]]))
│ │ -        20, 55.686, 3.379, 0.361, 3834.921418
│ │ -        """
│ │ -        # Check if cropping is specified
│ │ -        if crop == 1:
│ │ -            # crop image according to selection
│ │ -            img = self.getBoundingBox(img=img)
│ │ -
│ │ -        # Calculate SNR
│ │ -        sig_noi_rat = round(float(self.getSignalToNoise(img=img)), 3)
│ │ -
│ │ -        # Estimat image noise using Laplacian filter
│ │ -        # The higher the blur value the better
│ │ -        blur = cv2.Laplacian(img, cv2.CV_64F).var()
│ │ -
│ │ -        # flatten image
│ │ -        pixels = img.ravel()
│ │ -
│ │ -        # Calculate spacial parameters
│ │ -        mittel = round(np.mean(pixels))
│ │ -        std_dev = round(pixels.std(), 3)
│ │ -
│ │ -        # estimate Sigma as representative for noise
│ │ -        sigma = round(restoration.estimate_sigma(img), 3)
│ │ -
│ │ -        return mittel, std_dev, sigma, sig_noi_rat, blur
│ │ -
│ │ -    def makeImageContrast(
│ │ -        self, img: np.ndarray, brightness: int, contrast: int
│ │ -    ) -> np.ndarray:
│ │ -        """
│ │ -        Instance method to change image contrast and/or brightness.
│ │ -
│ │ -        Both, the image brightness and the image contrast can be augmented either
│ │ -        seperately or simultaneously. Both values are adapted using cv2.addWeighted
│ │ -        which basically performs a linear transform of pixel values using
│ │ -        the input parameters brightness and contrast.
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        img : np.ndarray
│ │ -            Grayscale image to be analysed as an numpy array. The image must
│ │ -            be loaded prior to augmentation, specifying a path
│ │ -            is not valid.
│ │ -        brightness : int
│ │ -            Integer value by which the brightness should be increased/decreased.
│ │ -            Value can be negative, zero or positive.
│ │ -        contrast : int
│ │ -            Integer value by which the contrast should be increased/decreased.
│ │ -            Value can be negative, zero or positive.
│ │ -
│ │ -        Returns
│ │ -        -------
│ │ -        bc_img : np.ndarray
│ │ -            The augmented grayscale image based on contrast/brightness value as a numpy array.
│ │ -            The image can be binary if the contrast is increased by a high enough value.
│ │ -        """
│ │ -        # Adapt brightness
│ │ -        if brightness != 0:
│ │ -            if brightness > 0:
│ │ -                shadow = brightness
│ │ -                highlight = 255
│ │ -            else:
│ │ -                shadow = 0
│ │ -                highlight = 255 + brightness
│ │ -            alpha_b = (highlight - shadow) / 255
│ │ -            gamma_b = shadow
│ │ +    Notes
│ │ +    -----
│ │ +    The IoU is usually calculated as IoU = intersection / union.
│ │ +    The intersection is calculated as the overlap of y_true and
│ │ +    y_pred, whereas the union is the sum of y_true and y_pred.
│ │ +
│ │ +    Examples
│ │ +    --------
│ │ +    >>> IoU(y_true=Tensor("IteratorGetNext:1", shape=(1, 512, 512, 1), dtype=float32),
│ │ +            y_pred=Tensor("VGG16_U-Net/conv2d_8/Sigmoid:0",
│ │ +            shape=(1, 512, 512, 1), dtype=float32),
│ │ +            smooth=1)
│ │ +    Tensor("truediv:0", shape=(1, 512, 512), dtype=float32)
│ │ +    """
│ │ +    # Caclulate Intersection
│ │ +    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)
│ │ +    # Calculate Union
│ │ +    union = K.sum(y_true, -1) + K.sum(y_pred, -1) - intersection
│ │ +    # Calculate IoU
│ │ +    iou = (intersection + smooth) / (union + smooth)
│ │  
│ │ -            # add weighted is basically y = a*(img) + b for brightness and contrast of pixels
│ │ -            bc_img = cv2.addWeighted(img, alpha_b, img, 0, gamma_b)
│ │ -        else:
│ │ -            bc_img = img.copy()
│ │ +    return iou
│ │  
│ │ -        # Adapt contrast
│ │ -        if contrast != 0:
│ │ -            f = 131 * (contrast + 127) / (127 * (131 - contrast))
│ │ -            alpha_c = f
│ │ -            gamma_c = 127 * (1 - f)
│ │ -
│ │ -            bc_img = cv2.addWeighted(bc_img, alpha_c, bc_img, 0, gamma_c)
│ │ -
│ │ -        return bc_img
│ │ -
│ │ -    def makeImageNoise(
│ │ -        self, img: np.ndarray, var: float, mean: float, noise: str = "speckle"
│ │ -    ) -> np.ndarray:
│ │ -        """
│ │ -        Instance method to add noise to an image.
│ │ -
│ │ -        The noise that is added is either speckle noise or gaussian
│ │ -        noise based on user specefication. Based on specification the
│ │ -        extend of the added noise can be altered via mean and variance
│ │ -        specification. The noise is added using the skimage.util.random_noise
│ │ -        function.
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        img : np.ndarray
│ │ -            Grayscale image to be analysed as an numpy array. The image must
│ │ -            be loaded prior to augmentation, specifying a path
│ │ -            is not valid.
│ │ -        var : int
│ │ -            Float value defining the variance of the noise distribution.
│ │ -            The larger the variance, the more intense the noise.
│ │ -        mean : int
│ │ -            Float value defining the mean of the noise distribution.
│ │ -            This shifts the whole noise appearance.
│ │ -        noise : str, default = speckle
│ │ -            String value containing specification what noiseis to be applied.
│ │ -            There are two options of noise:
│ │ -            - noise = "speckle"
│ │ -            - noise = "gaussian"
│ │ -            An error will be raised when another option is specified.
│ │ -
│ │ -        Returns
│ │ -        -------
│ │ -        n_img : np.ndarray
│ │ -            The augmented grayscale image based on noise specification as a numpy array.
│ │ -        """
│ │ -        # Check if image needs to be adapted
│ │ -        if var == 0 & mean == 0:
│ │ -            n_img = img.copy()
│ │  
│ │ -        else:
│ │ -            # Check which noise type should be applied
│ │ -            if noise == "speckle":
│ │ -                # Apply specle noise with defined varaince & mean
│ │ -                n_img = util.random_noise(img, mode="speckle", mean=mean, var=var)
│ │ -                # The above returns floating point on rang [0,1] so it need to be converted
│ │ -                n_img = np.array(255 * n_img, dtype="uint8")
│ │ -            else:
│ │ -                # Apply gaussian noise with defined varaince & mean
│ │ -                n_img = util.random_noise(img, mode="gaussian", mean=mean, var=var)
│ │ -                # The above returns floating point on rang [0,1] so it need to be converted
│ │ -                n_img = np.array(255 * n_img, dtype="uint8")
│ │ -
│ │ -        return n_img
│ │ -
│ │ -    def makeImageBlur(self, img: np.ndarray, sigma: int) -> np.ndarray:
│ │ -        """
│ │ -        Instance method to blur the input image.
│ │ -
│ │ -        The blur applied to the image is gaussian blur. To apply the blur,
│ │ -        the cv2.GuassianBlur function is used. If the sigma values equals 0,
│ │ -        then the input image is returned.
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        img : np.ndarray
│ │ -            Grayscale image to be analysed as an numpy array. The image must
│ │ -            be loaded prior to augmentation, specifying a path
│ │ -            is not valid.
│ │ -        sigma : int
│ │ -            Integer value defining the sigma used for the gaussian kernel
│ │ -            during blurring operation. The higher the sigma, the higher
│ │ -            the applied blur. When sigma = 0, the original value is returned.
│ │ -
│ │ -        Returns
│ │ -        -------
│ │ -        b_img : np.ndarray
│ │ -            The augmented grayscale image based on blur specification as a numpy array.
│ │ -        """
│ │ -        if sigma == 0:
│ │ -            b_img = img.copy()
│ │ +def dice_score(y_true, y_pred) -> float:
│ │ +    """Function to compute the Dice score, a measure of prediction accuracy.
│ │  
│ │ -        else:
│ │ -            # Apply gaussian blur
│ │ -            b_img = cv2.GaussianBlur(img, (11, 11), sigmaX=sigma, sigmaY=sigma)
│ │ +    The Dice score can be used as a loss metric during binary segmentation when
│ │ +    convolutional neural networks are applied. The Dice score is calculated
│ │ +    for both the training and validation set.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    y_true : tf.Tensor
│ │ +        True positive image segmentation label predefined by the user.
│ │ +        This is the mask that is provided prior to model training.
│ │ +    y_pred : tf.Tensor
│ │ +        Predicted image segmentation by the network.
│ │ +
│ │ +    Returns
│ │ +    -------
│ │ +    score : tf.Tensor
│ │ +        Dice score representation in the same shape as y_true, y_pred.
│ │ +
│ │ +    Notes
│ │ +    -----
│ │ +    The IoU is usually calculated as Dice = 2 * intersection / union.
│ │ +    The intersection is calculated as the overlap of y_true and
│ │ +    y_pred, whereas the union is the sum of y_true and y_pred.
│ │ +
│ │ +    Examples
│ │ +    --------
│ │ +    >>> IoU(y_true=Tensor("IteratorGetNext:1", shape=(1, 512, 512, 1),
│ │ +            dtype=float32),
│ │ +            y_pred=Tensor("VGG16_U-Net/conv2d_8/Sigmoid:0",
│ │ +            shape=(1, 512, 512, 1), dtype=float32),
│ │ +            smooth=1)
│ │ +    Tensor("dice_score/truediv:0", shape=(1, 512, 512), dtype=float32)
│ │ +    """
│ │ +    # Cacluate intersection
│ │ +    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)
│ │ +    # Calculate Dice score
│ │ +    score = (2.0 * intersection) / (K.sum(y_true, -1) + K.sum(y_pred, -1))
│ │ +    print("Score: ", score, type(score))
│ │ +
│ │ +    return score
│ │  
│ │ -        return b_img
│ │  
│ │ -    def adaptImages(
│ │ -        self,
│ │ -        brightness: int,
│ │ -        contrast: int,
│ │ -        mean: int,
│ │ -        variance: int,
│ │ -        save_img: str,
│ │ -        filetype: str,
│ │ -        sigma_b: int = 0,
│ │ -    ):
│ │ -        """
│ │ -        Instance method to adapt brightness, contrast and noise of an image. Also,
│ │ -        the quality of an image is assessed.
│ │ -
│ │ -        The created images and the results are saved to the specified
│ │ -        output directory. When all image augmentation parameters are set to 0,
│ │ -        the images in the root directory are not augmented rather their quality
│ │ -        is evaluated. Therefore, when only the image quality should be evaluated,
│ │ -        all augmentation parameters should be set to 0.
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        brightness : int
│ │ -            Integer value by which the brightness should be increased/decreased.
│ │ -            Value can be negative, zero or positive.
│ │ -        contrast : int
│ │ -            Integer value by which the contrast should be increased/decreased.
│ │ -            Value can be negative, zero or positive.
│ │ -        var : int
│ │ -            Float value defining the variance of the noise distribution.
│ │ -            The larger the variance, the more intense the noise.
│ │ -        mean : int
│ │ -            Float value defining the mean of the noise distribution.
│ │ -            This shifts the whole noise appearance.
│ │ -        save_img : str
│ │ -            String value defining if the adapted should be saved.
│ │ -            There are two options for save_img:
│ │ -            - save_img = "Yes". Images will be saved.
│ │ -            - save_img = "No". Images will not be saved.
│ │ -            An error will be raised when something other is specified.
│ │ -        sigma_b : int, default = 0
│ │ -            Integer value defining the sigma used for the gaussian kernel
│ │ -            during blurring operation. The higher the sigma, the higher
│ │ -            the applied blur. When sigma = 0, the original value is returned.
│ │ -
│ │ -        See Also
│ │ -        --------
│ │ -        makeImageContrast, makeImageNoise, calculateNoise, makeImageBlur,
│ │ -        saveResults function.
│ │ -
│ │ -        Examples
│ │ -        --------
│ │ -        >>> adaptImages(brightness=0, contrast=0, mean=0, variance=0,
│ │ -                            filetype=/**/*.tif, sigma_b=0, save_img="No")
│ │ -        """
│ │ -        # Get all image files to be adapted
│ │ -        list_of_files = glob.glob(self.root + filetype, recursive=True)
│ │ -
│ │ -        # Create DF for export
│ │ -        dataframe = pd.DataFrame(
│ │ -            columns=["File", "Mean", "Standard Dev", "Sigma", "SNR", "Blur", "ImScore"]
│ │ +def focal_loss(y_true, y_pred, alpha: float = 0.8, gamma: float = 2) -> float:
│ │ +    """Function to compute the focal loss, a measure of prediction accuracy.
│ │ +
│ │ +    The focal loss can be used as a loss metric during binary segmentation when
│ │ +    convolutional neural networks are applied. The focal loss score is
│ │ +    calculated for both, the training and validation set. The focal loss
│ │ +    is specifically applicable when class imbalances, i.e. between foregroung
│ │ +    (muscle aponeurosis) and background (not muscle aponeurosis), are existent.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    y_true : tf.Tensor
│ │ +        True positive image segmentation label predefined by the user.
│ │ +        This is the mask that is provided prior to model training.
│ │ +    y_pred : tf.Tensor
│ │ +        Predicted image segmentation by the network.
│ │ +    alpha : float, default = 0.8
│ │ +        Coefficient used on positive exaples, must be non-negative and
│ │ +        non-zero.
│ │ +    gamma : float, default = 2
│ │ +        Focussing parameter, must be non-negative and non-zero.
│ │ +
│ │ +    Returns
│ │ +    -------
│ │ +    f_loss : tf.Tensor
│ │ +        Tensor containing the calculated focal loss score.
│ │ +
│ │ +    Examples
│ │ +    --------
│ │ +    >>> IoU(y_true=Tensor("IteratorGetNext:1", shape=(1, 512, 512, 1),
│ │ +            dtype=float32),
│ │ +            y_pred=Tensor("VGG16_U-Net/conv2d_8/Sigmoid:0",
│ │ +            shape=(1, 512, 512, 1), dtype=float32),
│ │ +            smooth=1)
│ │ +    Tensor("focal_loss/Mean:0", shape=(), dtype=float32)
│ │ +    """
│ │ +    # Cacluate binary crossentropy loss
│ │ +    BCE = K.binary_crossentropy(y_true, y_pred)
│ │ +    # calculate exponentiated BCE
│ │ +    BCE_EXP = K.exp(-BCE)
│ │ +    # calculate focal loss
│ │ +    f_loss = K.mean(alpha * K.pow((1 - BCE_EXP), gamma) * BCE)
│ │ +
│ │ +    return f_loss
│ │ +
│ │ +
│ │ +def loadImages(img_path: str, mask_path: str) -> list:
│ │ +    """Function to load images and manually labeled masks from a specified
│ │ +    directory.
│ │ +
│ │ +    The images and masks are loaded, resized and normalized in order
│ │ +    to be suitable and usable for model training. The specified directories
│ │ +    must lead to the images and masks. The number of images and masks must be
│ │ +    equal. The images and masks can be in any common image format.
│ │ +    The names of the images and masks must match. The image and corresponding
│ │ +    mask must have the same name.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    img_path : str
│ │ +        Path that leads to the directory containing the training images.
│ │ +        Image must be in RGB format.
│ │ +    mask_path : str
│ │ +        Path that leads to the directory containing the mask images.
│ │ +        Masks must be binary.
│ │ +
│ │ +    Returns
│ │ +    -------
│ │ +    train_imgs : np.ndarray
│ │ +        Resized, normalized training images stored in a numpy array.
│ │ +    mask_imgs : np.ndarray
│ │ +        Resized, normalized training masks stored in a numpy array.
│ │ +
│ │ +    Notes
│ │ +    -----
│ │ +    See labelling instruction for correct masks creation and use,
│ │ +    if needed, the supplied ImageJ script to label your images.
│ │ +
│ │ +    Example
│ │ +    -------
│ │ +    >>> loadImages(img_path = "C:/Users/admin/Dokuments/images",
│ │ +                   mask_path = "C:/Users/admin/Dokuments/masks")
│ │ +    train_imgs([[[[0.22414216 0.19730392 0.22414216] ... [0.22414216 0.19730392 0.22414216]]])
│ │ +    mask_imgs([[[[0.] ... [0.]]])
│ │ +    """
│ │ +    # Images will be re-scaled accordingly
│ │ +    im_width = 512
│ │ +    im_height = 512
│ │ +
│ │ +    # list of all images in the path
│ │ +    ids = os.listdir(img_path)
│ │ +    print("Total no. of images = ", len(ids))
│ │ +
│ │ +    # Create empty numpy arrays
│ │ +    train_imgs = np.zeros((len(ids), im_height, im_width, 3), dtype=np.float32)
│ │ +    train_masks = np.zeros((len(ids), im_height, im_width, 1),
│ │ +                           dtype=np.float32)
│ │ +
│ │ +    # ´Loop through list of ids found in img_path and mask_path
│ │ +    for n, id_ in enumerate(tqdm(ids)):
│ │ +        # Load and resize image
│ │ +        img = load_img(img_path + id_, color_mode="rgb")
│ │ +        img = img_to_array(img)
│ │ +        img = resize(
│ │ +            img, (im_width, im_height, 3), mode="constant", preserve_range=True
│ │ +        )
│ │ +
│ │ +        # Load and resize mask
│ │ +        mask = img_to_array(load_img(mask_path + id_, color_mode="grayscale"))
│ │ +        mask = resize(
│ │ +            mask, (im_width, im_height, 1), mode="constant",
│ │ +            preserve_range=True
│ │          )
│ │  
│ │ -        # Test for image files in directory
│ │ -        if len(list_of_files) > 1:
│ │ +        # Normalize image & mask and insert in array
│ │ +        train_imgs[n] = img / 255.0
│ │ +        train_masks[n] = mask / 255.0
│ │ +
│ │ +    return train_imgs, train_masks
│ │ +
│ │ +
│ │ +def trainModel(
│ │ +    img_path: str,
│ │ +    mask_path: str,
│ │ +    out_path: str,
│ │ +    batch_size: int,
│ │ +    learning_rate: float,
│ │ +    epochs: int,
│ │ +    loss: str,
│ │ +    gui,
│ │ +) -> None:
│ │ +    """Function to train a convolutional neural network with VGG16 encoder and
│ │ +    U-net decoder. All the steps necessary to properly train an neural
│ │ +    network are included in this function.
│ │ +
│ │ +    This functions build upon all the other functions included in this module.
│ │ +    Given that all input parameters are correctly specified, the images and
│ │ +    masks are loaded, splittet into test and training sets, the model is
│ │ +    compiled according to user specification and the model is trained.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    img_path : str
│ │ +        Path that leads to the directory containing the training images.
│ │ +        Image must be in RGB format.
│ │ +    mask_path : str
│ │ +        Path that leads to the directory containing the mask images.
│ │ +        Masks must be binary.
│ │ +    out_path:
│ │ +        Path that leads to the directory where the trained model is saved.
│ │ +    batch_size : int
│ │ +        Integer value that determines the batch size per iteration through the
│ │ +        network during model training. Although a larger batch size has
│ │ +        advantages during model trainig, the images used here are large. Thus,
│ │ +        the larger the batch size, the more compute power is needed or the
│ │ +        longer the training duration. Must be non-negative and non-zero.
│ │ +    learning_rate : float
│ │ +        Float value determining the learning rate used during model training.
│ │ +        Must be non-negative and non-zero.
│ │ +    epochs : int
│ │ +        Integer value that determines the amount of epochs that the model
│ │ +        is trained befor training is aborted. The total amount of epochs
│ │ +        will only be used if early stopping does not happen.
│ │ +        Must be non-negative and non-zero.
│ │ +    loss : {"BCE"}
│ │ +        String variable that determines the loss function used during training.
│ │ +        So far, only one type is supported here:
│ │ +        - Binary cross-entropy. loss == "BCE"
│ │ +    gui : tk.TK
│ │ +        A tkinter.TK class instance that represents a GUI. By passing this
│ │ +        argument, interaction with the GUI is possible i.e., stopping
│ │ +        the model training model process.
│ │  
│ │ -            # Iterate through images
│ │ -            for image in list_of_files:
│ │ +    Notes
│ │ +    -----
│ │ +    For specific explanations for the included functions see the respective
│ │ +    function docstrings in this module.
│ │ +    This function can either be run from the command prompt or is called
│ │ +    by the GUI. Note that the functioned was specifically designed to be
│ │ +    called from the GUI. Thus, tk.messagebox will pop up when errors are
│ │ +    raised even if the GUI is not started.
│ │ +
│ │ +    Examples
│ │ +    --------
│ │ +    >>> trainModel(img_path= "C:/Users/admin/Dokuments/images",
│ │ +                   mask_path="C:/Users/admin/Dokuments/masks",
│ │ +                   out_path="C:/Users/admin/Dokuments/results",
│ │ +                   batch_size=1, learning_rate=0.005,
│ │ +                   epochs=3, loss="BCE", gui)
│ │ +
│ │ +    """
│ │ +    # Check input paramters
│ │ +    if batch_size <= 0 or learning_rate <= 0 or epochs <= 0:
│ │ +        # Make sure some kind of filetype is specified.
│ │ +        tk.messagebox.showerror(
│ │ +            "Information", "Training parameters must be non-zero" +
│ │ +            " and non-negative."
│ │ +        )
│ │ +        gui.should_stop = False
│ │ +        gui.is_running = False
│ │ +        gui.do_break()
│ │ +        return
│ │ +
│ │ +    # Images will be re-scaled accordingly
│ │ +    im_width = 512
│ │ +    im_height = 512
│ │ +
│ │ +    # Adapt folder paths
│ │ +    # This is necessary to concattenate id to path
│ │ +    img_path = img_path + "/"
│ │ +    mask_path = mask_path + "/"
│ │ +    out_path = out_path + "/"
│ │ +
│ │ +    try:
│ │ +        # Load images
│ │ +        train_imgs, train_masks = loadImages(img_path=img_path,
│ │ +                                             mask_path=mask_path)
│ │ +
│ │ +        # Inform user in GUI
│ │ +        cont = tk.messagebox.askokcancel(
│ │ +            "Information",
│ │ +            "Images & Masks were successfully loaded!" +
│ │ +            "\nDou you wish to proceed?",
│ │ +        )
│ │ +        if cont is True:
│ │  
│ │ -                # load image
│ │ -                img = cv2.imread(image, 0)
│ │ -                img = cv2.resize(img, (512, 512))
│ │ -                filename = os.path.splitext(os.path.basename(image))[0]
│ │ -
│ │ -                # Adapt image brightness and contrast
│ │ -                adapted_img = self.makeImageContrast(
│ │ -                    img=img, brightness=brightness, contrast=contrast
│ │ +            # Prepare data for model training
│ │ +            # Split data into training and validation
│ │ +            img_train, img_valid, mask_train, mask_valid = train_test_split(
│ │ +                train_imgs, train_masks, test_size=0.1, random_state=42
│ │ +            )
│ │ +
│ │ +            # Compose the VGG16 Unet model for aponeurosis detection
│ │ +            # Compile the aponeurosis model VGG16
│ │ +            VGG16_UNet = build_vgg16_unet((im_width, im_height, 3))
│ │ +            model_apo = VGG16_UNet
│ │ +
│ │ +            # Decide which loss metric is used
│ │ +            if loss == "BCE":
│ │ +                model_apo.compile(
│ │ +                    optimizer=Adam(),
│ │ +                    loss="binary_crossentropy",
│ │ +                    metrics=["accuracy", IoU],
│ │                  )
│ │ -                # Adapt image noise
│ │ -                adapted_img = self.makeImageNoise(
│ │ -                    img=img, var=variance, mean=mean, noise="gaussian"
│ │ +            elif loss == "Dice":
│ │ +                model_apo.compile(
│ │ +                    optimizer=Adam(), loss=dice_score,
│ │ +                    metrics=["accuracy", IoU]
│ │                  )
│ │ +            elif loss == "FL":
│ │ +                model_apo.compile(
│ │ +                    optimizer=Adam(), loss=focal_loss,
│ │ +                    metrics=["accuracy", IoU]
│ │ +                )
│ │ +            else:
│ │ +                raise TypeError("Specify correct loss metric.")
│ │  
│ │ -                # Adapt image blur
│ │ -                adapted_img = self.makeImageBlur(img=img, sigma=sigma_b)
│ │ +            # Show a summary of the model structure
│ │ +            model_apo.summary()
│ │  
│ │ -                # Calculate Noise parameters
│ │ -                mittel, std_dev, sigma, sig_noi_rat, blur = self.calculateNoise(
│ │ -                    img=adapted_img
│ │ +            # VGG16
│ │ +            # Set some training parameters
│ │ +            callbacks = [
│ │ +                EarlyStopping(patience=8, verbose=1),
│ │ +                ReduceLROnPlateau(
│ │ +                    factor=0.1, patience=10, min_lr=learning_rate, verbose=1
│ │ +                ),
│ │ +                ModelCheckpoint(
│ │ +                    out_path + "Test_Apo.h5",
│ │ +                    verbose=1,
│ │ +                    save_best_only=True,
│ │ +                    save_weights_only=False,
│ │ +                ),  # Give the model a name (the .h5 part)
│ │ +                CSVLogger(out_path + "Test_apo.csv", separator=",",
│ │ +                          append=False),
│ │ +            ]
│ │ +
│ │ +            # Inform user in GUI
│ │ +            cont2 = tk.messagebox.askokcancel(
│ │ +                "Information",
│ │ +                "Model was successfully compiled!" +
│ │ +                "\nDo you wish to proceed?",
│ │ +            )
│ │ +            # User chose to continue
│ │ +            if cont2 is True:
│ │ +                # VGG16
│ │ +                results = model_apo.fit(
│ │ +                    img_train,
│ │ +                    mask_train,
│ │ +                    batch_size=batch_size,
│ │ +                    epochs=epochs,
│ │ +                    callbacks=callbacks,
│ │ +                    validation_data=(img_valid, mask_valid),
│ │                  )
│ │  
│ │ -                # Take weighted geometric mean
│ │ -                params = np.array([mittel, std_dev, sigma, sig_noi_rat, blur])
│ │ -                geo_m = stats.gmean(params)
│ │ -
│ │ -                # Append to dataframe
│ │ -                dataframe = dataframe.append(
│ │ -                    {
│ │ -                        "File": filename,
│ │ -                        "Mean": mittel,
│ │ -                        "Standard Dev": std_dev,
│ │ -                        "Sigma": sigma,
│ │ -                        "SNR": sig_noi_rat,
│ │ -                        "Blur": blur,
│ │ -                        "ImScore": geo_m,
│ │ -                    },
│ │ -                    ignore_index=True,
│ │ +                # Inform user in GUI
│ │ +                tk.messagebox.showinfo(
│ │ +                    "Information",
│ │ +                    "Model was successfully trained"
│ │ +                    + "\nResults are saved to specified output path.",
│ │                  )
│ │  
│ │ -                if save_img == "Yes":
│ │ -                    cv2.imwrite(
│ │ -                        self.out + "/" + filename + "adapt" + ".tif", adapted_img
│ │ -                    )
│ │ +                # Variables stored in results.history: val_loss, val_acc,
│ │ +                # val_IoU, loss, acc, IoU, lr
│ │ +                fig, ax = plt.subplots(1, 2, figsize=(7, 7))
│ │ +                ax[0].plot(results.history["loss"],
│ │ +                           label="Training loss")
│ │ +                ax[0].plot(results.history["val_loss"],
│ │ +                           label="Validation loss")
│ │ +                ax[0].set_title("Learning curve")
│ │ +                ax[0].plot(
│ │ +                    np.argmin(results.history["val_loss"]),
│ │ +                    np.min(results.history["val_loss"]),
│ │ +                    marker="x",
│ │ +                    color="r",
│ │ +                    label="best model",
│ │ +                )
│ │ +                ax[0].set_xlabel("Epochs")
│ │ +                ax[0].set_ylabel("log_loss")
│ │ +                ax[0].legend()
│ │ +
│ │ +                ax[1].plot(results.history["val_IoU"], label="Training IoU")
│ │ +                ax[1].plot(results.history["IoU"], label="Validation IoU")
│ │ +                ax[1].set_title("IoU curve")
│ │ +                ax[1].set_xlabel("Epochs")
│ │ +                ax[1].set_ylabel("IoU score")
│ │ +                ax[1].legend()
│ │ +                plt.savefig(out_path + "Training_Results.tif")
│ │  
│ │ -            # Save Results
│ │ -            self.saveResults(path=self.out, dataframe=dataframe)
│ │ +            else:
│ │ +                # User cancelled process after model compilation
│ │ +                # clean up
│ │ +                gui.do_break()
│ │ +                gui.should_stop = False
│ │ +                gui.is_running = False
│ │  
│ │          else:
│ │ -            raise NameError("No files found in directory")
│ │ -
│ │ -
│ │ -# ---------------------------------------------------------------------------------------------------
│ │ -# Usage from command prompt
│ │ -# Specify the values according to your needs and uncomment the codeblock below.
│ │ -# To run type <<< python image_quality.py >>>
│ │ -
│ │ -# if __name__ == '__main__':
│ │ -
│ │ -#     brightness = 0
│ │ -#     contrast = 0
│ │ -#     mean = 0
│ │ -#     variance = 0
│ │ -#     sigma = 0
│ │ -#     root = "C:/Users/admin/Documents/images"
│ │ -#     out = "C:/Users/admin/Documents"
│ │ -
│ │ -#     imqual = ImageQuali(rootpath=root, outpath=out)
│ │ -#     imqual.adaptImages(brightness=brightness, contrast=contrast, mean=mean, variance=variance,
│ │ -#                        sigma_b=sigma, save_img="No")
│ │ +            # User cancelled process after image loading
│ │ +            # clean up
│ │ +            gui.do_break()
│ │ +            gui.should_stop = False
│ │ +            gui.is_running = False
│ │ +
│ │ +    # Error handling
│ │ +    except ValueError:
│ │ +        tk.messagebox.showerror(
│ │ +            "Information",
│ │ +            "Check input parameters."
│ │ +            + "\nPotential error sources:"
│ │ +            + "\n - Training parameters invalid",
│ │ +        )
│ │ +        # clean up
│ │ +        gui.do_break()
│ │ +        gui.should_stop = False
│ │ +        gui.is_running = False
│ │ +
│ │ +    except FileNotFoundError:
│ │ +        tk.messagebox.showerror(
│ │ +            "Information",
│ │ +            "Check input directories."
│ │ +            + "\nPotential error sources:"
│ │ +            + "\n - Invalid specified input directories"
│ │ +            + "\n - Unequal number of images or masks"
│ │ +            + "\n - Names for images and masks don't match",
│ │ +        )
│ │ +        # clean up
│ │ +        gui.do_break()
│ │ +        gui.should_stop = False
│ │ +        gui.is_running = False
│ │ +
│ │ +    except PermissionError:
│ │ +        tk.messagebox.showerror(
│ │ +            "Information",
│ │ +            "Check input directories."
│ │ +            + "\nPotential error sources:"
│ │ +            + "\n - Invalid specified input directories",
│ │ +        )
│ │ +        # clean up
│ │ +        gui.do_break()
│ │ +        gui.should_stop = False
│ │ +        gui.is_running = False
│ │ +
│ │ +    finally:
│ │ +        # clean up
│ │ +        gui.should_stop = False
│ │ +        gui.is_running = False
│ │ ├── encoding
│ │ │ @@ -1 +1 @@
│ │ │ -us-ascii
│ │ │ +utf-8
│ │   --- dl_track_us-0.1.1/DL_Track/gui_helpers/manual_tracing.py
│ ├── +++ dl_track_us-0.1.2/DL_Track_US/gui_helpers/manual_tracing.py
│ │┄ Files 10% similar despite different names
│ │ @@ -1,135 +1,141 @@
│ │  """
│ │  Description
│ │  -----------
│ │  This module contains a class to manually annotate longitudinal ultrasonography
│ │  images and videos. When the class is initiated, a graphical user interface is
│ │  opened. There, the user can annotate muscle fascicle length, pennation angle
│ │ -and muscle thickness. Moreover, the images can be scaled in order to get measurements
│ │ -in centimeters rather than pixels. By clicking the respective buttons in the GUI,
│ │ -the user can switch between the different parameters to analyze. The analysis is
│ │ -not restricted to any specific muscles. However, its use is restricted to a specific
│ │ -method for assessing muscle thickness, fascicle length and pennation angles.
│ │ -Moreover, each video frame is analyzed separately. An .xlsx file is retuned containing
│ │ -the analysis results for muscle fascicle length, pennation angle and muscle thickness.
│ │ +and muscle thickness. Moreover, the images can be scaled in order to get
│ │ +measurements in centimeters rather than pixels. By clicking the respective
│ │ +buttons in the GUI, the user can switch between the different parameters to
│ │ +analyze. The analysis is not restricted to any specific muscles. However,
│ │ +its use is restricted to a specific method for assessing muscle thickness,
│ │ +fascicle length and pennation angles. Moreover, each video frame is analyzed
│ │ +separately. An .xlsx file is retuned containing the analysis results for
│ │ +muscle fascicle length, pennation angle and muscle thickness.
│ │  
│ │  Functions scope
│ │  ---------------
│ │  For scope of the functions see class documentation.
│ │  
│ │  Notes
│ │  -----
│ │  Additional information and usage examples can be found at the respective
│ │  functions docstrings.
│ │  """
│ │  
│ │ -import glob
│ │  import math
│ │  import os
│ │  import tkinter as tk
│ │  from tkinter import messagebox, ttk
│ │  
│ │  import numpy as np
│ │  import pandas as pd
│ │  from PIL import Image, ImageGrab, ImageTk
│ │  from scipy.spatial import distance
│ │  
│ │  
│ │  class ManualAnalysis:
│ │ -    """
│ │ -    Python class to manually annotate longitudinal muscle
│ │ +    """Python class to manually annotate longitudinal muscle
│ │      ultrasonography images/videos of human lower limb muscles.
│ │      An analysis tkinter GUI is opened upon initialization of the class.
│ │      By clicking the buttons, the user can switch between different
│ │      parameters to analyze in the images.
│ │  
│ │      - Muscle thickness:
│ │ -                       Exactly one segment reaching from the superficial to the
│ │ -                       deep aponeuroses of the muscle must be drawn. If multiple
│ │ -                       measurement are drawn, these are averaged. Drawing can
│ │ -                       be started by clickling the left mouse button and keeping
│ │ -                       it pressed until it is not further required to draw the line
│ │ -                       (i.e., the other aponeurosis border is reached). Only the
│ │ -                       respective y-coordinates of the points where the cursor
│ │ -                       was clicked and released are considered for calculation of
│ │ -                       muscle thickness.
│ │ +                       Exactly one segment reaching from the superficial to
│ │ +                       the deep aponeuroses of the muscle must be drawn.
│ │ +                       If multiple measurement are drawn, these are averaged.
│ │ +                       Drawing can be started by clickling the left mouse
│ │ +                       button and keeping it pressed until it is not further
│ │ +                       required to draw the line (i.e., the other aponeurosis
│ │ +                       border is reached). Only the respective y-coordinates
│ │ +                       of the points where the cursor was clicked and released
│ │ +                       are considered for calculation of muscle thickness.
│ │      - Fascicle length:
│ │ -                      Exactly three segments along the fascicleof the muscle must
│ │ -                      be drawn. If multiple fascicle are drawn, their lengths are
│ │ -                      averaged. Drawing can be started by clickling the left mouse
│ │ -                      button and keeping it pressed until one segment is finished
│ │ -                      (mostly where fascicle curvature occurs the other aponeurosis
│ │ -                      border is reached). Using the euclidean distance, the total
│ │ +                      Exactly three segments along the fascicleof the muscle
│ │ +                      must be drawn. If multiple fascicle are drawn, their
│ │ +                      lengths are averaged. Drawing can be started by
│ │ +                      clickling the left mouse button and keeping it pressed
│ │ +                      until one segment is finished (mostly where fascicle
│ │ +                      curvature occurs the other aponeurosis border is
│ │ +                      reached). Using the euclidean distance, the total
│ │                        fascicle length is computed as a sum of the segments.
│ │      - Pennation angle:
│ │ -                      Exactly two segments, one along the fascicle orientation, the
│ │ -                      other along the aponeurosis orientation must be drawn. The line
│ │ -                      along the aponeurosis must be started where the line along the
│ │ -                      fascicle ends. If multiple angle are drawn, they are averaged.
│ │ -                      Drawing can be started by clickling the left mouse button and keeping
│ │ -                      it pressed until it is not further required to draw the line
│ │ -                      (i.e., the aponeurosis border is reached by the fascicle). The
│ │ -                      angle is calculated using the arc-tan function.
│ │ -    In order to scale the image/video frame, it is required to draw a line of length 10 milimeter
│ │ -    somewhere in the image. The line can be drawn in the same fashion as for example
│ │ -    the muscle thickness. Here however, the euclidean distance is used to calculate
│ │ -    the pixel / centimeter ratio. This has to be done for every frame.
│ │ -    We also provide the functionality to extend the muscle aponeuroses to more easily
│ │ -    extrapolate fascicles. The lines can be drawn in the same fashion as for example
│ │ -    the muscle thickness. During the analysis process, care must be taken to not
│ │ -    accidentally click the left mouse button as those coordinates might mess up the
│ │ -    results given that calculations are based on a strict analysis protocol.
│ │ +                      Exactly two segments, one along the fascicle
│ │ +                      orientation, the other along the aponeurosis orientation
│ │ +                      must be drawn. The line along the aponeurosis must be
│ │ +                      started where the line along the fascicle ends. If
│ │ +                      multiple angle are drawn, they are averaged. Drawing
│ │ +                      can be started by clickling the left mouse button and
│ │ +                      keeping it pressed until it is not further required to
│ │ +                      draw the line (i.e., the aponeurosis border is reached
│ │ +                      by the fascicle). The angle is calculated using the
│ │ +                      arc-tan function.
│ │ +    In order to scale the image/video frame, it is required to draw a line
│ │ +    of length 10 milimeter somewhere in the image. The line can be drawn in
│ │ +    the same fashion as for example the muscle thickness. Here however, the
│ │ +    euclidean distance is used to calculate the pixel / centimeter ratio.
│ │ +    This has to be done for every frame. We also provide the functionality
│ │ +    to extend the muscle aponeuroses to more easily extrapolate fascicles.
│ │ +    The lines can be drawn in the same fashion as for example the muscle
│ │ +    thickness. During the analysis process, care must be taken to not
│ │ +    accidentally click the left mouse button as those coordinates might
│ │ +    mess up the results given that calculations are based on a strict
│ │ +    analysis protocol.
│ │  
│ │      Attributes
│ │      ----------
│ │      self.image_list : list
│ │ -        A list variable containing the absolute paths to all images / video to be
│ │ -        analyzed.
│ │ +        A list variable containing the absolute paths to all images / video to
│ │ +        be analyzed.
│ │      self.rootpath : str
│ │          Path to root directory where images / videos for the analysis are
│ │          saved.
│ │      self.lines : list
│ │ -        A list variable containing all lines that are drawn upon the
│ │ -        image by the user. The list is emptied each time the analyzed
│ │ -        parameter is changed.
│ │ +        A list variable containing all lines that are drawn upon the image
│ │ +        by the user. The list is emptied each time the analyzed parameter
│ │ +        is changed.
│ │      self.scale_coords : list
│ │ -        A list variable containing the xy-coordinates coordinates of the scaling
│ │ -        line start- and endpoints to calculate the distance between.
│ │ +        A list variable containing the xy-coordinates coordinates of the
│ │ +        scaling line start- and endpoints to calculate the distance between.
│ │          The list is emptied each time a new image is scaled.
│ │      self.thick_coords : list
│ │ -        A list variable containing the xy-coordinates coordinates of the muscle
│ │ -        thickness line start- and endpoints to calculate the distance between.
│ │ -        The list is emptied each time a new image is scaled. Only the y-coordintes
│ │ -        are used for further analysis.
│ │ +        A list variable containing the xy-coordinates coordinates of the
│ │ +        muscle thickness line start- and endpoints to calculate the distance
│ │ +        between. The list is emptied each time a new image is scaled. Only
│ │ +        the y-coordintes are used for further analysis.
│ │      self.fasc_coords : list
│ │ -        A list variable containing the xy-coordinates coordinates of the fascicle
│ │ -        length line segments start- and endpoints to calculate the total length
│ │ -        of the fascicle. The list is emptied each time a new image is analyzed.
│ │ +        A list variable containing the xy-coordinates coordinates of the
│ │ +        fascicle length line segments start- and endpoints to calculate
│ │ +        the total length of the fascicle. The list is emptied each time a
│ │ +        new image is analyzed.
│ │      self.pen_coords : list
│ │ -        A list variable containing the xy-coordinates coordinates of the pennation
│ │ -        angle line segments start- and endpoints to calculate the angle
│ │ -        of the fascicle. The list is emptied each time a new image is analyzed.
│ │ +        A list variable containing the xy-coordinates coordinates of the
│ │ +        pennation angle line segments start- and endpoints to calculate
│ │ +        the angle of the fascicle. The list is emptied each time a new
│ │ +        image is analyzed.
│ │      self.coords : dict
│ │ -        Dictionary variable storing the xy-coordinates of mouse events during
│ │ -        analysis. Mouse events are clicking and releasing of the left mouse
│ │ -        button as well as dragging of the cursor.
│ │ +        Dictionary variable storing the xy-coordinates of mouse events
│ │ +        during analysis. Mouse events are clicking and releasing of the
│ │ +        left mouse button as well as dragging of the cursor.
│ │      self.count : int, default = 0
│ │ -        Index of image / video frame currently analysis in the list of image
│ │ -        file / video frame paths. The default is 0 as the first image / frame
│ │ -        analyzed alwas has the idex 0 in the list.
│ │ +        Index of image / video frame currently analysis in the list of
│ │ +        image file / video frame paths. The default is 0 as the first
│ │ +        image / frame analyzed always has the idex 0 in the list.
│ │      self.dataframe : pd.DataFrame
│ │ -        Panadas dataframe that stores the analysis results such as file name,
│ │ -        fascicle length, pennation angle and muscle thickness. This dataframe
│ │ -        is then saved in an .xlsx file.
│ │ +        Panadas dataframe that stores the analysis results such as file
│ │ +        name, fascicle length, pennation angle and muscle thickness.
│ │ +        This dataframe is then saved in an .xlsx file.
│ │      self.head : tk.TK
│ │ -        tk.Toplevel instance opening a window containing the manual image
│ │ -        analysis options.
│ │ +        tk.Toplevel instance opening a window containing the manual
│ │ +        image analysis options.
│ │      self.mode : tk.Stringvar, default = thick
│ │ -        tk.Stringvar variable containing the current parameter analysed by
│ │ -        the user. The parameters are
│ │ +        tk.Stringvar variable containing the current parameter analysed
│ │ +        by the user. The parameters are
│ │          - muscle thickness : thick
│ │          - fascicle length : fasc
│ │          - pennation angle : pen
│ │          - scaling : scale
│ │          - aponeurosis drawing : apo
│ │          The variable is updaten upon selection of the user. The default
│ │          is muscle thickness.
│ │ @@ -137,45 +143,43 @@
│ │          tk.Canvas variable representing the canvas the image is plotted
│ │          on in the GUI. The canvas is used to draw on the image.
│ │      self.img : ImageTk.PhotoImage
│ │          ImageTk.PhotoImage variable containing the current image that is
│ │          analyzed. It is necessary to load the image in this way in order
│ │          to plot the image.
│ │      self.dist : int
│ │ -        Integer variable containing the length of the scaling line
│ │ -        in pixel units. This variable is then used to scale the image.
│ │ +        Integer variable containing the length of the scaling line in
│ │ +        pixel units. This variable is then used to scale the image.
│ │  
│ │      Methods
│ │      -------
│ │      __init__
│ │ -        Instance method to initialize the class
│ │ +        Instance method to initialize the class.
│ │      calculateBatchManual
│ │ -        Instance method creating the GUI for manual
│ │ -        image analysis
│ │ +        Instance method creating the GUI for manual image analysis.
│ │      """
│ │ -
│ │      def __init__(self, img_list: str, rootpath: str):
│ │ -        """
│ │ -        Instance method to initialize the Manual Analysis class.
│ │ +        """Instance method to initialize the Manual Analysis class.
│ │  
│ │          Parameters
│ │          ----------
│ │          img_list : str
│ │ -            A list variable containing the absolute paths to all images / video to be
│ │ -            analyzed.
│ │ +            A list variable containing the absolute paths to all images /
│ │ +            video to be analyzed.
│ │          rootpath : str
│ │              Path to root directory where images / videos for the analysis are
│ │              saved.
│ │  
│ │          Examples
│ │          --------
│ │ -        >>> man_analysis = ManualAnalysis(img_list=["C:/user/Dokuments/images/image1.tif",
│ │ -                                                    "C:/user/Dokuments/images/image2.tif",
│ │ -                                                    "C:/user/Dokuments/images/image3.tif"],
│ │ -                                          rootpath="C:/user/Dokuments/images")
│ │ +        >>> man_analysis = ManualAnalysis(
│ │ +            img_list=["C:/user/Dokuments/images/image1.tif",
│ │ +            "C:/user/Dokuments/images/image2.tif",
│ │ +            "C:/user/Dokuments/images/image3.tif"],
│ │ +            rootpath="C:/user/Dokuments/images")
│ │          """
│ │          # Get input images
│ │          self.image_list = img_list
│ │          self.rootpath = rootpath
│ │  
│ │          # keep a reference to all lines by keeping them in a list
│ │          self.lines = []
│ │ @@ -194,25 +198,26 @@
│ │  
│ │          # Define dataframe for export
│ │          self.dataframe = pd.DataFrame(
│ │              columns=["File", "Fasicle Length", "Pennation Angle", "Thickness"]
│ │          )
│ │  
│ │      def calculateBatchManual(self):
│ │ -        """
│ │ -        Instance method creating a GUI for manual annotation of longitudinal ultrasoud
│ │ -        images of human lower limb muscles.
│ │ +        """Instance method creating a GUI for manual annotation of longitudinal
│ │ +        ultrasoud images of human lower limb muscles.
│ │  
│ │ -        The GUI contains several analysis options for the current image openend. The
│ │ -        user is able to switch between analysis of muscle thickness, fascicle length
│ │ -        and pennation angle. Moreover, the image can be scaled and the aponeuroses
│ │ -        can be extendet to easy fascicle extrapolation. When one image is finished,
│ │ -        the GUI updated by clicking "next image". The Results can be saved by clicking
│ │ -        "save results". It is possible to save each image seperately. The GUI can be
│ │ -        closed by clicking "break analysis" or simply close the window.
│ │ +        The GUI contains several analysis options for the current image
│ │ +        openend. The user is able to switch between analysis of muscle
│ │ +        thickness, fascicle length and pennation angle. Moreover, the
│ │ +        image can be scaled and the aponeuroses can be extendet to easy
│ │ +        fascicle extrapolation. When one image is finished, the GUI
│ │ +        updated by clicking "next image". The Results can be saved by
│ │ +        clicking "save results". It is possible to save each image
│ │ +        seperately. The GUI can be closed by clicking "break analysis"
│ │ +        or simply close the window.
│ │  
│ │          Notes
│ │          -----
│ │          The GUI can be initated from the main DL_Track GUI. It is also
│ │          possible to initiate the ManualAnalis class from the command
│ │          promt and interact with its GUI as a standalone. To do so,
│ │          the lines 231 (self.head = tk.Tk()) and 314 (self.head.mainloop())
│ │ @@ -220,16 +225,18 @@
│ │          must be commented.
│ │          """
│ │          # This line must be uncommented when the class is initiated from
│ │          # the command prompt
│ │          # self.head = tk.Tk()
│ │  
│ │          self.head = tk.Toplevel()
│ │ -        self.head.title("DLTrack - Manual Analysis")
│ │ -        # self.head.iconbitmap("home_im.ico")
│ │ +        self.head.title("DL_Track_US - Manual Analysis")
│ │ +        master_path = os.path.dirname(os.path.abspath(__file__))
│ │ +        iconpath = master_path + "/home_im.ico"
│ │ +        self.head.iconbitmap(iconpath)
│ │  
│ │          # Style
│ │          style = ttk.Style()
│ │          style.theme_use("clam")
│ │          style.configure("TFrame", background="DarkSeaGreen3")
│ │          style.configure(
│ │              "TRadiobutton",
│ │ @@ -294,23 +301,29 @@
│ │              variable=self.mode,
│ │              value="pen",
│ │              command=self.setAngles,
│ │          )
│ │          pennation.pack(side="left")
│ │  
│ │          # Define Button fot break
│ │ -        stop = ttk.Button(toolbar, text="Break Analysis", command=self.stopAnalysis)
│ │ +        stop = ttk.Button(
│ │ +            toolbar, text="Break Analysis", command=self.stopAnalysis
│ │ +        )
│ │          stop.pack(side="right")
│ │  
│ │          # Define Button for next image
│ │ -        next_img = ttk.Button(toolbar, text="Next Image", command=self.updateImage)
│ │ +        next_img = ttk.Button(
│ │ +            toolbar, text="Next Image", command=self.updateImage
│ │ +        )
│ │          next_img.pack(side="right")
│ │  
│ │          # Define Button for save image
│ │ -        save = ttk.Button(toolbar, text="Save Results", command=self.saveResults)
│ │ +        save = ttk.Button(
│ │ +            toolbar, text="Save Results", command=self.saveResults
│ │ +        )
│ │          save.pack(side="right")
│ │  
│ │          # Define Canvas for image
│ │          self.canvas = tk.Canvas(self.head, bg="black", height=1500, width=1500)
│ │          self.canvas.pack(fill="both")
│ │  
│ │          # Add padding
│ │ @@ -319,31 +332,32 @@
│ │  
│ │          # Load image
│ │          resized_img = Image.open(self.image_list[0]).resize((1024, 800))
│ │          self.img = ImageTk.PhotoImage(
│ │              resized_img,
│ │              master=self.canvas,
│ │          )
│ │ -        my_image = self.canvas.create_image(750, 500, anchor="center", image=self.img)
│ │ +        my_image = self.canvas.create_image(
│ │ +            750, 500, anchor="center", image=self.img
│ │ +        )
│ │  
│ │          # Bind mouse events to canvas
│ │          self.canvas.bind("<ButtonPress-1>", self.click)
│ │          self.canvas.bind("<ButtonRelease-1>", self.release)
│ │          self.canvas.bind("<B1-Motion>", self.drag)
│ │  
│ │          # This line must be uncommented when the class is initiated from
│ │          # the command prompt
│ │          # self.head.mainloop()
│ │  
│ │ -    # --------------------------------------------------------------------------------------------------------------------
│ │ +    # ----------------------------------------------------------------
│ │      # Functionalities used in head
│ │  
│ │      def click(self, event: str):
│ │ -        """
│ │ -        Instance method to record mouse clicks on canvas.
│ │ +        """Instance method to record mouse clicks on canvas.
│ │  
│ │          When the left mouse button is clicked on the canvas, the
│ │          xy-coordinates are stored for further analysis. When the button
│ │          is clicked multiple times, multiple values are stored.
│ │  
│ │          Parameters
│ │          ----------
│ │ @@ -378,16 +392,15 @@
│ │              self.thick_coords.append([self.coords["x"], self.coords["y"]])
│ │          elif self.mode.get() == "fasc":
│ │              self.fasc_coords.append([self.coords["x"], self.coords["y"]])
│ │          elif self.mode.get() == "pen":
│ │              self.pen_coords.append([self.coords["x"], self.coords["y"]])
│ │  
│ │      def release(self, event: str):
│ │ -        """
│ │ -        Instance method to record mouse button releases on canvas.
│ │ +        """Instance method to record mouse button releases on canvas.
│ │  
│ │          When the left mouse button is released on the canvas, the
│ │          xy-coordinates are stored for further analysis. When the button
│ │          is released multiple times, multiple values are stored.
│ │  
│ │          Parameters
│ │          ----------
│ │ @@ -410,16 +423,15 @@
│ │              self.thick_coords.append([self.coords["x2"], self.coords["y2"]])
│ │          elif self.mode.get() == "fasc":
│ │              self.fasc_coords.append([self.coords["x2"], self.coords["y2"]])
│ │          elif self.mode.get() == "pen":
│ │              self.pen_coords.append([self.coords["x2"], self.coords["y2"]])
│ │  
│ │      def drag(self, event: str):
│ │ -        """
│ │ -        Instance method to record mouse cursor dragging on canvas.
│ │ +        """Instance method to record mouse cursor dragging on canvas.
│ │  
│ │          When the cursor is dragged on the canvas, the
│ │          xy-coordinates are stored and updated for further analysis.
│ │          This is used to draw the line that follows the cursor
│ │          on the screen. Coordinates are only recorded when the left
│ │          mouse button is pressed.
│ │  
│ │ @@ -433,40 +445,39 @@
│ │          --------
│ │          >>> self.canvas.bind("<B1-Motion>", self.click)
│ │          """
│ │          # update the coordinates from the event
│ │          self.coords["x2"] = self.canvas.canvasx(event.x)
│ │          self.coords["y2"] = self.canvas.canvasy(event.y)
│ │  
│ │ -        # Change the coordinates of the last created line to the new coordinates
│ │ +        # Change the coordinates of the last created line to the new
│ │ +        # coordinates
│ │          self.canvas.coords(
│ │              self.lines[-1],
│ │              self.coords["x"],
│ │              self.coords["y"],
│ │              self.coords["x2"],
│ │              self.coords["y2"],
│ │          )
│ │  
│ │      def setScale(self):
│ │ -        """
│ │ -        Instance method to display the instructions for image scaling.
│ │ +        """Instance method to display the instructions for image scaling.
│ │  
│ │          This function is bound to the "Scale Image" radiobutton and
│ │          appears each time the button is selected. In this way, the user
│ │          is reminded on how to scale the image.
│ │          """
│ │          messagebox.showinfo(
│ │              "Information",
│ │              "Draw 10mm long scaling line."
│ │              + "\nThis is only necessary in the first image.",
│ │          )
│ │  
│ │      def setApo(self):
│ │ -        """
│ │ -        Instance method to display the instructions for aponeuroses
│ │ +        """Instance method to display the instructions for aponeuroses
│ │          extending.
│ │  
│ │          This function is bound to the "Draw Aponeurosis" radiobutton and
│ │          appears each time the button is selected. In this way, the user
│ │          is reminded on how to draw aponeurosis extension on the image.
│ │          """
│ │          messagebox.showinfo("Information", "Draw lines to extend aponeuroses.")
│ │ @@ -483,48 +494,45 @@
│ │          messagebox.showinfo(
│ │              "Information",
│ │              "Draw lines to measure muscle thickness."
│ │              + "\nSelect at least three sites.",
│ │          )
│ │  
│ │      def setFascicles(self):
│ │ -        """
│ │ -        Instance method to display the instructions for muscle fascicle
│ │ +        """Instance method to display the instructions for muscle fascicle
│ │          analysis.
│ │  
│ │          This function is bound to the "Muscle Fascicles" radiobutton and
│ │          appears each time the button is selected. In this way, the user
│ │          is reminded on how to analyze muscle fascicles.
│ │          """
│ │          messagebox.showinfo(
│ │              "Information",
│ │              "Draw lines to measure muscle fascicles."
│ │              + "\nSelect at least three fascicles."
│ │              + "\nLines MUST consist of three segments.",
│ │          )
│ │  
│ │      def setAngles(self):
│ │ -        """
│ │ -        Instance method to display the instructions for pennation angle
│ │ +        """Instance method to display the instructions for pennation angle
│ │          analysis.
│ │  
│ │          This function is bound to the "Pennation Angle" radiobutton and
│ │          appears each time the button is selected. In this way, the user
│ │          is reminded on how to analyze pennation angle.
│ │          """
│ │          messagebox.showinfo(
│ │              "Information",
│ │              "Draw lines to measure pennation angles."
│ │              + "\nSelect at least three angles."
│ │              + "\nAngle MUST consist of two segments.",
│ │          )
│ │  
│ │      def saveResults(self):
│ │ -        """
│ │ -        Instance Method to save the analysis results to a pd.DataFrame.
│ │ +        """Instance Method to save the analysis results to a pd.DataFrame.
│ │  
│ │          A list of each variable to be saved must be used. The list must
│ │          contain the coordinates of the recorded events during
│ │          parameter analysis. Then, the respective parameters are calculated
│ │          using the coordinates in each list and inculded in a dataframe.
│ │          Estimates or fascicle length, pennation angle,
│ │          muscle thickness are saved.
│ │ @@ -533,24 +541,27 @@
│ │          -----
│ │          In order to correctly calculate the muscle parameters, the amount of
│ │          coordinates must be specific. See class documentatio of documentation
│ │          of functions used to calculate parameters.
│ │  
│ │          See Also
│ │          --------
│ │ -        self.calculateThickness, self.calculateFascicle, self.calculatePennation
│ │ +        self.calculateThickness, self.calculateFascicle,
│ │ +        self.calculatePennation
│ │          """
│ │          # Get results
│ │          thickness = self.calculateThickness(self.thick_coords)
│ │          fascicles = self.calculateFascicles(self.fasc_coords)
│ │          pennation = self.calculatePennation(self.pen_coords)
│ │  
│ │          # Check number of input images and counter
│ │          if len(self.scale_coords) >= 1:
│ │ -            self.dist = np.abs(self.scale_coords[0][1] - self.scale_coords[1][1])
│ │ +            self.dist = np.abs(
│ │ +                self.scale_coords[0][1] - self.scale_coords[1][1]
│ │ +            )
│ │  
│ │              # Save Results with scaling
│ │              dataframe2 = pd.DataFrame(
│ │                  {
│ │                      "File": self.image_list[self.count],
│ │                      "Fasicle Length": np.mean(fascicles) / self.dist,
│ │                      "Pennation Angle": np.mean(pennation),
│ │ @@ -571,17 +582,31 @@
│ │                      "Thickness": np.mean(thickness),
│ │                  },
│ │                  index=[self.count],
│ │              )
│ │  
│ │              self.dataframe = pd.concat([self.dataframe, dataframe2], axis=0)
│ │  
│ │ +        # Save the drawing on the canvas to an image
│ │ +        # Define coordinates for cropping
│ │ +        x = (
│ │ +            self.head.winfo_rootx() + self.head.winfo_x()
│ │ +        )  # Top x of root + head
│ │ +        y = (
│ │ +            self.head.winfo_rooty() + self.head.winfo_y()
│ │ +        )  # Top y of root + head
│ │ +        x1 = x + 2 * self.head.winfo_width()  # include twice the width
│ │ +        y1 = y + 2 * self.head.winfo_height()  # include twice the height
│ │ +        # Save the screenshot to root location
│ │ +        ImageGrab.grab().crop((x, y, x1, y1)).save(
│ │ +            f"{self.image_list[self.count]}_analyzed.png"
│ │ +        )
│ │ +
│ │      def updateImage(self):
│ │ -        """
│ │ -        Instance method to update the current image displayed
│ │ +        """Instance method to update the current image displayed
│ │          on the canvas in the GUI.
│ │  
│ │          The image is updated upon clicking of "Next Image"
│ │          button in the GUI. Images can be updated until all
│ │          images have been analyzed. Even when the image is updated,
│ │          the analysis results are stored.
│ │          """
│ │ @@ -612,52 +637,52 @@
│ │              # Save Results to Excel
│ │              self.compileSaveResults()
│ │              messagebox.showerror("Error", "No more images available.")
│ │              # Quit session
│ │              self.closeWindow()
│ │  
│ │      def stopAnalysis(self):
│ │ -        """
│ │ -        Instance method to stop the current analysis
│ │ +        """Instance method to stop the current analysis
│ │          on the canvas in the GUI.
│ │  
│ │          The analysis is stopped upon clicking of "Break Analysis"
│ │          button in the GUI. The current analysis results are saved
│ │          when the analysis is terminated. The analysis can be
│ │          terminated at any timepoint during manual annotation. Prior to
│ │          terminating, the user is asked for confirmation.
│ │          """
│ │          # Save results
│ │          self.compileSaveResults()
│ │          # Terminate analysis
│ │ -        messagebox.askyesno("Attention", "Would you like to stop the Analysis?")
│ │ +        messagebox.askyesno(
│ │ +            "Attention", "Would you like to stop the Analysis?"
│ │ +        )
│ │          self.closeWindow()
│ │  
│ │      # --------------------------------------------------------------------------------------------------------------------
│ │      # Utilities used by functionalities in head
│ │  
│ │ -    def getAngle(self, a: list, b: list, c: list):
│ │ -        """
│ │ -        Instance method to calculate angle between three points.
│ │ +    def getAngle(self, a: list, b: list, c: list) -> float:
│ │ +        """Instance method to calculate angle between three points.
│ │  
│ │ -        The angle is calculated using the arc tangent. The arc tangent is used to
│ │ -        find the slope in radians when Y and X co-ordinates are given. The output is
│ │ -        the arc tangent of y/x in radians, which is between PI and -PI. Then the
│ │ -        output is converted to degrees.
│ │ +        The angle is calculated using the arc tangent. The arc tangent is used
│ │ +        to find the slope in radians when Y and X co-ordinates are given. The
│ │ +        output is the arc tangent of y/x in radians, which is between PI and
│ │ +        -PI. Then the output is converted to degrees.
│ │  
│ │          Parameters
│ │          ----------
│ │          a : list
│ │ -            List variable containing the xy-coordinates of the first mouse click event
│ │ -            of the pennation angle annotation. This must be the point
│ │ -            at the beginning of the fascicle segment.
│ │ +            List variable containing the xy-coordinates of the first mouse
│ │ +            click event of the pennation angle annotation. This must be the
│ │ +            point at the beginning of the fascicle segment.
│ │          b : list
│ │ -            List variable containing the xy-coordinates of the second mouse click event
│ │ -            of the pennation angle annotation. This must be the point
│ │ -            at the intersection between fascicle and aponeurosis.
│ │ +            List variable containing the xy-coordinates of the second mouse
│ │ +            click event of the pennation angle annotation. This must be the
│ │ +            point at the intersection between fascicle and aponeurosis.
│ │          c : list
│ │              List variable containing the xy-coordinates of the fourth
│ │              mouse event of the pennation angle annotation. This must be
│ │              the endpoint of the aponeurosis segment.
│ │  
│ │          Returns
│ │          -------
│ │ @@ -668,40 +693,42 @@
│ │  
│ │          Example
│ │          -------
│ │          >>> getAngle(a=[904.0, 41.0], b=[450,380], c=[670,385])
│ │          25.6
│ │          """
│ │          ang = math.degrees(
│ │ -            math.atan2(c[1] - b[1], c[0] - b[0]) - math.atan2(a[1] - b[1], a[0] - b[0])
│ │ +            math.atan2(c[1] - b[1], c[0] - b[0])
│ │ +            - math.atan2(a[1] - b[1], a[0] - b[0])
│ │          )
│ │  
│ │          if ang < 0:
│ │              ang = ang * (-1)
│ │  
│ │          return ang if ang < 180 else 360 - ang
│ │  
│ │ -    def calculateThickness(self, thick_list: list):
│ │ -        """
│ │ -        Instance method to calculate distance between deep and superficial
│ │ +    def calculateThickness(self, thick_list: list) -> list:
│ │ +        """Instance method to calculate distance between deep and superficial
│ │          muscle aponeuroses, also known as muscle thickness.
│ │  
│ │          The length of the segment is calculated by determining the absolute
│ │          distance of the y-coordinates of two points.
│ │ -        Here, the muscle thickness is calculated considering only the y-coordinates
│ │ -        of the start and end points of the drawn segments. In this way,
│ │ -        incorrect placement of the segments by drawing skewed lines can be
│ │ -        accounted for. Then the thickness is outputted in pixel units.
│ │ +        Here, the muscle thickness is calculated considering only the
│ │ +        y-coordinates of the start and end points of the drawn segments.
│ │ +        In this way, incorrect placement of the segments by drawing skewed
│ │ +        lines can be accounted for. Then the thickness is outputted in
│ │ +        pixel units.
│ │  
│ │          Parameters
│ │          ----------
│ │          thick_list : list
│ │ -            List variable containing the xy-coordinates of the first mouse click event
│ │ -            and the mouse release event of the muscle thickness annotation.
│ │ -            This must be the points at the beginning and end of the thickness segment.
│ │ +            List variable containing the xy-coordinates of the first mouse
│ │ +            click event and the mouse release event of the muscle thickness
│ │ +            annotation. This must be the points at the beginning and end of
│ │ +            the thickness segment.
│ │  
│ │          Returns
│ │          -------
│ │          thickness : list
│ │              List variable containing the muscle thickness in pixel units.
│ │              This value is calculated using only the
│ │              difference of the y-coordinates of the two points. If multiple
│ │ @@ -723,106 +750,118 @@
│ │  
│ │              dist = np.abs(thick_list[count][1] - thick_list[count + 1][1])
│ │              thickness.append(dist)
│ │              count += 2
│ │  
│ │          return thickness
│ │  
│ │ -    def calculateFascicles(self, fasc_list: list):
│ │ -        """
│ │ -        Instance method to calculate muscle fascicle legth as a sum of three
│ │ +    def calculateFascicles(self, fasc_list: list) -> list:
│ │ +        """Instance method to calculate muscle fascicle legth as a sum of three
│ │          annotated segments.
│ │  
│ │ -        The length of three line segment is calculated by summing their individual
│ │ -        length. Here, the length of a single annotated fascicle is calculated considering the three
│ │ -        drawn segments of the respective fascicle. The euclidean distance between
│ │ -        the start and endpoint of each segment is calculated and summed.
│ │ -        Then the length of the fascicle is outputted in pixel units.
│ │ +        The length of three line segment is calculated by summing their
│ │ +        individual length. Here, the length of a single annotated fascicle
│ │ +        is calculated considering the three drawn segments of the respective
│ │ +        fascicle. The euclidean distance between the start and endpoint of
│ │ +        each segment is calculated and summed. Then the length of the
│ │ +        fascicle is outputted in pixel units.
│ │  
│ │          Parameters
│ │          ----------
│ │          fasc_list : list
│ │ -            List variable containing the xy-coordinates of the first mouse click event
│ │ -            and the mouse release event of each annotated fascicle segment.
│ │ +            List variable containing the xy-coordinates of the first mouse
│ │ +            click event and the mouse release event of each annotated fascicle
│ │ +            segment.
│ │  
│ │          Returns
│ │          -------
│ │          fascicles : list
│ │              List variable containing the fascicle length in pixel units.
│ │              This value is calculated by summing the euclidean distance of
│ │              each drawn fascicle segment. If multiple fascicles are drawn,
│ │              multiple fascicle length values are outputted.
│ │  
│ │          Example
│ │          -------
│ │ -        >>> calculateFascicles(fasc_list=[[392.0, 622.0], [632.0, 544.0], [632.0, 544.0],
│ │ -                                          [1090.0, 415.0], [1090.0, 415.0], [1274.0, 381.0],
│ │ -                                          [449.0, 627.0], [748.0, 541.0], [748.0, 541.0],
│ │ -                                          [1109.0, 429.0], [1109.0, 429.0], [1297.0, 390.0]])
│ │ +        >>> calculateFascicles(fasc_list=[[392.0, 622.0], [632.0, 544.0],
│ │ +                                          [632.0, 544.0],
│ │ +                                          [1090.0, 415.0], [1090.0, 415.0],
│ │ +                                          [1274.0, 381.0],
│ │ +                                          [449.0, 627.0], [748.0, 541.0],
│ │ +                                          [748.0, 541.0],
│ │ +                                          [1109.0, 429.0], [1109.0, 429.0],
│ │ +                                          [1297.0, 390.0]])
│ │          [915.2921723246823, 881.0996335404545]
│ │          """
│ │          # turn list to array
│ │          fasc_list = np.asarray(fasc_list)
│ │          # Keep track of fascicle lengths
│ │          fascicles = []
│ │          # Keep count of segments
│ │          count = 0
│ │  
│ │          # loop through points in fasc_list but part the length
│ │          # by 6 because 6 points belong to one fascicle
│ │          for _ in range(0, int(len(fasc_list) / 6)):
│ │  
│ │              # Iterate through segments to calculate and draw one fascicle
│ │ -            # and iterate three times because a fascicle consists of three segments
│ │ +            # and iterate three times because a fascicle consists of three
│ │ +            # segments
│ │              fascicle = []  # Segment length are stored here
│ │              for _ in range(3):
│ │  
│ │ -                # calculate the distance between two points belonging to one segment
│ │ -                dist = distance.euclidean(fasc_list[count], fasc_list[count + 1])
│ │ +                # calculate the distance between two points belonging to one
│ │ +                # segment
│ │ +                dist = distance.euclidean(
│ │ +                    fasc_list[count], fasc_list[count + 1]
│ │ +                )
│ │                  fascicle.append(dist)
│ │                  # increase count to jump to next segment
│ │                  count += 2
│ │  
│ │              # When looped through all segments append length of whole fascicle
│ │              fascicles.append(sum(fascicle))
│ │  
│ │          return fascicles
│ │  
│ │ -    def calculatePennation(self, pen_list: list):
│ │ -        """
│ │ -        Instance method to calculate muscle pennation angle between three points
│ │ +    def calculatePennation(self, pen_list: list) -> list:
│ │ +        """Instance method to calculate muscle pennation angle between three
│ │ +        points.
│ │  
│ │          The angle between three points is calculated using the arc tangent.
│ │ -        Here, the points used for calculation are the start and endpoint of the
│ │ -        line segment drawn alongside the fascicle as well as the endpoint of the
│ │ -        segment drawn along the aponeurosis. The pennation angle
│ │ +        Here, the points used for calculation are the start and endpoint of
│ │ +        the line segment drawn alongside the fascicle as well as the endpoint
│ │ +        of the segment drawn along the aponeurosis. The pennation angle
│ │          is outputted in degrees.
│ │  
│ │          Parameters
│ │          ----------
│ │          pen_list : list
│ │ -            List variable containing the xy-coordinates of the first mouse click event
│ │ -            and the mouse release event of each annotated pennation angle segment.
│ │ +            List variable containing the xy-coordinates of the first mouse
│ │ +            click event and the mouse release event of each annotated
│ │ +            pennation angle segment.
│ │  
│ │          Returns
│ │          -------
│ │          pen_angles : list
│ │              List variable containing the pennation angle in degrees.
│ │              If multiple pennation angles are drawn, multiple angle values
│ │              are outputted.
│ │  
│ │          See Also
│ │          --------
│ │          self.getAngle()
│ │  
│ │          Example
│ │          -------
│ │ -        >>> calculateFascicles(pen_list=[[760.0, 579.0], [620.0, 629.0], [620.0, 629.0],
│ │ +        >>> calculateFascicles(pen_list=[[760.0, 579.0], [620.0, 629.0],
│ │ +                                         [620.0, 629.0],
│ │                                           [780.0, 631.0], [533.0, 571.0],
│ │ -                                         [378.0, 627.0], [378.0, 627.0], [558.0, 631.0]] )
│ │ +                                         [378.0, 627.0], [378.0, 627.0],
│ │ +                                         [558.0, 631.0]] )
│ │          [20.369984003523715, 21.137327423466722]
│ │          """
│ │          # Keep track of angles
│ │          pen_angles = []
│ │          # Keep count of segments
│ │          count = 0
│ │  
│ │ @@ -838,16 +877,15 @@
│ │              pen_angles.append(angle)
│ │              # Increase count to jump to next angle
│ │              count += 4
│ │  
│ │          return pen_angles
│ │  
│ │      def compileSaveResults(self):
│ │ -        """
│ │ -        Instance method to save the analysis results.
│ │ +        """Instance method to save the analysis results.
│ │  
│ │          A pd.DataFrame object must be used. The results
│ │          inculded in the dataframe are saved to an .xlsx file. Depending
│ │          on the form of class initialization, the .xlsx file is
│ │          either saved to self.root (GUI) or self.out (command prompt).
│ │  
│ │          Notes
│ │ @@ -864,27 +902,28 @@
│ │                  data.to_excel(writer, sheet_name="Results")
│ │          else:
│ │              with pd.ExcelWriter(excelpath, mode="w") as writer:
│ │                  data = self.dataframe
│ │                  data.to_excel(writer, sheet_name="Results")
│ │  
│ │      def closeWindow(self):
│ │ -        """
│ │ -        Instance method to close window upon call.
│ │ +        """Instance method to close window upon call.
│ │  
│ │          This method is evoked by clicking the button "Break Analysis".
│ │          It is necessary to use a different function, otherwise
│ │          the window would be destroyed upon starting.
│ │          """
│ │          self.head.destroy()
│ │  
│ │  
│ │ -# When using standalone uncomment To do so, uncomment the lines 231 (self.head = tk.Tk()) and 314 (self.head.mainloop())
│ │ +# When using standalone uncomment To do so, uncomment the lines 231
│ │ +# (self.head = tk.Tk()) and 314 (self.head.mainloop())
│ │  # and comment the line 233 (self.head = tk.Toplevel()).
│ │ -# The class can then be initalized by typing "python manual_tracing.py" and specifying the filetype and rootpath
│ │ +# The class can then be initalized by typing
│ │ +# "python manual_tracing.py" and specifying the filetype and rootpath
│ │  # in the code below. The whole codeblock below must be uncommented
│ │  
│ │  # if __name__ == "__main__":
│ │  
│ │  #     filetype = "/**/*.avi"
│ │  #     rootpath = "C:/Users/admin/Desktop"
│ │  #     img_list = glob.glob(rootpath + filetype, recursive=True)
│ │   --- dl_track_us-0.1.1/Figures/Figure_B-A.png
│ ├── +++ dl_track_us-0.1.2/Figures/Figure_B-A.png
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/Figures/Figure_video.png
│ ├── +++ dl_track_us-0.1.2/Figures/Figure_video.png
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/Figures/home_im.png
│ ├── +++ dl_track_us-0.1.2/Figures/home_im.png
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/Paper/paper.bib
│ ├── +++ dl_track_us-0.1.2/Paper/paper.bib
│ │┄ Files 3% similar despite different names
│ │ @@ -205,21 +205,36 @@
│ │  
│ │  @article{marzilger2018,
│ │  	title = {Reliability of a semi-automated algorithm for the vastus lateralis muscle architecture measurement based on ultrasound images},
│ │  	volume = {118},
│ │  	issn = {1439-6319, 1439-6327},
│ │  	url = {http://link.springer.com/10.1007/s00421-017-3769-8},
│ │  	doi = {10.1007/s00421-017-3769-8},
│ │ -	abstract = {Purpose  The assessment of muscle architecture with B-mode ultrasound is an established method in muscle physiology and mechanics. There are several manual, semi-automated and automated approaches available for muscle architecture analysis from ultrasound images or videos. However, most approaches have limitations such as workload, subjectivity, drift or they are applicable to short muscle fascicles only. Addressing these issues, an algorithm was developed to analyse architectural parameters of the vastus lateralis muscle (VL).
│ │ -Methods  In 17 healthy young men and women, ultrasound images were taken five times on two different days during passive knee joint flexion. From the images, fascicle length (FL), pennation angle (PAN) and muscle thickness (MTH) were calculated for both test days using the algorithm. Interday differences were determined using a two-way ANOVA. Interday and intraday reliability were assessed using intraclass correlation coefficients (ICC) and root mean square (RMS) differences.
│ │ -Results  FL, MTH and PAN did not differ between day one and two. The within day ICCs were above 0.94 for all tested parameters. The average interday ICCs were 0.86 for the FL, 0.96 for MTH and 0.60 for PAN. The average RMS differences between both days were 5.0\%, 8.5\% and 12.0\% for MTH, FL and PAN, respectively.
│ │ -Conclusion  The proposed algorithm provides high measurement reliability. However, the interday reliability might be influenced by small differences in probe position between days.},
│ │ +	abs:tract = {Purpose: The assessment of muscle architecture with B-mode ultrasound is an established method in muscle physiology and mechanics. There are several manual, semi-automated and automated approaches available for muscle architecture analysis from ultrasound images or videos. However, most approaches have limitations such as workload, subjectivity, drift or they are applicable to short muscle fascicles only. Addressing these issues, an algorithm was developed to analyse architectural parameters of the vastus lateralis muscle (VL).
│ │ +Methods In 17 healthy young men and women, ultrasound images were taken five times on two different days during passive knee joint flexion. From the images, fascicle length (FL), pennation angle (PAN) and muscle thickness (MTH) were calculated for both test days using the algorithm. Interday differences were determined using a two-way ANOVA. Interday and intraday reliability were assessed using intraclass correlation coefficients (ICC) and root mean square (RMS) differences.
│ │ +Results: FL, MTH and PAN did not differ between day one and two. The within day ICCs were above 0.94 for all tested parameters. The average interday ICCs were 0.86 for the FL, 0.96 for MTH and 0.60 for PAN. The average RMS differences between both days were 5.0\%, 8.5\% and 12.0\% for MTH, FL and PAN, respectively.
│ │ +Conclusion: The proposed algorithm provides high measurement reliability. However, the interday reliability might be influenced by small differences in probe position between days.},
│ │  	language = {en},
│ │  	number = {2},
│ │  	urldate = {2022-11-08},
│ │  	journal = {Eur J Appl Physiol},
│ │  	author = {Marzilger, Robert and Legerlotz, Kirsten and Panteli, Chrystalla and Bohm, Sebastian and Arampatzis, Adamantios},
│ │  	month = feb,
│ │  	year = {2018},
│ │  	pages = {291--301},
│ │  	file = {Marzilger et al. - 2018 - Reliability of a semi-automated algorithm for the .pdf:C\:\\Users\\ritpau00\\Zotero\\storage\\N6V6GE2L\\Marzilger et al. - 2018 - Reliability of a semi-automated algorithm for the .pdf:application/pdf},
│ │  }
│ │ +
│ │ +@article{ritsche2022,
│ │ +	title = {DeepACSA: Automatic Segmentation of Cross-sectional Area in Ultrasound Images of Lower Limb Muscles Using Deep Learning},
│ │ +	volume = {Publish Ahead of Print},
│ │ +	issn = {1530-0315, 0195-9131},
│ │ +	shorttitle = {{DeepACSA}},
│ │ +	url = {https://journals.lww.com/10.1249/MSS.0000000000003010},
│ │ +	doi = {10.1249/MSS.0000000000003010},
│ │ +	language = {en},
│ │ +	urldate = {2022-08-11},
│ │ +	journal = {Medicine \& Science in Sports \& Exercise},
│ │ +	author = {Ritsche, Paul and Wirth, Philipp and Cronin, Neil J. and Sarto, Fabio and Narici, Marco V. and Faude, Oliver and Franchi, Martino V.},
│ │ +	month = aug,
│ │ +	year = {2022},
│ │ +}
│ │   --- dl_track_us-0.1.1/Paper/paper.md
│ ├── +++ dl_track_us-0.1.2/Paper/paper.md
│ │┄ Files 21% similar despite different names
│ │ @@ -1,9 +1,9 @@
│ │  ---
│ │ -title: 'DL_Track: a python package to analyse muscle ultrasonography images'
│ │ +title: 'DL_Track_US: a python package to analyse muscle ultrasonography images'
│ │  tags:
│ │    - Python
│ │    - muscle
│ │    - ultrasonography
│ │    - muscle architecture
│ │    - deep learning
│ │  authors:
│ │ @@ -28,20 +28,25 @@
│ │     index: 4
│ │  date: 08 November 2022
│ │  bibliography: paper.bib
│ │  ---
│ │  
│ │  # Summary
│ │  
│ │ -Ultrasonography can be used to assess muscle architectural parameters during static and dynamic conditions. Nevertheless, the analysis of the acquired ultrasonography images presents a major difficulty. Muscle architectural parameters such as muscle thickness, fascicle length and pennation angle are mainly segmented manually. Manual analysis is time expensive, subjective and requires thorough expertise. Within recent years, several algorithms were developed to solve these issues. Yet, these are only partly automated, are not openly available, or lack in user friendliness. The DL_Track python package is designed to allow fully automated and rapid analyis of muscle architectural parameters in lower limb ultrasonography images.
│ │ +Ultrasonography can be used to assess muscle architectural parameters during static and dynamic conditions. Nevertheless, the analysis of the acquired ultrasonography images presents a major difficulty. Muscle architectural parameters such as muscle thickness, fascicle length and pennation angle are mainly segmented manually. Manual analysis is time expensive, subjective and requires thorough expertise. Within recent years, several algorithms were developed to solve these issues. Yet, these are only partly automated, are not openly available, or lack in user friendliness. The DL_Track_US python package is designed to allow fully automated and rapid analyis of muscle architectural parameters in lower limb ultrasonography images.
│ │  
│ │  # Statement of need
│ │  
│ │ -`DL_Track` is a python package to automatically segment and analyse muscle architectural parameters in longitudinal ultrasonography images and videos of human lower limb muscles. The use of ultrasonography to assess muscle morphological and architectural parameters is increasing in different scientific fields [@naruse2022]. This is due to the high portability, cost-effectiveness and patient-friendlyness compared to other imaging modalities such as MRI. Muscle architectural parameters such as muscle thickness, fascicle length and pennation angle are used to assess muscular adaptations to training, disuse and ageing, especially in the lower limbs [@sarto2021]. Albeit many disadvantages such as subjectivity, time expensiveness and required expertise, muscle architectural parameters in ultrasonography images are most often analyzed manually. A potential reason is the lack of versatile, accessible and easy to use analysis tools. Several algorithms to analyze muscle architectural parameters have been developed within the last years [@cronin2011; @rana2009; @marzilger2018; @drazan2019; @farris2016; @seynnes2020]. Nonetheless, most algorithms only partly automate the analysis, introducing some subjectivity in the analysis process. Moreover, many analyse only single architectural parameters and can be exclusively used for image or video analysis. Most existing methods further rely on hardcoded image filtering processes developed on few example images. Given that image parameters do no suit the designed filters of the if the filters are improperly adjusted, these filtering processes likely fail. On top of that, most previously proposed algorithms lack in user friendliness as the provided code and usage documentation is limited.
│ │ +`DL_Track_US` is a python package to automatically segment and analyse muscle architectural parameters in longitudinal ultrasonography images and videos of human lower limb muscles. The use of ultrasonography to assess muscle morphological and architectural parameters is increasing in different scientific fields [@naruse2022]. This is due to the high portability, cost-effectiveness and patient-friendlyness compared to other imaging modalities such as MRI. Muscle architectural parameters such as muscle thickness, fascicle length and pennation angle are used to assess muscular adaptations to training, disuse and ageing, especially in the lower limbs [@sarto2021]. Albeit many disadvantages such as subjectivity, time expensiveness and required expertise, muscle architectural parameters in ultrasonography images are most often analyzed manually. A potential reason is the lack of versatile, accessible and easy to use analysis tools. Several algorithms to analyze muscle architectural parameters have been developed within the last years [@cronin2011; @rana2009; @marzilger2018; @drazan2019; @farris2016; @seynnes2020]. Nonetheless, most algorithms only partly automate the analysis, introducing some subjectivity in the analysis process. Moreover, many analyse only single architectural parameters and can be exclusively used for image or video analysis. Most existing methods further rely on hardcoded image filtering processes developed on few example images. Given that image parameters do no suit the designed filters of the if the filters are improperly adjusted, these filtering processes likely fail. On top of that, most previously proposed algorithms lack in user friendliness as the provided code and usage documentation is limited.
│ │  
│ │ -`DL_Track` incorporates analysis modalities for fully automated analysis of ultrasonography images. `DL_Track` employs convolutional neural networks (CNNs) for the semantic segmentation of muscle fascicle fragments and aponeuroses. The employed CNNs consist of a VGG16 encoder path pre-trained on ImageNet [@simonyan2015; @deng2009] and a U-net decoder [@ronneberger2015]. Based on the semantic segmentation, muscle thickness, fascicle length and pennation angle are calculated as the distance between the detected aponeuroses, the intersection between extrapolated fascicle trajectories and aponeuroses and the intersection angle between extrapolated fascicle trajectories and detected deep aponeuroses, respectively. The workflow of the `DL_Track` analysis algorithm is demonstrated in \autoref{fig:1}.
│ │ +`DL_Track_US` incorporates analysis modalities for fully automated analysis of ultrasonography images. `DL_Track_US` employs convolutional neural networks (CNNs) for the semantic segmentation of muscle fascicle fragments and aponeuroses. The employed CNNs consist of a VGG16 encoder path pre-trained on ImageNet [@simonyan2015; @deng2009] and a U-net decoder [@ronneberger2015]. Based on the semantic segmentation, muscle thickness, fascicle length and pennation angle are calculated as the distance between the detected aponeuroses, the intersection between extrapolated fascicle trajectories and aponeuroses and the intersection angle between extrapolated fascicle trajectories and detected deep aponeuroses, respectively. An example of the analysis process of two muscles included in our dataset (vastus lateralis and gastrocnemius medialis) is demonstrated in \autoref{fig:1}. 
│ │  
│ │ -![Workflow of the DL_Track analysis algorithm.\label{fig:1}](figure1.png)
│ │ +![Process from original input image to output result for images of two muscles, gastrocnemius medialis (GM) and vastus lateralis (VL). Subsequent to inputting the original images into the models, predictions are generated by the models for the aponeuroses (apo) and fascicles as displayed in the binary images. Based on the binary image, the output result is calculated by post-processing operations, fascicles and aponeuroses are drawn and the values for fascicle length, pennation angle and muscle thickness are displayed.\label{fig:1}](figure1.png)
│ │  
│ │ -All `DL_Track` functionalities are embedded in a graphical user interface (GUI) to allow easy and intuitive usage. Apart from automated analysis, a manual analysis option included in the GUI as well. This is provided in case the provided pre-trained CNNs perform badly on input images or videos and subsequently avoid switching between softwares. Although we included images of the human gastrocnemius medialis, tibialis anterior, soleus and vastus lateralis from four different devices, it is likely the provided pre-trained CNNs fail when images from different muscles or devices are analyzed. However, an option to train CNNs bases on user data is also included in the GUI. Users are thereby enabled to train own CNNs based on own image or video data. The training ultrasonography image data, the pre-trained CNNs, example usage files as well as usage and testing instructions are provided.
│ │ +All `DL_Track_US` functionalities are embedded in a graphical user interface (GUI) to allow easy and intuitive usage. Apart from automated analysis, a manual analysis option is included in the GUI as well. This is provided in case the pre-trained CNNs perform badly on input images or videos and subsequently to avoid switching between softwares. The start window of the GUI is presented in \autoref{fig:2} and more information on usage can be found in our repository.
│ │ +
│ │ +![Start window of the DL_Track_US graphical user interface. Here, the input and model directories can be specified, the analysis type (image, video, image manual, video manual) selected and the analysis parameters specified. Moreover, users can choose to train their own models based on their own input images using the "Train Model" button to open a seperate window.\label{fig:2}](figure2.png)
│ │ +
│ │ +Although we included images of the human gastrocnemius medialis, tibialis anterior, soleus and vastus lateralis from four different devices, it is likely the provided pre-trained CNNs fail when images from different muscles or devices are analyzed. However, an option to train CNNs bases on user data is also included in the GUI. Users are thereby enabled to train own CNNs based on own image or video data. The training ultrasonography image data, the pre-trained CNNs, example usage files as well as usage and testing instructions are provided.
│ │ +`DL_Track_US` was developed to automatically segment muscle architectural parameters in longitudinal ultrasonography images or videos. Nevertheless, ultrasonography can be used to assess muscle anatomical cross-sectional area (ACSA) as well. We recently published DeepACSA [@ritsche], the counterpiece of DL_Track_US. DeepACSA was developed for automatic segmentation of muscle ACSA in transversal, panoramic ultrasonography images of the human rectus femoris, vastus lateralis and gastrocnemius muscles. 
│ │  
│ │  # References
│ │   --- dl_track_us-0.1.1/changelog.d/20221107_105509_paul.ritsche_pip_package_pr.rst
│ ├── +++ dl_track_us-0.1.2/changelog.d/20221107_105509_paul.ritsche_pip_package_pr.rst
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/changelog.d/20221114_112818_paul.ritsche.rst
│ ├── +++ dl_track_us-0.1.2/changelog.d/20221114_112818_paul.ritsche.rst
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/docs/.DS_Store
│ ├── +++ dl_track_us-0.1.2/docs/.DS_Store
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/docs/Makefile
│ ├── +++ dl_track_us-0.1.2/docs/Makefile
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/docs/make.bat
│ ├── +++ dl_track_us-0.1.2/docs/make.bat
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/docs/image_labelling/Image_Labeling_DLTrack.ijm
│ ├── +++ dl_track_us-0.1.2/docs/image_labelling/Image_Labeling_DLTrack.ijm
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/docs/source/DL_Track.gui_helpers.rst
│ ├── +++ dl_track_us-0.1.2/docs/source/DL_Track_US.gui_helpers.rst
│ │┄ Files 12% similar despite different names
│ │ @@ -1,78 +1,70 @@
│ │ -DL\_Track.gui\_helpers package
│ │ -==============================
│ │ +DL\_Track\_US.gui\_helpers package
│ │ +==================================
│ │  
│ │  Submodules
│ │  ----------
│ │  
│ │ -DL\_Track.gui\_helpers.calculate\_architecture module
│ │ ------------------------------------------------------
│ │ +DL\_Track\_US.gui\_helpers.calculate\_architecture module
│ │ +---------------------------------------------------------
│ │  
│ │ -.. automodule:: DL_Track.gui_helpers.calculate_architecture
│ │ +.. automodule:: DL_Track_US.gui_helpers.calculate_architecture
│ │     :members:
│ │     :undoc-members:
│ │     :show-inheritance:
│ │  
│ │ -DL\_Track.gui\_helpers.calculate\_architecture\_video module
│ │ -------------------------------------------------------------
│ │ +DL\_Track\_US.gui\_helpers.calculate\_architecture\_video module
│ │ +----------------------------------------------------------------
│ │  
│ │ -.. automodule:: DL_Track.gui_helpers.calculate_architecture_video
│ │ +.. automodule:: DL_Track_US.gui_helpers.calculate_architecture_video
│ │     :members:
│ │     :undoc-members:
│ │     :show-inheritance:
│ │  
│ │ -DL\_Track.gui\_helpers.calibrate module
│ │ ----------------------------------------
│ │ +DL\_Track\_US.gui\_helpers.calibrate module
│ │ +-------------------------------------------
│ │  
│ │ -.. automodule:: DL_Track.gui_helpers.calibrate
│ │ +.. automodule:: DL_Track_US.gui_helpers.calibrate
│ │     :members:
│ │     :undoc-members:
│ │     :show-inheritance:
│ │  
│ │ -DL\_Track.gui\_helpers.calibrate\_video module
│ │ -----------------------------------------------
│ │ +DL\_Track\_US.gui\_helpers.calibrate\_video module
│ │ +--------------------------------------------------
│ │  
│ │ -.. automodule:: DL_Track.gui_helpers.calibrate_video
│ │ +.. automodule:: DL_Track_US.gui_helpers.calibrate_video
│ │     :members:
│ │     :undoc-members:
│ │     :show-inheritance:
│ │  
│ │ -DL\_Track.gui\_helpers.do\_calculations module
│ │ -----------------------------------------------
│ │ +DL\_Track\_US.gui\_helpers.do\_calculations module
│ │ +--------------------------------------------------
│ │  
│ │ -.. automodule:: DL_Track.gui_helpers.do_calculations
│ │ +.. automodule:: DL_Track_US.gui_helpers.do_calculations
│ │     :members:
│ │     :undoc-members:
│ │     :show-inheritance:
│ │  
│ │ -DL\_Track.gui\_helpers.do\_calculations\_video module
│ │ ------------------------------------------------------
│ │ +DL\_Track\_US.gui\_helpers.do\_calculations\_video module
│ │ +---------------------------------------------------------
│ │  
│ │ -.. automodule:: DL_Track.gui_helpers.do_calculations_video
│ │ +.. automodule:: DL_Track_US.gui_helpers.do_calculations_video
│ │     :members:
│ │     :undoc-members:
│ │     :show-inheritance:
│ │  
│ │ -DL\_Track.gui\_helpers.image\_quality module
│ │ ---------------------------------------------
│ │ +DL\_Track\_US.gui\_helpers.manual\_tracing module
│ │ +-------------------------------------------------
│ │  
│ │ -.. automodule:: DL_Track.gui_helpers.image_quality
│ │ +.. automodule:: DL_Track_US.gui_helpers.manual_tracing
│ │     :members:
│ │     :undoc-members:
│ │     :show-inheritance:
│ │  
│ │ -DL\_Track.gui\_helpers.manual\_tracing module
│ │ ----------------------------------------------
│ │ +DL\_Track\_US.gui\_helpers.model\_training module
│ │ +-------------------------------------------------
│ │  
│ │ -.. automodule:: DL_Track.gui_helpers.manual_tracing
│ │ -   :members:
│ │ -   :undoc-members:
│ │ -   :show-inheritance:
│ │ -
│ │ -DL\_Track.gui\_helpers.model\_training module
│ │ ----------------------------------------------
│ │ -
│ │ -.. automodule:: DL_Track.gui_helpers.model_training
│ │ +.. automodule:: DL_Track_US.gui_helpers.model_training
│ │     :members:
│ │     :undoc-members:
│ │     :show-inheritance:
│ │   --- dl_track_us-0.1.1/docs/source/conf.py
│ ├── +++ dl_track_us-0.1.2/docs/source/conf.py
│ │┄ Files 12% similar despite different names
│ │ @@ -7,18 +7,18 @@
│ │  import os
│ │  import sys
│ │  sys.path.insert(0, os.path.abspath('../..'))
│ │  
│ │  # -- Project information -----------------------------------------------------
│ │  # https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
│ │  
│ │ -project = 'DLTrack'
│ │ +project = 'DL_Track_US'
│ │  copyright = '2022, Paul Ritsche, Olivier Seynnes, Neil Cronin'
│ │  author = 'Paul Ritsche, Olivier Seynnes, Neil Cronin'
│ │ -release = '0.1.1'
│ │ +release = '0.1.2'
│ │  
│ │  # -- General configuration ---------------------------------------------------
│ │  # https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
│ │  
│ │  extensions = [
│ │  	"sphinx.ext.autodoc"
│ │  	]
│ │   --- dl_track_us-0.1.1/docs/source/contribute.rst
│ ├── +++ dl_track_us-0.1.2/docs/source/contribute.rst
│ │┄ Files 2% similar despite different names
│ │ @@ -1,40 +1,40 @@
│ │  Contributing
│ │  ============
│ │  
│ │ -We are happy whenever you decide to contribute to DL_Track. However, when contributing to DL_Track, please first file an issue in our Github `issue section <https://github.com/PaulRitsche/DLTrack/issues>`_. Label the issue with "improvement" and describe your suggestion. Please formulate the title of the issue similar to this: Contribution - Your contribution idea. Please also state in the issue wheter you want to implement it yourself / already implemented it in your code or if you would like us to implement it. We will be in touch with you.
│ │ +We are happy whenever you decide to contribute to DL_Track_US. However, when contributing to DL_Track_US, please first file an issue in our Github `issue section <https://github.com/PaulRitsche/DL_Track_US/issues>`_. Label the issue with "improvement" and describe your suggestion. Please formulate the title of the issue similar to this: Contribution - Your contribution idea. Please also state in the issue wheter you want to implement it yourself / already implemented it in your code or if you would like us to implement it. We will be in touch with you.
│ │  Please note we have a code of conduct, please follow it in all your interactions with the project.
│ │  
│ │  In case you have decided to implement your suggestion yourself and we agreed that you should file a pull request, take a look at the steps listed below.
│ │  
│ │  Pull Request Process for contributing own material
│ │  --------------------------------------------------
│ │  
│ │ -1. Update the DL_Track_tutorial.pdf in the `docs folder <https://github.com/PaulRitsche/DLTrack/docs/usage>`_ with details of changes to the interface should they be relevant for the user. Simply add the step where it is during the analysis or create a new chapter.
│ │ +1. Update the DL_Track_US_tutorial.pdf in the `docs folder <https://github.com/PaulRitsche/DL_Track_US/tree/main/docs/usage>`_ with details of changes to the interface should they be relevant for the user. Simply add the step where it is during the analysis or create a new chapter.
│ │  2. Add a changelog to the changelog.d folder describing exactly what you changed in the project and the environment.
│ │  3. When adding new functions, please follow the code and docstring styles used throughout the code. FYI, we used the `Numpy styleguide <https://numpydoc.readthedocs.io/en/latest/format.html>`_.
│ │  4. Create a new branch (named yourchange_yourinitials) and a Pull Request to merge your work on the **main** branch of the project.
│ │  
│ │  Report a bug
│ │  ------------
│ │  
│ │ -In order to report a bug, please file an issue in our `issue section <https://github.com/PaulRitsche/DLTrack/issues>`_  on Github. Label your issue with the "bug" label and describe the bug you found. Please formulate the title of the issue similar to this: Bugreport - Your bug that  occured. Please describe the occurence of the bug as reproducible as possible. It's best to share with us the following in the issue:
│ │ +In order to report a bug, please file an issue in our `issue section <https://github.com/PaulRitsche/DL_Track_US/issues>`_  on Github. Label your issue with the "bug" label and describe the bug you found. Please formulate the title of the issue similar to this: Bugreport - Your bug that  occured. Please describe the occurence of the bug as reproducible as possible. It's best to share with us the following in the issue:
│ │  
│ │  - operating system
│ │  - the error raised by your code
│ │  - all steps to reproduce the bug
│ │  - code that produced the bug
│ │  
│ │  We will then be in touch with you and try to solve the problem as quickly as possible.
│ │  Please note we have a code of conduct, please follow it in all your interactions with the project.
│ │  
│ │  Getting Support
│ │  ---------------
│ │  
│ │ -If you have any questions about the project, encountered problems / need help during the installation procedure or encountered problems / need help during the usage of DL_Track not related to bugs, don't hesitate to report this in the Q&A section in the `DL_Track discussion forum <https://github.com/PaulRitsche/DLTrack/discussions>`_. This is the space to have conversations, ask questions and post answers without opening issues.
│ │ +If you have any questions about the project, encountered problems / need help during the installation procedure or encountered problems / need help during the usage of DL_Track_US_US not related to bugs, don't hesitate to report this in the Q&A section in the `DL_Track_US discussion forum <https://github.com/PaulRitsche/DL_Track_US/discussions>`_. This is the space to have conversations, ask questions and post answers without opening issues.
│ │  
│ │  Code of Conduct
│ │  ===============
│ │  
│ │  Our Pledge
│ │  ----------
│ │   --- dl_track_us-0.1.1/docs/source/index.rst
│ ├── +++ dl_track_us-0.1.2/docs/source/index.rst
│ │┄ Files 6% similar despite different names
│ │ @@ -1,28 +1,28 @@
│ │ -.. DL_Track documentation master file, created by
│ │ +.. DL_Track_US documentation master file, created by
│ │     sphinx-quickstart on Mon Nov  7 13:31:24 2022.
│ │     You can adapt this file completely to your liking, but it should at least
│ │     contain the root `toctree` directive.
│ │  
│ │ -DL_Track
│ │ -========
│ │ +DL_Track_US
│ │ +===========
│ │  *Automated analysis of human lower limb ultrasonography images*
│ │  
│ │ -So, what is DL_Track all about? The DL_Track algorithm was first presented by Neil Cronin,
│ │ -Olivier Seynnes and Taija Finni in `2020 <https://arxiv.org/pdf/2009.04790.pdf>`_. The algorithm makes extensive use of fully convolutional neural networks trained on a fair amount of ultrasonography images of the human lower limb. Specifically, the dataset included longitudinal ultrasonography images from the human gastrocnemius medialis, tibialis anterior, soleus and vastus lateralis. The algorithm is able to analyse muscle architectural parameters (muscle thickness, fasciclelength and pennation angle) in both, single image files as well as videos. By employing deep learning models, the DL_Track algorithm is one of the first fully automated algorithms, requiring no user input during the analysis. Then in 2022, we (Paul Ritsche, Olivier Synnes, Neil Cronin) have updated the code and deep learning models substantially, added a graphical user interface, manual analysis and an extensive documentation. Moreover we turned everything into an openly available Pypi package.
│ │ +So, what is DL_Track_US all about? The DL_Track_US algorithm was first presented by Neil Cronin,
│ │ +Olivier Seynnes and Taija Finni in `2020 <https://arxiv.org/pdf/2009.04790.pdf>`_. The algorithm makes extensive use of fully convolutional neural networks trained on a fair amount of ultrasonography images of the human lower limb. Specifically, the dataset included longitudinal ultrasonography images from the human gastrocnemius medialis, tibialis anterior, soleus and vastus lateralis. The algorithm is able to analyse muscle architectural parameters (muscle thickness, fasciclelength and pennation angle) in both, single image files as well as videos. By employing deep learning models, the DL_Track_US algorithm is one of the first fully automated algorithms, requiring no user input during the analysis. Then in 2022, we (Paul Ritsche, Olivier Synnes, Neil Cronin) have updated the code and deep learning models substantially, added a graphical user interface, manual analysis and an extensive documentation. Moreover we turned everything into an openly available Pypi package.
│ │  
│ │ -Why use DL_Track?
│ │ -=================
│ │ +Why use DL_Track_US?
│ │ +====================
│ │  
│ │ -Using the DL_Track python package to analyze muscle architectural parameters in human lower limb muscle ultrasonography images hase two main advantages. The analysis is objectified when using the automated analysis types for images and videos because no user input is required during the analysis process. Secondly, the required analysis time for image or video analysis is drastically reduced compared to manual analysis. Whereas an image or video frame manual analysis takes about one minute, DL_Track analyzes images and video frames in less than one second. This allows users to analyze large amounts of images without supervision during the analysis process in relatively short amounts of time.
│ │ +Using the DL_Track_US python package to analyze muscle architectural parameters in human lower limb muscle ultrasonography images hase two main advantages. The analysis is objectified when using the automated analysis types for images and videos because no user input is required during the analysis process. Secondly, the required analysis time for image or video analysis is drastically reduced compared to manual analysis. Whereas an image or video frame manual analysis takes about one minute, DL_Track_US analyzes images and video frames in less than one second. This allows users to analyze large amounts of images without supervision during the analysis process in relatively short amounts of time.
│ │  
│ │  Limitations
│ │  ===========
│ │  
│ │ -Currently, we have not provided unit testing for the functions and modules included in the DL_Track package. Moreover, the muscles included in the training data set are limited to the lower extremities. Although we included images from as many ultrasonography devices as possible, we were only able to include images from four different devices. Therefore, users aiming to analyze images from different muscles or different ultrasonography devices might be required to train their own models because the provided pre-trained models result in bad segmentations. The time required for image analysis compared to manual analysis is tremendously reduced. However, employing the networks for analysis of long videos containing many frames (>2000) may still require a few hours. Lastly, even though  DL_Track objectifies the analysis of ultrasonography images when using the automated analysis types, we labeled the images manually. Therefore, we introduced some subjectivity into the datasets.
│ │ +Currently, we have not provided unit testing for the functions and modules included in the DL_Track_US package. Moreover, the muscles included in the training data set are limited to the lower extremities. Although we included images from as many ultrasonography devices as possible, we were only able to include images from four different devices. Therefore, users aiming to analyze images from different muscles or different ultrasonography devices might be required to train their own models because the provided pre-trained models result in bad segmentations. The time required for image analysis compared to manual analysis is tremendously reduced. However, employing the networks for analysis of long videos containing many frames (>2000) may still require a few hours. Lastly, even though  DL_Track_US objectifies the analysis of ultrasonography images when using the automated analysis types, we labeled the images manually. Therefore, we introduced some subjectivity into the datasets.
│ │  
│ │  
│ │  .. toctree::
│ │     :caption: Contents
│ │     :hidden:
│ │  
│ │     modules
│ │   --- dl_track_us-0.1.1/docs/source/installation.rst
│ ├── +++ dl_track_us-0.1.2/docs/source/installation.rst
│ │┄ Files 8% similar despite different names
│ │ @@ -1,116 +1,112 @@
│ │  Installation
│ │  ============
│ │  
│ │  We offer two possible installation approaches for our DL_Track software. The first option is to download the DL_Track executable file. The second option we describe is DL_Track package installation via Github and pythons package manager pip. We want to inform you that there are more ways to install the package. However, we do not aim to be complete and rather demonstrate an (in our opinion) user friendly way for the installation of DL_Track. Moreover, we advise users with less programming experience to make use of the first option and download the executable file.
│ │  
│ │ -Download the DL\_Track executable
│ │ ----------------------------------
│ │ +Download the DL\_Track\_US executable
│ │ +-------------------------------------
│ │  
│ │  1. Got to the Zenodo webpage containing the DL_Track executable, the pre-trained models and the example files using this `link <https://zenodo.org/record/7318089#.Y3S2qKSZOUk>`_.
│ │ -2. Download the DL_Track_example.zip
│ │ -3. Download both pre-trained models (model-apo-VGG-BCE-512.h5 & model-VGG16-fasc-BCE-512.h5).
│ │ -4. Find the DL_Track.exe Executable in the DLTrack_example/executable folder.
│ │ -5. Create a specified DL_Track directory and put the DL_Track.exe, the model files and the example file in seperate subfolders (for example "Executable", "Models" and "Example"). Moreover, unpack the DL_Track_example.zip file.
│ │ -6. Open the DL_Track GUI by double clicking the DL_Track.exe file and start with the testing procedure to check that everything works properly (see `Examples <https://dltrack.readthedocs.io/en/latest/usage.html>`_ and `Testing <https://dltrack.readthedocs.io/en/latest/tests.html>`_).
│ │ +2. Download the DL_Track_US_example.zip folder and unpack the file.
│ │ +3. Find the DL_Track_US.exe executable located in the DLTrack_example/executable folder.
│ │ +4. Open the DL_Track_US_GUI by double clicking the DL_Track.exe file and start with the testing procedure to check that everything works properly (see `Examples <https://dltrack.readthedocs.io/en/latest/usage.html>`_ and `Testing <https://dltrack.readthedocs.io/en/latest/tests.html>`_). In case you get an anti-virus notification, trust us and click it away. We assure you the software is harmless. 
│ │  
│ │ -Install DL_Track via Github, pip and Pypi.org
│ │ ----------------------------------------------
│ │ +Install DL_Track_US via Github, pip and Pypi.org
│ │ +------------------------------------------------
│ │  
│ │  In case you want to use this way to install and run DL_Track, we advise you to setup conda (see step 1) and download the environment.yml file from the repo (see steps 5-8). If you want to actively contribute to the project or customize the code, it might be usefull to you to do all of the following steps (for more information see `Contributing Guidelines <https://dltrack.readthedocs.io/en/latest/contribute.html>`_).
│ │  
│ │ -1. Anaconda setup (only before first usage and if Anaconda/minicoda is not already installed).
│ │ +*Step 1.* Anaconda setup (only before first usage and if Anaconda/minicoda is not already installed).
│ │  
│ │  Install `Anaconda <https://www.anaconda.com/distribution/>`_ (click ‘Download’ and be sure to choose ‘Python 3.X Version’ (where the X represents the latest version being offered. IMPORTANT: Make sure you tick the ‘Add Anaconda to my PATH environment variable’ box).
│ │  
│ │ -2. **(Optional, only required for contributing or development)** Git setup (only before first usage and if Git is not already installed). This is optional and only required when you want to clone the whole DL_Track Github repository.
│ │ +*Step 2.* **(Only required for MacOS users, contributing or development)** Git setup (only before first usage and if Git is not already installed). This is optional and only required when you want to clone the whole DL_Track Github repository.
│ │  
│ │  In case you have never used Git before on you computer, please install it using the instructions provided `here <https://git-scm.com/download>`_.
│ │  
│ │ -3. **(Optional, only required for contributing or development)** Create a directory for DL_Track.
│ │ +*Step 3.* **(Only required for MacOS users, contributing or development)** Create a directory for DL_Track.
│ │  
│ │ -On your computer create a specific directory for DL_Track (for example "DL_Track") and navigate there. You can use Git as a version control system. Once there open a git bash with right click and then "Git Bash Here". In the bash terminal, type the following:
│ │ +On your computer create a specific directory for DL_Track (for example "DL_Track_US") and navigate there. You can use Git as a version control system. Once there open a git bash with right click and then "Git Bash Here". In the bash terminal, type the following:
│ │  
│ │  ``git init``
│ │  
│ │  This will initialize a git repository and allows you to continue. If run into problems, check this `website <https://git-scm.com/book/en/v2/Git-Basics-Getting-a-Git-Repository>`_.
│ │  
│ │ -4. **(Optional, only required for contributing or development)** Clone the DL_Track Github repository into a pre-specified folder (for example "DL_Track) by typing the following code in your bash window:
│ │ +*Step 4.* **(Only required for MacOS users, contributing or development)** Clone the DL_Track_US Github repository into a pre-specified folder (for example "DL_Track_US") by typing the following code in your bash window:
│ │  
│ │ -``git clone https://github.com/PaulRitsche/DL_Track.git``
│ │ +``git clone https://github.com/PaulRitsche/DL_Track_US.git``
│ │  
│ │ -This will clone the entire repository to your local computer. To make sure that everything worked, see if the files in your local directory match the ones you can find in the Github DL_Track repository. If you run into problem, check this `website <https://git-scm.com/book/en/v2/Git-Basics-Getting-a-Git-Repository>`_.
│ │ +This will clone the entire repository to your local computer. To make sure that everything worked, see if the files in your local directory match the ones you can find in the Github DL_Track_US repository. If you run into problem, check this `website <https://git-scm.com/book/en/v2/Git-Basics-Getting-a-Git-Repository>`_.
│ │  
│ │ -Alternatively, you can only download the environment.yml file from the `DL_Track repo <https://github.com/PaulRitsche/DLTrack/>`_ and continue to the next step.
│ │ +Alternatively, you can only download the environment.yml file from the `DL_Track_US repo <https://github.com/PaulRitsche/DL_Track_US/>`_ and continue to the next step.
│ │  
│ │ -5. Create the virtual environment required for DL_Track.
│ │ +*Step 5.* Create the virtual environment required for DL_Track_US.
│ │  
│ │ -DL_Track is bound to a specific python version (3.10). You now need to open an Anaconda terminal. Type the following command in your search window:
│ │ +DL_Track is bound to a specific python version (3.10). To create an environment for DL_Track_US, type the following command in your Git bash terminal:
│ │  
│ │ -``Anaconda``
│ │ +``conda create -n DL_Track_US python=3.10``
│ │  
│ │ -Select the app named "Anaconda Powershell Prompt". By clicking on it, a black terminal should appear. To create an environment for DL_Track, type the following command in the prompt:
│ │ -
│ │ -``conda create -n DL_Track python=3.10``
│ │ -
│ │ -6. Activate the environment for usage of DL_Track.
│ │ +*Step 6.* Activate the environment for usage of DL_Track_US.
│ │  
│ │  You can now activate the virtual environment by typing:
│ │  
│ │ -``conda activate DL_Track``
│ │ +``conda activate DL_Track_US``
│ │  
│ │ -An active conda environment is visible in () brackets befor your current path in the bash terminal. In this case, this should look something like (DL_Track) C:/user/.../DL_Track.Then, download the DL_Track package by typing:
│ │ +An active conda environment is visible in () brackets befor your current path in the bash terminal. In this case, this should look something like (DL_Track_US) C:/user/.../DL_Track_US.Then, download the DL_Track_US package by typing:
│ │  
│ │ -7. Install the DL_Track package.
│ │ +*Step 7.* Install the DL_Track_US package.
│ │  
│ │ -You can directly install the DL_Track package from Pypi. To do so, type the following command in your prompt:
│ │ +**Attention: The next part of Step 7 is NOT relevant for MacOS users:**
│ │  
│ │ -``pip install DL-Track-US`` 
│ │ +You can directly install the DL_Track_US package from Pypi. To do so, type the following command in your bash terminal:
│ │ +
│ │ +``pip install DL-Track-US==0.1.1`` 
│ │  
│ │  All the package dependencies will be installed automatically. You can verify whether the environment was correctly created by typing the following command in your bash terminal:
│ │  
│ │  ``conda list``
│ │  
│ │ -Now, all packages included in the DL_Track environment will be listed and you can check if all packages listed in the "DLTrack/environment.yml" file under the section "- pip" are included in the DL_Track environment.
│ │ -If you run into problems open a discussion in the Q&A section of `DL_Track discussions <https://github.com/PaulRitsche/DLTrack/discussions/categories/q-a>`_ and assign the label "Problem".
│ │ +Now, all packages included in the DL_Track_US environment will be listed and you can check if all packages listed in the "DL_Track_US/environment.yml" file under the section "- pip" are included in the DL_Track environment.
│ │ +If you run into problems open a discussion in the Q&A section of `DL_Track_US discussions <https://github.com/PaulRitsche/DL_Track_US/discussions/categories/q-a>`_ and assign the label "Problem".
│ │  
│ │ -**Attention MacOS users:** 
│ │ -Do not install the DL_Track package from Pypi. We advise you to use the provided requirements.txt file for environment creation. You need to slightly modify it. Change *tensorflow==2.10.0* to *tensorflow-macos=2.10.0*.  You need to create and activate the environment first:
│ │ +**Attention: The next part of Step 7 is ONLY relevant for MacOS users:**
│ │  
│ │ -``conda create -n DL_Track python=3.10``
│ │ +Do not install the DL_Track_US package from Pypi. We advise you to use the provided requirements.txt file for environment creation. You need to create and activate the environment first (see Step 5 & 6) and navigate into the folder that you cloned from Github (DL_Track_US) with the bash terminal. You can do that by typing "cd" followed by the path to the folder containing the requirements.txt file. This should look something like:
│ │  
│ │ -Activate the environment as described in the next section. Then you can install the requirements of DL_Track with: 
│ │ -
│ │ -``pip install -r requirements.txt``
│ │ -
│ │ -There are some more steps necessary for DL_Track usage, you'll finde the instructions in the `usage <https://dltrack.readthedocs.io/en/latest/usage.html>`_ section. 
│ │ +``cd /.../.../DLTrack/DL_Track_US``
│ │  
│ │ +Then you can install the requirements of DL_Track with: 
│ │  
│ │ +``pip install -r requirements.txt``
│ │  
│ │ +Install the DL_Track package locally to make use of its functionalities with:
│ │  
│ │ +``python -m pip install -e .``
│ │  
│ │ +There are some more steps necessary for DL_Track_US usage, you'll finde the instructions in the `usage <https://dltrack.readthedocs.io/en/latest/usage.html>`_ section. 
│ │  
│ │ -7. The First option of running DL_Track is installing the DL_Track package from Pypi.org. You do not need the whole cloned repository for this, only the active DL_Track environment. You do moreover not need be any specific directory. Type in your bash terminal:
│ │ +*Step 8.* The First option of running DL_Track_US is using the installed DL_Track package. You do not need the whole cloned repository for this, only the active DL_Track_US environment. You do moreover not need be any specific directory. Type in your bash terminal:
│ │  
│ │ -``python -m DL_Track``
│ │ +``python -m DL_Track_US``
│ │  
│ │ -The main GUI should now open. If you run into problems, open a discussion in the Q&A section of `DL_Track discussions <https://github.com/PaulRitsche/DLTrack/discussions/categories/q-a>`_ and assign the label "Problem".  For usage of DL_Track please take a look at the `docs <https://github.com/PaulRitsche/DLTrack/tree/main/docs/usage>`_ directory in the Github repository.
│ │ +The main GUI should now open. If you run into problems, open a discussion in the Q&A section of `DL_Track_US discussions <https://github.com/PaulRitsche/DLTrack/discussions/categories/q-a>`_ and assign the label "Problem".  For usage of DL_Track please take a look at the `docs <https://github.com/PaulRitsche/DLTrack/tree/main/docs/usage>`_ directory in the Github repository.
│ │  
│ │ -8. The second option of running DL_Track is using the DLTrack_GUI python script. This requires you to clone the whole directory and navigate to the directory where the DLTrack_GUI.py file is located. Moreover, you need the active DL_Track environment.
│ │ +*Step 9.* The second option of running DL_Track_US is using the DLTrack_GUI python script. This requires you to clone the whole directory and navigate to the directory where the DL_Track_US_GUI.py file is located. Moreover, you need the active DL_Track_US environment.
│ │  
│ │ -The DLTrack_GUI.py file is located at the `DL_Track <https://github.com/PaulRitsche/DLTrack/DL_Track>`_ folder. To execute the module type the following command in your bash terminal.
│ │ +The DL_Track_US_GUI.py file is located at the `DL_Track_US/DL_Track_US <https://github.com/PaulRitsche/DL_Track_US/DL_Track_US>`_ folder. To execute the module type the following command in your bash terminal.
│ │  
│ │ -``python DLTrack_GUI.py``
│ │ +``python DL_Track_US_GUI.py``
│ │  
│ │ -The main GUI should now open. If you run into problems, open a discussion in the Q&A section of `DL_Track discussions <https://github.com/PaulRitsche/DLTrack/discussions/categories/q-a>`_ and assign the label "Problem". You can find an example discussion there. For usage of DL_Track please take a look at the `docs <https://github.com/PaulRitsche/DLTrack/tree/main/docs/usage>`_ directory in the Github repository.
│ │ +The main GUI should now open. If you run into problems, open a discussion in the Q&A section of `DL_Track_US discussions <https://github.com/PaulRitsche/DL_Track_US/discussions/categories/q-a>`_ and assign the label "Problem". You can find an example discussion there. For usage of DL_Track_US please take a look at the `docs <https://github.com/PaulRitsche/DL_Track_US/tree/main/docs/usage>`_ directory in the Github repository.
│ │  
│ │  
│ │  GPU setup
│ │  ---------
│ │  
│ │  **Attention: The next section is only relevant for windows users!**
│ │  
│ │ -The processing speed of a single image or video frame analyzed with DL_Track is highly dependent on computing power. While possible, model inference and model training using a CPU only will decrese processing speed and prolong the model training process. Therefore, we advise to use a GPU whenever possible. Prior to using a GPU it needs to be set up. Firstly the GPU drivers must be locally installed on your computer. You can find out which drivers are right for your GPU `here <https://www.nvidia.com/Download/index.aspx?lang=en-us>`_. Subsequent to installing the drivers, you need to install the interdependant CUDA and cuDNN software packages. To use DL_Track with tensorflow version 2.10 you need to install CUDA version 11.2 from `here <https://developer.nvidia.com/cuda-11.2.0-download-archive>`_ and cuDNN version 8.5 for CUDA version 11.x from `here <https://developer.nvidia.com/rdp/cudnn-archive>`_ (you may need to create an nvidia account). As a next step, you need to be your own installation wizard. We refer to this `video <https://www.youtube.com/watch?v=OEFKlRSd8Ic>`_ (up to date, minute 9 to minute 13) or this `video <https://www.youtube.com/watch?v=IubEtS2JAiY&list=PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL&index=2>`_ (older, entire video but replace CUDA and cuDNN versions). There are procedures at the end of each video testing whether a GPU is detected by tensorflow or not. If you run into problems with the GPU/CUDA setup, please open a discussion in the Q&A section of `DL_Track discussions <https://github.com/PaulRitsche/DLTrack_US/discussions/categories/q-a>`_ and assign the label "Problem".
│ │ +The processing speed of a single image or video frame analyzed with DL_Track_US is highly dependent on computing power. While possible, model inference and model training using a CPU only will decrese processing speed and prolong the model training process. Therefore, we advise to use a GPU whenever possible. Prior to using a GPU it needs to be set up. Firstly the GPU drivers must be locally installed on your computer. You can find out which drivers are right for your GPU `here <https://www.nvidia.com/Download/index.aspx?lang=en-us>`_. Subsequent to installing the drivers, you need to install the interdependant CUDA and cuDNN software packages. To use DL_Track_US with tensorflow version 2.10 you need to install CUDA version 11.2 from `here <https://developer.nvidia.com/cuda-11.2.0-download-archive>`_ and cuDNN version 8.5 for CUDA version 11.x from `here <https://developer.nvidia.com/rdp/cudnn-archive>`_ (you may need to create an nvidia account). As a next step, you need to be your own installation wizard. We refer to this `video <https://www.youtube.com/watch?v=OEFKlRSd8Ic>`_ (up to date, minute 9 to minute 13) or this `video <https://www.youtube.com/watch?v=IubEtS2JAiY&list=PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL&index=2>`_ (older, entire video but replace CUDA and cuDNN versions). There are procedures at the end of each video testing whether a GPU is detected by tensorflow or not. If you run into problems with the GPU/CUDA setup, please open a discussion in the Q&A section of `DL_Track_US discussions <https://github.com/PaulRitsche/DL_Track_US/discussions/categories/q-a>`_ and assign the label "Problem".
│ │  
│ │  **Attention : The next section is only relevant for MacOS users!**
│ │  
│ │  In case you want to make use of you M1 / M2 chips for model training and / or inference, we refer you to this `tutorial <https://caffeinedev.medium.com/how-to-install-tensorflow-on-m1-mac-8e9b91d93706>`_. There you will find a detailed description of how to enable GPU support for tensorflow. It is not strictly necessary to do that for model training or inference, but will speed up the process.
│ │   --- dl_track_us-0.1.1/docs/source/modules.rst
│ ├── +++ dl_track_us-0.1.2/docs/source/modules.rst
│ │┄ Files 21% similar despite different names
│ │ @@ -1,46 +1,46 @@
│ │  **Welcome to the DLTrack user information page. Please choose the section where you want to head!**
│ │  
│ │  Installation
│ │  ============
│ │ -Here you can find all relevant information concerning the installation of DL_Track.
│ │ +Here you can find all relevant information concerning the installation of DL_Track_US.
│ │  
│ │  .. toctree::
│ │     :maxdepth: 1
│ │  
│ │     installation
│ │  
│ │  Usage Guidelines
│ │  ================
│ │ -Here you can find all relevant information concerning the usage of DL_Track.
│ │ +Here you can find all relevant information concerning the usage of DL_Track_US.
│ │  
│ │  .. toctree::
│ │     :maxdepth: 1
│ │  
│ │     usage
│ │  
│ │  Test Guidelines
│ │  ===============
│ │ -Here you can find all relevant information concerning the testing of DL_Track.
│ │ +Here you can find all relevant information concerning the testing of DL_Track_US.
│ │  
│ │  .. toctree::
│ │     :maxdepth: 1
│ │  
│ │     tests
│ │  
│ │  Contributing Guidelines
│ │  =======================
│ │ -Here you can find all relevant information concerning contributing to DL_Track.
│ │ +Here you can find all relevant information concerning contributing to DL_Track_US.
│ │  
│ │  .. toctree::
│ │     :maxdepth: 1
│ │  
│ │     contribute
│ │  
│ │  Documentation
│ │  =============
│ │ -Here you can find the code documentation of DL_Track.
│ │ +Here you can find the code documentation of DL_Track_US.
│ │  
│ │  .. toctree::
│ │ -   :maxdepth: 3
│ │ +   :maxdepth: 1
│ │  
│ │ -   DL_Track
│ │ +   DL_Track_US
│ │   --- dl_track_us-0.1.1/docs/source/usage.rst
│ ├── +++ dl_track_us-0.1.2/docs/source/usage.rst
│ │┄ Files 25% similar despite different names
│ │ @@ -1,9 +1,8 @@
│ │ -DL_Track Usage
│ │ -==============
│ │ +DL_Track_US Usage
│ │ +=================
│ │  
│ │ -We have provided detailed usage instructions and examples for DL_Track in the DL_Track_tutorial.pdf file. The file is located in the `docs <https://github.com/PaulRitsche/DLTrack/tree/main/docs/usage>`_ directory in our Github repository.
│ │ +We have provided detailed usage instructions and examples for DL_Track_US in the DL_Track_US_tutorial.pdf file. The file is located in the `docs <https://github.com/PaulRitsche/DL_Track_US/tree/main/docs/usage>`_ directory in our Github repository. In general, should the graphical user interface freeze and you can't interact with it, close it and restart. Pay attention that all parameters in the interface are specified correctly.
│ │  
│ │  **Attention MacOS users:**
│ │ -The DL_Track package is only fully functional on windows OS. However, with restricted functionality and slight adaptations in the source code, macOS users can employ the DL_Track as well. With macOS, the manual scaling option vor video and image analysis is not functional. Therefore, images cannot be scaled this way in the GUI. A possible solution is to scale the analysis results subsequent to completion of the analysis. Therefore, the pixel per centimeter must be calculated elsewhere. One option is to use `FIJI <https://imagej.net/software/fiji/downloads>`_. By drawing a line on the image, it is possible to see the length of the line in pixel units. Open the respective image in FIJI by drag and drop. Draw a line on the image with a known distance of one centimetre, click `cmd + m` and get the length of the line in pixel unit from the result window. Do that for every image with varying scanning depth. Divide the analysis results for muscle thickness and fascicle length by the linelength in pixel units. The result will be the muscle thickness and fascicle length in centimeter units.
│ │ +The DL_Track_US package is only fully functional on windows OS. However, with restricted functionality macOS users can employ the DL_Track as well. With macOS, the manual scaling option for video and image analysis is not functional. Therefore, images cannot be scaled this way in the GUI. A possible solution is to scale the analysis results subsequent to completion of the analysis. Therefore, the pixel per centimeter must be calculated elsewhere. One option is to use `FIJI <https://imagej.net/software/fiji/downloads>`_. By drawing a line on the image, it is possible to see the length of the line in pixel units. Open the respective image in FIJI by drag and drop. Draw a line on the image with a known distance of one centimetre, click `cmd + m` and get the length of the line in pixel unit from the result window. Do that for every image with varying scanning depth. Divide the analysis results for muscle thickness and fascicle length by the linelength in pixel units. The result will be the muscle thickness and fascicle length in centimeter units.
│ │  
│ │ -In order for the video analysis to work, line 605 ``cv2.imshow("Analysed image", comb)`` in the file      ``do_calculations_video.py`` must be commented out using the # symbol. The file is located at "DLTrack/DL_Track/gui_helpers/do_caclualtions_video.py".  Do the same for the ``do_calculations_video.py`` file in the directory ".../.../.../Anaconda/envs/DL_Track2/Lib/site-packages/DL_Track/gui_helpers" to modify the source file of the DL_Track package. The video analysis will work now, but the analysis will not be displayed directly on the scree. In case you need help, don't hesitate to contact us.
│ │   --- dl_track_us-0.1.1/docs/usage/DL_Track_tutorial.pdf
│ ├── +++ dl_track_us-0.1.2/docs/usage/DL_Track_tutorial.pdf
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/tests/DL_Track_tests.pdf
│ ├── +++ dl_track_us-0.1.2/tests/DL_Track_tests.pdf
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/LICENSE
│ ├── +++ dl_track_us-0.1.2/LICENSE
│ │┄ Files identical despite different names
│ │   --- dl_track_us-0.1.1/README_Pypi.md
│ ├── +++ dl_track_us-0.1.2/README_Pypi.md
│ │┄ Files 14% similar despite different names
│ │ @@ -1,36 +1,36 @@
│ │ -# DL_Track
│ │ +# DL_Track_US
│ │  
│ │  [![Documentation Status](https://readthedocs.org/projects/dltrack/badge/?version=latest)](https://dltrack.readthedocs.io/en/latest/?badge=latest)
│ │  
│ │ -The DL_Track package provides an easy to use graphical user interface (GUI) for deep learning based analysis of muscle architectural parameters from longitudinal ultrasonography images of human lower limb muscles. Please take a look at our [documentation](https://dltrack.readthedocs.io/en/latest/index.html) for more information.
│ │ +The DL_Track_US package provides an easy to use graphical user interface (GUI) for deep learning based analysis of muscle architectural parameters from longitudinal ultrasonography images of human lower limb muscles. Please take a look at our [documentation](https://dltrack.readthedocs.io/en/latest/index.html) for more information.
│ │  This code is based on a previously published [algorithm](https://github.com/njcronin/DL_Track) and replaces it. We have extended the functionalities of the previously proposed code. The previous code will not be updated and future updates will be included in this repository.
│ │  
│ │  ## Getting started
│ │  
│ │ -For detailled information about installaion of the DL_Track python package we refer you to our [documentation](https://dltrack.readthedocs.io/en/latest/installation.html). There you will finde guidelines not only for the installation procedure of DL_Track, but also concerding conda and GPU setup.
│ │ +For detailled information about installaion of the DL_Track_US python package we refer you to our [documentation](https://dltrack.readthedocs.io/en/latest/installation.html). There you will finde guidelines not only for the installation procedure of DL_Track_US, but also concerning conda and GPU setup.
│ │  
│ │  ## Quickstart
│ │  
│ │ -Once installed, DL_Track can be started from the command prompt with the respective environment activated:
│ │ +Once installed, DL_Track_US can be started from the command prompt with the respective environment activated:
│ │  
│ │ -``(DL_Track) C:/User/Desktop/ python DL_Track`` 
│ │ +``(DL_Track_US) C:/User/Desktop/ python DL_Track_US`` 
│ │  
│ │ -In case you have downloaded the executable, simply double-click the DL_Track icon.
│ │ +In case you have downloaded the executable, simply double-click the DL_Track_US icon.
│ │  
│ │ -Regardless of the used method, the GUI should open. For detailed the desciption of our GUI as well as usage examples, please take a look at the [user instruction](https://github.com/PaulRitsche/DLTrack/docs/usage).
│ │ +Regardless of the used method, the GUI should open. For detailed the desciption of our GUI as well as usage examples, please take a look at the [user instruction](https://github.com/PaulRitsche/DL_Track_US/tree/main/docs/usage).
│ │  
│ │  ## Testing
│ │  
│ │ -We have not yet integrated unit testing for DL_Track. Nonetheless, we have provided instructions to objectively test whether DL_Track, once installed, is functionable. Do perform the testing procedures yourself, check out the [test instructions](https://github.com/PaulRitsche/DLTrack/tests).
│ │ +We have not yet integrated unit testing for DL_Track_US. Nonetheless, we have provided instructions to objectively test whether DL_Track_US, once installed, is functionable. To perform the testing procedures yourself, check out the [test instructions](https://github.com/PaulRitsche/DL_Track_US/tree/main/tests).
│ │  
│ │  ## Code documentation 
│ │  
│ │ -In order to see the detailled scope and description of the modules and functions included in the DL_Track package, you can do so either directly in the code, or in the [Documentation](https://dltrack.readthedocs.io/en/latest/modules.html#documentation) section of our online documentation.
│ │ +In order to see the detailled scope and description of the modules and functions included in the DL_Track_US package, you can do so either directly in the code, or in the [Documentation](https://dltrack.readthedocs.io/en/latest/modules.html#documentation) section of our online documentation.
│ │  
│ │  ## Previous research
│ │  
│ │ -The previously published [algorithm](https://github.com/njcronin/DL_Track) was developed with the aim to compare the performance of the trained deep learning models with manual analysis of muscle fascicle length, muscle fascicle pennation angle and muscle thickness. The results were presented in a published [preprint](https://arxiv.org/pdf/2009.04790.pdf). The results demonstrated in the article described the DL_Track algorithm to be comparable with manual analysis of muscle fascicle length, muscle fascicle pennation angle and muscle thickness in ultrasonography images as well as videos.
│ │ +The previously published [algorithm](https://github.com/njcronin/DL_Track) was developed with the aim to compare the performance of the trained deep learning models with manual analysis of muscle fascicle length, muscle fascicle pennation angle and muscle thickness. The results were presented in a published [preprint](https://arxiv.org/pdf/2009.04790.pdf). The results demonstrated in the article described the DL_Track_US algorithm to be comparable with manual analysis of muscle fascicle length, muscle fascicle pennation angle and muscle thickness in ultrasonography images as well as videos.
│ │  
│ │  ## Community guidelines
│ │  
│ │ -Wheter you want to contribute, report a bug or have troubles with the DL_Track package, take a look at the provided [instructions](https://dltrack.readthedocs.io/en/latest/contribute.html) how to best do so. You can also contact us via email at paul.ritsche@unibas.ch, but we would prefer you to open a discussion as described in the instructions.
│ │ +Wheter you want to contribute, report a bug or have troubles with the DL_Track_US package, take a look at the provided [instructions](https://dltrack.readthedocs.io/en/latest/contribute.html) how to best do so. You can also contact us via email at paul.ritsche@unibas.ch, but we would prefer you to open a discussion as described in the instructions.
│ │   --- dl_track_us-0.1.1/pyproject.toml
│ ├── +++ dl_track_us-0.1.2/pyproject.toml
│ │┄ Files 20% similar despite different names
│ │ @@ -1,15 +1,15 @@
│ │  00000000: 5b62 7569 6c64 2d73 7973 7465 6d5d 0d0a  [build-system]..
│ │  00000010: 7265 7175 6972 6573 203d 205b 2268 6174  requires = ["hat
│ │  00000020: 6368 6c69 6e67 225d 0d0a 6275 696c 642d  chling"]..build-
│ │  00000030: 6261 636b 656e 6420 3d20 2268 6174 6368  backend = "hatch
│ │  00000040: 6c69 6e67 2e62 7569 6c64 220d 0a0d 0a5b  ling.build"....[
│ │  00000050: 7072 6f6a 6563 745d 0d0a 6e61 6d65 203d  project]..name =
│ │  00000060: 2022 444c 5f54 7261 636b 5f55 5322 0d0a   "DL_Track_US"..
│ │ -00000070: 7665 7273 696f 6e20 3d20 2230 2e31 2e31  version = "0.1.1
│ │ +00000070: 7665 7273 696f 6e20 3d20 2230 2e31 2e32  version = "0.1.2
│ │  00000080: 220d 0a61 7574 686f 7273 203d 205b 0d0a  "..authors = [..
│ │  00000090: 2020 7b20 6e61 6d65 3d22 5061 756c 2052    { name="Paul R
│ │  000000a0: 6974 7363 6865 222c 2065 6d61 696c 3d22  itsche", email="
│ │  000000b0: 7061 756c 2e72 6974 7363 6865 4075 6e69  paul.ritsche@uni
│ │  000000c0: 6261 732e 6368 2220 7d2c 0d0a 2020 7b20  bas.ch" },..  { 
│ │  000000d0: 6e61 6d65 3d22 4f6c 6976 6965 7220 5365  name="Olivier Se
│ │  000000e0: 796e 6e65 7322 2c20 656d 6169 6c3d 226f  ynnes", email="o
│ │ @@ -33,40 +33,41 @@
│ │  00000200: 332e 3130 222c 0d0a 2020 2020 224c 6963  3.10",..    "Lic
│ │  00000210: 656e 7365 203a 3a20 4f53 4920 4170 7072  ense :: OSI Appr
│ │  00000220: 6f76 6564 203a 3a20 4170 6163 6865 2053  oved :: Apache S
│ │  00000230: 6f66 7477 6172 6520 4c69 6365 6e73 6522  oftware License"
│ │  00000240: 2c0d 0a20 2020 2022 4f70 6572 6174 696e  ,..    "Operatin
│ │  00000250: 6720 5379 7374 656d 203a 3a20 4f53 2049  g System :: OS I
│ │  00000260: 6e64 6570 656e 6465 6e74 220d 0a5d 0d0a  ndependent"..]..
│ │ -00000270: 6465 7065 6e64 656e 6369 6573 203d 205b  dependencies = [
│ │ -00000280: 0d0a 2020 2020 226a 7570 7974 6572 3d3d  ..    "jupyter==
│ │ -00000290: 312e 302e 3022 2c0d 0a20 2020 2022 4b65  1.0.0",..    "Ke
│ │ -000002a0: 7261 733d 3d32 2e31 302e 3022 2c0d 0a20  ras==2.10.0",.. 
│ │ -000002b0: 2020 2022 6d61 7470 6c6f 746c 6962 3d3d     "matplotlib==
│ │ -000002c0: 332e 362e 3122 2c0d 0a20 2020 2022 6e75  3.6.1",..    "nu
│ │ -000002d0: 6d70 793d 3d31 2e32 332e 3422 2c0d 0a20  mpy==1.23.4",.. 
│ │ -000002e0: 2020 2022 6f70 656e 6376 2d63 6f6e 7472     "opencv-contr
│ │ -000002f0: 6962 2d70 7974 686f 6e3d 3d34 2e36 2e30  ib-python==4.6.0
│ │ -00000300: 2e36 3622 2c0d 0a20 2020 2022 6f70 656e  .66",..    "open
│ │ -00000310: 7079 786c 3d3d 332e 302e 3130 222c 0d0a  pyxl==3.0.10",..
│ │ -00000320: 2020 2020 2270 616e 6461 733d 3d31 2e35      "pandas==1.5
│ │ -00000330: 2e31 222c 0d0a 2020 2020 2250 696c 6c6f  .1",..    "Pillo
│ │ -00000340: 773d 3d39 2e32 2e30 222c 0d0a 2020 2020  w==9.2.0",..    
│ │ -00000350: 2270 7265 2d63 6f6d 6d69 743d 3d32 2e31  "pre-commit==2.1
│ │ -00000360: 372e 3022 2c0d 0a20 2020 2022 7363 696b  7.0",..    "scik
│ │ -00000370: 6974 2d69 6d61 6765 3d3d 302e 3139 2e33  it-image==0.19.3
│ │ -00000380: 222c 0d0a 2020 2020 2273 6369 6b69 742d  ",..    "scikit-
│ │ -00000390: 6c65 6172 6e3d 3d31 2e31 2e32 222c 0d0a  learn==1.1.2",..
│ │ -000003a0: 2020 2020 2273 6577 6172 3d3d 302e 342e      "sewar==0.4.
│ │ -000003b0: 3522 2c0d 0a20 2020 2022 7465 6e73 6f72  5",..    "tensor
│ │ -000003c0: 666c 6f77 3d3d 322e 3130 2e30 222c 0d0a  flow==2.10.0",..
│ │ -000003d0: 2020 2020 2274 7164 6d3d 3d34 2e36 342e      "tqdm==4.64.
│ │ -000003e0: 3122 0d0a 2020 5d0d 0a0d 0a5b 7072 6f6a  1"..  ]....[proj
│ │ -000003f0: 6563 742e 7572 6c73 5d0d 0a22 486f 6d65  ect.urls].."Home
│ │ -00000400: 7061 6765 2220 3d20 2268 7474 7073 3a2f  page" = "https:/
│ │ -00000410: 2f67 6974 6875 622e 636f 6d2f 5061 756c  /github.com/Paul
│ │ -00000420: 5269 7473 6368 652f 444c 5472 6163 6b22  Ritsche/DLTrack"
│ │ -00000430: 0d0a 2242 7567 2054 7261 636b 6572 2220  .."Bug Tracker" 
│ │ -00000440: 3d20 2268 7474 7073 3a2f 2f67 6974 6875  = "https://githu
│ │ -00000450: 622e 636f 6d2f 5061 756c 5269 7473 6368  b.com/PaulRitsch
│ │ -00000460: 652f 444c 5472 6163 6b2f 6973 7375 6573  e/DLTrack/issues
│ │ -00000470: 220d 0a                                  "..
│ │ +00000270: 2364 6570 656e 6465 6e63 6965 7320 3d20  #dependencies = 
│ │ +00000280: 5b0d 0a23 2020 2020 226a 7570 7974 6572  [..#    "jupyter
│ │ +00000290: 3d3d 312e 302e 3022 2c0d 0a23 2020 2020  ==1.0.0",..#    
│ │ +000002a0: 224b 6572 6173 3d3d 322e 3130 2e30 222c  "Keras==2.10.0",
│ │ +000002b0: 0d0a 2320 2020 2022 6d61 7470 6c6f 746c  ..#    "matplotl
│ │ +000002c0: 6962 3d3d 332e 362e 3122 2c0d 0a23 2020  ib==3.6.1",..#  
│ │ +000002d0: 2020 226e 756d 7079 3d3d 312e 3233 2e34    "numpy==1.23.4
│ │ +000002e0: 222c 0d0a 2320 2020 2022 6f70 656e 6376  ",..#    "opencv
│ │ +000002f0: 2d63 6f6e 7472 6962 2d70 7974 686f 6e3d  -contrib-python=
│ │ +00000300: 3d34 2e36 2e30 2e36 3622 2c0d 0a23 2020  =4.6.0.66",..#  
│ │ +00000310: 2020 226f 7065 6e70 7978 6c3d 3d33 2e30    "openpyxl==3.0
│ │ +00000320: 2e31 3022 2c0d 0a23 2020 2020 2270 616e  .10",..#    "pan
│ │ +00000330: 6461 733d 3d31 2e35 2e31 222c 0d0a 2320  das==1.5.1",..# 
│ │ +00000340: 2020 2022 5069 6c6c 6f77 3d3d 392e 322e     "Pillow==9.2.
│ │ +00000350: 3022 2c0d 0a23 2020 2020 2270 7265 2d63  0",..#    "pre-c
│ │ +00000360: 6f6d 6d69 743d 3d32 2e31 372e 3022 2c0d  ommit==2.17.0",.
│ │ +00000370: 0a23 2020 2020 2273 6369 6b69 742d 696d  .#    "scikit-im
│ │ +00000380: 6167 653d 3d30 2e31 392e 3322 2c0d 0a23  age==0.19.3",..#
│ │ +00000390: 2020 2020 2273 6369 6b69 742d 6c65 6172      "scikit-lear
│ │ +000003a0: 6e3d 3d31 2e31 2e32 222c 0d0a 2320 2020  n==1.1.2",..#   
│ │ +000003b0: 2022 7365 7761 723d 3d30 2e34 2e35 222c   "sewar==0.4.5",
│ │ +000003c0: 0d0a 2320 2020 2022 7465 6e73 6f72 666c  ..#    "tensorfl
│ │ +000003d0: 6f77 3d3d 322e 3130 2e30 222c 0d0a 2320  ow==2.10.0",..# 
│ │ +000003e0: 2020 2022 7471 646d 3d3d 342e 3634 2e31     "tqdm==4.64.1
│ │ +000003f0: 220d 0a23 2020 5d0d 0a0d 0a5b 7072 6f6a  "..#  ]....[proj
│ │ +00000400: 6563 742e 7572 6c73 5d0d 0a22 486f 6d65  ect.urls].."Home
│ │ +00000410: 7061 6765 2220 3d20 2268 7474 7073 3a2f  page" = "https:/
│ │ +00000420: 2f67 6974 6875 622e 636f 6d2f 5061 756c  /github.com/Paul
│ │ +00000430: 5269 7473 6368 652f 444c 5f54 7261 636b  Ritsche/DL_Track
│ │ +00000440: 5f55 5322 0d0a 2242 7567 2054 7261 636b  _US".."Bug Track
│ │ +00000450: 6572 2220 3d20 2268 7474 7073 3a2f 2f67  er" = "https://g
│ │ +00000460: 6974 6875 622e 636f 6d2f 5061 756c 5269  ithub.com/PaulRi
│ │ +00000470: 7473 6368 652f 444c 5f54 7261 636b 5f55  tsche/DL_Track_U
│ │ +00000480: 532f 6973 7375 6573 220d 0a              S/issues"..
