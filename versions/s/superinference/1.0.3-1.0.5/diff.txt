--- tmp/superinference-1.0.3.tar.gz
+++ tmp/superinference-1.0.5.tar.gz
â”œâ”€â”€ filetype from file(1)
â”‚ @@ -1 +1 @@
â”‚ -gzip compressed data, was "superinference-1.0.3.tar", last modified: Wed Mar 22 03:38:53 2023, max compression
â”‚ +gzip compressed data, was "superinference-1.0.5.tar", last modified: Fri Apr  7 05:45:49 2023, max compression
â”‚   --- superinference-1.0.3.tar
â”œâ”€â”€ +++ superinference-1.0.5.tar
â”‚ â”œâ”€â”€ file list
â”‚ â”‚ @@ -1,16 +1,16 @@
â”‚ â”‚ -drwxrwxrwx   0        0        0        0 2023-03-22 03:38:53.525583 superinference-1.0.3/
â”‚ â”‚ --rw-rw-rw-   0        0        0     1069 2023-03-21 13:42:01.000000 superinference-1.0.3/LICENSE.txt
â”‚ â”‚ --rw-rw-rw-   0        0        0     2258 2023-03-22 03:38:53.525583 superinference-1.0.3/PKG-INFO
â”‚ â”‚ --rw-rw-rw-   0        0        0      817 2023-03-21 13:42:01.000000 superinference-1.0.3/README.md
â”‚ â”‚ --rw-rw-rw-   0        0        0       97 2023-03-21 13:42:01.000000 superinference-1.0.3/pyproject.toml
â”‚ â”‚ --rw-rw-rw-   0        0        0      531 2023-03-22 03:38:53.529773 superinference-1.0.3/setup.cfg
â”‚ â”‚ -drwxrwxrwx   0        0        0        0 2023-03-22 03:38:53.471103 superinference-1.0.3/src/
â”‚ â”‚ -drwxrwxrwx   0        0        0        0 2023-03-22 03:38:53.502984 superinference-1.0.3/src/superinference/
â”‚ â”‚ --rw-rw-rw-   0        0        0        0 2023-03-21 09:18:24.000000 superinference-1.0.3/src/superinference/__init__.py
â”‚ â”‚ --rw-rw-rw-   0        0        0     1095 2023-03-22 03:35:34.000000 superinference-1.0.3/src/superinference/devto.py
â”‚ â”‚ --rw-rw-rw-   0        0        0    21524 2023-03-22 03:19:42.000000 superinference-1.0.3/src/superinference/github.py
â”‚ â”‚ -drwxrwxrwx   0        0        0        0 2023-03-22 03:38:53.522523 superinference-1.0.3/src/superinference.egg-info/
â”‚ â”‚ --rw-rw-rw-   0        0        0     2258 2023-03-22 03:38:53.000000 superinference-1.0.3/src/superinference.egg-info/PKG-INFO
â”‚ â”‚ --rw-rw-rw-   0        0        0      314 2023-03-22 03:38:53.000000 superinference-1.0.3/src/superinference.egg-info/SOURCES.txt
â”‚ â”‚ --rw-rw-rw-   0        0        0        1 2023-03-22 03:38:53.000000 superinference-1.0.3/src/superinference.egg-info/dependency_links.txt
â”‚ â”‚ --rw-rw-rw-   0        0        0       15 2023-03-22 03:38:53.000000 superinference-1.0.3/src/superinference.egg-info/top_level.txt
â”‚ â”‚ +drwxrwxrwx   0        0        0        0 2023-04-07 05:45:48.993099 superinference-1.0.5/
â”‚ â”‚ +-rw-rw-rw-   0        0        0     1069 2023-03-21 13:42:01.000000 superinference-1.0.5/LICENSE.txt
â”‚ â”‚ +-rw-rw-rw-   0        0        0    11544 2023-04-07 05:45:48.994454 superinference-1.0.5/PKG-INFO
â”‚ â”‚ +-rw-rw-rw-   0        0        0    10103 2023-04-07 04:27:04.000000 superinference-1.0.5/README.md
â”‚ â”‚ +-rw-rw-rw-   0        0        0       97 2023-03-21 13:42:01.000000 superinference-1.0.5/pyproject.toml
â”‚ â”‚ +-rw-rw-rw-   0        0        0      531 2023-04-07 05:45:48.998459 superinference-1.0.5/setup.cfg
â”‚ â”‚ +drwxrwxrwx   0        0        0        0 2023-04-07 05:45:48.944921 superinference-1.0.5/src/
â”‚ â”‚ +drwxrwxrwx   0        0        0        0 2023-04-07 05:45:48.974171 superinference-1.0.5/src/superinference/
â”‚ â”‚ +-rw-rw-rw-   0        0        0        0 2023-03-21 09:18:24.000000 superinference-1.0.5/src/superinference/__init__.py
â”‚ â”‚ +-rw-rw-rw-   0        0        0     1095 2023-03-22 03:35:34.000000 superinference-1.0.5/src/superinference/devto.py
â”‚ â”‚ +-rw-rw-rw-   0        0        0    24947 2023-04-07 04:24:26.000000 superinference-1.0.5/src/superinference/github.py
â”‚ â”‚ +drwxrwxrwx   0        0        0        0 2023-04-07 05:45:48.990851 superinference-1.0.5/src/superinference.egg-info/
â”‚ â”‚ +-rw-rw-rw-   0        0        0    11544 2023-04-07 05:45:48.000000 superinference-1.0.5/src/superinference.egg-info/PKG-INFO
â”‚ â”‚ +-rw-rw-rw-   0        0        0      314 2023-04-07 05:45:48.000000 superinference-1.0.5/src/superinference.egg-info/SOURCES.txt
â”‚ â”‚ +-rw-rw-rw-   0        0        0        1 2023-04-07 05:45:48.000000 superinference-1.0.5/src/superinference.egg-info/dependency_links.txt
â”‚ â”‚ +-rw-rw-rw-   0        0        0       15 2023-04-07 05:45:48.000000 superinference-1.0.5/src/superinference.egg-info/top_level.txt
â”‚ â”‚   --- superinference-1.0.3/LICENSE.txt
â”‚ â”œâ”€â”€ +++ superinference-1.0.5/LICENSE.txt
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- superinference-1.0.3/setup.cfg
â”‚ â”œâ”€â”€ +++ superinference-1.0.5/setup.cfg
â”‚ â”‚â”„ Files 1% similar despite different names
â”‚ â”‚ @@ -1,11 +1,11 @@
â”‚ â”‚  00000000: 5b6d 6574 6164 6174 615d 0d0a 6e61 6d65  [metadata]..name
â”‚ â”‚  00000010: 203d 2073 7570 6572 696e 6665 7265 6e63   = superinferenc
â”‚ â”‚  00000020: 650d 0a76 6572 7369 6f6e 203d 2031 2e30  e..version = 1.0
â”‚ â”‚ -00000030: 2e33 0d0a 6465 7363 7269 7074 696f 6e20  .3..description 
â”‚ â”‚ +00000030: 2e35 0d0a 6465 7363 7269 7074 696f 6e20  .5..description 
â”‚ â”‚  00000040: 3d20 5375 7065 7269 6e66 6572 656e 6365  = Superinference
â”‚ â”‚  00000050: 2069 7320 6120 6c69 6272 6172 7920 7468   is a library th
â”‚ â”‚  00000060: 6174 2069 6e66 6572 7320 616e 616c 7973  at infers analys
â”‚ â”‚  00000070: 6973 2d72 6561 6479 2061 7474 7269 6275  is-ready attribu
â”‚ â”‚  00000080: 7465 7320 6672 6f6d 2061 2070 6572 736f  tes from a perso
â”‚ â”‚  00000090: 6e27 2773 2073 6f63 6961 6c20 6d65 6469  n''s social medi
â”‚ â”‚  000000a0: 6120 7573 6572 6e61 6d65 206f 7220 756e  a username or un
â”‚ â”‚   --- superinference-1.0.3/src/superinference/devto.py
â”‚ â”œâ”€â”€ +++ superinference-1.0.5/src/superinference/devto.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- superinference-1.0.3/src/superinference/github.py
â”‚ â”œâ”€â”€ +++ superinference-1.0.5/src/superinference/github.py
â”‚ â”‚â”„ Files 12% similar despite different names
â”‚ â”‚ @@ -1,10 +1,10 @@
â”‚ â”‚  import requests
â”‚ â”‚  import re
â”‚ â”‚ -from datetime import datetime
â”‚ â”‚ +from datetime import datetime, timedelta
â”‚ â”‚  from base64 import b64decode
â”‚ â”‚  from rich.console import Console
â”‚ â”‚  from rich.theme import Theme
â”‚ â”‚  
â”‚ â”‚  console = Console(theme=Theme({"repr.str":"#54A24B", "repr.number": "#FF7F0E", "repr.none":"#808080"}))
â”‚ â”‚  
â”‚ â”‚  class GithubProfile:
â”‚ â”‚ @@ -16,145 +16,186 @@
â”‚ â”‚              access_token (str, optional): Github access token to increase API rate limit and access private repositories. Defaults to None.
â”‚ â”‚          """
â”‚ â”‚          self.username = username
â”‚ â”‚          self.access_token = access_token
â”‚ â”‚          self.inference = None
â”‚ â”‚          self._api_url = "https://api.github.com"
â”‚ â”‚  
â”‚ â”‚ +    def _error_handling(self, response, graphql=False):
â”‚ â”‚ +        """Handles errors from the Github API
â”‚ â”‚ +
â”‚ â”‚ +        Args:
â”‚ â”‚ +            response (requests.models.Response): Response from the Github API
â”‚ â”‚ +
â”‚ â”‚ +        Returns:
â”‚ â”‚ +            requests.models.Response: Response from the Github API, if no errors are found
â”‚ â”‚ +        """
â”‚ â”‚ +        if response.status_code == 200:
â”‚ â”‚ +            if graphql:
â”‚ â”‚ +                json_data = response.json()
â”‚ â”‚ +                if "errors" in json_data and json_data["errors"][0]["message"]:
â”‚ â”‚ +                    raise Exception(f"GraphQL API query error - \"{json_data['errors'][0]['message']}\"")
â”‚ â”‚ +            return response
â”‚ â”‚ +        elif response.status_code == 401:
â”‚ â”‚ +            if self.access_token:    
â”‚ â”‚ +                raise Exception("Invalid access token. Please check your access token and try again.")
â”‚ â”‚ +            else:
â”‚ â”‚ +                raise Exception("This feature requires an access token. Please provide an access token and try again.")
â”‚ â”‚ +        elif response.status_code == 403:
â”‚ â”‚ +            if self.access_token:
â”‚ â”‚ +                raise Exception("API rate limit exceeded, please try again later.")
â”‚ â”‚ +            else:
â”‚ â”‚ +                raise Exception("API rate limit exceeded, please provide an access token to increase rate limit.")
â”‚ â”‚ +        elif response.status_code == 404:
â”‚ â”‚ +            raise Exception("Invalid GitHub username inputted.")
â”‚ â”‚ +        else:
â”‚ â”‚ +            raise Exception(f"Error with status code of: {response.status_code}")
â”‚ â”‚ +        
â”‚ â”‚      def _request(self, url, error_handling=True):
â”‚ â”‚          """Makes a request to the Github API
â”‚ â”‚  
â”‚ â”‚          Args:
â”‚ â”‚              url (str): URL to be requested
â”‚ â”‚              error_handling (bool, optional): Whether to handle errors or not before returning the response. Defaults to True.
â”‚ â”‚  
â”‚ â”‚          Returns:
â”‚ â”‚ -            str: Response from the API
â”‚ â”‚ +            requests.models.Response: Response from the Github API
â”‚ â”‚          """
â”‚ â”‚          if self.access_token:
â”‚ â”‚              headers = {"Authorization": "token {}".format(self.access_token)}
â”‚ â”‚              response = requests.get(url, headers=headers)
â”‚ â”‚          else:
â”‚ â”‚              response = requests.get(url)
â”‚ â”‚              
â”‚ â”‚          if error_handling:
â”‚ â”‚ -            if response.status_code == 200:
â”‚ â”‚ -                return response
â”‚ â”‚ -            elif response.status_code == 401:
â”‚ â”‚ -                if self.access_token:    
â”‚ â”‚ -                    raise Exception("Invalid access token. Please check your access token and try again.")
â”‚ â”‚ -                else:
â”‚ â”‚ -                    raise Exception("This feature requires an access token. Please provide an access token and try again.")
â”‚ â”‚ -            elif response.status_code == 403:
â”‚ â”‚ -                if self.access_token:
â”‚ â”‚ -                    raise Exception("API rate limit exceeded, please try again later.")
â”‚ â”‚ -                else:
â”‚ â”‚ -                    raise Exception("API rate limit exceeded, please provide an access token to increase rate limit.")
â”‚ â”‚ -            elif response.status_code == 404:
â”‚ â”‚ -                raise Exception("Invalid GitHub username inputted.")
â”‚ â”‚ -            else:
â”‚ â”‚ -                raise Exception(f"Error with status code of: {response.status_code}")
â”‚ â”‚ +            return self._error_handling(response)
â”‚ â”‚          else:
â”‚ â”‚              return response
â”‚ â”‚      
â”‚ â”‚      def _parse_next_link(self, headers):
â”‚ â”‚          """Parses the next link from the Github API response headers
â”‚ â”‚  
â”‚ â”‚          Args:
â”‚ â”‚              headers (dict): Response headers from the Github API
â”‚ â”‚  
â”‚ â”‚          Returns:
â”‚ â”‚ -            str: Next link
â”‚ â”‚ +            str: Next link to be requested
â”‚ â”‚          """
â”‚ â”‚          if "Link" in headers:
â”‚ â”‚              links = headers["Link"]
â”‚ â”‚              if 'rel="next"' in links:
â”‚ â”‚                  next_link = links.split('rel="next"')[0].strip('<>; ')
â”‚ â”‚                  return next_link
â”‚ â”‚              else:
â”‚ â”‚                  return None
â”‚ â”‚          else:
â”‚ â”‚              return None
â”‚ â”‚      
â”‚ â”‚ -    def _multipage_request(self, url, item_name="items", json_key=None):
â”‚ â”‚ +    def _multipage_request(self, url, json_key=None):
â”‚ â”‚          """Makes a request to the Github API and handles pagination
â”‚ â”‚  
â”‚ â”‚          Args:
â”‚ â”‚              url (str): URL to be requested
â”‚ â”‚ -            item_name (str, optional): Name of the items requested, will be used in the message. Defaults to "items".
â”‚ â”‚              json_key (str, optional): Key of the items in the JSON response. Defaults to None.
â”‚ â”‚  
â”‚ â”‚          Returns:
â”‚ â”‚ -            list: List of items
â”‚ â”‚ +            item_list (list): List of combined items from all pages
â”‚ â”‚ +            incomplete_results (bool): Whether the results are incomplete or not (due to hitting rate limits)
â”‚ â”‚ +            total_count (int): The total count of items for search queries. Only returned for search endpoints.
â”‚ â”‚ +        
â”‚ â”‚          """
â”‚ â”‚ +        incomplete_results = False
â”‚ â”‚          item_list = []
â”‚ â”‚ -        message = None
â”‚ â”‚ +        is_search = "/search/" in url
â”‚ â”‚ +        
â”‚ â”‚          while url:
â”‚ â”‚              response = self._request(url)
â”‚ â”‚              if json_key:
â”‚ â”‚                  item_list.extend(response.json()[json_key])
â”‚ â”‚              else:
â”‚ â”‚                  item_list.extend(response.json())
â”‚ â”‚              url = self._parse_next_link(response.headers)
â”‚ â”‚              remaining_rate = int(response.headers["X-Ratelimit-Remaining"])
â”‚ â”‚              if url and remaining_rate == 0:
â”‚ â”‚ -                message = f"Hey there! Looks like the inference above is from the latest {len(item_list)} {item_name} since you've reached the API rate limit ðŸ˜‰."
â”‚ â”‚ +                incomplete_results = True
â”‚ â”‚                  url = None
â”‚ â”‚ -        return item_list, message
â”‚ â”‚ +        if is_search:
â”‚ â”‚ +            total_count = response.json()["total_count"]
â”‚ â”‚ +            return item_list, incomplete_results, total_count
â”‚ â”‚ +        else:
â”‚ â”‚ +            return item_list, incomplete_results
â”‚ â”‚      
â”‚ â”‚      def _username_token_check(self):
â”‚ â”‚          """Checks if the Github username is associated with the provided access token, which is called when the user wants to include private repositories"""
â”‚ â”‚          test_url = f"{self._api_url}/user"
â”‚ â”‚          response = self._request(test_url)
â”‚ â”‚          associated_username = response.json()['login']
â”‚ â”‚          if associated_username != self.username:
â”‚ â”‚              raise Exception("If you want to include private repositories, please ensure that the Github username is associated with the provided access token.")
â”‚ â”‚          
â”‚ â”‚ +    def _graphql_request(self, query):
â”‚ â”‚ +        """Makes a request to the Github GraphQL API
â”‚ â”‚ +
â”‚ â”‚ +        Args:
â”‚ â”‚ +            query (str): GraphQL query to be requested
â”‚ â”‚ +
â”‚ â”‚ +        Returns:
â”‚ â”‚ +            requests.models.Response: Response from the Github GraphQL API
â”‚ â”‚ +        """
â”‚ â”‚ +        url = f"{self._api_url}/graphql"
â”‚ â”‚ +        if self.access_token:
â”‚ â”‚ +            headers = {"Authorization": "token {}".format(self.access_token)}
â”‚ â”‚ +            response = requests.post(url, headers=headers, json={"query": query})
â”‚ â”‚ +        else:
â”‚ â”‚ +            response = requests.post(url)
â”‚ â”‚ +        return self._error_handling(response, graphql=True)
â”‚ â”‚ +        
â”‚ â”‚      def _profile_inference(self):
â”‚ â”‚          """Infer data regarding the user's Github profile
â”‚ â”‚  
â”‚ â”‚          Returns:
â”‚ â”‚ -            dict: Github profile data
â”‚ â”‚ +            dict: Github profile data and creation date
â”‚ â”‚          """
â”‚ â”‚          profile_url = f"{self._api_url}/users/{self.username}"
â”‚ â”‚          response = self._request(profile_url)
â”‚ â”‚ -        profile_data = response.json()
â”‚ â”‚ -        return {
â”‚ â”‚ -            "login": profile_data["login"],
â”‚ â”‚ -            "name": profile_data["name"],
â”‚ â”‚ -            "company": profile_data["company"],
â”‚ â”‚ -            "blog": profile_data["blog"],
â”‚ â”‚ -            "location": profile_data["location"],
â”‚ â”‚ -            "email": profile_data["email"],
â”‚ â”‚ -            "hireable": profile_data["hireable"],
â”‚ â”‚ -            "twitter_username": profile_data["twitter_username"],
â”‚ â”‚ -            "avatar_url": profile_data["avatar_url"],
â”‚ â”‚ -            "bio": profile_data["bio"],
â”‚ â”‚ -            "followers": profile_data["followers"],
â”‚ â”‚ -            "following": profile_data["following"]
â”‚ â”‚ +        json_data = response.json()
â”‚ â”‚ +        profile_data =  {
â”‚ â”‚ +            "login": json_data["login"],
â”‚ â”‚ +            "name": json_data["name"],
â”‚ â”‚ +            "company": json_data["company"],
â”‚ â”‚ +            "blog": json_data["blog"],
â”‚ â”‚ +            "location": json_data["location"],
â”‚ â”‚ +            "email": json_data["email"],
â”‚ â”‚ +            "hireable": json_data["hireable"],
â”‚ â”‚ +            "twitter_username": json_data["twitter_username"],
â”‚ â”‚ +            "avatar_url": json_data["avatar_url"],
â”‚ â”‚ +            "bio": json_data["bio"],
â”‚ â”‚ +            "followers": json_data["followers"],
â”‚ â”‚ +            "following": json_data["following"]
â”‚ â”‚          }
â”‚ â”‚ +        return {"data": profile_data, "created_at": json_data['created_at']}
â”‚ â”‚      
â”‚ â”‚  
â”‚ â”‚      def _repository_inference(self, top_repo_n=3, include_private=False):
â”‚ â”‚          """Infer data regarding the user's Github repositories
â”‚ â”‚  
â”‚ â”‚          Args:
â”‚ â”‚              top_repo_n (int, optional): Number of top repositories to be included in the inference. Defaults to 3.
â”‚ â”‚              include_private (bool, optional): Whether to include private repositories in the inference. Defaults to False.
â”‚ â”‚  
â”‚ â”‚          Returns:
â”‚ â”‚ -            dict: Github repository data and statistics
â”‚ â”‚ +            dict: Github repository statistics and list of original repositories
â”‚ â”‚          """
â”‚ â”‚          if include_private:
â”‚ â”‚              self._username_token_check()
â”‚ â”‚              repo_url = f"{self._api_url}/user/repos?per_page=100"
â”‚ â”‚          else:
â”‚ â”‚              repo_url = f"{self._api_url}/users/{self.username}/repos?per_page=100"
â”‚ â”‚              
â”‚ â”‚ -        repos, repo_message = self._multipage_request(repo_url, "repos")
â”‚ â”‚ +        repos, incomplete_results = self._multipage_request(repo_url)
â”‚ â”‚  
â”‚ â”‚          repos.sort(
â”‚ â”‚              key=lambda r: r["stargazers_count"] + r["forks_count"],
â”‚ â”‚              reverse=True,
â”‚ â”‚          )
â”‚ â”‚  
â”‚ â”‚          original_repos, forked_repos = [], []
â”‚ â”‚ @@ -179,195 +220,237 @@
â”‚ â”‚                      "top_language": r["language"],
â”‚ â”‚                      "stargazers_count": r["stargazers_count"],
â”‚ â”‚                      "forks_count": r["forks_count"],
â”‚ â”‚                  }
â”‚ â”‚              )
â”‚ â”‚  
â”‚ â”‚          stats = {
â”‚ â”‚ +            "incomplete_repo_results": incomplete_results,
â”‚ â”‚ +            "inference_from_repo_count": len(repos),
â”‚ â”‚              "original_repo_count": len(original_repos),
â”‚ â”‚              "forked_repo_count": len(forked_repos),
â”‚ â”‚              "counts": counts,
â”‚ â”‚              "top_repo_stars_forks": popular_repos,
â”‚ â”‚          }
â”‚ â”‚  
â”‚ â”‚ -        return {"stats": stats, "original_repos": original_repos, "repos": repos, "repo_api_message": repo_message}
â”‚ â”‚ +        return {"stats": stats, "original_repos": original_repos}
â”‚ â”‚      
â”‚ â”‚ -    def _contribution_inference(self, include_private=False):
â”‚ â”‚ -        """Infer data regarding the user's Github contributions (issues and pull requests)
â”‚ â”‚ +    def _contribution_inference(self, created_profile_date, original_repos):
â”‚ â”‚ +        """Infers data regarding a user's contributions to repositories on GitHub.
â”‚ â”‚  
â”‚ â”‚          Args:
â”‚ â”‚ -            include_private (bool, optional): Whether to include private repositories in the inference. Defaults to False.
â”‚ â”‚ +            created_profile_date (str): The user's Github profile creation date (from `_profile_inference()`)
â”‚ â”‚ +            original_repos (list): Original repository data (from `_repository_inference()`)
â”‚ â”‚  
â”‚ â”‚          Returns:
â”‚ â”‚ -            dict: Github contribution data and statistics
â”‚ â”‚ +            dict: Github contribution statistics
â”‚ â”‚          """
â”‚ â”‚ -        if include_private:
â”‚ â”‚ -            self._username_token_check()
â”‚ â”‚ -            issue_url = f"{self._api_url}/search/issues?q=type:issue author:{self.username}&sort=author-date&order=desc&per_page=100"
â”‚ â”‚ -            pr_url = f"{self._api_url}/search/issues?q=type:pr author:{self.username}&sort=author-date&order=desc&per_page=100"
â”‚ â”‚ -        else:
â”‚ â”‚ -            issue_url = f"{self._api_url}/search/issues?q=type:issue author:{self.username} is:public&sort=author-date&order=desc&per_page=100"
â”‚ â”‚ -            pr_url = f"{self._api_url}/search/issues?q=type:pr author:{self.username} is:public&sort=author-date&order=desc&per_page=100"
â”‚ â”‚ -            
â”‚ â”‚ -        issue_data, issue_message = self._multipage_request(issue_url, "issues", "items")
â”‚ â”‚ -        pr_data, pr_message = self._multipage_request(pr_url, "PRs", "items")
â”‚ â”‚ -        issues, prs = [], []
â”‚ â”‚ -
â”‚ â”‚ -        for i in issue_data:
â”‚ â”‚ -            if i['author_association'] != 'OWNER':
â”‚ â”‚ -                split_url = i['html_url'].split('/')
â”‚ â”‚ -                issues.append({
â”‚ â”‚ -                    'issue_title': i['title'],
â”‚ â”‚ -                    'created_at': i['created_at'],
â”‚ â”‚ -                    'state': i['state'],
â”‚ â”‚ -                    'state_reason': i['state_reason'],
â”‚ â”‚ -                    'repo_owner': split_url[3],
â”‚ â”‚ -                    'repo_name': split_url[4],
â”‚ â”‚ -                    'repo_url': f'https://github.com/{split_url[3]}/{split_url[4]}'
â”‚ â”‚ -                })
â”‚ â”‚ -
â”‚ â”‚ -        for p in pr_data:
â”‚ â”‚ -            if p['author_association'] != 'OWNER':
â”‚ â”‚ -                split_url = p['html_url'].split('/')
â”‚ â”‚ -                prs.append({
â”‚ â”‚ -                    'pr_title': p['title'],
â”‚ â”‚ -                    'created_at': p['created_at'],
â”‚ â”‚ -                    'merged_at': p['pull_request']['merged_at'],
â”‚ â”‚ -                    'state': p['state'],
â”‚ â”‚ -                    'state_reason': p['state_reason'],
â”‚ â”‚ -                    'repo_owner': split_url[3],
â”‚ â”‚ -                    'repo_name': split_url[4],
â”‚ â”‚ -                    'repo_url': f'https://github.com/{split_url[3]}/{split_url[4]}'
â”‚ â”‚ -                })
â”‚ â”‚ -                
â”‚ â”‚ -        merged_pr_count = len([p for p in prs if p['merged_at']])
â”‚ â”‚ -
â”‚ â”‚ -        contribution_count = {}
â”‚ â”‚ -        for ip in issues + prs:
â”‚ â”‚ -            repo_owner = ip['repo_owner']
â”‚ â”‚ -            contribution_count[repo_owner] = contribution_count.get(repo_owner, 0) + 1
â”‚ â”‚ -
â”‚ â”‚ -        contribution_count = dict(sorted(contribution_count.items(), key=lambda x: x[1], reverse=True))
â”‚ â”‚ +        if not self.access_token:
â”‚ â”‚ +            return None
â”‚ â”‚          
â”‚ â”‚ -        contribution = {
â”‚ â”‚ -                        'issue_count': len(issues),
â”‚ â”‚ -                        'total_pr_count': len(prs),
â”‚ â”‚ -                        'merged_pr_count': merged_pr_count,
â”‚ â”‚ -                        'contribution_count_per_repo_owner': contribution_count,
â”‚ â”‚ -                        'created_issue': issues,
â”‚ â”‚ -                        'created_pr': prs,
â”‚ â”‚ -                        'issue_api_message': issue_message,
â”‚ â”‚ -                        'pr_api_message': pr_message
â”‚ â”‚ -                        }
â”‚ â”‚ -
â”‚ â”‚ -        return {'contribution': contribution, 'issue_api_message': issue_message, 'pr_api_message': pr_message}
â”‚ â”‚ -    
â”‚ â”‚ -    def _activity_inference(self, original_repos, top_repo_n=3, include_private=False):
â”‚ â”‚ -        """Infer data regarding the user's Github activity (commits)
â”‚ â”‚ -
â”‚ â”‚ -        Args:
â”‚ â”‚ -            original_repos (dict): Original repository data (from _repo_inference)
â”‚ â”‚ -            top_repo_n (int, optional): Number of top repositories to be included in the inference. Defaults to 3.
â”‚ â”‚ -            include_private (bool, optional): Whether to include private repositories in the inference. Defaults to False.
â”‚ â”‚ -
â”‚ â”‚ -        Returns:
â”‚ â”‚ -            dict: Github activity data and statistics
â”‚ â”‚ +        created_date = datetime.strptime(created_profile_date, "%Y-%m-%dT%H:%M:%SZ")
â”‚ â”‚ +        created_year = created_date.year
â”‚ â”‚ +        today = datetime.now()
â”‚ â”‚ +        current_year = today.year
â”‚ â”‚ +        
â”‚ â”‚ +        def query_pattern_day(start_date, end_date):
â”‚ â”‚ +            return f"""
â”‚ â”‚ +                query {{
â”‚ â”‚ +                    user(login: "{self.username}") {{
â”‚ â”‚ +                        contributionsCollection(from: "{start_date.isoformat()}", to: "{end_date.isoformat()}") {{
â”‚ â”‚ +                            contributionCalendar {{
â”‚ â”‚ +                                totalContributions
â”‚ â”‚ +                                weeks {{
â”‚ â”‚ +                                    contributionDays {{
â”‚ â”‚ +                                        date
â”‚ â”‚ +                                        contributionCount
â”‚ â”‚ +                                    }}
â”‚ â”‚ +                                }}
â”‚ â”‚ +                            }}
â”‚ â”‚ +                        }}
â”‚ â”‚ +                    }}
â”‚ â”‚ +                }}
â”‚ â”‚ +            """
â”‚ â”‚ +        
â”‚ â”‚ +        contribution_detail = """
â”‚ â”‚ +            repository {
â”‚ â”‚ +                description
â”‚ â”‚ +                name
â”‚ â”‚ +                url
â”‚ â”‚ +                languages(first: 1, orderBy: {field: SIZE, direction: DESC}) {
â”‚ â”‚ +                    nodes {
â”‚ â”‚ +                        name
â”‚ â”‚ +                    }
â”‚ â”‚ +                }
â”‚ â”‚ +                owner {
â”‚ â”‚ +                    __typename
â”‚ â”‚ +                    ... on User {
â”‚ â”‚ +                        login
â”‚ â”‚ +                    }
â”‚ â”‚ +                    ... on Organization {
â”‚ â”‚ +                        login
â”‚ â”‚ +                    }
â”‚ â”‚ +                }
â”‚ â”‚ +            }
â”‚ â”‚ +            contributions {
â”‚ â”‚ +                totalCount
â”‚ â”‚ +            }
â”‚ â”‚          """
â”‚ â”‚ -        if include_private:
â”‚ â”‚ -            self._username_token_check()
â”‚ â”‚ -            commit_url = self._api_url + f"/search/commits?q=committer:{self.username}&sort=committer-date&order=desc&per_page=100"
â”‚ â”‚ -        else:
â”‚ â”‚ -            commit_url = self._api_url + f"/search/commits?q=committer:{self.username} is:public&sort=committer-date&order=desc&per_page=100"
â”‚ â”‚ -        commit_data, commit_message = self._multipage_request(commit_url, "commits", "items")
â”‚ â”‚ +        
â”‚ â”‚ +        def query_pattern_repo(start_date, end_date):
â”‚ â”‚ +            return f"""
â”‚ â”‚ +                query {{
â”‚ â”‚ +                    user(login: "{self.username}") {{
â”‚ â”‚ +                        contributionsCollection(from: "{start_date.isoformat()}", to: "{end_date.isoformat()}") {{
â”‚ â”‚ +                            commitContributionsByRepository(maxRepositories: 100) {{
â”‚ â”‚ +                                {contribution_detail}
â”‚ â”‚ +                            }}
â”‚ â”‚ +                            issueContributionsByRepository(maxRepositories: 100) {{
â”‚ â”‚ +                                {contribution_detail}
â”‚ â”‚ +                            }}
â”‚ â”‚ +                            pullRequestContributionsByRepository(maxRepositories: 100) {{
â”‚ â”‚ +                                {contribution_detail}
â”‚ â”‚ +                            }}
â”‚ â”‚ +                            pullRequestReviewContributionsByRepository(maxRepositories: 100) {{
â”‚ â”‚ +                                {contribution_detail}
â”‚ â”‚ +                            }}
â”‚ â”‚ +                        }}
â”‚ â”‚ +                    }}
â”‚ â”‚ +                }}
â”‚ â”‚ +            """
â”‚ â”‚ +        
â”‚ â”‚ +        contributions_per_day = []
â”‚ â”‚ +        contributions_per_repo = []
â”‚ â”‚ +        contributions_count = 0
â”‚ â”‚ +        for i in range(created_year, current_year + 1):
â”‚ â”‚ +            if i == created_year:
â”‚ â”‚ +                query_day = query_pattern_day(created_date, datetime(i, 12, 31))
â”‚ â”‚ +                query_repo = query_pattern_repo(created_date, datetime(i, 12, 31))
â”‚ â”‚ +            elif i == current_year:
â”‚ â”‚ +                query_day = query_pattern_day(datetime(i, 1, 1), today)
â”‚ â”‚ +                query_repo = query_pattern_repo(datetime(i, 1, 1), today)
â”‚ â”‚ +            else:
â”‚ â”‚ +                query_day = query_pattern_day(datetime(i, 1, 1), datetime(i, 12, 31))
â”‚ â”‚ +                query_repo = query_pattern_repo(datetime(i, 1, 1), datetime(i, 12, 31))
â”‚ â”‚  
â”‚ â”‚ -        commits = []
â”‚ â”‚ -        for c in commit_data:
â”‚ â”‚ -            commits.append({
â”‚ â”‚ -            "created_at": c["commit"]["committer"]["date"][:10],
â”‚ â”‚ -            "repo_owner": c["repository"]["owner"]["login"],
â”‚ â”‚ -            "repo_owner_type": c["repository"]["owner"]["type"],
â”‚ â”‚ -            "repo_name": c["repository"]["name"]})
â”‚ â”‚ -
â”‚ â”‚ -        counts = {
â”‚ â”‚ -            "date": {},
â”‚ â”‚ -            "day": {},
â”‚ â”‚ -            "month": {},
â”‚ â”‚ +            response_day = self._graphql_request(query_day)
â”‚ â”‚ +            data_day = response_day.json()['data']
â”‚ â”‚ +            new_contribution_day=[]
â”‚ â”‚ +            for week in data_day["user"]["contributionsCollection"]["contributionCalendar"]["weeks"]:
â”‚ â”‚ +                new_contribution_day.extend([day for day in week["contributionDays"]])
â”‚ â”‚ +            contributions_per_day.extend(new_contribution_day)
â”‚ â”‚ +            contributions_count += data_day["user"]["contributionsCollection"]["contributionCalendar"]["totalContributions"]
â”‚ â”‚ +
â”‚ â”‚ +            response_repo = self._graphql_request(query_repo)
â”‚ â”‚ +            data_repo = response_repo.json()['data']
â”‚ â”‚ +            def extract_repo_detail(repository, contributions):
â”‚ â”‚ +                return {
â”‚ â”‚ +                    "name": repository['name'],
â”‚ â”‚ +                    "owner": repository['owner']['login'],
â”‚ â”‚ +                    "owner_type": repository['owner']["__typename"],
â”‚ â”‚ +                    "html_url": repository['url'],
â”‚ â”‚ +                    "description": repository['description'],
â”‚ â”‚ +                    "top_language": repository['languages']['nodes'][0]['name'].lower().replace(" ", "-") if repository['languages']['nodes'] else None,
â”‚ â”‚ +                    "contributions_count": contributions['totalCount']
â”‚ â”‚ +                }
â”‚ â”‚ +            new_commits = [extract_repo_detail(item['repository'], item['contributions']) for item in data_repo['user']['contributionsCollection']['commitContributionsByRepository']]
â”‚ â”‚ +            new_issues = [extract_repo_detail(item['repository'], item['contributions']) for item in data_repo['user']['contributionsCollection']['issueContributionsByRepository']]
â”‚ â”‚ +            new_pr = [extract_repo_detail(item['repository'], item['contributions']) for item in data_repo['user']['contributionsCollection']['pullRequestContributionsByRepository']]
â”‚ â”‚ +            new_pr_review = [extract_repo_detail(item['repository'], item['contributions']) for item in data_repo['user']['contributionsCollection']['pullRequestReviewContributionsByRepository']]
â”‚ â”‚ +            contributions_per_repo.extend(new_commits + new_issues + new_pr + new_pr_review)
â”‚ â”‚ +            
â”‚ â”‚ +        one_year_ago = datetime.now() - timedelta(days=365)
â”‚ â”‚ +        count = {"day": {}, "month": {}}
â”‚ â”‚ +        for c in contributions_per_day:
â”‚ â”‚ +            c_date = datetime.strptime(c["date"], "%Y-%m-%d")
â”‚ â”‚ +            c_day = c_date.strftime("%a")
â”‚ â”‚ +            c_month = c_date.strftime("%b")
â”‚ â”‚ +            is_one_year_ago = c_date >= one_year_ago
â”‚ â”‚ +            
â”‚ â”‚ +            count["day"][c_day] = count["day"].get(c_day, [0, 0])
â”‚ â”‚ +            if is_one_year_ago:
â”‚ â”‚ +                count["day"][c_day][0] += c['contributionCount']
â”‚ â”‚ +            count["day"][c_day][1] +=  c['contributionCount']
â”‚ â”‚ +            
â”‚ â”‚ +            count["month"][c_month] = count["month"].get(c_month, [0, 0])
â”‚ â”‚ +            if is_one_year_ago:
â”‚ â”‚ +                count["month"][c_month][0] += c['contributionCount']
â”‚ â”‚ +            count["month"][c_month][1] += c['contributionCount']
â”‚ â”‚ +            
â”‚ â”‚ +        final_count = {
â”‚ â”‚ +            "day": count["day"],
â”‚ â”‚ +            "month": count["month"],
â”‚ â”‚              "owned_repo": {},
â”‚ â”‚ -            "other_repo": {},
â”‚ â”‚ -            "repo_org_owner": {},
â”‚ â”‚ -            "repo_user_owner": {},
â”‚ â”‚ +            "other_repo": [],
â”‚ â”‚ +            "User": {},
â”‚ â”‚ +            "Organization": {},
â”‚ â”‚          }
â”‚ â”‚ -        for c in commits:
â”‚ â”‚ -            c_date = c["created_at"]
â”‚ â”‚ -            c_day = datetime.strptime(c_date, "%Y-%m-%d").strftime("%a")
â”‚ â”‚ -            c_month = datetime.strptime(c_date, "%Y-%m-%d").strftime("%b")
â”‚ â”‚ -            counts["date"][c_date] = counts["date"].get(c_date, 0) + 1
â”‚ â”‚ -            counts["day"][c_day] = counts["day"].get(c_day, 0) + 1
â”‚ â”‚ -            counts["month"][c_month] = counts["month"].get(c_month, 0) + 1
â”‚ â”‚ -            repo_owner, repo_owner_type, repo_name = c["repo_owner"], c["repo_owner_type"], c["repo_name"]
â”‚ â”‚ -            if repo_owner == self.username:
â”‚ â”‚ -                counts["owned_repo"][repo_name] = counts["owned_repo"].get(repo_name, 0) + 1
â”‚ â”‚ -            else:
â”‚ â”‚ -                counts["other_repo"][repo_name] = counts["other_repo"].get(repo_name, 0) + 1
â”‚ â”‚ -            if repo_owner_type == "Organization":
â”‚ â”‚ -                counts["repo_org_owner"][repo_owner] = counts["repo_org_owner"].get(repo_owner, 0) + 1
â”‚ â”‚ -            else:
â”‚ â”‚ -                counts["repo_user_owner"][repo_owner] = counts["repo_user_owner"].get(repo_owner, 0) + 1
â”‚ â”‚          
â”‚ â”‚ -        sorted_counts = {}
â”‚ â”‚ -        for k in counts:
â”‚ â”‚ -            if k == 'date':
â”‚ â”‚ -                sorted_counts[k] = counts[k]
â”‚ â”‚ +        for c in contributions_per_repo:
â”‚ â”‚ +            repo_type = "owned_repo" if c['owner'] == self.username else "other_repo"
â”‚ â”‚ +
â”‚ â”‚ +            if repo_type == "owned_repo":
â”‚ â”‚ +                final_count[repo_type][c['name']] = final_count[repo_type].get(c['name'], 0) + c['contributions_count']
â”‚ â”‚              else:
â”‚ â”‚ -                sorted_counts[k] = dict(sorted(counts[k].items(), key=lambda x: x[1], reverse=True))
â”‚ â”‚ -                
â”‚ â”‚ -        most_active_repo_name = list(sorted_counts['owned_repo'].keys())[:top_repo_n]
â”‚ â”‚ -        most_active_repo = []
â”‚ â”‚ -        for r in original_repos:
â”‚ â”‚ -            if r['name'] in most_active_repo_name:
â”‚ â”‚ -                most_active_repo.append({
â”‚ â”‚ -                    'name': r['name'],
â”‚ â”‚ -                    'html_url': r['html_url'],
â”‚ â”‚ -                    'description': r['description'],
â”‚ â”‚ -                    'top_language': r['language'],
â”‚ â”‚ -                    'commits_count': sorted_counts['owned_repo'][r['name']]
â”‚ â”‚ -                })
â”‚ â”‚ -                
â”‚ â”‚ -        if len(commits) > 0:
â”‚ â”‚ -            last_commit_date = datetime.strptime(commits[0]['created_at'], '%Y-%m-%d')
â”‚ â”‚ -            first_commit_date = datetime.strptime(commits[-1]['created_at'], '%Y-%m-%d')
â”‚ â”‚ -            total_weeks = round((last_commit_date - first_commit_date).days / 7)
â”‚ â”‚ -            weekly_avg_commits = round(len(commits) / total_weeks, 3)
â”‚ â”‚ -        else:
â”‚ â”‚ -            weekly_avg_commits = 0
â”‚ â”‚ +                index = next((i for i, obj in enumerate(final_count[repo_type]) if obj["name"] == c['name']), -1)
â”‚ â”‚ +                if index == -1:
â”‚ â”‚ +                    data = {k: v for k, v in c.items() if k != "owner_type"}
â”‚ â”‚ +                    final_count[repo_type].append(data)             
â”‚ â”‚ +                else:
â”‚ â”‚ +                    final_count[repo_type][index]["contributions_count"] += c['contributions_count']      
â”‚ â”‚ +
â”‚ â”‚ +            final_count[c['owner_type']][c['owner']] = final_count[c['owner_type']].get(c['owner'], 0) + c['contributions_count']
â”‚ â”‚              
â”‚ â”‚ -        activity = {
â”‚ â”‚ -            'commit_count': len(commits),
â”‚ â”‚ -            'most_active_day': list(sorted_counts['day'].keys())[0] if len(commits) > 0 else None,
â”‚ â”‚ -            'most_active_month': list(sorted_counts['month'].keys())[0] if len(commits) > 0 else None,
â”‚ â”‚ -            'weekly_average_commits': weekly_avg_commits,
â”‚ â”‚ -            'commit_count_per_day': sorted_counts['day'],
â”‚ â”‚ -            'commit_count_per_month': sorted_counts['month'],
â”‚ â”‚ -            'commit_count_per_owned_repo': sorted_counts['owned_repo'],
â”‚ â”‚ -            'commit_count_per_other_repo': sorted_counts['other_repo'],
â”‚ â”‚ -            'commit_count_per_repo_org_owner': sorted_counts['repo_org_owner'],
â”‚ â”‚ -            'commit_count_per_repo_user_owner': sorted_counts['repo_user_owner'],
â”‚ â”‚ -            'commit_api_message': commit_message
â”‚ â”‚ -        }
â”‚ â”‚ +        sorted_count = {}
â”‚ â”‚ +        for k, v in final_count.items():
â”‚ â”‚ +            if k == "day" or k == "month":
â”‚ â”‚ +                sorted_count[k] = dict(sorted(v.items(), key=lambda item: item[1][0], reverse=True))
â”‚ â”‚ +            elif k == "other_repo":
â”‚ â”‚ +                sorted_count[k] = sorted(v, key=lambda v: v["contributions_count"], reverse=True)
â”‚ â”‚ +            else:
â”‚ â”‚ +                sorted_count[k] = dict(sorted(v.items(), key=lambda item: item[1], reverse=True))
â”‚ â”‚ +            
â”‚ â”‚ +        total_weeks = round((today - created_date).days / 7)
â”‚ â”‚ +        weekly_avg_contributions = round(contributions_count / total_weeks, 3)
â”‚ â”‚ +        
â”‚ â”‚ +        data_contrib = []
â”‚ â”‚ +        for r in original_repos[:10]:
â”‚ â”‚ +            response = self._request(r['contributors_url'])
â”‚ â”‚ +            repo_data = response.json()
â”‚ â”‚ +            data_contrib.extend(repo_data)
â”‚ â”‚ +
â”‚ â”‚ +        incoming_contribution = {}
â”‚ â”‚ +        for d in data_contrib:
â”‚ â”‚ +            login = d['login']
â”‚ â”‚ +            if login != self.username:
â”‚ â”‚ +                incoming_contribution[login] = incoming_contribution.get(login, 0) + d['contributions']
â”‚ â”‚  
â”‚ â”‚ -        return {'activity': activity, 'most_active_repo': most_active_repo, 'commit_api_message': commit_message}
â”‚ â”‚ +        sorted_incoming_contribution = dict(sorted(incoming_contribution.items(), key=lambda item: item[1], reverse=True))
â”‚ â”‚ +        
â”‚ â”‚ +        contribution = {
â”‚ â”‚ +            'contribution_count': contributions_count,
â”‚ â”‚ +            'weekly_average_contribution': weekly_avg_contributions,
â”‚ â”‚ +            'contribution_count_per_day': sorted_count["day"],
â”‚ â”‚ +            'contribution_count_per_month': sorted_count["month"],
â”‚ â”‚ +            'contribution_count_per_owned_repo': sorted_count["owned_repo"],
â”‚ â”‚ +            'contribution_count_per_other_repo': sorted_count["other_repo"],
â”‚ â”‚ +            'contribution_count_per_repo_org_owner': sorted_count["Organization"],
â”‚ â”‚ +            'contribution_count_per_repo_user_owner': sorted_count["User"],
â”‚ â”‚ +            'external_contribution_to_top_10_repo': sorted_incoming_contribution
â”‚ â”‚ +        }
â”‚ â”‚ +        
â”‚ â”‚ +        return contribution
â”‚ â”‚      
â”‚ â”‚ -    def _skill_inference(self, bio, original_repos, repo_message, top_language_n=3):
â”‚ â”‚ +    def _skill_inference(self, bio, original_repos, top_language_n=3):
â”‚ â”‚          """Infer data regarding the user's skills from their Github bio and repositories.
â”‚ â”‚  
â”‚ â”‚          Args:
â”‚ â”‚              bio (str): The user's Github bio
â”‚ â”‚ -            original_repos (dict): Original repository data (from _repo_inference)
â”‚ â”‚ -            repo_message (str): Message from _repo_inference
â”‚ â”‚ -            top_language_n (int): Number of top languages to be included in the inference. Defaults to 3.
â”‚ â”‚ +            original_repos (dict): Original repository data from _repository_inference()
â”‚ â”‚ +            top_language_n (int, optional): The number of top languages to consider. Defaults to 3.
â”‚ â”‚  
â”‚ â”‚          Returns:
â”‚ â”‚              dict: Inferred data regarding the user's skills
â”‚ â”‚          """
â”‚ â”‚          response = requests.get("https://raw.githubusercontent.com/supertypeai/collective/main/src/data/profileTagsChoices.json")
â”‚ â”‚          keywords = response.json()
â”‚ â”‚          
â”‚ â”‚ @@ -401,46 +484,50 @@
â”‚ â”‚  
â”‚ â”‚          languages_count = {}
â”‚ â”‚          if self.access_token:
â”‚ â”‚              for r in original_repos:
â”‚ â”‚                  response = self._request(r["languages_url"])
â”‚ â”‚                  r_lang = response.json()
â”‚ â”‚                  for key in r_lang.keys():
â”‚ â”‚ -                    languages_count[key] = languages_count.get(key, 0) + 1
â”‚ â”‚ +                    formatted_key = key.replace(" ", "-").lower()
â”‚ â”‚ +                    languages_count[formatted_key] = languages_count.get(formatted_key, 0) + 1
â”‚ â”‚              sorted_languages = sorted(languages_count, key=languages_count.get, reverse=True)
â”‚ â”‚              languages_percentage = {lang: round(languages_count[lang] / len(original_repos), 3) for lang in sorted_languages}
â”‚ â”‚          else:
â”‚ â”‚              for r in original_repos:
â”‚ â”‚ -                languages_count[r["language"]] = languages_count.get(r["language"], 0) + 1
â”‚ â”‚ +                if r['language']:
â”‚ â”‚ +                    formatted_lang = r["language"].replace(" ", "-").lower()
â”‚ â”‚ +                    languages_count[formatted_lang] = languages_count.get(formatted_lang, 0) + 1
â”‚ â”‚              sorted_languages = sorted(languages_count, key=languages_count.get, reverse=True)
â”‚ â”‚ -            languages_percentage = "Sorry, it looks like the information you're requesting is only available for authenticated requests ðŸ˜”"
â”‚ â”‚ +            languages_percentage = None
â”‚ â”‚          
â”‚ â”‚ -        return {
â”‚ â”‚ +        skill = {
â”‚ â”‚ +            "inference_from_originalrepo_count": len(original_repos),
â”‚ â”‚              "key_qualifications": key_qualifications, 
â”‚ â”‚              "top_n_languages": sorted_languages[:top_language_n], 
â”‚ â”‚              "languages_percentage": languages_percentage,
â”‚ â”‚ -            "repo_api_message": repo_message
â”‚ â”‚              }
â”‚ â”‚ +        
â”‚ â”‚ +        return skill
â”‚ â”‚      
â”‚ â”‚      def perform_inference(self, top_repo_n=3, top_language_n=3, include_private=False):
â”‚ â”‚          """Perform inference on the user's Github profile. This will print out a dictionary that includes data and statistics regarding their profile, repository, skill, activity, and contribution. The dictionary will also be stored in the inference attribute.
â”‚ â”‚  
â”‚ â”‚          Args:
â”‚ â”‚              top_repo_n (int, optional): Number of top repositories to be included in the inference. Defaults to 3.
â”‚ â”‚              top_language_n (int, optional): Number of top languages to be included in the inference. Defaults to 3.
â”‚ â”‚              include_private (bool, optional): Whether to include private repositories in the inference. Defaults to False.
â”‚ â”‚          """
â”‚ â”‚          profile = self._profile_inference()
â”‚ â”‚          repository = self._repository_inference(top_repo_n=top_repo_n, include_private=include_private)
â”‚ â”‚ -        skill = self._skill_inference(bio=profile['bio'], original_repos=repository['original_repos'], repo_message=repository['repo_api_message'], top_language_n=top_language_n)
â”‚ â”‚ -        activity = self._activity_inference(original_repos=repository['original_repos'], top_repo_n=top_repo_n, include_private=include_private)
â”‚ â”‚ -        contribution = self._contribution_inference(include_private=include_private)
â”‚ â”‚ +        skill = self._skill_inference(bio=profile['data']['bio'], original_repos=repository['original_repos'], top_language_n=top_language_n)
â”‚ â”‚ +        contribution = self._contribution_inference(created_profile_date=profile['created_at'], original_repos=repository['original_repos'])
â”‚ â”‚          
â”‚ â”‚          self.inference = {
â”‚ â”‚ -            "profile": profile,
â”‚ â”‚ +            "profile": profile['data'],
â”‚ â”‚              "skill": skill,
â”‚ â”‚              "stats": repository["stats"],
â”‚ â”‚ -            "activity": activity,
â”‚ â”‚ +            # "activity": activity,
â”‚ â”‚              "contribution": contribution
â”‚ â”‚          }
â”‚ â”‚          
â”‚ â”‚          console.print(self.inference)
