# Comparing `tmp/pod5-0.1.4-py3-none-any.whl.zip` & `tmp/pod5-0.1.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,25 +1,25 @@
-Zip file size: 47726 bytes, number of entries: 23
--rw-r--r--  2.0 unx      751 b- defN 22-Dec-09 11:16 pod5/__init__.py
--rw-r--r--  2.0 unx     2168 b- defN 22-Dec-09 11:16 pod5/api_utils.py
--rw-r--r--  2.0 unx    13111 b- defN 22-Dec-16 11:36 pod5/pod5_types.py
--rw-r--r--  2.0 unx    33936 b- defN 22-Dec-16 11:36 pod5/reader.py
--rw-r--r--  2.0 unx     6422 b- defN 22-Dec-09 11:16 pod5/repack.py
--rw-r--r--  2.0 unx     4711 b- defN 22-Dec-09 11:16 pod5/signal_tools.py
--rw-r--r--  2.0 unx    12239 b- defN 22-Dec-16 11:36 pod5/writer.py
--rw-r--r--  2.0 unx       24 b- defN 22-Dec-09 11:16 pod5/tools/__init__.py
--rw-r--r--  2.0 unx     1537 b- defN 22-Dec-09 11:16 pod5/tools/main.py
--rw-r--r--  2.0 unx    14129 b- defN 22-Dec-09 11:16 pod5/tools/parsers.py
--rw-r--r--  2.0 unx    21306 b- defN 22-Dec-16 11:36 pod5/tools/pod5_convert_from_fast5.py
--rw-r--r--  2.0 unx    11662 b- defN 22-Dec-22 09:32 pod5/tools/pod5_convert_to_fast5.py
--rw-r--r--  2.0 unx     6533 b- defN 22-Dec-16 11:36 pod5/tools/pod5_inspect.py
--rw-r--r--  2.0 unx     2447 b- defN 22-Dec-09 11:16 pod5/tools/pod5_merge.py
--rw-r--r--  2.0 unx     1740 b- defN 22-Dec-09 11:16 pod5/tools/pod5_repack.py
--rw-r--r--  2.0 unx    13727 b- defN 22-Dec-09 11:16 pod5/tools/pod5_subset.py
--rw-r--r--  2.0 unx     1009 b- defN 22-Dec-09 11:16 pod5/tools/pod5_update.py
--rw-r--r--  2.0 unx     1004 b- defN 22-Dec-09 11:16 pod5/tools/utils.py
--rw-r--r--  2.0 unx    14729 b- defN 22-Dec-23 09:03 pod5-0.1.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 22-Dec-23 09:03 pod5-0.1.4.dist-info/WHEEL
--rw-r--r--  2.0 unx       46 b- defN 22-Dec-23 09:03 pod5-0.1.4.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        5 b- defN 22-Dec-23 09:03 pod5-0.1.4.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1798 b- defN 22-Dec-23 09:03 pod5-0.1.4.dist-info/RECORD
-23 files, 165126 bytes uncompressed, 44870 bytes compressed:  72.8%
+Zip file size: 47852 bytes, number of entries: 23
+-rw-r--r--  2.0 unx      751 b- defN 23-Jan-17 15:47 pod5/__init__.py
+-rw-r--r--  2.0 unx     2168 b- defN 23-Jan-17 15:47 pod5/api_utils.py
+-rw-r--r--  2.0 unx    13142 b- defN 23-Jan-17 15:47 pod5/pod5_types.py
+-rw-r--r--  2.0 unx    34648 b- defN 23-Jan-17 15:47 pod5/reader.py
+-rw-r--r--  2.0 unx     6469 b- defN 23-Jan-17 15:47 pod5/repack.py
+-rw-r--r--  2.0 unx     4827 b- defN 23-Jan-17 15:47 pod5/signal_tools.py
+-rw-r--r--  2.0 unx    12922 b- defN 23-Jan-17 15:47 pod5/writer.py
+-rw-r--r--  2.0 unx       24 b- defN 23-Jan-17 15:47 pod5/tools/__init__.py
+-rw-r--r--  2.0 unx     1567 b- defN 23-Jan-17 15:47 pod5/tools/main.py
+-rw-r--r--  2.0 unx    14129 b- defN 23-Jan-17 15:47 pod5/tools/parsers.py
+-rw-r--r--  2.0 unx    21306 b- defN 23-Jan-17 15:47 pod5/tools/pod5_convert_from_fast5.py
+-rw-r--r--  2.0 unx    10456 b- defN 23-Jan-17 15:47 pod5/tools/pod5_convert_to_fast5.py
+-rw-r--r--  2.0 unx     6533 b- defN 23-Jan-17 15:47 pod5/tools/pod5_inspect.py
+-rw-r--r--  2.0 unx     2447 b- defN 23-Jan-17 15:47 pod5/tools/pod5_merge.py
+-rw-r--r--  2.0 unx     1740 b- defN 23-Jan-17 15:47 pod5/tools/pod5_repack.py
+-rw-r--r--  2.0 unx    13768 b- defN 23-Jan-17 15:47 pod5/tools/pod5_subset.py
+-rw-r--r--  2.0 unx     1009 b- defN 23-Jan-17 15:47 pod5/tools/pod5_update.py
+-rw-r--r--  2.0 unx     1004 b- defN 23-Jan-17 15:47 pod5/tools/utils.py
+-rw-r--r--  2.0 unx    14925 b- defN 23-Jan-20 11:29 pod5-0.1.5.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jan-20 11:29 pod5-0.1.5.dist-info/WHEEL
+-rw-r--r--  2.0 unx       46 b- defN 23-Jan-20 11:29 pod5-0.1.5.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        5 b- defN 23-Jan-20 11:29 pod5-0.1.5.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     1798 b- defN 23-Jan-20 11:29 pod5-0.1.5.dist-info/RECORD
+23 files, 165776 bytes uncompressed, 44996 bytes compressed:  72.9%
```

## zipnote {}

```diff
@@ -48,23 +48,23 @@
 
 Filename: pod5/tools/pod5_update.py
 Comment: 
 
 Filename: pod5/tools/utils.py
 Comment: 
 
-Filename: pod5-0.1.4.dist-info/METADATA
+Filename: pod5-0.1.5.dist-info/METADATA
 Comment: 
 
-Filename: pod5-0.1.4.dist-info/WHEEL
+Filename: pod5-0.1.5.dist-info/WHEEL
 Comment: 
 
-Filename: pod5-0.1.4.dist-info/entry_points.txt
+Filename: pod5-0.1.5.dist-info/entry_points.txt
 Comment: 
 
-Filename: pod5-0.1.4.dist-info/top_level.txt
+Filename: pod5-0.1.5.dist-info/top_level.txt
 Comment: 
 
-Filename: pod5-0.1.4.dist-info/RECORD
+Filename: pod5-0.1.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pod5/pod5_types.py

```diff
@@ -86,15 +86,17 @@
 
     #: Calibration offset used to convert raw ADC data into pA readings.
     offset: float
     #: Calibration scale factor used to convert raw ADC data into pA readings.
     scale: float
 
     @classmethod
-    def from_range(cls, offset: float, adc_range: float, digitisation: float):
+    def from_range(
+        cls, offset: float, adc_range: float, digitisation: float
+    ) -> "Calibration":
         """Create a Calibration instance from offset, adc_range and digitisation"""
         return cls(offset, adc_range / digitisation)
 
 
 @dataclass()
 class Pore:
     """
```

## pod5/reader.py

```diff
@@ -69,22 +69,22 @@
 )
 
 
 @total_ordering
 class ReadTableVersion(enum.Enum):
     """Version of read table"""
 
-    V3 = 3
+    V3: int = 3
 
-    def __lt__(self, other):
+    def __lt__(self, other) -> bool:
         if self.__class__ is other.__class__:
             return self.value < other.value
         return NotImplemented
 
-    def __eq__(self, other):
+    def __eq__(self, other) -> bool:
         if self.__class__ is other.__class__:
             return self.value == other.value
         return NotImplemented
 
 
 Signal = namedtuple("Signal", ["signal", "samples"])
 SignalRowInfo = namedtuple(
@@ -99,16 +99,16 @@
     """
 
     def __init__(
         self,
         reader: "Reader",
         batch: "ReadRecordBatch",
         row: int,
-        batch_signal_cache=None,
-        selected_batch_index=None,
+        batch_signal_cache: Optional[List[npt.NDArray[np.int16]]] = None,
+        selected_batch_index: Optional[int] = None,
     ):
         """ """
         self._reader = reader
         self._batch = batch
         self._row = row
         self._batch_signal_cache = batch_signal_cache
         self._selected_batch_index = selected_batch_index
@@ -121,43 +121,43 @@
         return UUID(bytes=self._batch.columns.read_id[self._row].as_py())
 
     @property
     def read_number(self) -> int:
         """
         Get the integer read number of the read.
         """
-        return self._batch.columns.read_number[self._row].as_py()
+        return self._batch.columns.read_number[self._row].as_py()  # type: ignore
 
     @property
     def start_sample(self) -> int:
         """
         Get the absolute sample which the read started.
         """
-        return self._batch.columns.start[self._row].as_py()
+        return self._batch.columns.start[self._row].as_py()  # type: ignore
 
     @property
     def num_samples(self) -> int:
         """
         Get the number of samples in the reads signal data.
         """
-        return self._batch.columns.num_samples[self._row].as_py()
+        return self._batch.columns.num_samples[self._row].as_py()  # type: ignore
 
     @property
     def median_before(self) -> float:
         """
         Get the median before level (in pico amps) for the read.
         """
-        return self._batch.columns.median_before[self._row].as_py()
+        return self._batch.columns.median_before[self._row].as_py()  # type: ignore
 
     @property
     def num_minknow_events(self) -> float:
         """
         Find the number of minknow events in the read.
         """
-        return self._batch.columns.num_minknow_events[self._row].as_py()
+        return self._batch.columns.num_minknow_events[self._row].as_py()  # type: ignore
 
     @property
     def tracked_scaling(self) -> ShiftScalePair:
         """
         Find the tracked scaling value in the read.
         """
         return ShiftScalePair(
@@ -176,22 +176,22 @@
         )
 
     @property
     def num_reads_since_mux_change(self) -> int:
         """
         Number of selected reads since the last mux change on this reads channel.
         """
-        return self._batch.columns.num_reads_since_mux_change[self._row].as_py()
+        return self._batch.columns.num_reads_since_mux_change[self._row].as_py()  # type: ignore
 
     @property
     def time_since_mux_change(self) -> int:
         """
         Time in seconds since the last mux change on this reads channel.
         """
-        return self._batch.columns.time_since_mux_change[self._row].as_py()
+        return self._batch.columns.time_since_mux_change[self._row].as_py()  # type: ignore
 
     @property
     def pore(self) -> Pore:
         """
         Get the pore data associated with the read.
         """
         return Pore(
@@ -249,22 +249,22 @@
 
     @property
     def end_reason_index(self) -> int:
         """
         Get the dictionary index of the end reason data associated with the read.
         This property is the same as the EndReason enumeration value.
         """
-        return self._batch.columns.end_reason[self._row].index.as_py()
+        return self._batch.columns.end_reason[self._row].index.as_py()  # type: ignore
 
     @property
     def run_info_index(self) -> int:
         """
         Get the dictionary index of the run info data associated with the read.
         """
-        return self._batch.columns.run_info[self._row].index.as_py()
+        return self._batch.columns.run_info[self._row].index.as_py()  # type: ignore
 
     @property
     def sample_count(self) -> int:
         """
         Get the number of samples in the reads signal data.
         """
         return self.num_samples
@@ -277,27 +277,27 @@
         return sum(r.byte_count for r in self.signal_rows)
 
     @property
     def has_cached_signal(self) -> bool:
         """
         Get if cached signal is available for this read.
         """
-        return self._batch_signal_cache
+        return self._batch_signal_cache is not None
 
     @property
     def signal(self) -> npt.NDArray[np.int16]:
         """
         Get the full signal for the read.
 
         Returns
         -------
         numpy.ndarray[int16]
             A numpy array of signal data with int16 type.
         """
-        if self.has_cached_signal:
+        if self._batch_signal_cache is not None:
             if self._selected_batch_index is not None:
                 return self._batch_signal_cache[self._selected_batch_index]
             return self._batch_signal_cache[self._row]
 
         rows = self._batch.columns.signal[self._row]
         batch_data = [self._find_signal_row_index(r.as_py()) for r in rows]
         sample_counts = []
@@ -354,15 +354,15 @@
 
         Returns
         -------
         list[SignalRowInfo]
             A list of signal row data (as SignalRowInfo) in the read.
         """
 
-        def map_signal_row(sig_row):
+        def map_signal_row(sig_row) -> SignalRowInfo:
             sig_row = sig_row.as_py()
 
             batch, batch_index, batch_row_index = self._find_signal_row_index(sig_row)
             return SignalRowInfo(
                 batch_index,
                 batch_row_index,
                 batch.samples[batch_row_index].as_py(),
@@ -506,15 +506,15 @@
     def get_read(self, row: int) -> ReadRecord:
         """Get the ReadRecord at row index"""
         return ReadRecord(self._reader, self, row)
 
     @property
     def num_reads(self) -> int:
         """Return the number of rows in this RecordBatch"""
-        return self._batch.num_rows
+        return int(self._batch.num_rows)
 
     @property
     def read_id_column(self):
         """
         Get the column of read ids for this batch
         """
         if self._selected_batch_rows is not None:
@@ -623,14 +623,19 @@
     def __init__(self, path: PathOrStr):
         """
         Open a pod5 filepath for reading
         """
 
         self._path = Path(path).absolute()
 
+        self._file_reader: Optional[p5b.Pod5FileReader] = None
+        self._read_handle: Optional[ArrowTableHandle] = None
+        self._run_info_handle: Optional[ArrowTableHandle] = None
+        self._signal_handle: Optional[ArrowTableHandle] = None
+
         (
             self._file_reader,
             self._read_handle,
             self._run_info_handle,
             self._signal_handle,
         ) = self._open_arrow_table_handles(self._path)
 
@@ -642,15 +647,15 @@
         writing_version_str = schema_metadata[b"MINKNOW:pod5_version"].decode("utf-8")
         writing_version = packaging.version.parse(writing_version_str)
 
         self._columns_type = ReadRecordV3Columns
         self._reads_table_version = ReadTableVersion.V3
 
         self._file_version = writing_version
-        self._file_version_pre_migration = (
+        self._file_version_pre_migration = packaging.version.Version(
             self._file_reader.get_file_version_pre_migration()
         )
 
         # Warning: The cached signal maintains an open file handle. So ensure that
         # this dictionary is cleared before closing.
         self._cached_signal_batches: Dict[int, Signal] = {}
         self._cached_run_infos: Dict[str, RunInfo] = {}
@@ -1032,14 +1037,18 @@
             acquisition_id_col = run_info_batch.column("acquisition_id")
             for row in range(run_info_batch.num_rows):
                 if acquisition_id_col[row].as_py() == acquisition_id:
                     values = {}
                     for field in fields(RunInfo):
                         col = run_info_batch.column(field.name)
                         values[field.name] = col[row].as_py()
+
+                        if field.name in ("tracking_id", "context_tags"):
+                            values[field.name] = {k: v for k, v in values[field.name]}
+
                     run_info = RunInfo(**values)
                     break
 
         if not run_info:
             raise Exception(
                 f"Failed to find run info '{acquisition_id}' in run info table"
             )
```

## pod5/repack.py

```diff
@@ -63,14 +63,15 @@
 
         Returns
         -------
         repacker_object: p5b.Pod5RepackerOutput
             Use this as "output_ref" in calls to :py:meth:`add_selected_reads_to_output`
             or :py:meth:`add_reads_to_output`
         """
+        assert output_file._writer is not None
         return self._repacker.add_output(output_file._writer)
 
     def add_selected_reads_to_output(
         self,
         output_ref: p5b.Pod5RepackerOutput,
         reader: p5.Reader,
         selected_read_ids: Collection[str],
```

## pod5/signal_tools.py

```diff
@@ -63,22 +63,25 @@
             f"Inconsistent number of chunks to decompress - "
             f"signals: {len(compressed_signal_chunks)}, counts: {len(sample_counts)}"
         )
 
     if len(compressed_signal_chunks) == 0:
         return np.array([], dtype=np.int16)
 
-    return np.concatenate(
+    decompressed_signal: npt.NDArray[
+        np.int16
+    ] = np.concatenate(  # type:ignore [no-untyped-call]
         [
             vbz_decompress_signal(signal_chunk, sample_count)
             for signal_chunk, sample_count in zip(
                 compressed_signal_chunks, sample_counts
             )
         ]
     )
+    return decompressed_signal
 
 
 def vbz_decompress_signal_into(
     compressed_signal: Union[npt.NDArray[np.uint8], memoryview],
     output_array: npt.NDArray[np.int16],
 ) -> npt.NDArray[np.int16]:
     """
```

## pod5/writer.py

```diff
@@ -1,14 +1,25 @@
 """
 Tools for writing POD5 data
 """
 import datetime
 import itertools
 from pathlib import Path
-from typing import Any, Callable, Dict, List, Sequence, Tuple, Type, TypeVar, Union
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    List,
+    Optional,
+    Sequence,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
+)
 
 import lib_pod5 as p5b
 import numpy as np
 import pytz
 
 from pod5.api_utils import Pod5ApiException
 from pod5.pod5_types import (
@@ -29,15 +40,15 @@
 def force_type_and_default(value, dtype, count, default_value=None):
     if default_value is not None and value is None:
         value = np.array([default_value] * count, dtype=dtype)
     assert value is not None
     return value.astype(type, copy=False)
 
 
-def map_to_tuples(info_map) -> List[Tuple[str, str]]:
+def map_to_tuples(info_map: Any) -> List[Tuple[str, str]]:
     """
     Convert a fast5 property map (e.g. context_tags and tracking_id) to a
     tuple or string pairs to pass to pod5 C API
     """
     if isinstance(info_map, dict):
         return list((str(key), str(value)) for key, value in info_map.items())
     if isinstance(info_map, list):
@@ -68,15 +79,17 @@
         """
         self._path = Path(path).absolute()
         self._software_name = software_name
 
         if self._path.is_file():
             raise FileExistsError("Input path already exists. Refusing to overwrite.")
 
-        self._writer = p5b.create_file(str(self._path), software_name, None)
+        self._writer: Optional[p5b.FileWriter] = p5b.create_file(
+            str(self._path), software_name, None
+        )
         if not self._writer:
             raise Pod5ApiException(
                 f"Failed to open writer at {self._path} : {p5b.get_error_string()}"
             )
 
         self._end_reasons: Dict[EndReason, int] = {}
         self._pores: Dict[PoreType, int] = {}
@@ -92,18 +105,18 @@
         # Internal lookup of _add functions based on their respective type
         self._adder_funcs: Dict[Type, Callable[[Any], int]] = {
             EndReason: self._add_end_reason,
             PoreType: self._add_pore_type,
             RunInfo: self._add_run_info,
         }
 
-    def __enter__(self):
+    def __enter__(self) -> "Writer":
         return self
 
-    def __exit__(self, *exc_details):
+    def __exit__(self, *exc_details) -> None:
         self.close()
 
     def close(self) -> None:
         """Close the FileWriter handle"""
         if self._writer:
             self._writer.close()
             self._writer = None
@@ -147,22 +160,29 @@
         index_cache[obj] = added_index
 
         # Return the newly added index
         return added_index
 
     def _add_end_reason(self, end_reason: EndReason) -> int:
         """Add the given EndReason instance to the pod5 file returning its index"""
+        if self._writer is None:
+            raise Pod5ApiException("Writer handle has been closed")
         return self._writer.add_end_reason(end_reason.reason.value)
 
     def _add_pore_type(self, pore_type: PoreType) -> int:
         """Add the given PoreType instance to the pod5 file returning its index"""
+        if self._writer is None:
+            raise Pod5ApiException("Writer handle has been closed")
         return self._writer.add_pore(pore_type)
 
     def _add_run_info(self, run_info: RunInfo) -> int:
         """Add the given RunInfo instance to the pod5 file returning its index"""
+        if self._writer is None:
+            raise Pod5ApiException("Writer handle has been closed")
+
         return self._writer.add_run_info(
             run_info.acquisition_id,
             timestamp_to_int(run_info.acquisition_start_time),
             run_info.adc_max,
             run_info.adc_min,
             map_to_tuples(run_info.context_tags),
             run_info.experiment_name,
@@ -245,39 +265,42 @@
             List of Read object to be added to this POD5 file
         """
 
         # Nothing to do
         if not reads:
             return
 
+        if self._writer is None:
+            raise Pod5ApiException("Writer handle has been closed")
+
         if isinstance(reads[0], Read):
-            return self._writer.add_reads(
+            return self._writer.add_reads(  # type: ignore [call-arg]
                 *self._prepare_add_reads_args(reads),
-                [r.signal for r in reads],
+                [r.signal for r in reads],  # type: ignore
             )
         elif isinstance(reads[0], CompressedRead):
-            signal_chunks = [r.signal_chunks for r in reads]
-            signal_chunk_lengths = [r.signal_chunk_lengths for r in reads]
+            signal_chunks = [r.signal_chunks for r in reads]  # type: ignore
+            signal_chunk_lengths = [r.signal_chunk_lengths for r in reads]  # type: ignore
 
             # Array containing the number of chunks for each signal
             signal_chunk_counts = np.array(
                 [len(samples_per_chunk) for samples_per_chunk in signal_chunk_lengths],
                 dtype=np.uint32,
             )
 
-            return self._writer.add_reads_pre_compressed(
+            return self._writer.add_reads_pre_compressed(  # type: ignore [call-arg]
                 *self._prepare_add_reads_args(reads),
                 # Join all signal data into one list
                 list(itertools.chain(*signal_chunks)),
                 # Join all read sample counts into one array
-                np.concatenate(signal_chunk_lengths).astype(np.uint32),
+                np.concatenate(signal_chunk_lengths).astype(np.uint32),  # type: ignore [no-untyped-call]
                 signal_chunk_counts,
             )
 
-    def _prepare_add_reads_args(self, reads: Sequence[BaseRead]):
+    def _prepare_add_reads_args(self, reads: Sequence[BaseRead]) -> List[Any]:
         """
         Converts the List of reads into the list of ctypes arrays of data to be supplied
         to the c api.
         """
         read_id = np.array(
             [np.frombuffer(read.read_id.bytes, dtype=np.uint8) for read in reads]
         )
```

## pod5/tools/main.py

```diff
@@ -1,24 +1,25 @@
 """Main entry point for pod5 tools"""
 import argparse
+from typing import Any
 
 from pod5 import __version__
 from pod5.tools.parsers import (
     SubcommandHelpFormatter,
     prepare_pod5_convert,
     prepare_pod5_inspect_argparser,
     prepare_pod5_merge_argparser,
     prepare_pod5_repack_argparser,
     prepare_pod5_subset_argparser,
     prepare_pod5_update_argparser,
     run_tool,
 )
 
 
-def main():
+def main() -> Any:
     """
     The core pod5 tools function which assembles the argparser and executes the required
     pod5 tool.
     """
     desc = (
         "**********      POD5 Tools      **********\n\n"
         "Tools for inspecting, converting, subsetting and formatting POD5 files"
```

## pod5/tools/pod5_convert_to_fast5.py

```diff
@@ -2,15 +2,15 @@
 Tool for converting pod5 files to the legacy fast5 format
 """
 import multiprocessing as mp
 import time
 from collections import namedtuple
 from pathlib import Path
 from queue import Empty
-from typing import List
+from typing import List, Sequence
 
 import h5py
 import numpy
 import vbz_h5py_plugin  # noqa: F401
 
 import pod5 as p5
 from pod5.tools.parsers import pod5_convert_to_fast5_argparser, run_tool
@@ -30,46 +30,20 @@
         if count > div:
             return f"{count/div:.1f} {unit}Samples"
 
     return f"{count} Samples"
 
 
 WriteRequest = namedtuple("WriteRequest", [])
-Read = namedtuple(
-    "Read",
-    [
-        "read_id",
-        "signal",
-        "pore_type",
-        "digitisation",
-        "offset",
-        "range",
-        "sampling_rate",
-        "channel_number",
-        "channel_mux",
-        "start_time",
-        "duration",
-        "read_number",
-        "median_before",
-        "end_reason",
-        "tracking_id",
-        "context_tags",
-        "num_minknow_events",
-        "tracked_scaling_scale",
-        "tracked_scaling_shift",
-        "predicted_scaling_scale",
-        "predicted_scaling_shift",
-        "num_reads_since_mux_change",
-        "time_since_mux_change",
-    ],
-)
 Fast5FileData = namedtuple("Fast5FileData", ["filename", "reads"])
 
 
-def do_write_fast5_files(write_request_queue, write_data_queue, exit_queue):
+def do_write_fast5_files(
+    write_request_queue: mp.Queue, write_data_queue: mp.Queue, exit_queue: mp.Queue
+):
 
     # Pod5 does not have 'partial' so need to add that back in here.
     fast5_end_reasons = {
         "unknown": 0,
         "partial": 1,  # Do not remove, required by fast5.
         "mux_change": 2,
         "unblock_mux_change": 3,
@@ -99,63 +73,72 @@
             file.attrs.create(
                 "file_version", "3.0".encode("ascii"), dtype=ascii_string_type
             )
             file.attrs.create(
                 "file_type", "multi-read".encode("ascii"), dtype=ascii_string_type
             )
 
-            for read in file_data.reads:
-                tracking_id = dict(read.tracking_id)
+            reads: Sequence[p5.Read] = file_data.reads
+            for read in reads:
+                tracking_id = dict(read.run_info.tracking_id)
                 read_group = file.create_group(f"read_{read.read_id}")
                 read_group.attrs.create(
                     "run_id",
                     tracking_id["run_id"].encode("ascii"),
                     dtype=ascii_string_type,
                 )
                 read_group.attrs.create(
-                    "pore_type", read.pore_type.encode("ascii"), dtype=ascii_string_type
+                    "pore_type",
+                    read.pore.pore_type.encode("ascii"),
+                    dtype=ascii_string_type,
                 )
 
                 tracking_id_group = read_group.create_group("tracking_id")
                 for k, v in tracking_id.items():
                     tracking_id_group.attrs[k] = v
 
                 context_tags_group = read_group.create_group("context_tags")
-                for k, v in read.context_tags:
+                for k, v in read.run_info.context_tags.items():
                     context_tags_group.attrs[k] = v
 
                 channel_id_group = read_group.create_group("channel_id")
+                digitisation = read.run_info.adc_max - read.run_info.adc_min + 1
                 channel_id_group.attrs.create(
-                    "digitisation", read.digitisation, dtype=numpy.float64
+                    "digitisation", digitisation, dtype=numpy.float64
                 )
                 channel_id_group.attrs.create(
-                    "offset", read.offset, dtype=numpy.float64
+                    "offset", read.calibration.offset, dtype=numpy.float64
+                )
+
+                channel_id_group.attrs.create(
+                    "range", digitisation * read.calibration.scale, dtype=numpy.float64
                 )
-                channel_id_group.attrs.create("range", read.range, dtype=numpy.float64)
                 channel_id_group.attrs.create(
-                    "sampling_rate", read.sampling_rate, dtype=numpy.float64
+                    "sampling_rate", read.run_info.sample_rate, dtype=numpy.float64
                 )
-                channel_id_group.attrs["channel_number"] = str(read.channel_number)
+                channel_id_group.attrs["channel_number"] = str(read.pore.channel)
 
                 raw_group = read_group.create_group("Raw")
                 raw_group.create_dataset(
                     "Signal",
                     data=read.signal,
                     dtype=numpy.int16,
                     compression=32020,
                     compression_opts=(0, 2, 1, 1),
                 )
                 raw_group.attrs.create(
-                    "start_time", read.start_time, dtype=numpy.uint64
+                    "start_time", read.start_sample, dtype=numpy.uint64
+                )
+                raw_group.attrs.create(
+                    "duration", read.sample_count, dtype=numpy.uint32
                 )
-                raw_group.attrs.create("duration", read.duration, dtype=numpy.uint32)
                 raw_group.attrs.create(
                     "read_number", read.read_number, dtype=numpy.int32
                 )
-                raw_group.attrs.create("start_mux", read.channel_mux, dtype=numpy.uint8)
+                raw_group.attrs.create("start_mux", read.pore.well, dtype=numpy.uint8)
                 raw_group.attrs["read_id"] = str(read.read_id)
                 raw_group.attrs.create(
                     "median_before", read.median_before, dtype=numpy.float64
                 )
 
                 # Lookup the fast5 enumeration values, which should include "partial: 1"
                 # This will ensure that the enumeration is valid on a round-trip
@@ -167,30 +150,30 @@
 
                 raw_group.attrs.create(
                     "num_minknow_events", read.num_minknow_events, dtype=numpy.uint64
                 )
 
                 raw_group.attrs.create(
                     "tracked_scaling_scale",
-                    read.tracked_scaling_scale,
+                    read.tracked_scaling.scale,
                     dtype=numpy.float32,
                 )
                 raw_group.attrs.create(
                     "tracked_scaling_shift",
-                    read.tracked_scaling_shift,
+                    read.tracked_scaling.shift,
                     dtype=numpy.float32,
                 )
                 raw_group.attrs.create(
                     "predicted_scaling_scale",
-                    read.predicted_scaling_scale,
+                    read.predicted_scaling.scale,
                     dtype=numpy.float32,
                 )
                 raw_group.attrs.create(
                     "predicted_scaling_shift",
-                    read.predicted_scaling_shift,
+                    read.predicted_scaling.shift,
                     dtype=numpy.float32,
                 )
                 raw_group.attrs.create(
                     "num_reads_since_mux_change",
                     read.num_reads_since_mux_change,
                     dtype=numpy.uint32,
                 )
@@ -208,44 +191,14 @@
     file_reads: Fast5FileData, write_request_queue, write_data_queue
 ):
 
     write_request_queue.get()
     write_data_queue.put(file_reads)
 
 
-def extract_read(read_table_version: p5.reader.ReadTableVersion, read: p5.ReadRecord):
-    run_info = read.run_info
-
-    return Read(
-        read.read_id,
-        read.signal,
-        read.pore.pore_type,
-        read.calibration_digitisation,
-        read.calibration.offset,
-        read.calibration_range,
-        run_info.sample_rate,
-        read.pore.channel,
-        read.pore.well,
-        read.start_sample,
-        read.sample_count,
-        read.read_number,
-        read.median_before,
-        read.end_reason,
-        run_info.tracking_id,
-        run_info.context_tags,
-        read.num_minknow_events,
-        read.tracked_scaling.scale,
-        read.tracked_scaling.shift,
-        read.predicted_scaling.scale,
-        read.predicted_scaling.shift,
-        read.num_reads_since_mux_change,
-        read.time_since_mux_change,
-    )
-
-
 def make_fast5_filename(output_location, file_index):
     output_location.mkdir(parents=True, exist_ok=True)
     return output_location / f"output_{file_index}.fast5"
 
 
 def convert_to_fast5(
     inputs: List[Path],
@@ -313,18 +266,17 @@
                 print(
                     f"{file_count} fast5s\t"
                     f"{read_count} reads\t"
                     f"{format_sample_count(sample_count)}\t"
                     f"{mb_total/time_total:.1f} MB/s"
                 )
 
-            extracted_read = extract_read(reader.reads_table_version, read)
-            current_reads_batch.append(extracted_read)
+            current_reads_batch.append(read.to_read())
             read_count += 1
-            sample_count += len(extracted_read.signal)
+            sample_count += read.num_samples
 
             # Write a batch of reads to a fast5 file
             if len(current_reads_batch) >= file_read_count:
                 put_write_fast5_file(
                     Fast5FileData(
                         make_fast5_filename(output, file_count),
                         current_reads_batch,
```

## pod5/tools/pod5_subset.py

```diff
@@ -180,15 +180,15 @@
         json_data = json_load(_fh)
         jsonschema.validate(instance=json_data, schema=JSON_SCHEMA)
 
     return json_data
 
 
 def get_total_selection(
-    mapping: typing.Dict[str, typing.Iterable[str]], duplicate_ok: bool
+    mapping: typing.Mapping[str, typing.Iterable[str]], duplicate_ok: bool
 ) -> typing.Set[str]:
     """
     Create a set of all read_ids from the mapping checking for duplicates if necessary
     """
     # Create a set of the required output read_ids
     total_selection: typing.Set[str] = set()
     for requested_read_ids in mapping.values():
@@ -199,31 +199,35 @@
 
     if not duplicate_ok:
         assert_no_duplicate_reads(mapping=mapping)
 
     return total_selection
 
 
-def assert_no_duplicate_reads(mapping: typing.Dict[str, typing.Iterable[str]]) -> None:
+def assert_no_duplicate_reads(
+    mapping: typing.Mapping[str, typing.Iterable[str]]
+) -> None:
     """
     Raise AssertionError if we detect any duplicate read_ids in the outputs
     """
     # Count the total number of outputs to detect if duplicates were requested
     counter: typing.Counter[str] = Counter()
     for ids in mapping.values():
         counter.update(ids)
 
     if any(count > 1 for count in counter.values()):
         raise AssertionError("Duplicate outputs detected but --duplicate_ok not set")
 
+    return None
+
 
 def prepare_repacker_outputs(
     repacker: p5_repack.Repacker,
     output: Path,
-    mapping: typing.Dict[str, typing.Iterable[str]],
+    mapping: typing.Mapping[str, typing.Iterable[str]],
 ) -> OutputMap:
     """
     Create a dictionary of the output filepath and their and associated
     FileWriter and Pod5RepackerOutput objects.
     """
     outputs: OutputMap = {}
     for target in mapping:
@@ -282,15 +286,15 @@
     transfers: TransferMap = defaultdict(set)
 
     for input_path in inputs:
         # Open a FileReader from input_path
         p5_reader = p5.Reader(input_path)
 
         # Iterate over batches of read_ids
-        for batch in p5_reader.read_batches(selection=selection, missing_ok=True):
+        for batch in p5_reader.read_batches(selection=list(selection), missing_ok=True):
             for read_id in p5.format_read_ids(batch.read_id_column):
 
                 # Get the repacker output destination
                 _, repacker_output = outputs[read_targets[read_id]]
 
                 # Add this read_id to the target_mapping
                 transfers[(p5_reader, repacker_output)].add(str(read_id))
@@ -324,15 +328,15 @@
 
     print("Done")
 
 
 def subset_pod5s_with_mapping(
     inputs: typing.Iterable[Path],
     output: Path,
-    mapping: typing.Dict[str, typing.Iterable[str]],
+    mapping: typing.Mapping[str, typing.Iterable[str]],
     missing_ok: bool = False,
     duplicate_ok: bool = False,
     force_overwrite: bool = False,
 ) -> typing.List[Path]:
     """
     Given an iterable of input pod5 paths and an output directory, create output pod5
     files containing the read_ids specified in the given mapping of output filename to
```

## Comparing `pod5-0.1.4.dist-info/METADATA` & `pod5-0.1.5.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,38 +1,42 @@
 Metadata-Version: 2.1
 Name: pod5
-Version: 0.1.4
+Version: 0.1.5
 Summary: Oxford Nanopore Technologies Pod5 File Format Python API and Tools
 Author-email: support@nanoporetech.com
 Keywords: nanopore
 Classifier: Environment :: Console
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)
 Classifier: Natural Language :: English
 Classifier: Programming Language :: Python :: 3
 Classifier: Topic :: Scientific/Engineering :: Bio-Informatics
 Requires-Python: ~=3.7
 Description-Content-Type: text/markdown
 Requires-Dist: iso8601
 Requires-Dist: jsonschema
-Requires-Dist: lib-pod5 (~=0.1.4)
+Requires-Dist: lib-pod5 (~=0.1.5)
 Requires-Dist: more-itertools
 Requires-Dist: numpy (>=1.20.0)
 Requires-Dist: pyarrow (~=8.0.0)
 Requires-Dist: pytz
 Requires-Dist: packaging
 Requires-Dist: pandas
 Requires-Dist: vbz-h5py-plugin
 Provides-Extra: dev
 Requires-Dist: black (==22.3.0) ; extra == 'dev'
-Requires-Dist: mypy ; extra == 'dev'
+Requires-Dist: mypy (==0.991) ; extra == 'dev'
 Requires-Dist: pre-commit ; extra == 'dev'
 Requires-Dist: pytest (~=6.2) ; extra == 'dev'
 Requires-Dist: pytest-cov (~=3.0) ; extra == 'dev'
+Requires-Dist: types-jsonschema ; extra == 'dev'
+Requires-Dist: types-setuptools ; extra == 'dev'
+Requires-Dist: types-pytz ; extra == 'dev'
+Requires-Dist: pandas-stubs ; extra == 'dev'
 
 POD5 Python Package
 ===================
 
 The `pod5` Python package contains the tools and python API wrapping the compiled bindings
 for the POD5 file format from `lib_pod5`.
```

## Comparing `pod5-0.1.4.dist-info/RECORD` & `pod5-0.1.5.dist-info/RECORD`

 * *Files 18% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 pod5/__init__.py,sha256=oV8b_pKy0lZdDL7Yej7cnw3wylDPr8hkvWQaIqfh_Co,751
 pod5/api_utils.py,sha256=xsD_0UfD2xTYShZ-aZtn5mrKzOQtrL4X_7ek9x6H-Ok,2168
-pod5/pod5_types.py,sha256=lF3PPtyJjA59h6qx_T3EPmGGKqMOUUfufwxWx7_cdEM,13111
-pod5/reader.py,sha256=b1Azr7tHBFZRPlW3hqNarE5cZ5_uRiG71eM0v3r8Ynw,33936
-pod5/repack.py,sha256=UydvgfecX69N_be4J31y3hRpezXdbfK_Riv0FOGwKgg,6422
-pod5/signal_tools.py,sha256=bzmOW8cdRkyTQZgz3zX4gkzMZcPqTAm8Z0hky4ljsQ4,4711
-pod5/writer.py,sha256=9qLom1CuoEqWJkuPfxv3TYHkLi71qgjCEEmk18o4zPU,12239
+pod5/pod5_types.py,sha256=KRcEpRyfeTLfmXfxYAohn-TUFklMALP_3f8S_yAYigE,13142
+pod5/reader.py,sha256=jyEON1sORCDHI90OArnfZST50FESIO_LjgcRAdFyjQo,34648
+pod5/repack.py,sha256=igvfRS0OuS7GriAc2T2O1oYD7_iNq5dTc1G7YvckSsc,6469
+pod5/signal_tools.py,sha256=abhJbH6wKjBwG2edXeXEIk6ULCBshH8b-J5fk_UIUaE,4827
+pod5/writer.py,sha256=3Fsdk-TH1xMOJ91Sl7Gi0Vd5GH2F9-ZBEZ5eA9gJukI,12922
 pod5/tools/__init__.py,sha256=1s_72oZeqBAn3tANjY_auA_SrtP3c4Ye3kft9t1-H90,24
-pod5/tools/main.py,sha256=3EfPi9NkOFMS2zjt1hvGP68y7TLdlCID10xygRpVxJo,1537
+pod5/tools/main.py,sha256=d4EZ1YBZo2XaRzFERLnVm6Zr5UI-_6moV2JD2q_MYl4,1567
 pod5/tools/parsers.py,sha256=HXzR3UpkFT_Z4EXlL-m1nESDCeQ68eNvxRHEmP4YsMg,14129
 pod5/tools/pod5_convert_from_fast5.py,sha256=x7mc9qQtUNM40hbLs9cAoQy2m5D1ocGd0qv_b94lXnM,21306
-pod5/tools/pod5_convert_to_fast5.py,sha256=F19HW3SttsgW3uwReLJG0dRIMxekbAUziHgIvv7-6y4,11662
+pod5/tools/pod5_convert_to_fast5.py,sha256=_Ltwk9nO7CsT-1JRKpDdmy9mR-MFYaHBA5t3odHXLxM,10456
 pod5/tools/pod5_inspect.py,sha256=0vk8Sbd4C1wWvuYm1hkuh2LUpx7Dw-3SKbeqJU7OAec,6533
 pod5/tools/pod5_merge.py,sha256=lkvABHit_BTLdZYKW_xJsE5kOz0shS5fuY_aZ3mePiY,2447
 pod5/tools/pod5_repack.py,sha256=s4bcNKmr61z8H00HcdAMFKZ93L9yFYknZq9KdggM_Xo,1740
-pod5/tools/pod5_subset.py,sha256=Nv6_9a2gpYjU-slfD3KrVv8ZyHvZ0HlX-bAOykqjJwg,13727
+pod5/tools/pod5_subset.py,sha256=PSWHLy1WdgV7yddpytupkRB6PhUWQjheHv07hfHW1C4,13768
 pod5/tools/pod5_update.py,sha256=31WVi1btmZcW6Uzn_SmzvM3wK5ZSbVCe9cUgEhWKD5s,1009
 pod5/tools/utils.py,sha256=3EJllNHqxv4EVN7RWIiU6Umn0ez5MRdgmMA6xqSir1c,1004
-pod5-0.1.4.dist-info/METADATA,sha256=LhWLSh3PU7wK-aHh_x7tmqFHBKwdBStAxwMjBUCpGyk,14729
-pod5-0.1.4.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-pod5-0.1.4.dist-info/entry_points.txt,sha256=CuZYKwUC2PuRgzHz7RI7JuZdWsTGvniyB_qo9vtig1g,46
-pod5-0.1.4.dist-info/top_level.txt,sha256=G6L1QAUTcDXbZIHuDIWHt4-rGWZdgw3eB0hM3a495Ec,5
-pod5-0.1.4.dist-info/RECORD,,
+pod5-0.1.5.dist-info/METADATA,sha256=ZB62FIpDPRVQuWpkghqEkdq48XXTGp-C-isTzBtOmZk,14925
+pod5-0.1.5.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+pod5-0.1.5.dist-info/entry_points.txt,sha256=CuZYKwUC2PuRgzHz7RI7JuZdWsTGvniyB_qo9vtig1g,46
+pod5-0.1.5.dist-info/top_level.txt,sha256=G6L1QAUTcDXbZIHuDIWHt4-rGWZdgw3eB0hM3a495Ec,5
+pod5-0.1.5.dist-info/RECORD,,
```

