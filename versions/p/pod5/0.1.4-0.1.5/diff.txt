--- tmp/pod5-0.1.4-py3-none-any.whl.zip
+++ tmp/pod5-0.1.5-py3-none-any.whl.zip
├── zipinfo {}
│ @@ -1,25 +1,25 @@
│ -Zip file size: 47726 bytes, number of entries: 23
│ --rw-r--r--  2.0 unx      751 b- defN 22-Dec-09 11:16 pod5/__init__.py
│ --rw-r--r--  2.0 unx     2168 b- defN 22-Dec-09 11:16 pod5/api_utils.py
│ --rw-r--r--  2.0 unx    13111 b- defN 22-Dec-16 11:36 pod5/pod5_types.py
│ --rw-r--r--  2.0 unx    33936 b- defN 22-Dec-16 11:36 pod5/reader.py
│ --rw-r--r--  2.0 unx     6422 b- defN 22-Dec-09 11:16 pod5/repack.py
│ --rw-r--r--  2.0 unx     4711 b- defN 22-Dec-09 11:16 pod5/signal_tools.py
│ --rw-r--r--  2.0 unx    12239 b- defN 22-Dec-16 11:36 pod5/writer.py
│ --rw-r--r--  2.0 unx       24 b- defN 22-Dec-09 11:16 pod5/tools/__init__.py
│ --rw-r--r--  2.0 unx     1537 b- defN 22-Dec-09 11:16 pod5/tools/main.py
│ --rw-r--r--  2.0 unx    14129 b- defN 22-Dec-09 11:16 pod5/tools/parsers.py
│ --rw-r--r--  2.0 unx    21306 b- defN 22-Dec-16 11:36 pod5/tools/pod5_convert_from_fast5.py
│ --rw-r--r--  2.0 unx    11662 b- defN 22-Dec-22 09:32 pod5/tools/pod5_convert_to_fast5.py
│ --rw-r--r--  2.0 unx     6533 b- defN 22-Dec-16 11:36 pod5/tools/pod5_inspect.py
│ --rw-r--r--  2.0 unx     2447 b- defN 22-Dec-09 11:16 pod5/tools/pod5_merge.py
│ --rw-r--r--  2.0 unx     1740 b- defN 22-Dec-09 11:16 pod5/tools/pod5_repack.py
│ --rw-r--r--  2.0 unx    13727 b- defN 22-Dec-09 11:16 pod5/tools/pod5_subset.py
│ --rw-r--r--  2.0 unx     1009 b- defN 22-Dec-09 11:16 pod5/tools/pod5_update.py
│ --rw-r--r--  2.0 unx     1004 b- defN 22-Dec-09 11:16 pod5/tools/utils.py
│ --rw-r--r--  2.0 unx    14729 b- defN 22-Dec-23 09:03 pod5-0.1.4.dist-info/METADATA
│ --rw-r--r--  2.0 unx       92 b- defN 22-Dec-23 09:03 pod5-0.1.4.dist-info/WHEEL
│ --rw-r--r--  2.0 unx       46 b- defN 22-Dec-23 09:03 pod5-0.1.4.dist-info/entry_points.txt
│ --rw-r--r--  2.0 unx        5 b- defN 22-Dec-23 09:03 pod5-0.1.4.dist-info/top_level.txt
│ -?rw-rw-r--  2.0 unx     1798 b- defN 22-Dec-23 09:03 pod5-0.1.4.dist-info/RECORD
│ -23 files, 165126 bytes uncompressed, 44870 bytes compressed:  72.8%
│ +Zip file size: 47852 bytes, number of entries: 23
│ +-rw-r--r--  2.0 unx      751 b- defN 23-Jan-17 15:47 pod5/__init__.py
│ +-rw-r--r--  2.0 unx     2168 b- defN 23-Jan-17 15:47 pod5/api_utils.py
│ +-rw-r--r--  2.0 unx    13142 b- defN 23-Jan-17 15:47 pod5/pod5_types.py
│ +-rw-r--r--  2.0 unx    34648 b- defN 23-Jan-17 15:47 pod5/reader.py
│ +-rw-r--r--  2.0 unx     6469 b- defN 23-Jan-17 15:47 pod5/repack.py
│ +-rw-r--r--  2.0 unx     4827 b- defN 23-Jan-17 15:47 pod5/signal_tools.py
│ +-rw-r--r--  2.0 unx    12922 b- defN 23-Jan-17 15:47 pod5/writer.py
│ +-rw-r--r--  2.0 unx       24 b- defN 23-Jan-17 15:47 pod5/tools/__init__.py
│ +-rw-r--r--  2.0 unx     1567 b- defN 23-Jan-17 15:47 pod5/tools/main.py
│ +-rw-r--r--  2.0 unx    14129 b- defN 23-Jan-17 15:47 pod5/tools/parsers.py
│ +-rw-r--r--  2.0 unx    21306 b- defN 23-Jan-17 15:47 pod5/tools/pod5_convert_from_fast5.py
│ +-rw-r--r--  2.0 unx    10456 b- defN 23-Jan-17 15:47 pod5/tools/pod5_convert_to_fast5.py
│ +-rw-r--r--  2.0 unx     6533 b- defN 23-Jan-17 15:47 pod5/tools/pod5_inspect.py
│ +-rw-r--r--  2.0 unx     2447 b- defN 23-Jan-17 15:47 pod5/tools/pod5_merge.py
│ +-rw-r--r--  2.0 unx     1740 b- defN 23-Jan-17 15:47 pod5/tools/pod5_repack.py
│ +-rw-r--r--  2.0 unx    13768 b- defN 23-Jan-17 15:47 pod5/tools/pod5_subset.py
│ +-rw-r--r--  2.0 unx     1009 b- defN 23-Jan-17 15:47 pod5/tools/pod5_update.py
│ +-rw-r--r--  2.0 unx     1004 b- defN 23-Jan-17 15:47 pod5/tools/utils.py
│ +-rw-r--r--  2.0 unx    14925 b- defN 23-Jan-20 11:29 pod5-0.1.5.dist-info/METADATA
│ +-rw-r--r--  2.0 unx       92 b- defN 23-Jan-20 11:29 pod5-0.1.5.dist-info/WHEEL
│ +-rw-r--r--  2.0 unx       46 b- defN 23-Jan-20 11:29 pod5-0.1.5.dist-info/entry_points.txt
│ +-rw-r--r--  2.0 unx        5 b- defN 23-Jan-20 11:29 pod5-0.1.5.dist-info/top_level.txt
│ +?rw-rw-r--  2.0 unx     1798 b- defN 23-Jan-20 11:29 pod5-0.1.5.dist-info/RECORD
│ +23 files, 165776 bytes uncompressed, 44996 bytes compressed:  72.9%
├── zipnote {}
│ @@ -48,23 +48,23 @@
│  
│  Filename: pod5/tools/pod5_update.py
│  Comment: 
│  
│  Filename: pod5/tools/utils.py
│  Comment: 
│  
│ -Filename: pod5-0.1.4.dist-info/METADATA
│ +Filename: pod5-0.1.5.dist-info/METADATA
│  Comment: 
│  
│ -Filename: pod5-0.1.4.dist-info/WHEEL
│ +Filename: pod5-0.1.5.dist-info/WHEEL
│  Comment: 
│  
│ -Filename: pod5-0.1.4.dist-info/entry_points.txt
│ +Filename: pod5-0.1.5.dist-info/entry_points.txt
│  Comment: 
│  
│ -Filename: pod5-0.1.4.dist-info/top_level.txt
│ +Filename: pod5-0.1.5.dist-info/top_level.txt
│  Comment: 
│  
│ -Filename: pod5-0.1.4.dist-info/RECORD
│ +Filename: pod5-0.1.5.dist-info/RECORD
│  Comment: 
│  
│  Zip file comment:
├── pod5/pod5_types.py
│ @@ -86,15 +86,17 @@
│  
│      #: Calibration offset used to convert raw ADC data into pA readings.
│      offset: float
│      #: Calibration scale factor used to convert raw ADC data into pA readings.
│      scale: float
│  
│      @classmethod
│ -    def from_range(cls, offset: float, adc_range: float, digitisation: float):
│ +    def from_range(
│ +        cls, offset: float, adc_range: float, digitisation: float
│ +    ) -> "Calibration":
│          """Create a Calibration instance from offset, adc_range and digitisation"""
│          return cls(offset, adc_range / digitisation)
│  
│  
│  @dataclass()
│  class Pore:
│      """
├── pod5/reader.py
│ @@ -69,22 +69,22 @@
│  )
│  
│  
│  @total_ordering
│  class ReadTableVersion(enum.Enum):
│      """Version of read table"""
│  
│ -    V3 = 3
│ +    V3: int = 3
│  
│ -    def __lt__(self, other):
│ +    def __lt__(self, other) -> bool:
│          if self.__class__ is other.__class__:
│              return self.value < other.value
│          return NotImplemented
│  
│ -    def __eq__(self, other):
│ +    def __eq__(self, other) -> bool:
│          if self.__class__ is other.__class__:
│              return self.value == other.value
│          return NotImplemented
│  
│  
│  Signal = namedtuple("Signal", ["signal", "samples"])
│  SignalRowInfo = namedtuple(
│ @@ -99,16 +99,16 @@
│      """
│  
│      def __init__(
│          self,
│          reader: "Reader",
│          batch: "ReadRecordBatch",
│          row: int,
│ -        batch_signal_cache=None,
│ -        selected_batch_index=None,
│ +        batch_signal_cache: Optional[List[npt.NDArray[np.int16]]] = None,
│ +        selected_batch_index: Optional[int] = None,
│      ):
│          """ """
│          self._reader = reader
│          self._batch = batch
│          self._row = row
│          self._batch_signal_cache = batch_signal_cache
│          self._selected_batch_index = selected_batch_index
│ @@ -121,43 +121,43 @@
│          return UUID(bytes=self._batch.columns.read_id[self._row].as_py())
│  
│      @property
│      def read_number(self) -> int:
│          """
│          Get the integer read number of the read.
│          """
│ -        return self._batch.columns.read_number[self._row].as_py()
│ +        return self._batch.columns.read_number[self._row].as_py()  # type: ignore
│  
│      @property
│      def start_sample(self) -> int:
│          """
│          Get the absolute sample which the read started.
│          """
│ -        return self._batch.columns.start[self._row].as_py()
│ +        return self._batch.columns.start[self._row].as_py()  # type: ignore
│  
│      @property
│      def num_samples(self) -> int:
│          """
│          Get the number of samples in the reads signal data.
│          """
│ -        return self._batch.columns.num_samples[self._row].as_py()
│ +        return self._batch.columns.num_samples[self._row].as_py()  # type: ignore
│  
│      @property
│      def median_before(self) -> float:
│          """
│          Get the median before level (in pico amps) for the read.
│          """
│ -        return self._batch.columns.median_before[self._row].as_py()
│ +        return self._batch.columns.median_before[self._row].as_py()  # type: ignore
│  
│      @property
│      def num_minknow_events(self) -> float:
│          """
│          Find the number of minknow events in the read.
│          """
│ -        return self._batch.columns.num_minknow_events[self._row].as_py()
│ +        return self._batch.columns.num_minknow_events[self._row].as_py()  # type: ignore
│  
│      @property
│      def tracked_scaling(self) -> ShiftScalePair:
│          """
│          Find the tracked scaling value in the read.
│          """
│          return ShiftScalePair(
│ @@ -176,22 +176,22 @@
│          )
│  
│      @property
│      def num_reads_since_mux_change(self) -> int:
│          """
│          Number of selected reads since the last mux change on this reads channel.
│          """
│ -        return self._batch.columns.num_reads_since_mux_change[self._row].as_py()
│ +        return self._batch.columns.num_reads_since_mux_change[self._row].as_py()  # type: ignore
│  
│      @property
│      def time_since_mux_change(self) -> int:
│          """
│          Time in seconds since the last mux change on this reads channel.
│          """
│ -        return self._batch.columns.time_since_mux_change[self._row].as_py()
│ +        return self._batch.columns.time_since_mux_change[self._row].as_py()  # type: ignore
│  
│      @property
│      def pore(self) -> Pore:
│          """
│          Get the pore data associated with the read.
│          """
│          return Pore(
│ @@ -249,22 +249,22 @@
│  
│      @property
│      def end_reason_index(self) -> int:
│          """
│          Get the dictionary index of the end reason data associated with the read.
│          This property is the same as the EndReason enumeration value.
│          """
│ -        return self._batch.columns.end_reason[self._row].index.as_py()
│ +        return self._batch.columns.end_reason[self._row].index.as_py()  # type: ignore
│  
│      @property
│      def run_info_index(self) -> int:
│          """
│          Get the dictionary index of the run info data associated with the read.
│          """
│ -        return self._batch.columns.run_info[self._row].index.as_py()
│ +        return self._batch.columns.run_info[self._row].index.as_py()  # type: ignore
│  
│      @property
│      def sample_count(self) -> int:
│          """
│          Get the number of samples in the reads signal data.
│          """
│          return self.num_samples
│ @@ -277,27 +277,27 @@
│          return sum(r.byte_count for r in self.signal_rows)
│  
│      @property
│      def has_cached_signal(self) -> bool:
│          """
│          Get if cached signal is available for this read.
│          """
│ -        return self._batch_signal_cache
│ +        return self._batch_signal_cache is not None
│  
│      @property
│      def signal(self) -> npt.NDArray[np.int16]:
│          """
│          Get the full signal for the read.
│  
│          Returns
│          -------
│          numpy.ndarray[int16]
│              A numpy array of signal data with int16 type.
│          """
│ -        if self.has_cached_signal:
│ +        if self._batch_signal_cache is not None:
│              if self._selected_batch_index is not None:
│                  return self._batch_signal_cache[self._selected_batch_index]
│              return self._batch_signal_cache[self._row]
│  
│          rows = self._batch.columns.signal[self._row]
│          batch_data = [self._find_signal_row_index(r.as_py()) for r in rows]
│          sample_counts = []
│ @@ -354,15 +354,15 @@
│  
│          Returns
│          -------
│          list[SignalRowInfo]
│              A list of signal row data (as SignalRowInfo) in the read.
│          """
│  
│ -        def map_signal_row(sig_row):
│ +        def map_signal_row(sig_row) -> SignalRowInfo:
│              sig_row = sig_row.as_py()
│  
│              batch, batch_index, batch_row_index = self._find_signal_row_index(sig_row)
│              return SignalRowInfo(
│                  batch_index,
│                  batch_row_index,
│                  batch.samples[batch_row_index].as_py(),
│ @@ -506,15 +506,15 @@
│      def get_read(self, row: int) -> ReadRecord:
│          """Get the ReadRecord at row index"""
│          return ReadRecord(self._reader, self, row)
│  
│      @property
│      def num_reads(self) -> int:
│          """Return the number of rows in this RecordBatch"""
│ -        return self._batch.num_rows
│ +        return int(self._batch.num_rows)
│  
│      @property
│      def read_id_column(self):
│          """
│          Get the column of read ids for this batch
│          """
│          if self._selected_batch_rows is not None:
│ @@ -623,14 +623,19 @@
│      def __init__(self, path: PathOrStr):
│          """
│          Open a pod5 filepath for reading
│          """
│  
│          self._path = Path(path).absolute()
│  
│ +        self._file_reader: Optional[p5b.Pod5FileReader] = None
│ +        self._read_handle: Optional[ArrowTableHandle] = None
│ +        self._run_info_handle: Optional[ArrowTableHandle] = None
│ +        self._signal_handle: Optional[ArrowTableHandle] = None
│ +
│          (
│              self._file_reader,
│              self._read_handle,
│              self._run_info_handle,
│              self._signal_handle,
│          ) = self._open_arrow_table_handles(self._path)
│  
│ @@ -642,15 +647,15 @@
│          writing_version_str = schema_metadata[b"MINKNOW:pod5_version"].decode("utf-8")
│          writing_version = packaging.version.parse(writing_version_str)
│  
│          self._columns_type = ReadRecordV3Columns
│          self._reads_table_version = ReadTableVersion.V3
│  
│          self._file_version = writing_version
│ -        self._file_version_pre_migration = (
│ +        self._file_version_pre_migration = packaging.version.Version(
│              self._file_reader.get_file_version_pre_migration()
│          )
│  
│          # Warning: The cached signal maintains an open file handle. So ensure that
│          # this dictionary is cleared before closing.
│          self._cached_signal_batches: Dict[int, Signal] = {}
│          self._cached_run_infos: Dict[str, RunInfo] = {}
│ @@ -1032,14 +1037,18 @@
│              acquisition_id_col = run_info_batch.column("acquisition_id")
│              for row in range(run_info_batch.num_rows):
│                  if acquisition_id_col[row].as_py() == acquisition_id:
│                      values = {}
│                      for field in fields(RunInfo):
│                          col = run_info_batch.column(field.name)
│                          values[field.name] = col[row].as_py()
│ +
│ +                        if field.name in ("tracking_id", "context_tags"):
│ +                            values[field.name] = {k: v for k, v in values[field.name]}
│ +
│                      run_info = RunInfo(**values)
│                      break
│  
│          if not run_info:
│              raise Exception(
│                  f"Failed to find run info '{acquisition_id}' in run info table"
│              )
├── pod5/repack.py
│ @@ -63,14 +63,15 @@
│  
│          Returns
│          -------
│          repacker_object: p5b.Pod5RepackerOutput
│              Use this as "output_ref" in calls to :py:meth:`add_selected_reads_to_output`
│              or :py:meth:`add_reads_to_output`
│          """
│ +        assert output_file._writer is not None
│          return self._repacker.add_output(output_file._writer)
│  
│      def add_selected_reads_to_output(
│          self,
│          output_ref: p5b.Pod5RepackerOutput,
│          reader: p5.Reader,
│          selected_read_ids: Collection[str],
├── pod5/signal_tools.py
│ @@ -63,22 +63,25 @@
│              f"Inconsistent number of chunks to decompress - "
│              f"signals: {len(compressed_signal_chunks)}, counts: {len(sample_counts)}"
│          )
│  
│      if len(compressed_signal_chunks) == 0:
│          return np.array([], dtype=np.int16)
│  
│ -    return np.concatenate(
│ +    decompressed_signal: npt.NDArray[
│ +        np.int16
│ +    ] = np.concatenate(  # type:ignore [no-untyped-call]
│          [
│              vbz_decompress_signal(signal_chunk, sample_count)
│              for signal_chunk, sample_count in zip(
│                  compressed_signal_chunks, sample_counts
│              )
│          ]
│      )
│ +    return decompressed_signal
│  
│  
│  def vbz_decompress_signal_into(
│      compressed_signal: Union[npt.NDArray[np.uint8], memoryview],
│      output_array: npt.NDArray[np.int16],
│  ) -> npt.NDArray[np.int16]:
│      """
├── pod5/writer.py
│ @@ -1,14 +1,25 @@
│  """
│  Tools for writing POD5 data
│  """
│  import datetime
│  import itertools
│  from pathlib import Path
│ -from typing import Any, Callable, Dict, List, Sequence, Tuple, Type, TypeVar, Union
│ +from typing import (
│ +    Any,
│ +    Callable,
│ +    Dict,
│ +    List,
│ +    Optional,
│ +    Sequence,
│ +    Tuple,
│ +    Type,
│ +    TypeVar,
│ +    Union,
│ +)
│  
│  import lib_pod5 as p5b
│  import numpy as np
│  import pytz
│  
│  from pod5.api_utils import Pod5ApiException
│  from pod5.pod5_types import (
│ @@ -29,15 +40,15 @@
│  def force_type_and_default(value, dtype, count, default_value=None):
│      if default_value is not None and value is None:
│          value = np.array([default_value] * count, dtype=dtype)
│      assert value is not None
│      return value.astype(type, copy=False)
│  
│  
│ -def map_to_tuples(info_map) -> List[Tuple[str, str]]:
│ +def map_to_tuples(info_map: Any) -> List[Tuple[str, str]]:
│      """
│      Convert a fast5 property map (e.g. context_tags and tracking_id) to a
│      tuple or string pairs to pass to pod5 C API
│      """
│      if isinstance(info_map, dict):
│          return list((str(key), str(value)) for key, value in info_map.items())
│      if isinstance(info_map, list):
│ @@ -68,15 +79,17 @@
│          """
│          self._path = Path(path).absolute()
│          self._software_name = software_name
│  
│          if self._path.is_file():
│              raise FileExistsError("Input path already exists. Refusing to overwrite.")
│  
│ -        self._writer = p5b.create_file(str(self._path), software_name, None)
│ +        self._writer: Optional[p5b.FileWriter] = p5b.create_file(
│ +            str(self._path), software_name, None
│ +        )
│          if not self._writer:
│              raise Pod5ApiException(
│                  f"Failed to open writer at {self._path} : {p5b.get_error_string()}"
│              )
│  
│          self._end_reasons: Dict[EndReason, int] = {}
│          self._pores: Dict[PoreType, int] = {}
│ @@ -92,18 +105,18 @@
│          # Internal lookup of _add functions based on their respective type
│          self._adder_funcs: Dict[Type, Callable[[Any], int]] = {
│              EndReason: self._add_end_reason,
│              PoreType: self._add_pore_type,
│              RunInfo: self._add_run_info,
│          }
│  
│ -    def __enter__(self):
│ +    def __enter__(self) -> "Writer":
│          return self
│  
│ -    def __exit__(self, *exc_details):
│ +    def __exit__(self, *exc_details) -> None:
│          self.close()
│  
│      def close(self) -> None:
│          """Close the FileWriter handle"""
│          if self._writer:
│              self._writer.close()
│              self._writer = None
│ @@ -147,22 +160,29 @@
│          index_cache[obj] = added_index
│  
│          # Return the newly added index
│          return added_index
│  
│      def _add_end_reason(self, end_reason: EndReason) -> int:
│          """Add the given EndReason instance to the pod5 file returning its index"""
│ +        if self._writer is None:
│ +            raise Pod5ApiException("Writer handle has been closed")
│          return self._writer.add_end_reason(end_reason.reason.value)
│  
│      def _add_pore_type(self, pore_type: PoreType) -> int:
│          """Add the given PoreType instance to the pod5 file returning its index"""
│ +        if self._writer is None:
│ +            raise Pod5ApiException("Writer handle has been closed")
│          return self._writer.add_pore(pore_type)
│  
│      def _add_run_info(self, run_info: RunInfo) -> int:
│          """Add the given RunInfo instance to the pod5 file returning its index"""
│ +        if self._writer is None:
│ +            raise Pod5ApiException("Writer handle has been closed")
│ +
│          return self._writer.add_run_info(
│              run_info.acquisition_id,
│              timestamp_to_int(run_info.acquisition_start_time),
│              run_info.adc_max,
│              run_info.adc_min,
│              map_to_tuples(run_info.context_tags),
│              run_info.experiment_name,
│ @@ -245,39 +265,42 @@
│              List of Read object to be added to this POD5 file
│          """
│  
│          # Nothing to do
│          if not reads:
│              return
│  
│ +        if self._writer is None:
│ +            raise Pod5ApiException("Writer handle has been closed")
│ +
│          if isinstance(reads[0], Read):
│ -            return self._writer.add_reads(
│ +            return self._writer.add_reads(  # type: ignore [call-arg]
│                  *self._prepare_add_reads_args(reads),
│ -                [r.signal for r in reads],
│ +                [r.signal for r in reads],  # type: ignore
│              )
│          elif isinstance(reads[0], CompressedRead):
│ -            signal_chunks = [r.signal_chunks for r in reads]
│ -            signal_chunk_lengths = [r.signal_chunk_lengths for r in reads]
│ +            signal_chunks = [r.signal_chunks for r in reads]  # type: ignore
│ +            signal_chunk_lengths = [r.signal_chunk_lengths for r in reads]  # type: ignore
│  
│              # Array containing the number of chunks for each signal
│              signal_chunk_counts = np.array(
│                  [len(samples_per_chunk) for samples_per_chunk in signal_chunk_lengths],
│                  dtype=np.uint32,
│              )
│  
│ -            return self._writer.add_reads_pre_compressed(
│ +            return self._writer.add_reads_pre_compressed(  # type: ignore [call-arg]
│                  *self._prepare_add_reads_args(reads),
│                  # Join all signal data into one list
│                  list(itertools.chain(*signal_chunks)),
│                  # Join all read sample counts into one array
│ -                np.concatenate(signal_chunk_lengths).astype(np.uint32),
│ +                np.concatenate(signal_chunk_lengths).astype(np.uint32),  # type: ignore [no-untyped-call]
│                  signal_chunk_counts,
│              )
│  
│ -    def _prepare_add_reads_args(self, reads: Sequence[BaseRead]):
│ +    def _prepare_add_reads_args(self, reads: Sequence[BaseRead]) -> List[Any]:
│          """
│          Converts the List of reads into the list of ctypes arrays of data to be supplied
│          to the c api.
│          """
│          read_id = np.array(
│              [np.frombuffer(read.read_id.bytes, dtype=np.uint8) for read in reads]
│          )
├── pod5/tools/main.py
│ @@ -1,24 +1,25 @@
│  """Main entry point for pod5 tools"""
│  import argparse
│ +from typing import Any
│  
│  from pod5 import __version__
│  from pod5.tools.parsers import (
│      SubcommandHelpFormatter,
│      prepare_pod5_convert,
│      prepare_pod5_inspect_argparser,
│      prepare_pod5_merge_argparser,
│      prepare_pod5_repack_argparser,
│      prepare_pod5_subset_argparser,
│      prepare_pod5_update_argparser,
│      run_tool,
│  )
│  
│  
│ -def main():
│ +def main() -> Any:
│      """
│      The core pod5 tools function which assembles the argparser and executes the required
│      pod5 tool.
│      """
│      desc = (
│          "**********      POD5 Tools      **********\n\n"
│          "Tools for inspecting, converting, subsetting and formatting POD5 files"
├── pod5/tools/pod5_convert_to_fast5.py
│ @@ -2,15 +2,15 @@
│  Tool for converting pod5 files to the legacy fast5 format
│  """
│  import multiprocessing as mp
│  import time
│  from collections import namedtuple
│  from pathlib import Path
│  from queue import Empty
│ -from typing import List
│ +from typing import List, Sequence
│  
│  import h5py
│  import numpy
│  import vbz_h5py_plugin  # noqa: F401
│  
│  import pod5 as p5
│  from pod5.tools.parsers import pod5_convert_to_fast5_argparser, run_tool
│ @@ -30,46 +30,20 @@
│          if count > div:
│              return f"{count/div:.1f} {unit}Samples"
│  
│      return f"{count} Samples"
│  
│  
│  WriteRequest = namedtuple("WriteRequest", [])
│ -Read = namedtuple(
│ -    "Read",
│ -    [
│ -        "read_id",
│ -        "signal",
│ -        "pore_type",
│ -        "digitisation",
│ -        "offset",
│ -        "range",
│ -        "sampling_rate",
│ -        "channel_number",
│ -        "channel_mux",
│ -        "start_time",
│ -        "duration",
│ -        "read_number",
│ -        "median_before",
│ -        "end_reason",
│ -        "tracking_id",
│ -        "context_tags",
│ -        "num_minknow_events",
│ -        "tracked_scaling_scale",
│ -        "tracked_scaling_shift",
│ -        "predicted_scaling_scale",
│ -        "predicted_scaling_shift",
│ -        "num_reads_since_mux_change",
│ -        "time_since_mux_change",
│ -    ],
│ -)
│  Fast5FileData = namedtuple("Fast5FileData", ["filename", "reads"])
│  
│  
│ -def do_write_fast5_files(write_request_queue, write_data_queue, exit_queue):
│ +def do_write_fast5_files(
│ +    write_request_queue: mp.Queue, write_data_queue: mp.Queue, exit_queue: mp.Queue
│ +):
│  
│      # Pod5 does not have 'partial' so need to add that back in here.
│      fast5_end_reasons = {
│          "unknown": 0,
│          "partial": 1,  # Do not remove, required by fast5.
│          "mux_change": 2,
│          "unblock_mux_change": 3,
│ @@ -99,63 +73,72 @@
│              file.attrs.create(
│                  "file_version", "3.0".encode("ascii"), dtype=ascii_string_type
│              )
│              file.attrs.create(
│                  "file_type", "multi-read".encode("ascii"), dtype=ascii_string_type
│              )
│  
│ -            for read in file_data.reads:
│ -                tracking_id = dict(read.tracking_id)
│ +            reads: Sequence[p5.Read] = file_data.reads
│ +            for read in reads:
│ +                tracking_id = dict(read.run_info.tracking_id)
│                  read_group = file.create_group(f"read_{read.read_id}")
│                  read_group.attrs.create(
│                      "run_id",
│                      tracking_id["run_id"].encode("ascii"),
│                      dtype=ascii_string_type,
│                  )
│                  read_group.attrs.create(
│ -                    "pore_type", read.pore_type.encode("ascii"), dtype=ascii_string_type
│ +                    "pore_type",
│ +                    read.pore.pore_type.encode("ascii"),
│ +                    dtype=ascii_string_type,
│                  )
│  
│                  tracking_id_group = read_group.create_group("tracking_id")
│                  for k, v in tracking_id.items():
│                      tracking_id_group.attrs[k] = v
│  
│                  context_tags_group = read_group.create_group("context_tags")
│ -                for k, v in read.context_tags:
│ +                for k, v in read.run_info.context_tags.items():
│                      context_tags_group.attrs[k] = v
│  
│                  channel_id_group = read_group.create_group("channel_id")
│ +                digitisation = read.run_info.adc_max - read.run_info.adc_min + 1
│                  channel_id_group.attrs.create(
│ -                    "digitisation", read.digitisation, dtype=numpy.float64
│ +                    "digitisation", digitisation, dtype=numpy.float64
│                  )
│                  channel_id_group.attrs.create(
│ -                    "offset", read.offset, dtype=numpy.float64
│ +                    "offset", read.calibration.offset, dtype=numpy.float64
│ +                )
│ +
│ +                channel_id_group.attrs.create(
│ +                    "range", digitisation * read.calibration.scale, dtype=numpy.float64
│                  )
│ -                channel_id_group.attrs.create("range", read.range, dtype=numpy.float64)
│                  channel_id_group.attrs.create(
│ -                    "sampling_rate", read.sampling_rate, dtype=numpy.float64
│ +                    "sampling_rate", read.run_info.sample_rate, dtype=numpy.float64
│                  )
│ -                channel_id_group.attrs["channel_number"] = str(read.channel_number)
│ +                channel_id_group.attrs["channel_number"] = str(read.pore.channel)
│  
│                  raw_group = read_group.create_group("Raw")
│                  raw_group.create_dataset(
│                      "Signal",
│                      data=read.signal,
│                      dtype=numpy.int16,
│                      compression=32020,
│                      compression_opts=(0, 2, 1, 1),
│                  )
│                  raw_group.attrs.create(
│ -                    "start_time", read.start_time, dtype=numpy.uint64
│ +                    "start_time", read.start_sample, dtype=numpy.uint64
│ +                )
│ +                raw_group.attrs.create(
│ +                    "duration", read.sample_count, dtype=numpy.uint32
│                  )
│ -                raw_group.attrs.create("duration", read.duration, dtype=numpy.uint32)
│                  raw_group.attrs.create(
│                      "read_number", read.read_number, dtype=numpy.int32
│                  )
│ -                raw_group.attrs.create("start_mux", read.channel_mux, dtype=numpy.uint8)
│ +                raw_group.attrs.create("start_mux", read.pore.well, dtype=numpy.uint8)
│                  raw_group.attrs["read_id"] = str(read.read_id)
│                  raw_group.attrs.create(
│                      "median_before", read.median_before, dtype=numpy.float64
│                  )
│  
│                  # Lookup the fast5 enumeration values, which should include "partial: 1"
│                  # This will ensure that the enumeration is valid on a round-trip
│ @@ -167,30 +150,30 @@
│  
│                  raw_group.attrs.create(
│                      "num_minknow_events", read.num_minknow_events, dtype=numpy.uint64
│                  )
│  
│                  raw_group.attrs.create(
│                      "tracked_scaling_scale",
│ -                    read.tracked_scaling_scale,
│ +                    read.tracked_scaling.scale,
│                      dtype=numpy.float32,
│                  )
│                  raw_group.attrs.create(
│                      "tracked_scaling_shift",
│ -                    read.tracked_scaling_shift,
│ +                    read.tracked_scaling.shift,
│                      dtype=numpy.float32,
│                  )
│                  raw_group.attrs.create(
│                      "predicted_scaling_scale",
│ -                    read.predicted_scaling_scale,
│ +                    read.predicted_scaling.scale,
│                      dtype=numpy.float32,
│                  )
│                  raw_group.attrs.create(
│                      "predicted_scaling_shift",
│ -                    read.predicted_scaling_shift,
│ +                    read.predicted_scaling.shift,
│                      dtype=numpy.float32,
│                  )
│                  raw_group.attrs.create(
│                      "num_reads_since_mux_change",
│                      read.num_reads_since_mux_change,
│                      dtype=numpy.uint32,
│                  )
│ @@ -208,44 +191,14 @@
│      file_reads: Fast5FileData, write_request_queue, write_data_queue
│  ):
│  
│      write_request_queue.get()
│      write_data_queue.put(file_reads)
│  
│  
│ -def extract_read(read_table_version: p5.reader.ReadTableVersion, read: p5.ReadRecord):
│ -    run_info = read.run_info
│ -
│ -    return Read(
│ -        read.read_id,
│ -        read.signal,
│ -        read.pore.pore_type,
│ -        read.calibration_digitisation,
│ -        read.calibration.offset,
│ -        read.calibration_range,
│ -        run_info.sample_rate,
│ -        read.pore.channel,
│ -        read.pore.well,
│ -        read.start_sample,
│ -        read.sample_count,
│ -        read.read_number,
│ -        read.median_before,
│ -        read.end_reason,
│ -        run_info.tracking_id,
│ -        run_info.context_tags,
│ -        read.num_minknow_events,
│ -        read.tracked_scaling.scale,
│ -        read.tracked_scaling.shift,
│ -        read.predicted_scaling.scale,
│ -        read.predicted_scaling.shift,
│ -        read.num_reads_since_mux_change,
│ -        read.time_since_mux_change,
│ -    )
│ -
│ -
│  def make_fast5_filename(output_location, file_index):
│      output_location.mkdir(parents=True, exist_ok=True)
│      return output_location / f"output_{file_index}.fast5"
│  
│  
│  def convert_to_fast5(
│      inputs: List[Path],
│ @@ -313,18 +266,17 @@
│                  print(
│                      f"{file_count} fast5s\t"
│                      f"{read_count} reads\t"
│                      f"{format_sample_count(sample_count)}\t"
│                      f"{mb_total/time_total:.1f} MB/s"
│                  )
│  
│ -            extracted_read = extract_read(reader.reads_table_version, read)
│ -            current_reads_batch.append(extracted_read)
│ +            current_reads_batch.append(read.to_read())
│              read_count += 1
│ -            sample_count += len(extracted_read.signal)
│ +            sample_count += read.num_samples
│  
│              # Write a batch of reads to a fast5 file
│              if len(current_reads_batch) >= file_read_count:
│                  put_write_fast5_file(
│                      Fast5FileData(
│                          make_fast5_filename(output, file_count),
│                          current_reads_batch,
├── pod5/tools/pod5_subset.py
│ @@ -180,15 +180,15 @@
│          json_data = json_load(_fh)
│          jsonschema.validate(instance=json_data, schema=JSON_SCHEMA)
│  
│      return json_data
│  
│  
│  def get_total_selection(
│ -    mapping: typing.Dict[str, typing.Iterable[str]], duplicate_ok: bool
│ +    mapping: typing.Mapping[str, typing.Iterable[str]], duplicate_ok: bool
│  ) -> typing.Set[str]:
│      """
│      Create a set of all read_ids from the mapping checking for duplicates if necessary
│      """
│      # Create a set of the required output read_ids
│      total_selection: typing.Set[str] = set()
│      for requested_read_ids in mapping.values():
│ @@ -199,31 +199,35 @@
│  
│      if not duplicate_ok:
│          assert_no_duplicate_reads(mapping=mapping)
│  
│      return total_selection
│  
│  
│ -def assert_no_duplicate_reads(mapping: typing.Dict[str, typing.Iterable[str]]) -> None:
│ +def assert_no_duplicate_reads(
│ +    mapping: typing.Mapping[str, typing.Iterable[str]]
│ +) -> None:
│      """
│      Raise AssertionError if we detect any duplicate read_ids in the outputs
│      """
│      # Count the total number of outputs to detect if duplicates were requested
│      counter: typing.Counter[str] = Counter()
│      for ids in mapping.values():
│          counter.update(ids)
│  
│      if any(count > 1 for count in counter.values()):
│          raise AssertionError("Duplicate outputs detected but --duplicate_ok not set")
│  
│ +    return None
│ +
│  
│  def prepare_repacker_outputs(
│      repacker: p5_repack.Repacker,
│      output: Path,
│ -    mapping: typing.Dict[str, typing.Iterable[str]],
│ +    mapping: typing.Mapping[str, typing.Iterable[str]],
│  ) -> OutputMap:
│      """
│      Create a dictionary of the output filepath and their and associated
│      FileWriter and Pod5RepackerOutput objects.
│      """
│      outputs: OutputMap = {}
│      for target in mapping:
│ @@ -282,15 +286,15 @@
│      transfers: TransferMap = defaultdict(set)
│  
│      for input_path in inputs:
│          # Open a FileReader from input_path
│          p5_reader = p5.Reader(input_path)
│  
│          # Iterate over batches of read_ids
│ -        for batch in p5_reader.read_batches(selection=selection, missing_ok=True):
│ +        for batch in p5_reader.read_batches(selection=list(selection), missing_ok=True):
│              for read_id in p5.format_read_ids(batch.read_id_column):
│  
│                  # Get the repacker output destination
│                  _, repacker_output = outputs[read_targets[read_id]]
│  
│                  # Add this read_id to the target_mapping
│                  transfers[(p5_reader, repacker_output)].add(str(read_id))
│ @@ -324,15 +328,15 @@
│  
│      print("Done")
│  
│  
│  def subset_pod5s_with_mapping(
│      inputs: typing.Iterable[Path],
│      output: Path,
│ -    mapping: typing.Dict[str, typing.Iterable[str]],
│ +    mapping: typing.Mapping[str, typing.Iterable[str]],
│      missing_ok: bool = False,
│      duplicate_ok: bool = False,
│      force_overwrite: bool = False,
│  ) -> typing.List[Path]:
│      """
│      Given an iterable of input pod5 paths and an output directory, create output pod5
│      files containing the read_ids specified in the given mapping of output filename to
│   --- pod5-0.1.4.dist-info/METADATA
├── +++ pod5-0.1.5.dist-info/METADATA
│┄ Files 2% similar despite different names
│ @@ -1,38 +1,42 @@
│  Metadata-Version: 2.1
│  Name: pod5
│ -Version: 0.1.4
│ +Version: 0.1.5
│  Summary: Oxford Nanopore Technologies Pod5 File Format Python API and Tools
│  Author-email: support@nanoporetech.com
│  Keywords: nanopore
│  Classifier: Environment :: Console
│  Classifier: Intended Audience :: Developers
│  Classifier: Intended Audience :: Science/Research
│  Classifier: License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)
│  Classifier: Natural Language :: English
│  Classifier: Programming Language :: Python :: 3
│  Classifier: Topic :: Scientific/Engineering :: Bio-Informatics
│  Requires-Python: ~=3.7
│  Description-Content-Type: text/markdown
│  Requires-Dist: iso8601
│  Requires-Dist: jsonschema
│ -Requires-Dist: lib-pod5 (~=0.1.4)
│ +Requires-Dist: lib-pod5 (~=0.1.5)
│  Requires-Dist: more-itertools
│  Requires-Dist: numpy (>=1.20.0)
│  Requires-Dist: pyarrow (~=8.0.0)
│  Requires-Dist: pytz
│  Requires-Dist: packaging
│  Requires-Dist: pandas
│  Requires-Dist: vbz-h5py-plugin
│  Provides-Extra: dev
│  Requires-Dist: black (==22.3.0) ; extra == 'dev'
│ -Requires-Dist: mypy ; extra == 'dev'
│ +Requires-Dist: mypy (==0.991) ; extra == 'dev'
│  Requires-Dist: pre-commit ; extra == 'dev'
│  Requires-Dist: pytest (~=6.2) ; extra == 'dev'
│  Requires-Dist: pytest-cov (~=3.0) ; extra == 'dev'
│ +Requires-Dist: types-jsonschema ; extra == 'dev'
│ +Requires-Dist: types-setuptools ; extra == 'dev'
│ +Requires-Dist: types-pytz ; extra == 'dev'
│ +Requires-Dist: pandas-stubs ; extra == 'dev'
│  
│  POD5 Python Package
│  ===================
│  
│  The `pod5` Python package contains the tools and python API wrapping the compiled bindings
│  for the POD5 file format from `lib_pod5`.
│   --- pod5-0.1.4.dist-info/RECORD
├── +++ pod5-0.1.5.dist-info/RECORD
│┄ Files 18% similar despite different names
│ @@ -1,23 +1,23 @@
│  pod5/__init__.py,sha256=oV8b_pKy0lZdDL7Yej7cnw3wylDPr8hkvWQaIqfh_Co,751
│  pod5/api_utils.py,sha256=xsD_0UfD2xTYShZ-aZtn5mrKzOQtrL4X_7ek9x6H-Ok,2168
│ -pod5/pod5_types.py,sha256=lF3PPtyJjA59h6qx_T3EPmGGKqMOUUfufwxWx7_cdEM,13111
│ -pod5/reader.py,sha256=b1Azr7tHBFZRPlW3hqNarE5cZ5_uRiG71eM0v3r8Ynw,33936
│ -pod5/repack.py,sha256=UydvgfecX69N_be4J31y3hRpezXdbfK_Riv0FOGwKgg,6422
│ -pod5/signal_tools.py,sha256=bzmOW8cdRkyTQZgz3zX4gkzMZcPqTAm8Z0hky4ljsQ4,4711
│ -pod5/writer.py,sha256=9qLom1CuoEqWJkuPfxv3TYHkLi71qgjCEEmk18o4zPU,12239
│ +pod5/pod5_types.py,sha256=KRcEpRyfeTLfmXfxYAohn-TUFklMALP_3f8S_yAYigE,13142
│ +pod5/reader.py,sha256=jyEON1sORCDHI90OArnfZST50FESIO_LjgcRAdFyjQo,34648
│ +pod5/repack.py,sha256=igvfRS0OuS7GriAc2T2O1oYD7_iNq5dTc1G7YvckSsc,6469
│ +pod5/signal_tools.py,sha256=abhJbH6wKjBwG2edXeXEIk6ULCBshH8b-J5fk_UIUaE,4827
│ +pod5/writer.py,sha256=3Fsdk-TH1xMOJ91Sl7Gi0Vd5GH2F9-ZBEZ5eA9gJukI,12922
│  pod5/tools/__init__.py,sha256=1s_72oZeqBAn3tANjY_auA_SrtP3c4Ye3kft9t1-H90,24
│ -pod5/tools/main.py,sha256=3EfPi9NkOFMS2zjt1hvGP68y7TLdlCID10xygRpVxJo,1537
│ +pod5/tools/main.py,sha256=d4EZ1YBZo2XaRzFERLnVm6Zr5UI-_6moV2JD2q_MYl4,1567
│  pod5/tools/parsers.py,sha256=HXzR3UpkFT_Z4EXlL-m1nESDCeQ68eNvxRHEmP4YsMg,14129
│  pod5/tools/pod5_convert_from_fast5.py,sha256=x7mc9qQtUNM40hbLs9cAoQy2m5D1ocGd0qv_b94lXnM,21306
│ -pod5/tools/pod5_convert_to_fast5.py,sha256=F19HW3SttsgW3uwReLJG0dRIMxekbAUziHgIvv7-6y4,11662
│ +pod5/tools/pod5_convert_to_fast5.py,sha256=_Ltwk9nO7CsT-1JRKpDdmy9mR-MFYaHBA5t3odHXLxM,10456
│  pod5/tools/pod5_inspect.py,sha256=0vk8Sbd4C1wWvuYm1hkuh2LUpx7Dw-3SKbeqJU7OAec,6533
│  pod5/tools/pod5_merge.py,sha256=lkvABHit_BTLdZYKW_xJsE5kOz0shS5fuY_aZ3mePiY,2447
│  pod5/tools/pod5_repack.py,sha256=s4bcNKmr61z8H00HcdAMFKZ93L9yFYknZq9KdggM_Xo,1740
│ -pod5/tools/pod5_subset.py,sha256=Nv6_9a2gpYjU-slfD3KrVv8ZyHvZ0HlX-bAOykqjJwg,13727
│ +pod5/tools/pod5_subset.py,sha256=PSWHLy1WdgV7yddpytupkRB6PhUWQjheHv07hfHW1C4,13768
│  pod5/tools/pod5_update.py,sha256=31WVi1btmZcW6Uzn_SmzvM3wK5ZSbVCe9cUgEhWKD5s,1009
│  pod5/tools/utils.py,sha256=3EJllNHqxv4EVN7RWIiU6Umn0ez5MRdgmMA6xqSir1c,1004
│ -pod5-0.1.4.dist-info/METADATA,sha256=LhWLSh3PU7wK-aHh_x7tmqFHBKwdBStAxwMjBUCpGyk,14729
│ -pod5-0.1.4.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
│ -pod5-0.1.4.dist-info/entry_points.txt,sha256=CuZYKwUC2PuRgzHz7RI7JuZdWsTGvniyB_qo9vtig1g,46
│ -pod5-0.1.4.dist-info/top_level.txt,sha256=G6L1QAUTcDXbZIHuDIWHt4-rGWZdgw3eB0hM3a495Ec,5
│ -pod5-0.1.4.dist-info/RECORD,,
│ +pod5-0.1.5.dist-info/METADATA,sha256=ZB62FIpDPRVQuWpkghqEkdq48XXTGp-C-isTzBtOmZk,14925
│ +pod5-0.1.5.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
│ +pod5-0.1.5.dist-info/entry_points.txt,sha256=CuZYKwUC2PuRgzHz7RI7JuZdWsTGvniyB_qo9vtig1g,46
│ +pod5-0.1.5.dist-info/top_level.txt,sha256=G6L1QAUTcDXbZIHuDIWHt4-rGWZdgw3eB0hM3a495Ec,5
│ +pod5-0.1.5.dist-info/RECORD,,
