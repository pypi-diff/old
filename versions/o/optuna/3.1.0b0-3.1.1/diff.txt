--- tmp/optuna-3.1.0b0.tar.gz
+++ tmp/optuna-3.1.1.tar.gz
├── filetype from file(1)
│ @@ -1 +1 @@
│ -gzip compressed data, was "optuna-3.1.0b0.tar", last modified: Thu Dec 22 07:11:17 2022, max compression
│ +gzip compressed data, was "optuna-3.1.1.tar", last modified: Fri Apr  7 07:55:15 2023, max compression
│   --- optuna-3.1.0b0.tar
├── +++ optuna-3.1.1.tar
│ ├── file list
│ │ @@ -1,229 +1,231 @@
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.272595 optuna-3.1.0b0/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1081 2022-12-22 07:11:04.000000 optuna-3.1.0b0/LICENSE
│ │ --rw-r--r--   0 runner    (1001) docker     (116)       16 2022-12-22 07:11:04.000000 optuna-3.1.0b0/MANIFEST.in
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    11379 2022-12-22 07:11:17.272595 optuna-3.1.0b0/PKG-INFO
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     8524 2022-12-22 07:11:04.000000 optuna-3.1.0b0/README.md
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.240594 optuna-3.1.0b0/benchmarks/
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.244594 optuna-3.1.0b0/benchmarks/asv/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:04.000000 optuna-3.1.0b0/benchmarks/asv/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2027 2022-12-22 07:11:04.000000 optuna-3.1.0b0/benchmarks/asv/optimize.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.244594 optuna-3.1.0b0/benchmarks/kurobako/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:04.000000 optuna-3.1.0b0/benchmarks/kurobako/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      969 2022-12-22 07:11:04.000000 optuna-3.1.0b0/benchmarks/kurobako/mo_create_study.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.244594 optuna-3.1.0b0/benchmarks/naslib/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:04.000000 optuna-3.1.0b0/benchmarks/naslib/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2758 2022-12-22 07:11:04.000000 optuna-3.1.0b0/benchmarks/naslib/problem.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.248594 optuna-3.1.0b0/optuna/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1264 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     6350 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/_callbacks.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2395 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/_convert_positional_args.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     6949 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/_deprecated.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3832 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/_experimental.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.248594 optuna-3.1.0b0/optuna/_hypervolume/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      373 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/_hypervolume/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2871 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/_hypervolume/base.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1683 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/_hypervolume/hssp.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1460 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/_hypervolume/utils.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4130 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/_hypervolume/wfg.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4069 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/_imports.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    11225 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/_transform.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    36404 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/cli.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    27969 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/distributions.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2442 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/exceptions.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.248594 optuna-3.1.0b0/optuna/importance/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4685 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/importance/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     6725 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/importance/_base.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.248594 optuna-3.1.0b0/optuna/importance/_fanova/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      117 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/importance/_fanova/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5265 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/importance/_fanova/_evaluator.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4207 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/importance/_fanova/_fanova.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    12189 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/importance/_fanova/_tree.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3888 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/importance/_mean_decrease_impurity.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.252594 optuna-3.1.0b0/optuna/integration/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5651 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/__init__.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.252594 optuna-3.1.0b0/optuna/integration/_lightgbm_tuner/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1698 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/_lightgbm_tuner/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3499 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/_lightgbm_tuner/alias.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    43814 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/_lightgbm_tuner/optimize.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1239 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/_lightgbm_tuner/sklearn.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.256594 optuna-3.1.0b0/optuna/integration/allennlp/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      294 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/allennlp/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2016 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/allennlp/_dump_best_config.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      367 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/allennlp/_environment.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     9781 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/allennlp/_executor.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     8052 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/allennlp/_pruner.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2706 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/allennlp/_variables.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    24515 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/botorch.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      940 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/catalyst.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4542 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/catboost.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4116 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/chainer.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    10506 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/chainermn.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    20288 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/cma.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    24604 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/dask.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2667 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/fastaiv1.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2646 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/fastaiv2.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2799 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/keras.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     6076 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/lightgbm.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    11856 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/mlflow.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2103 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/mxnet.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    10756 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/pytorch_distributed.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1411 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/pytorch_ignite.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5281 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/pytorch_lightning.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4282 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/shap.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    31328 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/sklearn.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    14132 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/skopt.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1397 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/skorch.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4030 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/tensorboard.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3036 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/tensorflow.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1885 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/tfkeras.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     8355 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/wandb.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4820 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/integration/xgboost.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    10009 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/logging.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.256594 optuna-3.1.0b0/optuna/multi_objective/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      437 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/multi_objective/__init__.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.256594 optuna-3.1.0b0/optuna/multi_objective/samplers/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      586 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/multi_objective/samplers/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1989 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/multi_objective/samplers/_adapter.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4462 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/multi_objective/samplers/_base.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     7401 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/multi_objective/samplers/_motpe.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    13788 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/multi_objective/samplers/_nsga2.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2734 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/multi_objective/samplers/_random.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    17554 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/multi_objective/study.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    12964 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/multi_objective/trial.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.256594 optuna-3.1.0b0/optuna/multi_objective/visualization/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      177 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/multi_objective/visualization/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     7361 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/multi_objective/visualization/_pareto_front.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3873 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/progress_bar.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.256594 optuna-3.1.0b0/optuna/pruners/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1107 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/pruners/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      910 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/pruners/_base.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    13907 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/pruners/_hyperband.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2958 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/pruners/_median.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1505 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/pruners/_nop.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4167 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/pruners/_patient.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     7187 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/pruners/_percentile.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    10373 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/pruners/_successive_halving.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4504 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/pruners/_threshold.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/py.typed
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.260594 optuna-3.1.0b0/optuna/samplers/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1007 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     8196 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_base.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4527 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_brute_force.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    26670 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_cmaes.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    10853 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_grid.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3704 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_partial_fixed.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    12881 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_qmc.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1893 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_random.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.260594 optuna-3.1.0b0/optuna/samplers/_search_space/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      464 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_search_space/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2320 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_search_space/group_decomposed.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5252 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_search_space/intersection.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.260594 optuna-3.1.0b0/optuna/samplers/_tpe/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_tpe/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4553 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_tpe/multi_objective_sampler.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    19532 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_tpe/parzen_estimator.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    34414 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/_tpe/sampler.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.260594 optuna-3.1.0b0/optuna/samplers/nsgaii/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      647 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/nsgaii/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5907 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/nsgaii/_crossover.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.260594 optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1972 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_base.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1650 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_blxalpha.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3901 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_sbx.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2130 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_spx.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4088 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_undx.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1522 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_uniform.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3250 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_vsbx.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    22130 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/samplers/nsgaii/_sampler.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.264594 optuna-3.1.0b0/optuna/storages/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1628 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    19746 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_base.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    18057 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_cached_storage.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5853 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_heartbeat.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    14900 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_in_memory.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.264594 optuna-3.1.0b0/optuna/storages/_journal/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_journal/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2128 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_journal/base.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     6567 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_journal/file.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3813 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_journal/redis.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    26350 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_journal/storage.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.264594 optuna-3.1.0b0/optuna/storages/_rdb/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/__init__.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.264594 optuna-3.1.0b0/optuna/storages/_rdb/alembic/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2161 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/alembic/env.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      494 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/alembic/script.py.mako
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.264594 optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5455 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v0.9.0.a.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      963 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v1.2.0.a.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2665 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v1.3.0.a.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     6243 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v2.4.0.a.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1722 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v2.6.0.a_.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     6043 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v3.0.0.a.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2792 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v3.0.0.b.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     6260 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v3.0.0.c.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5692 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v3.0.0.d.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1697 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/alembic.ini
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    19202 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/models.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    49062 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/storages/_rdb/storage.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.268594 optuna-3.1.0b0/optuna/study/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      625 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/study/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3791 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/study/_dataframe.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2826 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/study/_frozen.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3398 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/study/_multi_objective.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     8949 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/study/_optimize.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      421 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/study/_study_direction.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4212 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/study/_study_summary.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     6610 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/study/_tell.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    51592 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/study/study.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.268594 optuna-3.1.0b0/optuna/testing/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/testing/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      472 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/testing/distributions.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      197 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/testing/objectives.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      284 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/testing/pruners.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1996 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/testing/samplers.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3300 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/testing/storages.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      645 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/testing/threading.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2468 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/testing/visualization.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.268594 optuna-3.1.0b0/optuna/trial/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      377 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/trial/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     2766 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/trial/_base.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5395 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/trial/_fixed.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    19087 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/trial/_frozen.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1029 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/trial/_state.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    28080 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/trial/_trial.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)       24 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/version.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.272595 optuna-3.1.0b0/optuna/visualization/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      889 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    13632 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/_contour.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5873 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/_edf.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3383 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/_intermediate_values.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     8112 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/_optimization_history.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    10954 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/_parallel_coordinate.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     6290 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/_param_importances.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    17380 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/_pareto_front.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      851 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/_plotly_imports.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     7109 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/_slice.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5476 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/_utils.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.272595 optuna-3.1.0b0/optuna/visualization/matplotlib/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      926 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/matplotlib/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    13112 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/matplotlib/_contour.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4076 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/matplotlib/_edf.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     3190 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/matplotlib/_intermediate_values.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1253 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/matplotlib/_matplotlib_imports.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4823 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/matplotlib/_optimization_history.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5326 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/matplotlib/_parallel_coordinate.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     4428 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/matplotlib/_param_importances.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     8613 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/matplotlib/_pareto_front.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5499 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/matplotlib/_slice.py
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1827 2022-12-22 07:11:04.000000 optuna-3.1.0b0/optuna/visualization/matplotlib/_utils.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (116)        0 2022-12-22 07:11:17.248594 optuna-3.1.0b0/optuna.egg-info/
│ │ --rw-r--r--   0 runner    (1001) docker     (116)    11379 2022-12-22 07:11:17.000000 optuna-3.1.0b0/optuna.egg-info/PKG-INFO
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     6599 2022-12-22 07:11:17.000000 optuna-3.1.0b0/optuna.egg-info/SOURCES.txt
│ │ --rw-r--r--   0 runner    (1001) docker     (116)        1 2022-12-22 07:11:17.000000 optuna-3.1.0b0/optuna.egg-info/dependency_links.txt
│ │ --rw-r--r--   0 runner    (1001) docker     (116)       44 2022-12-22 07:11:17.000000 optuna-3.1.0b0/optuna.egg-info/entry_points.txt
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     1028 2022-12-22 07:11:17.000000 optuna-3.1.0b0/optuna.egg-info/requires.txt
│ │ --rw-r--r--   0 runner    (1001) docker     (116)       18 2022-12-22 07:11:17.000000 optuna-3.1.0b0/optuna.egg-info/top_level.txt
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      814 2022-12-22 07:11:04.000000 optuna-3.1.0b0/pyproject.toml
│ │ --rw-r--r--   0 runner    (1001) docker     (116)      531 2022-12-22 07:11:17.272595 optuna-3.1.0b0/setup.cfg
│ │ --rw-r--r--   0 runner    (1001) docker     (116)     5421 2022-12-22 07:11:04.000000 optuna-3.1.0b0/setup.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.304415 optuna-3.1.1/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3016 2023-01-13 07:07:35.000000 optuna-3.1.1/LICENSE
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)       16 2022-11-30 02:55:49.000000 optuna-3.1.1/MANIFEST.in
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    11331 2023-04-07 07:55:15.304534 optuna-3.1.1/PKG-INFO
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     8478 2023-04-07 07:49:53.000000 optuna-3.1.1/README.md
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.249302 optuna-3.1.1/benchmarks/
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.256885 optuna-3.1.1/benchmarks/asv/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)        0 2022-11-30 02:55:49.000000 optuna-3.1.1/benchmarks/asv/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2027 2022-12-23 05:37:15.000000 optuna-3.1.1/benchmarks/asv/optimize.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.257482 optuna-3.1.1/benchmarks/kurobako/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)        0 2022-11-30 02:55:49.000000 optuna-3.1.1/benchmarks/kurobako/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      969 2023-04-07 07:49:53.000000 optuna-3.1.1/benchmarks/kurobako/mo_create_study.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.257963 optuna-3.1.1/benchmarks/naslib/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)        0 2022-11-30 02:55:49.000000 optuna-3.1.1/benchmarks/naslib/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2758 2023-04-07 07:49:53.000000 optuna-3.1.1/benchmarks/naslib/problem.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.262105 optuna-3.1.1/optuna/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1264 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6350 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/_callbacks.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2395 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/_convert_positional_args.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6949 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/_deprecated.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3832 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/_experimental.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.264644 optuna-3.1.1/optuna/_hypervolume/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      373 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/_hypervolume/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2871 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/_hypervolume/base.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1683 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/_hypervolume/hssp.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1460 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/_hypervolume/utils.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4130 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/_hypervolume/wfg.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4069 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/_imports.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    12143 2023-04-05 02:26:58.000000 optuna-3.1.1/optuna/_transform.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    36895 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/cli.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    27969 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/distributions.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2442 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/exceptions.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.265366 optuna-3.1.1/optuna/importance/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4685 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/importance/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6725 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/importance/_base.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.266210 optuna-3.1.1/optuna/importance/_fanova/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      117 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/importance/_fanova/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5265 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/importance/_fanova/_evaluator.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4207 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/importance/_fanova/_fanova.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    12189 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/importance/_fanova/_tree.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3888 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/importance/_mean_decrease_impurity.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.271997 optuna-3.1.1/optuna/integration/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5651 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/__init__.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.273856 optuna-3.1.1/optuna/integration/_lightgbm_tuner/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1698 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/integration/_lightgbm_tuner/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3499 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/integration/_lightgbm_tuner/alias.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    43841 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/_lightgbm_tuner/optimize.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1239 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/_lightgbm_tuner/sklearn.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.275143 optuna-3.1.1/optuna/integration/allennlp/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      294 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/allennlp/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2016 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/allennlp/_dump_best_config.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      367 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/allennlp/_environment.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     9781 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/allennlp/_executor.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     8052 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/allennlp/_pruner.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2706 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/allennlp/_variables.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    24941 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/botorch.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      940 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/integration/catalyst.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4542 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/integration/catboost.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4284 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/chainer.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    11287 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/chainermn.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    20465 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/cma.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    26584 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/dask.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2667 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/fastaiv1.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2646 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/integration/fastaiv2.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2799 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/keras.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6076 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/lightgbm.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    11837 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/mlflow.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2103 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/mxnet.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    12567 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/pytorch_distributed.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1411 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/pytorch_ignite.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5361 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/pytorch_lightning.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4282 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/shap.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    31269 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/sklearn.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    14132 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/skopt.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1397 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/skorch.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4030 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/tensorboard.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3036 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/tensorflow.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1885 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/tfkeras.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     8336 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/wandb.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4822 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/integration/xgboost.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    10009 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/logging.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.276284 optuna-3.1.1/optuna/multi_objective/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      437 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/multi_objective/__init__.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.277700 optuna-3.1.1/optuna/multi_objective/samplers/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      586 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/multi_objective/samplers/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1989 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/multi_objective/samplers/_adapter.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4462 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/multi_objective/samplers/_base.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     7401 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/multi_objective/samplers/_motpe.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    13788 2023-01-04 00:56:42.000000 optuna-3.1.1/optuna/multi_objective/samplers/_nsga2.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2734 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/multi_objective/samplers/_random.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    17554 2023-03-08 09:46:04.000000 optuna-3.1.1/optuna/multi_objective/study.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    13674 2023-03-08 09:46:04.000000 optuna-3.1.1/optuna/multi_objective/trial.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.278136 optuna-3.1.1/optuna/multi_objective/visualization/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      177 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/multi_objective/visualization/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     7361 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/multi_objective/visualization/_pareto_front.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4237 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/progress_bar.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.280209 optuna-3.1.1/optuna/pruners/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1107 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/pruners/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      910 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/pruners/_base.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    13907 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/pruners/_hyperband.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2958 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/pruners/_median.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1505 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/pruners/_nop.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4167 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/pruners/_patient.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     7187 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/pruners/_percentile.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    10373 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/pruners/_successive_halving.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4504 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/pruners/_threshold.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)        0 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/py.typed
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.282481 optuna-3.1.1/optuna/samplers/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1007 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/samplers/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     8195 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_base.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4581 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_brute_force.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    26529 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_cmaes.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    10853 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_grid.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3704 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_partial_fixed.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    12886 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_qmc.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1893 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_random.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.283589 optuna-3.1.1/optuna/samplers/_search_space/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      464 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_search_space/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2320 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_search_space/group_decomposed.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5252 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_search_space/intersection.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.284753 optuna-3.1.1/optuna/samplers/_tpe/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)        0 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/samplers/_tpe/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6036 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_tpe/_erf.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     7552 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_tpe/_truncnorm.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4553 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_tpe/multi_objective_sampler.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    18802 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_tpe/parzen_estimator.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    34500 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/_tpe/sampler.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.285389 optuna-3.1.1/optuna/samplers/nsgaii/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      647 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/samplers/nsgaii/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5907 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/nsgaii/_crossover.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.286920 optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)        0 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1972 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_base.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1650 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_blxalpha.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3901 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_sbx.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2130 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_spx.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4088 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_undx.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1522 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_uniform.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3250 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_vsbx.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    22130 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/samplers/nsgaii/_sampler.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.288227 optuna-3.1.1/optuna/storages/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1628 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/storages/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    19165 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/storages/_base.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    17417 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/storages/_cached_storage.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5853 2023-01-04 00:56:42.000000 optuna-3.1.1/optuna/storages/_heartbeat.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    14394 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/storages/_in_memory.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.289546 optuna-3.1.1/optuna/storages/_journal/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)        0 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/storages/_journal/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2128 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/storages/_journal/base.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6567 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/storages/_journal/file.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3813 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/storages/_journal/redis.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    25816 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/storages/_journal/storage.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.290572 optuna-3.1.1/optuna/storages/_rdb/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)        0 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/storages/_rdb/__init__.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.291185 optuna-3.1.1/optuna/storages/_rdb/alembic/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2161 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/storages/_rdb/alembic/env.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      494 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/storages/_rdb/alembic/script.py.mako
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.293390 optuna-3.1.1/optuna/storages/_rdb/alembic/versions/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5455 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v0.9.0.a.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      963 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v1.2.0.a.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2828 2023-01-13 07:07:35.000000 optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v1.3.0.a.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6406 2023-01-13 07:07:35.000000 optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v2.4.0.a.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1722 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v2.6.0.a_.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6206 2023-01-13 07:07:35.000000 optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v3.0.0.a.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2955 2023-01-13 07:07:35.000000 optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v3.0.0.b.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6423 2023-01-13 07:07:35.000000 optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v3.0.0.c.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5855 2023-01-13 07:07:35.000000 optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v3.0.0.d.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1697 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/storages/_rdb/alembic.ini
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    19491 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/storages/_rdb/models.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    47949 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/storages/_rdb/storage.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.295525 optuna-3.1.1/optuna/study/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      625 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/study/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3791 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/study/_dataframe.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2826 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/study/_frozen.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3398 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/study/_multi_objective.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     9008 2023-01-30 01:49:04.000000 optuna-3.1.1/optuna/study/_optimize.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      421 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/study/_study_direction.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4212 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/study/_study_summary.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6610 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/study/_tell.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    51543 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/study/study.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.297389 optuna-3.1.1/optuna/testing/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)        0 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/testing/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      472 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/testing/distributions.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      197 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/testing/objectives.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      284 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/testing/pruners.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1996 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/testing/samplers.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3399 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/testing/storages.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      645 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/testing/threading.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     2468 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/testing/visualization.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.298592 optuna-3.1.1/optuna/trial/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      377 2022-11-30 02:55:49.000000 optuna-3.1.1/optuna/trial/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3620 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/trial/_base.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6105 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/trial/_fixed.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    19797 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/trial/_frozen.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1029 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/trial/_state.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    28829 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/trial/_trial.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)       22 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/version.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.301752 optuna-3.1.1/optuna/visualization/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      889 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    13632 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/_contour.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5873 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/_edf.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3383 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/_intermediate_values.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     8112 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/_optimization_history.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    10954 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/_parallel_coordinate.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6290 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/_param_importances.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    17377 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/_pareto_front.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      851 2022-12-23 05:37:15.000000 optuna-3.1.1/optuna/visualization/_plotly_imports.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     7109 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/_slice.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5476 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/_utils.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.304225 optuna-3.1.1/optuna/visualization/matplotlib/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      926 2023-04-07 07:49:39.000000 optuna-3.1.1/optuna/visualization/matplotlib/__init__.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    13194 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/matplotlib/_contour.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4076 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/matplotlib/_edf.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     3190 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/matplotlib/_intermediate_values.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1253 2023-04-07 07:49:39.000000 optuna-3.1.1/optuna/visualization/matplotlib/_matplotlib_imports.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4823 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/matplotlib/_optimization_history.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5326 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/matplotlib/_parallel_coordinate.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     4428 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/matplotlib/_param_importances.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     8610 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/matplotlib/_pareto_front.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5499 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/matplotlib/_slice.py
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1827 2023-04-07 07:49:53.000000 optuna-3.1.1/optuna/visualization/matplotlib/_utils.py
│ │ +drwxr-xr-x   0 mamu       (501) staff       (20)        0 2023-04-07 07:55:15.263112 optuna-3.1.1/optuna.egg-info/
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)    11331 2023-04-07 07:55:15.000000 optuna-3.1.1/optuna.egg-info/PKG-INFO
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     6663 2023-04-07 07:55:15.000000 optuna-3.1.1/optuna.egg-info/SOURCES.txt
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)        1 2023-04-07 07:55:15.000000 optuna-3.1.1/optuna.egg-info/dependency_links.txt
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)       44 2023-04-07 07:55:15.000000 optuna-3.1.1/optuna.egg-info/entry_points.txt
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     1101 2023-04-07 07:55:15.000000 optuna-3.1.1/optuna.egg-info/requires.txt
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)        7 2023-04-07 07:55:15.000000 optuna-3.1.1/optuna.egg-info/top_level.txt
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      814 2023-04-07 07:49:53.000000 optuna-3.1.1/pyproject.toml
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)      531 2023-04-07 07:55:15.304894 optuna-3.1.1/setup.cfg
│ │ +-rw-r--r--   0 mamu       (501) staff       (20)     5994 2023-04-07 07:49:53.000000 optuna-3.1.1/setup.py
│ │   --- optuna-3.1.0b0/PKG-INFO
│ ├── +++ optuna-3.1.1/PKG-INFO
│ │┄ Files 4% similar despite different names
│ │ @@ -1,10 +1,10 @@
│ │  Metadata-Version: 2.1
│ │  Name: optuna
│ │ -Version: 3.1.0b0
│ │ +Version: 3.1.1
│ │  Summary: A hyperparameter optimization framework
│ │  Home-page: https://optuna.org/
│ │  Author: Takuya Akiba
│ │  Author-email: akiba@preferred.jp
│ │  License: UNKNOWN
│ │  Project-URL: Source, https://github.com/optuna/optuna
│ │  Project-URL: Documentation, https://optuna.readthedocs.io
│ │ @@ -13,22 +13,22 @@
│ │          
│ │          # Optuna: A hyperparameter optimization framework
│ │          
│ │          [![Python](https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9%20%7C%203.10%20%7C%203.11-blue)](https://www.python.org)
│ │          [![pypi](https://img.shields.io/pypi/v/optuna.svg)](https://pypi.python.org/pypi/optuna)
│ │          [![conda](https://img.shields.io/conda/vn/conda-forge/optuna.svg)](https://anaconda.org/conda-forge/optuna)
│ │          [![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/optuna/optuna)
│ │ -        [![CircleCI](https://circleci.com/gh/optuna/optuna.svg?style=svg)](https://circleci.com/gh/optuna/optuna)
│ │          [![Read the Docs](https://readthedocs.org/projects/optuna/badge/?version=stable)](https://optuna.readthedocs.io/en/stable/)
│ │          [![Codecov](https://codecov.io/gh/optuna/optuna/branch/master/graph/badge.svg)](https://codecov.io/gh/optuna/optuna/branch/master)
│ │          
│ │          [**Website**](https://optuna.org/)
│ │          | [**Docs**](https://optuna.readthedocs.io/en/stable/)
│ │          | [**Install Guide**](https://optuna.readthedocs.io/en/stable/installation.html)
│ │          | [**Tutorial**](https://optuna.readthedocs.io/en/stable/tutorial/index.html)
│ │ +        | [**Examples**](https://github.com/optuna/optuna-examples)
│ │          
│ │          *Optuna* is an automatic hyperparameter optimization software framework, particularly designed
│ │          for machine learning. It features an imperative, *define-by-run* style user API. Thanks to our
│ │          *define-by-run* API, the code written with Optuna enjoys high modularity, and the user of
│ │          Optuna can dynamically construct the search spaces for the hyperparameters.
│ │          
│ │          ## Key Features
│ │   --- optuna-3.1.0b0/README.md
│ ├── +++ optuna-3.1.1/README.md
│ │┄ Files 2% similar despite different names
│ │ @@ -2,22 +2,22 @@
│ │  
│ │  # Optuna: A hyperparameter optimization framework
│ │  
│ │  [![Python](https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9%20%7C%203.10%20%7C%203.11-blue)](https://www.python.org)
│ │  [![pypi](https://img.shields.io/pypi/v/optuna.svg)](https://pypi.python.org/pypi/optuna)
│ │  [![conda](https://img.shields.io/conda/vn/conda-forge/optuna.svg)](https://anaconda.org/conda-forge/optuna)
│ │  [![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/optuna/optuna)
│ │ -[![CircleCI](https://circleci.com/gh/optuna/optuna.svg?style=svg)](https://circleci.com/gh/optuna/optuna)
│ │  [![Read the Docs](https://readthedocs.org/projects/optuna/badge/?version=stable)](https://optuna.readthedocs.io/en/stable/)
│ │  [![Codecov](https://codecov.io/gh/optuna/optuna/branch/master/graph/badge.svg)](https://codecov.io/gh/optuna/optuna/branch/master)
│ │  
│ │  [**Website**](https://optuna.org/)
│ │  | [**Docs**](https://optuna.readthedocs.io/en/stable/)
│ │  | [**Install Guide**](https://optuna.readthedocs.io/en/stable/installation.html)
│ │  | [**Tutorial**](https://optuna.readthedocs.io/en/stable/tutorial/index.html)
│ │ +| [**Examples**](https://github.com/optuna/optuna-examples)
│ │  
│ │  *Optuna* is an automatic hyperparameter optimization software framework, particularly designed
│ │  for machine learning. It features an imperative, *define-by-run* style user API. Thanks to our
│ │  *define-by-run* API, the code written with Optuna enjoys high modularity, and the user of
│ │  Optuna can dynamically construct the search spaces for the hyperparameters.
│ │  
│ │  ## Key Features
│ │   --- optuna-3.1.0b0/benchmarks/asv/optimize.py
│ ├── +++ optuna-3.1.1/benchmarks/asv/optimize.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/benchmarks/kurobako/mo_create_study.py
│ ├── +++ optuna-3.1.1/benchmarks/kurobako/mo_create_study.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/benchmarks/naslib/problem.py
│ ├── +++ optuna-3.1.1/benchmarks/naslib/problem.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/__init__.py
│ ├── +++ optuna-3.1.1/optuna/__init__.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/_callbacks.py
│ ├── +++ optuna-3.1.1/optuna/_callbacks.py
│ │┄ Files 8% similar despite different names
│ │ @@ -8,18 +8,18 @@
│ │  from optuna.trial import FrozenTrial
│ │  from optuna.trial import TrialState
│ │  
│ │  
│ │  class MaxTrialsCallback:
│ │      """Set a maximum number of trials before ending the study.
│ │  
│ │ -    While the :obj:`n_trials` argument of :obj:`optuna.optimize` sets the number of trials that
│ │ -    will be run, you may want to continue running until you have a certain number of successfullly
│ │ -    completed trials or stop the study when you have a certain number of trials that fail.
│ │ -    This :obj:`MaxTrialsCallback` class allows you to set a maximum number of trials for a
│ │ +    While the ``n_trials`` argument of :meth:`optuna.study.Study.optimize` sets the number of
│ │ +    trials that will be run, you may want to continue running until you have a certain number of
│ │ +    successfully completed trials or stop the study when you have a certain number of trials that
│ │ +    fail. This ``MaxTrialsCallback`` class allows you to set a maximum number of trials for a
│ │      particular :class:`~optuna.trial.TrialState` before stopping the study.
│ │  
│ │      Example:
│ │  
│ │          .. testcode::
│ │  
│ │              import optuna
│ │ @@ -39,15 +39,15 @@
│ │              )
│ │  
│ │      Args:
│ │          n_trials:
│ │              The max number of trials. Must be set to an integer.
│ │          states:
│ │              Tuple of the :class:`~optuna.trial.TrialState` to be counted
│ │ -            towards the max trials limit. Default value is :obj:`(TrialState.COMPLETE,)`.
│ │ +            towards the max trials limit. Default value is ``(TrialState.COMPLETE,)``.
│ │              If :obj:`None`, count all states.
│ │      """
│ │  
│ │      def __init__(
│ │          self, n_trials: int, states: Optional[Container[TrialState]] = (TrialState.COMPLETE,)
│ │      ) -> None:
│ │          self._n_trials = n_trials
│ │ @@ -60,16 +60,16 @@
│ │              study.stop()
│ │  
│ │  
│ │  @experimental_class("2.8.0")
│ │  class RetryFailedTrialCallback:
│ │      """Retry a failed trial up to a maximum number of times.
│ │  
│ │ -    When a trial fails, this callback can be used with the :class:`optuna.storage` class to
│ │ -    recreate the trial in :obj:`TrialState.WAITING` to queue up the trial to be run again.
│ │ +    When a trial fails, this callback can be used with a class in :mod:`optuna.storages` to
│ │ +    recreate the trial in ``TrialState.WAITING`` to queue up the trial to be run again.
│ │  
│ │      The failed trial can be identified by the
│ │      :func:`~optuna.storages.RetryFailedTrialCallback.retried_trial_number` function.
│ │      Even if repetitive failure occurs (a retried trial fails again),
│ │      this method returns the number of the original trial.
│ │      To get a full list including the numbers of the retried trials as well as their original trial,
│ │      call the :func:`~optuna.storages.RetryFailedTrialCallback.retry_history` function.
│ │   --- optuna-3.1.0b0/optuna/_convert_positional_args.py
│ ├── +++ optuna-3.1.1/optuna/_convert_positional_args.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/_deprecated.py
│ ├── +++ optuna-3.1.1/optuna/_deprecated.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/_experimental.py
│ ├── +++ optuna-3.1.1/optuna/_experimental.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/_hypervolume/base.py
│ ├── +++ optuna-3.1.1/optuna/_hypervolume/base.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/_hypervolume/hssp.py
│ ├── +++ optuna-3.1.1/optuna/_hypervolume/hssp.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/_hypervolume/utils.py
│ ├── +++ optuna-3.1.1/optuna/_hypervolume/utils.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/_hypervolume/wfg.py
│ ├── +++ optuna-3.1.1/optuna/_hypervolume/wfg.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/_imports.py
│ ├── +++ optuna-3.1.1/optuna/_imports.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/_transform.py
│ ├── +++ optuna-3.1.1/optuna/_transform.py
│ │┄ Files 5% similar despite different names
│ │ @@ -34,14 +34,17 @@
│ │              Should always be :obj:`True` if any parameters are going to be sampled from the
│ │              transformed space.
│ │          transform_step:
│ │              If :obj:`True`, offset the lower and higher bounds by a half step each, increasing the
│ │              space by one step. This allows fair sampling for values close to the bounds.
│ │              Should always be :obj:`True` if any parameters are going to be sampled from the
│ │              transformed space.
│ │ +        transform_0_1:
│ │ +            If :obj:`True`, apply a linear transformation to the bounds and parameters so that
│ │ +            they are in the unit cube.
│ │  
│ │      Attributes:
│ │          bounds:
│ │              Constructed bounds from the given search space.
│ │          column_to_encoded_columns:
│ │              Constructed mapping from original parameter column index to encoded column indices.
│ │          encoded_column_to_column:
│ │ @@ -58,28 +61,32 @@
│ │      """
│ │  
│ │      def __init__(
│ │          self,
│ │          search_space: Dict[str, BaseDistribution],
│ │          transform_log: bool = True,
│ │          transform_step: bool = True,
│ │ +        transform_0_1: bool = False,
│ │      ) -> None:
│ │          bounds, column_to_encoded_columns, encoded_column_to_column = _transform_search_space(
│ │              search_space, transform_log, transform_step
│ │          )
│ │ -
│ │ -        self._bounds = bounds
│ │ +        self._raw_bounds = bounds
│ │          self._column_to_encoded_columns = column_to_encoded_columns
│ │          self._encoded_column_to_column = encoded_column_to_column
│ │          self._search_space = search_space
│ │          self._transform_log = transform_log
│ │ +        self._transform_0_1 = transform_0_1
│ │  
│ │      @property
│ │      def bounds(self) -> numpy.ndarray:
│ │ -        return self._bounds
│ │ +        if self._transform_0_1:
│ │ +            return numpy.array([[0.0, 1.0]] * self._raw_bounds.shape[0])
│ │ +        else:
│ │ +            return self._raw_bounds
│ │  
│ │      @property
│ │      def column_to_encoded_columns(self) -> List[numpy.ndarray]:
│ │          return self._column_to_encoded_columns
│ │  
│ │      @property
│ │      def encoded_column_to_column(self) -> numpy.ndarray:
│ │ @@ -93,15 +100,15 @@
│ │                  A parameter configuration to transform.
│ │  
│ │          Returns:
│ │              A 1-dimensional ``numpy.ndarray`` holding the transformed parameters in the
│ │              configuration.
│ │  
│ │          """
│ │ -        trans_params = numpy.zeros(self._bounds.shape[0], dtype=numpy.float64)
│ │ +        trans_params = numpy.zeros(self._raw_bounds.shape[0], dtype=numpy.float64)
│ │  
│ │          bound_idx = 0
│ │          for name, distribution in self._search_space.items():
│ │              assert name in params, "Parameter configuration must contain all distributions."
│ │              param = params[name]
│ │  
│ │              if isinstance(distribution, CategoricalDistribution):
│ │ @@ -110,14 +117,21 @@
│ │                  bound_idx += len(distribution.choices)
│ │              else:
│ │                  trans_params[bound_idx] = _transform_numerical_param(
│ │                      param, distribution, self._transform_log
│ │                  )
│ │                  bound_idx += 1
│ │  
│ │ +        if self._transform_0_1:
│ │ +            single_mask = self._raw_bounds[:, 0] == self._raw_bounds[:, 1]
│ │ +            trans_params[single_mask] = 0.5
│ │ +            trans_params[~single_mask] = (
│ │ +                trans_params[~single_mask] - self._raw_bounds[~single_mask, 0]
│ │ +            ) / (self._raw_bounds[~single_mask, 1] - self._raw_bounds[~single_mask, 0])
│ │ +
│ │          return trans_params
│ │  
│ │      def untransform(self, trans_params: numpy.ndarray) -> Dict[str, Any]:
│ │          """Untransform a parameter configuration from continuous space to actual values.
│ │  
│ │          Args:
│ │              trans_params:
│ │ @@ -125,15 +139,20 @@
│ │                  parameter configuration.
│ │  
│ │          Returns:
│ │              A dictionary of an untransformed parameter configuration. Keys are parameter names.
│ │              Values are untransformed parameter values.
│ │  
│ │          """
│ │ -        assert trans_params.shape == (self._bounds.shape[0],)
│ │ +        assert trans_params.shape == (self._raw_bounds.shape[0],)
│ │ +
│ │ +        if self._transform_0_1:
│ │ +            trans_params = self._raw_bounds[:, 0] + trans_params * (
│ │ +                self._raw_bounds[:, 1] - self._raw_bounds[:, 0]
│ │ +            )
│ │  
│ │          params = {}
│ │  
│ │          for (name, distribution), encoded_columns in zip(
│ │              self._search_space.items(), self.column_to_encoded_columns
│ │          ):
│ │              trans_param = trans_params[encoded_columns]
│ │   --- optuna-3.1.0b0/optuna/cli.py
│ ├── +++ optuna-3.1.1/optuna/cli.py
│ │┄ Files 2% similar despite different names
│ │ @@ -5,14 +5,15 @@
│ │  from argparse import Namespace
│ │  import datetime
│ │  from enum import Enum
│ │  from importlib.machinery import SourceFileLoader
│ │  import inspect
│ │  import json
│ │  import logging
│ │ +import os
│ │  import sys
│ │  import types
│ │  from typing import Any
│ │  from typing import Dict
│ │  from typing import List
│ │  from typing import Optional
│ │  from typing import Tuple
│ │ @@ -33,17 +34,26 @@
│ │  _dataframe = _LazyImport("optuna.study._dataframe")
│ │  
│ │  _DATETIME_FORMAT = "%Y-%m-%d %H:%M:%S"
│ │  
│ │  
│ │  def _check_storage_url(storage_url: Optional[str]) -> str:
│ │  
│ │ -    if storage_url is None:
│ │ -        raise CLIUsageError("Storage URL is not specified.")
│ │ -    return storage_url
│ │ +    if storage_url is not None:
│ │ +        return storage_url
│ │ +
│ │ +    env_storage = os.environ.get("OPTUNA_STORAGE")
│ │ +    if env_storage is not None:
│ │ +        warnings.warn(
│ │ +            "Specifying the storage url via 'OPTUNA_STORAGE' environment variable"
│ │ +            " is an experimental feature. The interface can change in the future.",
│ │ +            ExperimentalWarning,
│ │ +        )
│ │ +        return env_storage
│ │ +    raise CLIUsageError("Storage URL is not specified.")
│ │  
│ │  
│ │  def _format_value(value: Any) -> Any:
│ │      #  Format value that can be serialized to JSON or YAML.
│ │      if value is None or isinstance(value, (int, float)):
│ │          return value
│ │      elif isinstance(value, datetime.datetime):
│ │ @@ -897,15 +907,22 @@
│ │      "storage upgrade": _StorageUpgrade,
│ │      "ask": _Ask,
│ │      "tell": _Tell,
│ │  }
│ │  
│ │  
│ │  def _add_common_arguments(parser: ArgumentParser) -> ArgumentParser:
│ │ -    parser.add_argument("--storage", default=None, help="DB URL. (e.g. sqlite:///example.db)")
│ │ +    parser.add_argument(
│ │ +        "--storage",
│ │ +        default=None,
│ │ +        help=(
│ │ +            "DB URL. (e.g. sqlite:///example.db) "
│ │ +            "Also can be specified via OPTUNA_STORAGE environment variable."
│ │ +        ),
│ │ +    )
│ │      verbose_group = parser.add_mutually_exclusive_group()
│ │      verbose_group.add_argument(
│ │          "-v",
│ │          "--verbose",
│ │          action="count",
│ │          dest="verbose_level",
│ │          default=1,
│ │   --- optuna-3.1.0b0/optuna/distributions.py
│ ├── +++ optuna-3.1.1/optuna/distributions.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/exceptions.py
│ ├── +++ optuna-3.1.1/optuna/exceptions.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/importance/__init__.py
│ ├── +++ optuna-3.1.1/optuna/importance/__init__.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/importance/_base.py
│ ├── +++ optuna-3.1.1/optuna/importance/_base.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/importance/_fanova/_evaluator.py
│ ├── +++ optuna-3.1.1/optuna/importance/_fanova/_evaluator.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/importance/_fanova/_fanova.py
│ ├── +++ optuna-3.1.1/optuna/importance/_fanova/_fanova.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/importance/_fanova/_tree.py
│ ├── +++ optuna-3.1.1/optuna/importance/_fanova/_tree.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/importance/_mean_decrease_impurity.py
│ ├── +++ optuna-3.1.1/optuna/importance/_mean_decrease_impurity.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/__init__.py
│ ├── +++ optuna-3.1.1/optuna/integration/__init__.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/_lightgbm_tuner/__init__.py
│ ├── +++ optuna-3.1.1/optuna/integration/_lightgbm_tuner/__init__.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/_lightgbm_tuner/alias.py
│ ├── +++ optuna-3.1.1/optuna/integration/_lightgbm_tuner/alias.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/_lightgbm_tuner/optimize.py
│ ├── +++ optuna-3.1.1/optuna/integration/_lightgbm_tuner/optimize.py
│ │┄ Files 1% similar despite different names
│ │ @@ -763,18 +763,18 @@
│ │              Each function must accept two parameters with the following types in this order:
│ │              :class:`~optuna.study.Study` and :class:`~optuna.trial.FrozenTrial`.
│ │              Please note that this is not a ``callbacks`` argument of `lightgbm.train()`_ .
│ │  
│ │          model_dir:
│ │              A directory to save boosters. By default, it is set to :obj:`None` and no boosters are
│ │              saved. Please set shared directory (e.g., directories on NFS) if you want to access
│ │ -            :meth:`~optuna.integration.LightGBMTuner.get_best_booster` in distributed environments.
│ │ -            Otherwise, it may raise :obj:`ValueError`. If the directory does not exist, it will be
│ │ -            created. The filenames of the boosters will be ``{model_dir}/{trial_number}.pkl``
│ │ -            (e.g., ``./boosters/0.pkl``).
│ │ +            :meth:`~optuna.integration.lightgbm.LightGBMTuner.get_best_booster` in distributed
│ │ +            environments. Otherwise, it may raise :obj:`ValueError`. If the directory does not
│ │ +            exist, it will be created. The filenames of the boosters will be
│ │ +            ``{model_dir}/{trial_number}.pkl`` (e.g., ``./boosters/0.pkl``).
│ │  
│ │          verbosity:
│ │              A verbosity level to change Optuna's logging level. The level is aligned to
│ │              `LightGBM's verbosity`_ .
│ │  
│ │              .. warning::
│ │                  Deprecated in v2.0.0. ``verbosity`` argument will be removed in the future.
│ │ @@ -916,15 +916,15 @@
│ │              Each function must accept two parameters with the following types in this order:
│ │              :class:`~optuna.study.Study` and :class:`~optuna.trial.FrozenTrial`.
│ │              Please note that this is not a ``callbacks`` argument of `lightgbm.train()`_ .
│ │  
│ │          model_dir:
│ │              A directory to save boosters. By default, it is set to :obj:`None` and no boosters are
│ │              saved. Please set shared directory (e.g., directories on NFS) if you want to access
│ │ -            :meth:`~optuna.integration.LightGBMTunerCV.get_best_booster`
│ │ +            :meth:`~optuna.integration.lightgbm.LightGBMTunerCV.get_best_booster`
│ │              in distributed environments.
│ │              Otherwise, it may raise :obj:`ValueError`. If the directory does not exist, it will be
│ │              created. The filenames of the boosters will be ``{model_dir}/{trial_number}.pkl``
│ │              (e.g., ``./boosters/0.pkl``).
│ │  
│ │          verbosity:
│ │              A verbosity level to change Optuna's logging level. The level is aligned to
│ │ @@ -941,15 +941,15 @@
│ │              Flag to show progress bars or not. To disable progress bar, set this :obj:`False`.
│ │  
│ │              .. note::
│ │                  Progress bars will be fragmented by logging messages of LightGBM and Optuna.
│ │                  Please suppress such messages to show the progress bars properly.
│ │  
│ │          return_cvbooster:
│ │ -            Flag to enable :meth:`~optuna.integration.LightGBMTunerCV.get_best_booster`.
│ │ +            Flag to enable :meth:`~optuna.integration.lightgbm.LightGBMTunerCV.get_best_booster`.
│ │  
│ │          optuna_seed:
│ │              ``seed`` of :class:`~optuna.samplers.TPESampler` for random number generator
│ │              that affects sampling for ``num_leaves``, ``bagging_fraction``, ``bagging_freq``,
│ │              ``lambda_l1``, and ``lambda_l2``.
│ │  
│ │              .. note::
│ │   --- optuna-3.1.0b0/optuna/integration/_lightgbm_tuner/sklearn.py
│ ├── +++ optuna-3.1.1/optuna/integration/_lightgbm_tuner/sklearn.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/allennlp/_dump_best_config.py
│ ├── +++ optuna-3.1.1/optuna/integration/allennlp/_dump_best_config.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/allennlp/_executor.py
│ ├── +++ optuna-3.1.1/optuna/integration/allennlp/_executor.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/allennlp/_pruner.py
│ ├── +++ optuna-3.1.1/optuna/integration/allennlp/_pruner.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/allennlp/_variables.py
│ ├── +++ optuna-3.1.1/optuna/integration/allennlp/_variables.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/botorch.py
│ ├── +++ optuna-3.1.1/optuna/integration/botorch.py
│ │┄ Files 4% similar despite different names
│ │ @@ -28,19 +28,32 @@
│ │  
│ │  with try_import() as _imports:
│ │      from botorch.acquisition.monte_carlo import qExpectedImprovement
│ │      from botorch.acquisition.multi_objective import monte_carlo
│ │      from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective
│ │      from botorch.acquisition.objective import ConstrainedMCObjective
│ │      from botorch.acquisition.objective import GenericMCObjective
│ │ -    from botorch.fit import fit_gpytorch_model
│ │      from botorch.models import SingleTaskGP
│ │      from botorch.models.transforms.outcome import Standardize
│ │      from botorch.optim import optimize_acqf
│ │ -    from botorch.sampling.samplers import SobolQMCNormalSampler
│ │ +    from botorch.sampling import SobolQMCNormalSampler
│ │ +    import botorch.version
│ │ +
│ │ +    if botorch.version.version_tuple < (0, 8, 0):
│ │ +        from botorch.fit import fit_gpytorch_model as fit_gpytorch_mll
│ │ +
│ │ +        def _get_sobol_qmc_normal_sampler(num_samples: int) -> SobolQMCNormalSampler:
│ │ +            return SobolQMCNormalSampler(num_samples)
│ │ +
│ │ +    else:
│ │ +        from botorch.fit import fit_gpytorch_mll
│ │ +
│ │ +        def _get_sobol_qmc_normal_sampler(num_samples: int) -> SobolQMCNormalSampler:
│ │ +            return SobolQMCNormalSampler(torch.Size((num_samples,)))
│ │ +
│ │      from botorch.utils.multi_objective.box_decompositions import NondominatedPartitioning
│ │      from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization
│ │      from botorch.utils.sampling import manual_seed
│ │      from botorch.utils.sampling import sample_simplex
│ │      from botorch.utils.transforms import normalize
│ │      from botorch.utils.transforms import unnormalize
│ │      from gpytorch.mlls import ExactMarginalLogLikelihood
│ │ @@ -120,20 +133,20 @@
│ │  
│ │          objective = None  # Using the default identity objective.
│ │  
│ │      train_x = normalize(train_x, bounds=bounds)
│ │  
│ │      model = SingleTaskGP(train_x, train_y, outcome_transform=Standardize(m=train_y.size(-1)))
│ │      mll = ExactMarginalLogLikelihood(model.likelihood, model)
│ │ -    fit_gpytorch_model(mll)
│ │ +    fit_gpytorch_mll(mll)
│ │  
│ │      acqf = qExpectedImprovement(
│ │          model=model,
│ │          best_f=best_f,
│ │ -        sampler=SobolQMCNormalSampler(num_samples=256),
│ │ +        sampler=_get_sobol_qmc_normal_sampler(256),
│ │          objective=objective,
│ │      )
│ │  
│ │      standard_bounds = torch.zeros_like(bounds)
│ │      standard_bounds[1] = 1
│ │  
│ │      candidates, _ = optimize_acqf(
│ │ @@ -190,15 +203,15 @@
│ │  
│ │          additional_qehvi_kwargs = {}
│ │  
│ │      train_x = normalize(train_x, bounds=bounds)
│ │  
│ │      model = SingleTaskGP(train_x, train_y, outcome_transform=Standardize(m=train_y.shape[-1]))
│ │      mll = ExactMarginalLogLikelihood(model.likelihood, model)
│ │ -    fit_gpytorch_model(mll)
│ │ +    fit_gpytorch_mll(mll)
│ │  
│ │      # Approximate box decomposition similar to Ax when the number of objectives is large.
│ │      # https://github.com/facebook/Ax/blob/master/ax/models/torch/botorch_moo_defaults
│ │      if n_objectives > 2:
│ │          alpha = 10 ** (-8 + n_objectives)
│ │      else:
│ │          alpha = 0.0
│ │ @@ -209,18 +222,17 @@
│ │  
│ │      ref_point_list = ref_point.tolist()
│ │  
│ │      acqf = monte_carlo.qExpectedHypervolumeImprovement(
│ │          model=model,
│ │          ref_point=ref_point_list,
│ │          partitioning=partitioning,
│ │ -        sampler=SobolQMCNormalSampler(num_samples=256),
│ │ +        sampler=_get_sobol_qmc_normal_sampler(256),
│ │          **additional_qehvi_kwargs,
│ │      )
│ │ -
│ │      standard_bounds = torch.zeros_like(bounds)
│ │      standard_bounds[1] = 1
│ │  
│ │      candidates, _ = optimize_acqf(
│ │          acq_function=acqf,
│ │          bounds=standard_bounds,
│ │          q=1,
│ │ @@ -270,15 +282,15 @@
│ │  
│ │          additional_qnehvi_kwargs = {}
│ │  
│ │      train_x = normalize(train_x, bounds=bounds)
│ │  
│ │      model = SingleTaskGP(train_x, train_y, outcome_transform=Standardize(m=train_y.shape[-1]))
│ │      mll = ExactMarginalLogLikelihood(model.likelihood, model)
│ │ -    fit_gpytorch_model(mll)
│ │ +    fit_gpytorch_mll(mll)
│ │  
│ │      # Approximate box decomposition similar to Ax when the number of objectives is large.
│ │      # https://github.com/facebook/Ax/blob/master/ax/models/torch/botorch_moo_defaults
│ │      if n_objectives > 2:
│ │          alpha = 10 ** (-8 + n_objectives)
│ │      else:
│ │          alpha = 0.0
│ │ @@ -291,15 +303,15 @@
│ │      # cf. https://botorch.org/api/acquisition.html (accessed on 2022/11/18)
│ │      acqf = monte_carlo.qNoisyExpectedHypervolumeImprovement(
│ │          model=model,
│ │          ref_point=ref_point_list,
│ │          X_baseline=train_x,
│ │          alpha=alpha,
│ │          prune_baseline=True,
│ │ -        sampler=SobolQMCNormalSampler(num_samples=256),
│ │ +        sampler=_get_sobol_qmc_normal_sampler(256),
│ │          **additional_qnehvi_kwargs,
│ │      )
│ │  
│ │      standard_bounds = torch.zeros_like(bounds)
│ │      standard_bounds[1] = 1
│ │  
│ │      candidates, _ = optimize_acqf(
│ │ @@ -353,20 +365,20 @@
│ │  
│ │          objective = GenericMCObjective(scalarization)
│ │  
│ │      train_x = normalize(train_x, bounds=bounds)
│ │  
│ │      model = SingleTaskGP(train_x, train_y, outcome_transform=Standardize(m=train_y.size(-1)))
│ │      mll = ExactMarginalLogLikelihood(model.likelihood, model)
│ │ -    fit_gpytorch_model(mll)
│ │ +    fit_gpytorch_mll(mll)
│ │  
│ │      acqf = qExpectedImprovement(
│ │          model=model,
│ │          best_f=objective(train_y).max(),
│ │ -        sampler=SobolQMCNormalSampler(num_samples=256),
│ │ +        sampler=_get_sobol_qmc_normal_sampler(256),
│ │          objective=objective,
│ │      )
│ │  
│ │      standard_bounds = torch.zeros_like(bounds)
│ │      standard_bounds[1] = 1
│ │  
│ │      candidates, _ = optimize_acqf(
│ │   --- optuna-3.1.0b0/optuna/integration/catalyst.py
│ ├── +++ optuna-3.1.1/optuna/integration/catalyst.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/catboost.py
│ ├── +++ optuna-3.1.1/optuna/integration/catboost.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/chainer.py
│ ├── +++ optuna-3.1.1/optuna/integration/chainer.py
│ │┄ Files 5% similar despite different names
│ │ @@ -3,19 +3,19 @@
│ │  
│ │  import optuna
│ │  
│ │  
│ │  with optuna._imports.try_import() as _imports:
│ │      import chainer
│ │      from chainer.training.extension import Extension
│ │ -    from chainer.training.triggers import IntervalTrigger
│ │ -    from chainer.training.triggers import ManualScheduleTrigger
│ │ +    from chainer.training.triggers import IntervalTrigger  # type: ignore[attr-defined]
│ │ +    from chainer.training.triggers import ManualScheduleTrigger  # type: ignore[attr-defined]
│ │  
│ │  if not _imports.is_successful():
│ │ -    Extension = object  # type: ignore  # NOQA
│ │ +    Extension = object  # type: ignore[assignment, misc]  # NOQA
│ │  
│ │  
│ │  class ChainerPruningExtension(Extension):
│ │      """Chainer extension to prune unpromising trials.
│ │  
│ │      See `the example <https://github.com/optuna/optuna-examples/blob/main/
│ │      chainer/chainer_integration.py>`__
│ │ @@ -50,46 +50,50 @@
│ │          pruner_trigger: Union[Tuple[(int, str)], "IntervalTrigger", "ManualScheduleTrigger"],
│ │      ) -> None:
│ │  
│ │          _imports.check()
│ │  
│ │          self._trial = trial
│ │          self._observation_key = observation_key
│ │ -        self._pruner_trigger = chainer.training.get_trigger(pruner_trigger)  # type: ignore
│ │ +        self._pruner_trigger = chainer.training.get_trigger(  # type: ignore[attr-defined]
│ │ +            pruner_trigger
│ │ +        )
│ │          if not isinstance(self._pruner_trigger, (IntervalTrigger, ManualScheduleTrigger)):
│ │              pruner_type = type(self._pruner_trigger)
│ │              raise TypeError(
│ │                  "Invalid trigger class: " + str(pruner_type) + "\n"
│ │                  "Pruner trigger is supposed to be an instance of "
│ │                  "IntervalTrigger or ManualScheduleTrigger."
│ │              )
│ │  
│ │      @staticmethod
│ │      def _get_float_value(
│ │ -        observation_value: Union[float, "chainer.Variable"]  # type: ignore
│ │ +        observation_value: Union[float, "chainer.Variable"]  # type: ignore[name-defined]
│ │      ) -> float:
│ │  
│ │          _imports.check()
│ │  
│ │          try:
│ │ -            if isinstance(observation_value, chainer.Variable):  # type: ignore
│ │ -                return float(observation_value.data)  # type: ignore
│ │ +            if isinstance(observation_value, chainer.Variable):  # type: ignore[attr-defined]
│ │ +                return float(observation_value.data)
│ │              else:
│ │                  return float(observation_value)
│ │          except TypeError:
│ │              raise TypeError(
│ │                  "Type of observation value is not supported by ChainerPruningExtension.\n"
│ │                  "{} cannot be cast to float.".format(type(observation_value))
│ │              ) from None
│ │  
│ │ -    def _observation_exists(self, trainer: "chainer.training.Trainer") -> bool:  # type: ignore
│ │ +    def _observation_exists(
│ │ +        self, trainer: "chainer.training.Trainer"  # type: ignore[name-defined]
│ │ +    ) -> bool:
│ │  
│ │          return self._pruner_trigger(trainer) and self._observation_key in trainer.observation
│ │  
│ │ -    def __call__(self, trainer: "chainer.training.Trainer") -> None:  # type: ignore
│ │ +    def __call__(self, trainer: "chainer.training.Trainer") -> None:  # type: ignore[name-defined]
│ │  
│ │          if not self._observation_exists(trainer):
│ │              return
│ │  
│ │          current_score = self._get_float_value(trainer.observation[self._observation_key])
│ │          current_step = getattr(trainer.updater, self._pruner_trigger.unit)
│ │          self._trial.report(current_score, step=current_step)
│ │   --- optuna-3.1.0b0/optuna/integration/chainermn.py
│ ├── +++ optuna-3.1.1/optuna/integration/chainermn.py
│ │┄ Files 7% similar despite different names
│ │ @@ -1,12 +1,13 @@
│ │  from datetime import datetime
│ │  from typing import Any
│ │  from typing import Callable
│ │  from typing import Dict
│ │  from typing import Optional
│ │ +from typing import overload
│ │  from typing import Sequence
│ │  from typing import Tuple
│ │  from typing import Type
│ │  import warnings
│ │  
│ │  from optuna import TrialPruned
│ │  from optuna._deprecated import deprecated_func
│ │ @@ -201,15 +202,43 @@
│ │          def func() -> int:
│ │  
│ │              assert self.delegate is not None
│ │              return self.delegate.suggest_int(name, low, high, step=step, log=log)
│ │  
│ │          return self._call_with_mpi(func)
│ │  
│ │ -    def suggest_categorical(self, name: str, choices: Sequence[CategoricalChoiceType]) -> Any:
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[None]) -> None:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[bool]) -> bool:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[int]) -> int:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[float]) -> float:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[str]) -> str:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(
│ │ +        self, name: str, choices: Sequence[CategoricalChoiceType]
│ │ +    ) -> CategoricalChoiceType:
│ │ +        ...
│ │ +
│ │ +    def suggest_categorical(
│ │ +        self, name: str, choices: Sequence[CategoricalChoiceType]
│ │ +    ) -> CategoricalChoiceType:
│ │          def func() -> CategoricalChoiceType:
│ │  
│ │              assert self.delegate is not None
│ │              return self.delegate.suggest_categorical(name, choices)
│ │  
│ │          return self._call_with_mpi(func)
│ │  
│ │ @@ -285,14 +314,15 @@
│ │  
│ │              assert self.delegate is not None
│ │              return self.delegate.user_attrs
│ │  
│ │          return self._call_with_mpi(func)
│ │  
│ │      @property
│ │ +    @deprecated_func("3.1.0", "6.0.0")
│ │      def system_attrs(self) -> Dict[str, Any]:
│ │          def func() -> Dict[str, Any]:
│ │  
│ │              assert self.delegate is not None
│ │              return self.delegate.system_attrs
│ │  
│ │          return self._call_with_mpi(func)
│ │   --- optuna-3.1.0b0/optuna/integration/cma.py
│ ├── +++ optuna-3.1.1/optuna/integration/cma.py
│ │┄ Files 1% similar despite different names
│ │ @@ -86,15 +86,16 @@
│ │  
│ │          seed:
│ │              A random seed for CMA-ES.
│ │  
│ │          cma_opts:
│ │              Options passed to the constructor of cma.CMAEvolutionStrategy_ class.
│ │  
│ │ -            Note that ``BoundaryHandler``, ``bounds``, ``CMA_stds`` and ``seed`` arguments in
│ │ +            Note that default option is cma_default_options_,
│ │ +            but ``BoundaryHandler``, ``bounds``, ``CMA_stds`` and ``seed`` arguments in
│ │              ``cma_opts`` will be ignored because it is added by
│ │              :class:`~optuna.integration.PyCmaSampler` automatically.
│ │  
│ │          n_startup_trials:
│ │              The independent sampling is used instead of the CMA-ES algorithm until the given number
│ │              of trials finish in the same study.
│ │  
│ │ @@ -116,16 +117,18 @@
│ │          warn_independent_sampling:
│ │              If this is :obj:`True`, a warning message is emitted when
│ │              the value of a parameter is sampled by using an independent sampler.
│ │  
│ │              Note that the parameters of the first trial in a study are always sampled
│ │              via an independent sampler, so no warning messages are emitted in this case.
│ │  
│ │ -    .. _cma.CMAEvolutionStrategy: http://cma.gforge.inria.fr/apidocs-pycma/\
│ │ +    .. _cma.CMAEvolutionStrategy: https://cma-es.github.io/apidocs-pycma/\
│ │      cma.evolution_strategy.CMAEvolutionStrategy.html
│ │ +    .. _cma_default_options: https://cma-es.github.io/apidocs-pycma/\
│ │ +    cma.evolution_strategy.html#cma_default_options_
│ │      """
│ │  
│ │      def __init__(
│ │          self,
│ │          x0: Optional[Dict[str, Any]] = None,
│ │          sigma0: Optional[float] = None,
│ │          cma_stds: Optional[Dict[str, float]] = None,
│ │   --- optuna-3.1.0b0/optuna/integration/dask.py
│ ├── +++ optuna-3.1.1/optuna/integration/dask.py
│ │┄ Files 18% similar despite different names
│ │ @@ -25,26 +25,32 @@
│ │  from optuna.trial import TrialState
│ │  
│ │  
│ │  with try_import() as _imports:
│ │      import distributed
│ │      from distributed.protocol.pickle import dumps
│ │      from distributed.protocol.pickle import loads
│ │ -    from distributed.utils import thread_state
│ │ +    from distributed.utils import thread_state  # type: ignore[attr-defined]
│ │      from distributed.worker import get_client
│ │  
│ │  
│ │  def _serialize_frozentrial(trial: FrozenTrial) -> dict:
│ │      data = trial.__dict__.copy()
│ │      data["state"] = data["state"].name
│ │      attrs = [a for a in data.keys() if a.startswith("_")]
│ │      for attr in attrs:
│ │          data[attr[1:]] = data.pop(attr)
│ │ -    data["system_attrs"] = dumps(data["system_attrs"]) if data["system_attrs"] else {}
│ │ -    data["user_attrs"] = dumps(data["user_attrs"]) if data["user_attrs"] else {}
│ │ +    data["system_attrs"] = (
│ │ +        dumps(data["system_attrs"])  # type: ignore[no-untyped-call]
│ │ +        if data["system_attrs"]
│ │ +        else {}
│ │ +    )
│ │ +    data["user_attrs"] = (
│ │ +        dumps(data["user_attrs"]) if data["user_attrs"] else {}  # type: ignore[no-untyped-call]
│ │ +    )
│ │      data["distributions"] = {k: distribution_to_json(v) for k, v in data["distributions"].items()}
│ │      if data["datetime_start"] is not None:
│ │          data["datetime_start"] = data["datetime_start"].isoformat(timespec="microseconds")
│ │      if data["datetime_complete"] is not None:
│ │          data["datetime_complete"] = data["datetime_complete"].isoformat(timespec="microseconds")
│ │      data["value"] = None
│ │      return data
│ │ @@ -53,49 +59,64 @@
│ │  def _deserialize_frozentrial(data: dict) -> FrozenTrial:
│ │      data["state"] = TrialState[data["state"]]
│ │      data["distributions"] = {k: json_to_distribution(v) for k, v in data["distributions"].items()}
│ │      if data["datetime_start"] is not None:
│ │          data["datetime_start"] = datetime.fromisoformat(data["datetime_start"])
│ │      if data["datetime_complete"] is not None:
│ │          data["datetime_complete"] = datetime.fromisoformat(data["datetime_complete"])
│ │ -    data["system_attrs"] = loads(data["system_attrs"]) if data["system_attrs"] else {}
│ │ -    data["user_attrs"] = loads(data["user_attrs"]) if data["user_attrs"] else {}
│ │ +    data["system_attrs"] = (
│ │ +        loads(data["system_attrs"])  # type: ignore[no-untyped-call]
│ │ +        if data["system_attrs"]
│ │ +        else {}
│ │ +    )
│ │ +    data["user_attrs"] = (
│ │ +        loads(data["user_attrs"]) if data["user_attrs"] else {}  # type: ignore[no-untyped-call]
│ │ +    )
│ │      return FrozenTrial(**data)
│ │  
│ │  
│ │  def _serialize_frozenstudy(study: FrozenStudy) -> dict:
│ │      data = {
│ │          "directions": [d.name for d in study._directions],
│ │          "study_id": study._study_id,
│ │          "study_name": study.study_name,
│ │ -        "user_attrs": dumps(study.user_attrs) if study.user_attrs else {},
│ │ -        "system_attrs": dumps(study.system_attrs) if study.system_attrs else {},
│ │ +        "user_attrs": dumps(study.user_attrs)  # type: ignore[no-untyped-call]
│ │ +        if study.user_attrs
│ │ +        else {},
│ │ +        "system_attrs": dumps(study.system_attrs)  # type: ignore[no-untyped-call]
│ │ +        if study.system_attrs
│ │ +        else {},
│ │      }
│ │      return data
│ │  
│ │  
│ │  def _deserialize_frozenstudy(data: dict) -> FrozenStudy:
│ │      data["directions"] = [StudyDirection[d] for d in data["directions"]]
│ │      data["direction"] = None
│ │ -    data["system_attrs"] = loads(data["system_attrs"]) if data["system_attrs"] else {}
│ │ -    data["user_attrs"] = loads(data["user_attrs"]) if data["user_attrs"] else {}
│ │ +    data["system_attrs"] = (
│ │ +        loads(data["system_attrs"])  # type: ignore[no-untyped-call]
│ │ +        if data["system_attrs"]
│ │ +        else {}
│ │ +    )
│ │ +    data["user_attrs"] = (
│ │ +        loads(data["user_attrs"]) if data["user_attrs"] else {}  # type: ignore[no-untyped-call]
│ │ +    )
│ │      return FrozenStudy(**data)
│ │  
│ │  
│ │  class _OptunaSchedulerExtension:
│ │      def __init__(self, scheduler: "distributed.Scheduler"):
│ │          self.scheduler = scheduler
│ │          self.storages: Dict[str, BaseStorage] = {}
│ │  
│ │          methods = [
│ │              "create_new_study",
│ │              "delete_study",
│ │              "set_study_user_attr",
│ │              "set_study_system_attr",
│ │ -            "set_study_directions",
│ │              "get_study_id_from_name",
│ │              "get_study_name_from_id",
│ │              "get_study_directions",
│ │              "get_study_user_attrs",
│ │              "get_study_system_attrs",
│ │              "get_all_studies",
│ │              "create_new_trial",
│ │ @@ -119,17 +140,21 @@
│ │      def get_storage(self, name: str) -> BaseStorage:
│ │          return self.storages[name]
│ │  
│ │      def create_new_study(
│ │          self,
│ │          comm: "distributed.comm.tcp.TCP",
│ │          storage_name: str,
│ │ +        directions: List[str],
│ │          study_name: Optional[str] = None,
│ │      ) -> int:
│ │ -        return self.get_storage(storage_name).create_new_study(study_name=study_name)
│ │ +        return self.get_storage(storage_name).create_new_study(
│ │ +            directions=[StudyDirection[direction] for direction in directions],
│ │ +            study_name=study_name,
│ │ +        )
│ │  
│ │      def delete_study(
│ │          self,
│ │          comm: "distributed.comm.tcp.TCP",
│ │          storage_name: str,
│ │          study_id: int,
│ │      ) -> None:
│ │ @@ -140,41 +165,29 @@
│ │          comm: "distributed.comm.tcp.TCP",
│ │          storage_name: str,
│ │          study_id: int,
│ │          key: str,
│ │          value: Any,
│ │      ) -> None:
│ │          return self.get_storage(storage_name).set_study_user_attr(
│ │ -            study_id=study_id, key=key, value=loads(value)
│ │ +            study_id=study_id, key=key, value=loads(value)  # type: ignore[no-untyped-call]
│ │          )
│ │  
│ │      def set_study_system_attr(
│ │          self,
│ │          comm: "distributed.comm.tcp.TCP",
│ │          storage_name: str,
│ │          study_id: int,
│ │          key: str,
│ │          value: Any,
│ │      ) -> None:
│ │          return self.get_storage(storage_name).set_study_system_attr(
│ │              study_id=study_id,
│ │              key=key,
│ │ -            value=loads(value),
│ │ -        )
│ │ -
│ │ -    def set_study_directions(
│ │ -        self,
│ │ -        comm: "distributed.comm.tcp.TCP",
│ │ -        storage_name: str,
│ │ -        study_id: int,
│ │ -        directions: List[str],
│ │ -    ) -> None:
│ │ -        return self.get_storage(storage_name).set_study_directions(
│ │ -            study_id=study_id,
│ │ -            directions=[StudyDirection[direction] for direction in directions],
│ │ +            value=loads(value),  # type: ignore[no-untyped-call]
│ │          )
│ │  
│ │      def get_study_id_from_name(
│ │          self,
│ │          comm: "distributed.comm.tcp.TCP",
│ │          storage_name: str,
│ │          study_name: str,
│ │ @@ -200,23 +213,31 @@
│ │  
│ │      def get_study_user_attrs(
│ │          self,
│ │          comm: "distributed.comm.tcp.TCP",
│ │          storage_name: str,
│ │          study_id: int,
│ │      ) -> Dict[str, Any]:
│ │ -        return dumps(self.get_storage(storage_name).get_study_user_attrs(study_id=study_id))
│ │ +        return dumps(
│ │ +            self.get_storage(storage_name).get_study_user_attrs(  # type: ignore[no-untyped-call]
│ │ +                study_id=study_id
│ │ +            )
│ │ +        )
│ │  
│ │      def get_study_system_attrs(
│ │          self,
│ │          comm: "distributed.comm.tcp.TCP",
│ │          storage_name: str,
│ │          study_id: int,
│ │      ) -> Dict[str, Any]:
│ │ -        return dumps(self.get_storage(storage_name).get_study_system_attrs(study_id=study_id))
│ │ +        return dumps(
│ │ +            self.get_storage(storage_name).get_study_system_attrs(  # type: ignore[no-untyped-call]
│ │ +                study_id=study_id
│ │ +            )
│ │ +        )
│ │  
│ │      def get_all_studies(self, comm: "distributed.comm.tcp.TCP", storage_name: str) -> List[dict]:
│ │          studies = self.get_storage(storage_name).get_all_studies()
│ │          return [_serialize_frozenstudy(s) for s in studies]
│ │  
│ │      def create_new_trial(
│ │          self,
│ │ @@ -312,29 +333,29 @@
│ │          trial_id: int,
│ │          key: str,
│ │          value: Any,
│ │      ) -> None:
│ │          return self.get_storage(storage_name).set_trial_user_attr(
│ │              trial_id=trial_id,
│ │              key=key,
│ │ -            value=loads(value),
│ │ +            value=loads(value),  # type: ignore[no-untyped-call]
│ │          )
│ │  
│ │      def set_trial_system_attr(
│ │          self,
│ │          comm: "distributed.comm.tcp.TCP",
│ │          storage_name: str,
│ │          trial_id: int,
│ │          key: str,
│ │          value: Any,
│ │      ) -> None:
│ │          return self.get_storage(storage_name).set_trial_system_attr(
│ │              trial_id=trial_id,
│ │              key=key,
│ │ -            value=loads(value),
│ │ +            value=loads(value),  # type: ignore[no-untyped-call]
│ │          )
│ │  
│ │      def get_trial(
│ │          self,
│ │          comm: "distributed.comm.tcp.TCP",
│ │          storage_name: str,
│ │          trial_id: int,
│ │ @@ -410,15 +431,15 @@
│ │          storage:
│ │              Optuna storage url to use for underlying Optuna storage class to wrap
│ │              (e.g. :obj:`None` for in-memory storage, ``sqlite:///example.db``
│ │              for SQLite storage). Defaults to :obj:`None`.
│ │  
│ │          name:
│ │              Unique identifier for the Dask storage class. Specifying a custom name can sometimes
│ │ -            be useful for logging or debugging. If no name if provided provided,
│ │ +            be useful for logging or debugging. If :obj:`None` is provided,
│ │              a random name will be automatically generated.
│ │  
│ │          client:
│ │              Dask ``Client`` to connect to. If not provided, will attempt to find an
│ │              existing ``Client``.
│ │  
│ │      """
│ │ @@ -432,22 +453,24 @@
│ │          _imports.check()
│ │          self.name = name or f"dask-storage-{uuid.uuid4().hex}"
│ │          self.client = client or get_client()
│ │  
│ │          if self.client.asynchronous or getattr(thread_state, "on_event_loop_thread", False):
│ │  
│ │              async def _register() -> DaskStorage:
│ │ -                await self.client.run_on_scheduler(  # type: ignore
│ │ +                await self.client.run_on_scheduler(  # type: ignore[no-untyped-call]
│ │                      _register_with_scheduler, storage=storage, name=self.name
│ │                  )
│ │                  return self
│ │  
│ │              self._started = asyncio.ensure_future(_register())
│ │          else:
│ │ -            self.client.run_on_scheduler(_register_with_scheduler, storage=storage, name=self.name)
│ │ +            self.client.run_on_scheduler(  # type: ignore[no-untyped-call]
│ │ +                _register_with_scheduler, storage=storage, name=self.name
│ │ +            )
│ │  
│ │      def __await__(self) -> Generator[Any, None, "DaskStorage"]:
│ │          if hasattr(self, "_started"):
│ │              return self._started.__await__()
│ │          else:
│ │  
│ │              async def _() -> DaskStorage:
│ │ @@ -468,226 +491,225 @@
│ │          This is a convenience method to extract the Optuna storage instance stored on
│ │          the Dask scheduler process to the local Python process.
│ │          """
│ │  
│ │          def _get_base_storage(dask_scheduler: distributed.Scheduler, name: str) -> BaseStorage:
│ │              return dask_scheduler.extensions["optuna"].storages[name]
│ │  
│ │ -        return self.client.run_on_scheduler(_get_base_storage, name=self.name)
│ │ +        return self.client.run_on_scheduler(  # type: ignore[no-untyped-call]
│ │ +            _get_base_storage, name=self.name
│ │ +        )
│ │  
│ │ -    def create_new_study(self, study_name: Optional[str] = None) -> int:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_create_new_study,
│ │ +    def create_new_study(
│ │ +        self, directions: Sequence[StudyDirection], study_name: Optional[str] = None
│ │ +    ) -> int:
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_create_new_study,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              study_name=study_name,
│ │ +            directions=[direction.name for direction in directions],
│ │          )
│ │  
│ │      def delete_study(self, study_id: int) -> None:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_delete_study,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_delete_study,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              study_id=study_id,
│ │          )
│ │  
│ │      def set_study_user_attr(self, study_id: int, key: str, value: Any) -> None:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_set_study_user_attr,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_set_study_user_attr,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              study_id=study_id,
│ │              key=key,
│ │ -            value=dumps(value),
│ │ +            value=dumps(value),  # type: ignore[no-untyped-call]
│ │          )
│ │  
│ │      def set_study_system_attr(self, study_id: int, key: str, value: Any) -> None:
│ │          return self.client.sync(
│ │ -            self.client.scheduler.optuna_set_study_system_attr,
│ │ +            self.client.scheduler.optuna_set_study_system_attr,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              study_id=study_id,
│ │              key=key,
│ │ -            value=dumps(value),
│ │ -        )
│ │ -
│ │ -    def set_study_directions(self, study_id: int, directions: Sequence[StudyDirection]) -> None:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_set_study_directions,
│ │ -            storage_name=self.name,
│ │ -            study_id=study_id,
│ │ -            directions=[direction.name for direction in directions],
│ │ +            value=dumps(value),  # type: ignore[no-untyped-call]
│ │          )
│ │  
│ │      # Basic study access
│ │  
│ │      def get_study_id_from_name(self, study_name: str) -> int:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_get_study_id_from_name,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_get_study_id_from_name,  # type: ignore[union-attr]
│ │              study_name=study_name,
│ │              storage_name=self.name,
│ │          )
│ │  
│ │      def get_study_name_from_id(self, study_id: int) -> str:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_get_study_name_from_id,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_get_study_name_from_id,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              study_id=study_id,
│ │          )
│ │  
│ │      def get_study_directions(self, study_id: int) -> List[StudyDirection]:
│ │ -        directions = self.client.sync(
│ │ -            self.client.scheduler.optuna_get_study_directions,
│ │ +        directions = self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_get_study_directions,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              study_id=study_id,
│ │          )
│ │          return [StudyDirection[direction] for direction in directions]
│ │  
│ │      def get_study_user_attrs(self, study_id: int) -> Dict[str, Any]:
│ │ -        return loads(
│ │ -            self.client.sync(
│ │ -                self.client.scheduler.optuna_get_study_user_attrs,
│ │ +        return loads(  # type: ignore[no-untyped-call]
│ │ +            self.client.sync(  # type: ignore[no-untyped-call]
│ │ +                self.client.scheduler.optuna_get_study_user_attrs,  # type: ignore[union-attr]
│ │                  storage_name=self.name,
│ │                  study_id=study_id,
│ │              )
│ │          )
│ │  
│ │      def get_study_system_attrs(self, study_id: int) -> Dict[str, Any]:
│ │ -        return loads(
│ │ -            self.client.sync(
│ │ -                self.client.scheduler.optuna_get_study_system_attrs,
│ │ +        return loads(  # type: ignore[no-untyped-call]
│ │ +            self.client.sync(  # type: ignore[no-untyped-call]
│ │ +                self.client.scheduler.optuna_get_study_system_attrs,  # type: ignore[union-attr]
│ │                  storage_name=self.name,
│ │                  study_id=study_id,
│ │              )
│ │          )
│ │  
│ │      def get_all_studies(self) -> List[FrozenStudy]:
│ │ -        results = self.client.sync(
│ │ -            self.client.scheduler.optuna_get_all_studies,
│ │ +        results = self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_get_all_studies,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │          )
│ │          return [_deserialize_frozenstudy(i) for i in results]
│ │  
│ │      # Basic trial manipulation
│ │  
│ │      def create_new_trial(self, study_id: int, template_trial: Optional[FrozenTrial] = None) -> int:
│ │          serialized_template_trial = None
│ │          if template_trial is not None:
│ │              serialized_template_trial = _serialize_frozentrial(template_trial)
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_create_new_trial,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_create_new_trial,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              study_id=study_id,
│ │              template_trial=serialized_template_trial,
│ │          )
│ │  
│ │      def set_trial_param(
│ │          self,
│ │          trial_id: int,
│ │          param_name: str,
│ │          param_value_internal: float,
│ │          distribution: BaseDistribution,
│ │      ) -> None:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_set_trial_param,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_set_trial_param,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              trial_id=trial_id,
│ │              param_name=param_name,
│ │              param_value_internal=param_value_internal,
│ │              distribution=distribution_to_json(distribution),
│ │          )
│ │  
│ │      def get_trial_id_from_study_id_trial_number(self, study_id: int, trial_number: int) -> int:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_get_trial_id_from_study_id_trial_number,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_get_trial_id_from_study_id_trial_number,  # type: ignore[union-attr]  # NOQA: E501
│ │              storage_name=self.name,
│ │              study_id=study_id,
│ │              trial_number=trial_number,
│ │          )
│ │  
│ │      def get_trial_number_from_id(self, trial_id: int) -> int:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_get_trial_number_from_id,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_get_trial_number_from_id,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              trial_id=trial_id,
│ │          )
│ │  
│ │      def get_trial_param(self, trial_id: int, param_name: str) -> float:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_get_trial_param,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_get_trial_param,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              trial_id=trial_id,
│ │              param_name=param_name,
│ │          )
│ │  
│ │      def set_trial_state_values(
│ │          self, trial_id: int, state: TrialState, values: Optional[Sequence[float]] = None
│ │      ) -> bool:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_set_trial_state_values,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_set_trial_state_values,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              trial_id=trial_id,
│ │              state=state.name,
│ │              values=values,
│ │          )
│ │  
│ │      def set_trial_intermediate_value(
│ │          self, trial_id: int, step: int, intermediate_value: float
│ │      ) -> None:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_set_trial_intermediate_value,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_set_trial_intermediate_value,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              trial_id=trial_id,
│ │              step=step,
│ │              intermediate_value=intermediate_value,
│ │          )
│ │  
│ │      def set_trial_user_attr(self, trial_id: int, key: str, value: Any) -> None:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_set_trial_user_attr,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_set_trial_user_attr,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              trial_id=trial_id,
│ │              key=key,
│ │ -            value=dumps(value),
│ │ +            value=dumps(value),  # type: ignore[no-untyped-call]
│ │          )
│ │  
│ │      def set_trial_system_attr(self, trial_id: int, key: str, value: Any) -> None:
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_set_trial_system_attr,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_set_trial_system_attr,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              trial_id=trial_id,
│ │              key=key,
│ │ -            value=dumps(value),
│ │ +            value=dumps(value),  # type: ignore[no-untyped-call]
│ │          )
│ │  
│ │      # Basic trial access
│ │  
│ │      async def _get_trial(self, trial_id: int) -> FrozenTrial:
│ │ -        serialized_trial = await self.client.scheduler.optuna_get_trial(
│ │ +        serialized_trial = await self.client.scheduler.optuna_get_trial(  # type: ignore[union-attr]  # NOQA: E501
│ │              trial_id=trial_id, storage_name=self.name
│ │          )
│ │          return _deserialize_frozentrial(serialized_trial)
│ │  
│ │      def get_trial(self, trial_id: int) -> FrozenTrial:
│ │ -        return self.client.sync(self._get_trial, trial_id=trial_id)
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self._get_trial, trial_id=trial_id
│ │ +        )
│ │  
│ │      async def _get_all_trials(
│ │          self, study_id: int, deepcopy: bool = True, states: Optional[Iterable[TrialState]] = None
│ │      ) -> List[FrozenTrial]:
│ │          serialized_states = None
│ │          if states is not None:
│ │              serialized_states = tuple(s.name for s in states)
│ │ -        serialized_trials = await self.client.scheduler.optuna_get_all_trials(
│ │ +        serialized_trials = await self.client.scheduler.optuna_get_all_trials(  # type: ignore[union-attr]  # NOQA: E501
│ │              storage_name=self.name,
│ │              study_id=study_id,
│ │              deepcopy=deepcopy,
│ │              states=serialized_states,
│ │          )
│ │          return [_deserialize_frozentrial(t) for t in serialized_trials]
│ │  
│ │      def get_all_trials(
│ │          self, study_id: int, deepcopy: bool = True, states: Optional[Container[TrialState]] = None
│ │      ) -> List[FrozenTrial]:
│ │ -        return self.client.sync(
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │              self._get_all_trials,
│ │              study_id=study_id,
│ │              deepcopy=deepcopy,
│ │              states=states,
│ │          )
│ │  
│ │      def get_n_trials(
│ │ @@ -695,13 +717,13 @@
│ │      ) -> int:
│ │          serialized_state: Optional[Union[Tuple[str, ...], str]] = None
│ │          if state is not None:
│ │              if isinstance(state, TrialState):
│ │                  serialized_state = state.name
│ │              else:
│ │                  serialized_state = tuple(s.name for s in state)
│ │ -        return self.client.sync(
│ │ -            self.client.scheduler.optuna_get_n_trials,
│ │ +        return self.client.sync(  # type: ignore[no-untyped-call]
│ │ +            self.client.scheduler.optuna_get_n_trials,  # type: ignore[union-attr]
│ │              storage_name=self.name,
│ │              study_id=study_id,
│ │              state=serialized_state,
│ │          )
│ │   --- optuna-3.1.0b0/optuna/integration/fastaiv1.py
│ ├── +++ optuna-3.1.1/optuna/integration/fastaiv1.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/fastaiv2.py
│ ├── +++ optuna-3.1.1/optuna/integration/fastaiv2.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/keras.py
│ ├── +++ optuna-3.1.1/optuna/integration/keras.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/lightgbm.py
│ ├── +++ optuna-3.1.1/optuna/integration/lightgbm.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/mlflow.py
│ ├── +++ optuna-3.1.1/optuna/integration/mlflow.py
│ │┄ Files 1% similar despite different names
│ │ @@ -198,15 +198,15 @@
│ │                      return (x - 2) ** 2
│ │  
│ │  
│ │                  study = optuna.create_study(study_name="my_other_study")
│ │                  study.optimize(objective, n_trials=10, callbacks=[mlflc])
│ │  
│ │          Returns:
│ │ -            ObjectiveFuncType: Objective function with tracking to MLflow enabled.
│ │ +            Objective function with tracking to MLflow enabled.
│ │          """
│ │  
│ │          def decorator(func: ObjectiveFuncType) -> ObjectiveFuncType:
│ │              @functools.wraps(func)
│ │              def wrapper(trial: optuna.trial.Trial) -> Union[float, Sequence[float]]:
│ │                  with self._lock:
│ │                      study = trial.study
│ │   --- optuna-3.1.0b0/optuna/integration/mxnet.py
│ ├── +++ optuna-3.1.1/optuna/integration/mxnet.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/pytorch_distributed.py
│ ├── +++ optuna-3.1.1/optuna/integration/pytorch_distributed.py
│ │┄ Files 24% similar despite different names
│ │ @@ -1,14 +1,16 @@
│ │  from datetime import datetime
│ │  import functools
│ │  import pickle
│ │  from typing import Any
│ │  from typing import Callable
│ │  from typing import Dict
│ │ +from typing import List
│ │  from typing import Optional
│ │ +from typing import overload
│ │  from typing import Sequence
│ │  from typing import TYPE_CHECKING
│ │  from typing import TypeVar
│ │  
│ │  import optuna
│ │  from optuna._deprecated import deprecated_func
│ │  from optuna._experimental import experimental_class
│ │ @@ -16,42 +18,45 @@
│ │  from optuna.distributions import BaseDistribution
│ │  from optuna.distributions import CategoricalChoiceType
│ │  
│ │  
│ │  with try_import() as _imports:
│ │      import torch
│ │      import torch.distributed as dist
│ │ +    from torch.distributed import ProcessGroup
│ │  
│ │  
│ │  if TYPE_CHECKING:
│ │      from typing_extensions import ParamSpec
│ │  
│ │      _T = TypeVar("_T")
│ │      _P = ParamSpec("_P")
│ │  
│ │  
│ │  _suggest_deprecated_msg = (
│ │      "Use :func:`~optuna.integration.TorchDistributedTrial.suggest_float` instead."
│ │  )
│ │  
│ │ +_g_pg: List[Optional["ProcessGroup"]] = [None]
│ │ +
│ │  
│ │  def broadcast_properties(f: "Callable[_P, _T]") -> "Callable[_P, _T]":
│ │      """Method decorator to fetch updated trial properties from rank 0 after ``f`` is run.
│ │  
│ │      This decorator ensures trial properties (params, distributions, etc.) on all distributed
│ │      processes are up-to-date with the wrapped trial stored on rank 0.
│ │      It should be applied to all :class:`~optuna.integration.TorchDistributedTrial`
│ │      methods that update property values.
│ │      """
│ │  
│ │      @functools.wraps(f)
│ │      def wrapped(*args: "_P.args", **kwargs: "_P.kwargs") -> "_T":
│ │          # TODO(nlgranger): Remove type ignore after mypy includes
│ │          # https://github.com/python/mypy/pull/12668
│ │ -        self: TorchDistributedTrial = args[0]  # type: ignore
│ │ +        self: TorchDistributedTrial = args[0]  # type: ignore[assignment]
│ │  
│ │          def fetch_properties() -> Sequence:
│ │              assert self._delegate is not None
│ │              return (
│ │                  self._delegate.number,
│ │                  self._delegate.params,
│ │                  self._delegate.distributions,
│ │ @@ -89,45 +94,65 @@
│ │      if you want to optimize an objective function that trains neural network
│ │      written with PyTorch distributed data parallel.
│ │  
│ │      Args:
│ │          trial:
│ │              A :class:`~optuna.trial.Trial` object or :obj:`None`. Please set trial object in
│ │              rank-0 node and set :obj:`None` in the other rank node.
│ │ -        device:
│ │ -            A `torch.device` to communicate with the other nodes. Please set a CUDA device
│ │ -            assigned to the current node if you use "nccl" as `torch.distributed` backend.
│ │ +        group:
│ │ +            A `torch.distributed.ProcessGroup` to communicate with the other nodes.
│ │ +            TorchDistributedTrial use CPU tensors to communicate, make sure the group
│ │ +            supports CPU tensors communications.
│ │ +
│ │ +            Use `gloo` backend when group is None.
│ │ +            Create a global `gloo` backend when group is None and WORLD is nccl.
│ │  
│ │      .. note::
│ │          The methods of :class:`~optuna.integration.TorchDistributedTrial` are expected to be
│ │          called by all workers at once. They invoke synchronous data transmission to share
│ │          processing results and synchronize timing.
│ │  
│ │      """
│ │  
│ │      def __init__(
│ │ -        self, trial: Optional[optuna.trial.Trial], device: Optional["torch.device"] = None
│ │ +        self,
│ │ +        trial: Optional[optuna.trial.Trial],
│ │ +        group: Optional["ProcessGroup"] = None,
│ │      ) -> None:
│ │ -
│ │          _imports.check()
│ │  
│ │ -        if dist.get_rank() == 0:  # type: ignore
│ │ +        if group is not None:
│ │ +            self._group: "ProcessGroup" = group
│ │ +        else:
│ │ +            if _g_pg[0] is None:
│ │ +                if dist.group.WORLD is None:
│ │ +                    raise RuntimeError("torch distributed is not initialized.")
│ │ +                default_pg: "ProcessGroup" = dist.group.WORLD
│ │ +                if dist.get_backend(default_pg) == "nccl":  # type: ignore[no-untyped-call]
│ │ +                    new_group: "ProcessGroup" = dist.new_group(  # type: ignore[no-untyped-call]
│ │ +                        backend="gloo"
│ │ +                    )
│ │ +                    _g_pg[0] = new_group
│ │ +                else:
│ │ +                    _g_pg[0] = default_pg
│ │ +            self._group = _g_pg[0]
│ │ +
│ │ +        if dist.get_rank(self._group) == 0:  # type: ignore[no-untyped-call]
│ │              if not isinstance(trial, optuna.trial.Trial):
│ │                  raise ValueError(
│ │                      "Rank 0 node expects an optuna.trial.Trial instance as the trial argument."
│ │                  )
│ │          else:
│ │              if trial is not None:
│ │                  raise ValueError(
│ │ -                    "Non-rank 0 node is supposed to recieve None as the trial argument."
│ │ +                    "Non-rank 0 node is supposed to receive None as the trial argument."
│ │                  )
│ │  
│ │              assert trial is None, "error message"
│ │          self._delegate = trial
│ │ -        self._device = device
│ │  
│ │          self._number = self._broadcast(getattr(self._delegate, "number", None))
│ │          self._params = self._broadcast(getattr(self._delegate, "params", None))
│ │          self._distributions = self._broadcast(getattr(self._delegate, "distributions", None))
│ │          self._user_attrs = self._broadcast(getattr(self._delegate, "user_attrs", None))
│ │          self._system_attrs = self._broadcast(getattr(self._delegate, "system_attrs", None))
│ │          self._datetime_start = self._broadcast(getattr(self._delegate, "datetime_start", None))
│ │ @@ -139,15 +164,14 @@
│ │          low: float,
│ │          high: float,
│ │          *,
│ │          step: Optional[float] = None,
│ │          log: bool = False,
│ │      ) -> float:
│ │          def func() -> float:
│ │ -
│ │              assert self._delegate is not None
│ │              return self._delegate.suggest_float(name, low, high, step=step, log=log)
│ │  
│ │          return self._call_and_communicate(func, torch.float)
│ │  
│ │      @deprecated_func("3.0.0", "6.0.0", text=_suggest_deprecated_msg)
│ │      def suggest_uniform(self, name: str, low: float, high: float) -> float:
│ │ @@ -163,33 +187,59 @@
│ │      def suggest_discrete_uniform(self, name: str, low: float, high: float, q: float) -> float:
│ │  
│ │          return self.suggest_float(name, low, high, step=q)
│ │  
│ │      @broadcast_properties
│ │      def suggest_int(self, name: str, low: int, high: int, step: int = 1, log: bool = False) -> int:
│ │          def func() -> float:
│ │ -
│ │              assert self._delegate is not None
│ │              return self._delegate.suggest_int(name, low, high, step=step, log=log)
│ │  
│ │          return self._call_and_communicate(func, torch.int)
│ │  
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[None]) -> None:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[bool]) -> bool:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[int]) -> int:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[float]) -> float:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[str]) -> str:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(
│ │ +        self, name: str, choices: Sequence[CategoricalChoiceType]
│ │ +    ) -> CategoricalChoiceType:
│ │ +        ...
│ │ +
│ │      @broadcast_properties
│ │ -    def suggest_categorical(self, name: str, choices: Sequence["CategoricalChoiceType"]) -> Any:
│ │ +    def suggest_categorical(
│ │ +        self, name: str, choices: Sequence[CategoricalChoiceType]
│ │ +    ) -> CategoricalChoiceType:
│ │          def func() -> CategoricalChoiceType:
│ │ -
│ │              assert self._delegate is not None
│ │              return self._delegate.suggest_categorical(name, choices)
│ │  
│ │          return self._call_and_communicate_obj(func)
│ │  
│ │      @broadcast_properties
│ │      def report(self, value: float, step: int) -> None:
│ │          err = None
│ │ -        if dist.get_rank() == 0:  # type: ignore
│ │ +        if dist.get_rank(self._group) == 0:  # type: ignore[no-untyped-call]
│ │              try:
│ │                  assert self._delegate is not None
│ │                  self._delegate.report(value, step)
│ │              except Exception as e:
│ │                  err = e
│ │              err = self._broadcast(err)
│ │          else:
│ │ @@ -197,27 +247,26 @@
│ │  
│ │          if err is not None:
│ │              raise err
│ │  
│ │      @broadcast_properties
│ │      def should_prune(self) -> bool:
│ │          def func() -> bool:
│ │ -
│ │              assert self._delegate is not None
│ │              # Some pruners return numpy.bool_, which is incompatible with bool.
│ │              return bool(self._delegate.should_prune())
│ │  
│ │          # torch.bool seems to be the correct type, but the communication fails
│ │          # due to the RuntimeError.
│ │          return self._call_and_communicate(func, torch.uint8)
│ │  
│ │      @broadcast_properties
│ │      def set_user_attr(self, key: str, value: Any) -> None:
│ │          err = None
│ │ -        if dist.get_rank() == 0:  # type: ignore
│ │ +        if dist.get_rank(self._group) == 0:  # type: ignore[no-untyped-call]
│ │              try:
│ │                  assert self._delegate is not None
│ │                  self._delegate.set_user_attr(key, value)
│ │              except Exception as e:
│ │                  err = e
│ │              err = self._broadcast(err)
│ │          else:
│ │ @@ -227,15 +276,15 @@
│ │              raise err
│ │  
│ │      @broadcast_properties
│ │      @deprecated_func("3.1.0", "6.0.0")
│ │      def set_system_attr(self, key: str, value: Any) -> None:
│ │          err = None
│ │  
│ │ -        if dist.get_rank() == 0:  # type: ignore
│ │ +        if dist.get_rank(self._group) == 0:  # type: ignore[no-untyped-call]
│ │              try:
│ │                  assert self._delegate is not None
│ │                  self._delegate.storage.set_trial_system_attr(self._delegate._trial_id, key, value)
│ │              except Exception as e:
│ │                  err = e
│ │              err = self._broadcast(err)
│ │          else:
│ │ @@ -257,54 +306,49 @@
│ │          return self._distributions
│ │  
│ │      @property
│ │      def user_attrs(self) -> Dict[str, Any]:
│ │          return self._user_attrs
│ │  
│ │      @property
│ │ +    @deprecated_func("3.1.0", "6.0.0")
│ │      def system_attrs(self) -> Dict[str, Any]:
│ │          return self._system_attrs
│ │  
│ │      @property
│ │      def datetime_start(self) -> Optional[datetime]:
│ │          return self._datetime_start
│ │  
│ │      def _call_and_communicate(self, func: Callable, dtype: "torch.dtype") -> Any:
│ │          buffer = torch.empty(1, dtype=dtype)
│ │ -        rank = dist.get_rank()  # type: ignore
│ │ +        rank = dist.get_rank(self._group)  # type: ignore[no-untyped-call]
│ │          if rank == 0:
│ │              result = func()
│ │              buffer[0] = result
│ │ -        if self._device is not None:
│ │ -            buffer = buffer.to(self._device)
│ │ -        dist.broadcast(buffer, src=0)  # type: ignore
│ │ +        dist.broadcast(buffer, src=0, group=self._group)  # type: ignore[no-untyped-call]
│ │          return buffer.item()
│ │  
│ │      def _call_and_communicate_obj(self, func: Callable) -> Any:
│ │ -        rank = dist.get_rank()  # type: ignore
│ │ +        rank = dist.get_rank(self._group)  # type: ignore[no-untyped-call]
│ │          result = func() if rank == 0 else None
│ │          return self._broadcast(result)
│ │  
│ │      def _broadcast(self, value: Optional[Any]) -> Any:
│ │          buffer = None
│ │          size_buffer = torch.empty(1, dtype=torch.int)
│ │ -        rank = dist.get_rank()  # type: ignore
│ │ +        rank = dist.get_rank(self._group)  # type: ignore[no-untyped-call]
│ │          if rank == 0:
│ │              buffer = _to_tensor(value)
│ │              size_buffer[0] = buffer.shape[0]
│ │ -        if self._device is not None:
│ │ -            size_buffer = size_buffer.to(self._device)
│ │ -        dist.broadcast(size_buffer, src=0)  # type: ignore
│ │ +        dist.broadcast(size_buffer, src=0, group=self._group)  # type: ignore[no-untyped-call]
│ │          buffer_size = int(size_buffer.item())
│ │          if rank != 0:
│ │              buffer = torch.empty(buffer_size, dtype=torch.uint8)
│ │          assert buffer is not None
│ │ -        if self._device is not None:
│ │ -            buffer = buffer.to(self._device)
│ │ -        dist.broadcast(buffer, src=0)  # type: ignore
│ │ +        dist.broadcast(buffer, src=0, group=self._group)  # type: ignore[no-untyped-call]
│ │          return _from_tensor(buffer)
│ │  
│ │  
│ │  def _to_tensor(obj: Any) -> "torch.Tensor":
│ │      b = bytearray(pickle.dumps(obj))
│ │      return torch.tensor(b, dtype=torch.uint8)
│ │   --- optuna-3.1.0b0/optuna/integration/pytorch_ignite.py
│ ├── +++ optuna-3.1.1/optuna/integration/pytorch_ignite.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/pytorch_lightning.py
│ ├── +++ optuna-3.1.1/optuna/integration/pytorch_lightning.py
│ │┄ Files 2% similar despite different names
│ │ @@ -115,16 +115,17 @@
│ │              return
│ │  
│ │          # Because on_validation_end is executed in spawned processes,
│ │          # _trial.report is necessary to update the memory in main process, not to update the RDB.
│ │          _trial_id = self._trial._trial_id
│ │          _study = self._trial.study
│ │          _trial = _study._storage._backend.get_trial(_trial_id)  # type: ignore
│ │ -        is_pruned = _trial.system_attrs.get(_PRUNED_KEY)
│ │ -        epoch = _trial.system_attrs.get(_EPOCH_KEY)
│ │ +        _trial_system_attrs = _study._storage.get_trial_system_attrs(_trial_id)
│ │ +        is_pruned = _trial_system_attrs.get(_PRUNED_KEY)
│ │ +        epoch = _trial_system_attrs.get(_EPOCH_KEY)
│ │          intermediate_values = _trial.intermediate_values
│ │          for step, value in intermediate_values.items():
│ │              self._trial.report(value, step=step)
│ │  
│ │          if is_pruned:
│ │              message = "Trial was pruned at epoch {}.".format(epoch)
│ │              raise optuna.TrialPruned(message)
│ │   --- optuna-3.1.0b0/optuna/integration/shap.py
│ ├── +++ optuna-3.1.1/optuna/integration/shap.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/sklearn.py
│ ├── +++ optuna-3.1.1/optuna/integration/sklearn.py
│ │┄ Files 2% similar despite different names
│ │ @@ -10,16 +10,14 @@
│ │  from typing import Iterable
│ │  from typing import List
│ │  from typing import Mapping
│ │  from typing import Optional
│ │  from typing import Union
│ │  
│ │  import numpy as np
│ │ -import scipy as sp
│ │ -from scipy.sparse import spmatrix
│ │  
│ │  from optuna import distributions
│ │  from optuna import logging
│ │  from optuna import samplers
│ │  from optuna import study as study_module
│ │  from optuna import TrialPruned
│ │  from optuna._experimental import experimental_class
│ │ @@ -28,14 +26,16 @@
│ │  from optuna.study import StudyDirection
│ │  from optuna.trial import FrozenTrial
│ │  from optuna.trial import Trial
│ │  
│ │  
│ │  with try_import() as _imports:
│ │      import pandas as pd
│ │ +    import scipy as sp
│ │ +    from scipy.sparse import spmatrix
│ │      import sklearn
│ │      from sklearn.base import BaseEstimator
│ │      from sklearn.base import clone
│ │      from sklearn.base import is_classifier
│ │      from sklearn.metrics import check_scoring
│ │      from sklearn.model_selection import BaseCrossValidator
│ │      from sklearn.model_selection import check_cv
│ │ @@ -376,15 +376,15 @@
│ │          cv:
│ │              Cross-validation strategy. Possible inputs for cv are:
│ │  
│ │              - integer to specify the number of folds in a CV splitter,
│ │              - a CV splitter,
│ │              - an iterable yielding (train, validation) splits as arrays of indices.
│ │  
│ │ -            For integer, if :obj:`estimator` is a classifier and :obj:`y` is
│ │ +            For integer, if ``estimator`` is a classifier and ``y`` is
│ │              either binary or multiclass,
│ │              ``sklearn.model_selection.StratifiedKFold`` is used. otherwise,
│ │              ``sklearn.model_selection.KFold`` is used.
│ │  
│ │          enable_pruning:
│ │              If :obj:`True`, pruning is performed in the case where the
│ │              underlying estimator supports ``partial_fit``.
│ │ @@ -396,26 +396,26 @@
│ │              affect the refit step, which will always raise the error.
│ │  
│ │          max_iter:
│ │              Maximum number of epochs. This is only used if the underlying
│ │              estimator supports ``partial_fit``.
│ │  
│ │          n_jobs:
│ │ -            Number of :obj:`threading` based parallel jobs. :obj:`-1` means
│ │ +            Number of :obj:`threading` based parallel jobs. ``-1`` means
│ │              using the number is set to CPU count.
│ │  
│ │                  .. note::
│ │                      ``n_jobs`` allows parallelization using :obj:`threading` and may suffer from
│ │                      `Python's GIL <https://wiki.python.org/moin/GlobalInterpreterLock>`_.
│ │                      It is recommended to use :ref:`process-based parallelization<distributed>`
│ │                      if ``func`` is CPU bound.
│ │  
│ │          n_trials:
│ │              Number of trials. If :obj:`None`, there is no limitation on the
│ │ -            number of trials. If :obj:`timeout` is also set to :obj:`None`,
│ │ +            number of trials. If ``timeout`` is also set to :obj:`None`,
│ │              the study continues to create trials until it receives a
│ │              termination signal such as Ctrl+C or SIGTERM. This trades off
│ │              runtime vs quality of the solution.
│ │  
│ │          random_state:
│ │              Seed of the pseudo random number generator. If int, this is the
│ │              seed used by the random number generator. If
│ │ @@ -451,15 +451,15 @@
│ │  
│ │              - If int, then draw ``subsample`` samples.
│ │              - If float, then draw ``subsample`` * ``X.shape[0]`` samples.
│ │  
│ │          timeout:
│ │              Time limit in seconds for the search of appropriate models. If
│ │              :obj:`None`, the study is executed without time limitation. If
│ │ -            :obj:`n_trials` is also set to :obj:`None`, the study continues to
│ │ +            ``n_trials`` is also set to :obj:`None`, the study continues to
│ │              create trials until it receives a termination signal such as
│ │              Ctrl+C or SIGTERM. This trades off runtime vs quality of the
│ │              solution.
│ │  
│ │          verbose:
│ │              Verbosity level. The higher, the more messages.
│ │  
│ │ @@ -821,16 +821,15 @@
│ │                  Group labels for the samples used while splitting the dataset
│ │                  into train/validation set.
│ │  
│ │              **fit_params:
│ │                  Parameters passed to ``fit`` on the estimator.
│ │  
│ │          Returns:
│ │ -            self:
│ │ -                Return self.
│ │ +            self.
│ │          """
│ │  
│ │          self._check_params()
│ │  
│ │          random_state = check_random_state(self.random_state)
│ │          max_samples = self.subsample
│ │          n_samples = _num_samples(X)
│ │ @@ -926,12 +925,11 @@
│ │              X:
│ │                  Data.
│ │  
│ │              y:
│ │                  Target variable.
│ │  
│ │          Returns:
│ │ -            score:
│ │ -                Scaler score.
│ │ +            Scaler score.
│ │          """
│ │  
│ │          return self.scorer_(self.best_estimator_, X, y)
│ │   --- optuna-3.1.0b0/optuna/integration/skopt.py
│ ├── +++ optuna-3.1.1/optuna/integration/skopt.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/skorch.py
│ ├── +++ optuna-3.1.1/optuna/integration/skorch.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/tensorboard.py
│ ├── +++ optuna-3.1.1/optuna/integration/tensorboard.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/tensorflow.py
│ ├── +++ optuna-3.1.1/optuna/integration/tensorflow.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/tfkeras.py
│ ├── +++ optuna-3.1.1/optuna/integration/tfkeras.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/integration/wandb.py
│ ├── +++ optuna-3.1.1/optuna/integration/wandb.py
│ │┄ Files 2% similar despite different names
│ │ @@ -209,15 +209,15 @@
│ │  
│ │  
│ │                  study = optuna.create_study()
│ │                  study.optimize(objective, n_trials=10, callbacks=[wandbc])
│ │  
│ │  
│ │          Returns:
│ │ -            ObjectiveFuncType: Objective function with W&B tracking enabled.
│ │ +            Objective function with W&B tracking enabled.
│ │          """
│ │  
│ │          def decorator(func: ObjectiveFuncType) -> ObjectiveFuncType:
│ │              @functools.wraps(func)
│ │              def wrapper(trial: optuna.trial.Trial) -> Union[float, Sequence[float]]:
│ │  
│ │                  run = wandb.run  # Uses global run when `as_multirun` is set to False.
│ │   --- optuna-3.1.0b0/optuna/integration/xgboost.py
│ ├── +++ optuna-3.1.1/optuna/integration/xgboost.py
│ │┄ Files 1% similar despite different names
│ │ @@ -56,15 +56,15 @@
│ │              evaluation_results = {}
│ │              # Flatten the evaluation history to `{dataset-metric: score}` layout.
│ │              for dataset, metrics in evals_log.items():
│ │                  for metric, scores in metrics.items():
│ │                      assert isinstance(scores, list), scores
│ │                      key = dataset + "-" + metric
│ │                      if self._is_cv:
│ │ -                        # Remove stddev of the metric across the cross-valdation
│ │ +                        # Remove stddev of the metric across the cross-validation
│ │                          # folds.
│ │                          evaluation_results[key] = scores[-1][0]
│ │                      else:
│ │                          evaluation_results[key] = scores[-1]
│ │  
│ │              current_score = evaluation_results[self._observation_key]
│ │              self._trial.report(current_score, step=epoch)
│ │ @@ -99,15 +99,15 @@
│ │  
│ │          def __call__(self, env: "xgb.core.CallbackEnv") -> None:  # type: ignore
│ │  
│ │              context = _get_callback_context(env)
│ │              evaluation_result_list = env.evaluation_result_list
│ │              if context == "cv":
│ │                  # Remove a third element: the stddev of the metric across the
│ │ -                # cross-valdation folds.
│ │ +                # cross-validation folds.
│ │                  evaluation_result_list = [
│ │                      (key, metric) for key, metric, _ in evaluation_result_list
│ │                  ]
│ │              current_score = dict(evaluation_result_list)[self._observation_key]
│ │              self._trial.report(current_score, step=env.iteration)
│ │              if self._trial.should_prune():
│ │                  message = "Trial was pruned at iteration {}.".format(env.iteration)
│ │   --- optuna-3.1.0b0/optuna/logging.py
│ ├── +++ optuna-3.1.1/optuna/logging.py
│ │┄ Files 1% similar despite different names
│ │ @@ -245,15 +245,15 @@
│ │      _get_library_root_logger().addHandler(_default_handler)
│ │  
│ │  
│ │  def disable_propagation() -> None:
│ │      """Disable propagation of the library log outputs.
│ │  
│ │      Note that log propagation is disabled by default. You only need to use this function
│ │ -    to stop log propagation when you use :func:`~optuna.logging.enable_propogation()`.
│ │ +    to stop log propagation when you use :func:`~optuna.logging.enable_propagation()`.
│ │  
│ │      Example:
│ │  
│ │          Stop propagating logs to the root logger on the second optimize call.
│ │  
│ │          .. testsetup::
│ │   --- optuna-3.1.0b0/optuna/multi_objective/samplers/__init__.py
│ ├── +++ optuna-3.1.1/optuna/multi_objective/samplers/__init__.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/multi_objective/samplers/_adapter.py
│ ├── +++ optuna-3.1.1/optuna/multi_objective/samplers/_adapter.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/multi_objective/samplers/_base.py
│ ├── +++ optuna-3.1.1/optuna/multi_objective/samplers/_base.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/multi_objective/samplers/_motpe.py
│ ├── +++ optuna-3.1.1/optuna/multi_objective/samplers/_motpe.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/multi_objective/samplers/_nsga2.py
│ ├── +++ optuna-3.1.1/optuna/multi_objective/samplers/_nsga2.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/multi_objective/samplers/_random.py
│ ├── +++ optuna-3.1.1/optuna/multi_objective/samplers/_random.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/multi_objective/study.py
│ ├── +++ optuna-3.1.1/optuna/multi_objective/study.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/multi_objective/trial.py
│ ├── +++ optuna-3.1.1/optuna/multi_objective/trial.py
│ │┄ Files 4% similar despite different names
│ │ @@ -1,12 +1,13 @@
│ │  from datetime import datetime
│ │  from typing import Any
│ │  from typing import Dict
│ │  from typing import List
│ │  from typing import Optional
│ │ +from typing import overload
│ │  from typing import Sequence
│ │  from typing import Union
│ │  
│ │  from optuna import multi_objective
│ │  from optuna._deprecated import deprecated_class
│ │  from optuna.distributions import BaseDistribution
│ │  from optuna.study._study_direction import StudyDirection
│ │ @@ -91,14 +92,40 @@
│ │  
│ │          Please refer to the documentation of :func:`optuna.trial.Trial.suggest_int`
│ │          for further details.
│ │          """
│ │  
│ │          return self._trial.suggest_int(name, low, high, step=step, log=log)
│ │  
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[None]) -> None:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[bool]) -> bool:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[int]) -> int:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[float]) -> float:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[str]) -> str:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(
│ │ +        self, name: str, choices: Sequence[CategoricalChoiceType]
│ │ +    ) -> CategoricalChoiceType:
│ │ +        ...
│ │ +
│ │      def suggest_categorical(
│ │          self, name: str, choices: Sequence[CategoricalChoiceType]
│ │      ) -> CategoricalChoiceType:
│ │          """Suggest a value for the categorical parameter.
│ │  
│ │          Please refer to the documentation of :func:`optuna.trial.Trial.suggest_categorical`
│ │          for further details.
│ │   --- optuna-3.1.0b0/optuna/multi_objective/visualization/_pareto_front.py
│ ├── +++ optuna-3.1.1/optuna/multi_objective/visualization/_pareto_front.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/progress_bar.py
│ ├── +++ optuna-3.1.1/optuna/progress_bar.py
│ │┄ Files 8% similar despite different names
│ │ @@ -1,17 +1,21 @@
│ │  import logging
│ │  from typing import Any
│ │  from typing import Optional
│ │ +from typing import TYPE_CHECKING
│ │  
│ │  from tqdm.auto import tqdm
│ │  
│ │  from optuna import logging as optuna_logging
│ │  from optuna._experimental import experimental_func
│ │  
│ │  
│ │ +if TYPE_CHECKING:
│ │ +    from optuna.study import Study
│ │ +
│ │  _tqdm_handler: Optional["_TqdmLoggingHandler"] = None
│ │  
│ │  
│ │  # Reference: https://gist.github.com/hvy/8b80c2cedf02b15c24f85d1fa17ebe02
│ │  class _TqdmLoggingHandler(logging.StreamHandler):
│ │      def emit(self, record: Any) -> None:
│ │          try:
│ │ @@ -33,15 +37,18 @@
│ │          n_trials:
│ │              The number of trials.
│ │          timeout:
│ │              Stop study after the given number of second(s).
│ │      """
│ │  
│ │      def __init__(
│ │ -        self, is_valid: bool, n_trials: Optional[int] = None, timeout: Optional[float] = None
│ │ +        self,
│ │ +        is_valid: bool,
│ │ +        n_trials: Optional[int] = None,
│ │ +        timeout: Optional[float] = None,
│ │      ) -> None:
│ │  
│ │          self._is_valid = is_valid and (n_trials or timeout) is not None
│ │          self._n_trials = n_trials
│ │          self._timeout = timeout
│ │          self._last_elapsed_seconds = 0.0
│ │  
│ │ @@ -53,41 +60,49 @@
│ │      @experimental_func("1.2.0", name="Progress bar")
│ │      def _init_valid(self) -> None:
│ │  
│ │          if self._n_trials is not None:
│ │              self._progress_bar = tqdm(total=self._n_trials)
│ │  
│ │          else:
│ │ -            fmt = "{percentage:3.0f}%|{bar}| {elapsed}/{desc}"
│ │ -            self._progress_bar = tqdm(total=self._timeout, bar_format=fmt)
│ │ -
│ │ -            # Using description string instead postfix string
│ │ -            # to display formatted timeout, since postfix carries
│ │ -            # extra comma space auto-format.
│ │ -            # https://github.com/tqdm/tqdm/issues/712
│ │              total = tqdm.format_interval(self._timeout)
│ │ -            self._progress_bar.set_description_str(total)
│ │ +            fmt = "{desc} {percentage:3.0f}%|{bar}| {elapsed}/" + total
│ │ +            self._progress_bar = tqdm(total=self._timeout, bar_format=fmt)
│ │  
│ │          global _tqdm_handler
│ │  
│ │          _tqdm_handler = _TqdmLoggingHandler()
│ │          _tqdm_handler.setLevel(logging.INFO)
│ │          _tqdm_handler.setFormatter(optuna_logging.create_default_formatter())
│ │          optuna_logging.disable_default_handler()
│ │          optuna_logging._get_library_root_logger().addHandler(_tqdm_handler)
│ │  
│ │ -    def update(self, elapsed_seconds: float) -> None:
│ │ +    def update(self, elapsed_seconds: float, study: "Study") -> None:
│ │          """Update the progress bars if ``is_valid`` is :obj:`True`.
│ │  
│ │          Args:
│ │              elapsed_seconds:
│ │                  The time past since :func:`~optuna.study.Study.optimize` started.
│ │ +            study:
│ │ +                The current study object.
│ │          """
│ │  
│ │          if self._is_valid:
│ │ +            if not study._is_multi_objective():
│ │ +                # Not updating the progress bar when there are no complete trial.
│ │ +                try:
│ │ +                    msg = (
│ │ +                        f"Best trial: {study.best_trial.number}. "
│ │ +                        f"Best value: {study.best_value:.6g}"
│ │ +                    )
│ │ +
│ │ +                    self._progress_bar.set_description(msg)
│ │ +                except ValueError:
│ │ +                    pass
│ │ +
│ │              if self._n_trials is not None:
│ │                  self._progress_bar.update(1)
│ │                  if self._timeout is not None:
│ │                      self._progress_bar.set_postfix_str(
│ │                          "{:.02f}/{} seconds".format(elapsed_seconds, self._timeout)
│ │                      )
│ │   --- optuna-3.1.0b0/optuna/pruners/__init__.py
│ ├── +++ optuna-3.1.1/optuna/pruners/__init__.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/pruners/_base.py
│ ├── +++ optuna-3.1.1/optuna/pruners/_base.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/pruners/_hyperband.py
│ ├── +++ optuna-3.1.1/optuna/pruners/_hyperband.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/pruners/_median.py
│ ├── +++ optuna-3.1.1/optuna/pruners/_median.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/pruners/_nop.py
│ ├── +++ optuna-3.1.1/optuna/pruners/_nop.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/pruners/_patient.py
│ ├── +++ optuna-3.1.1/optuna/pruners/_patient.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/pruners/_percentile.py
│ ├── +++ optuna-3.1.1/optuna/pruners/_percentile.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/pruners/_successive_halving.py
│ ├── +++ optuna-3.1.1/optuna/pruners/_successive_halving.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/pruners/_threshold.py
│ ├── +++ optuna-3.1.1/optuna/pruners/_threshold.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/__init__.py
│ ├── +++ optuna-3.1.1/optuna/samplers/__init__.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/_base.py
│ ├── +++ optuna-3.1.1/optuna/samplers/_base.py
│ │┄ Files 1% similar despite different names
│ │ @@ -152,15 +152,15 @@
│ │          study: Study,
│ │          trial: FrozenTrial,
│ │          state: TrialState,
│ │          values: Optional[Sequence[float]],
│ │      ) -> None:
│ │          """Trial post-processing.
│ │  
│ │ -        This method is called after the objective function returns and right before the trials is
│ │ +        This method is called after the objective function returns and right before the trial is
│ │          finished and its state is stored.
│ │  
│ │          .. note::
│ │              Added in v2.4.0 as an experimental feature. The interface may change in newer versions
│ │              without prior notice. See https://github.com/optuna/optuna/releases/tag/v2.4.0.
│ │  
│ │          Args:
│ │   --- optuna-3.1.0b0/optuna/samplers/_brute_force.py
│ ├── +++ optuna-3.1.1/optuna/samplers/_brute_force.py
│ │┄ Files 4% similar despite different names
│ │ @@ -41,15 +41,16 @@
│ │  
│ │  
│ │              study = optuna.create_study(sampler=optuna.samplers.BruteForceSampler())
│ │              study.optimize(objective)
│ │  
│ │      Note:
│ │          The defined search space must be finite. Therefore, when using
│ │ -        :class:`~optuna.distributions.FloatDistibution`, ``step=None`` is not allowed.
│ │ +        :class:`~optuna.distributions.FloatDistribution` or
│ │ +        :func:`~optuna.trial.Trial.suggest_float`, ``step=None`` is not allowed.
│ │  
│ │      Note:
│ │          This sampler assumes that it suggests all parameters and that the search space is fixed.
│ │          For example, the sampler may fail to try the entire search space in the following cases.
│ │  
│ │          * Using with other samplers or :meth:`~optuna.study.Study.enqueue_trial`
│ │          * Changing suggestion ranges or adding parameters in the same :class:`~optuna.study.Study`
│ │   --- optuna-3.1.0b0/optuna/samplers/_cmaes.py
│ ├── +++ optuna-3.1.1/optuna/samplers/_cmaes.py
│ │┄ Files 3% similar despite different names
│ │ @@ -6,46 +6,48 @@
│ │  from typing import cast
│ │  from typing import Dict
│ │  from typing import List
│ │  from typing import NamedTuple
│ │  from typing import Optional
│ │  from typing import Sequence
│ │  from typing import Tuple
│ │ +from typing import TYPE_CHECKING
│ │  from typing import Union
│ │  import warnings
│ │  
│ │ -from cmaes import CMA
│ │ -from cmaes import CMAwM
│ │ -from cmaes import get_warm_start_mgd
│ │ -from cmaes import SepCMA
│ │  import numpy as np
│ │  
│ │  import optuna
│ │  from optuna import logging
│ │ +from optuna._imports import _LazyImport
│ │  from optuna._transform import _SearchSpaceTransform
│ │  from optuna.distributions import BaseDistribution
│ │  from optuna.distributions import FloatDistribution
│ │  from optuna.distributions import IntDistribution
│ │  from optuna.exceptions import ExperimentalWarning
│ │  from optuna.samplers import BaseSampler
│ │  from optuna.study._study_direction import StudyDirection
│ │  from optuna.trial import FrozenTrial
│ │  from optuna.trial import TrialState
│ │  
│ │  
│ │ +if TYPE_CHECKING:
│ │ +    import cmaes
│ │ +
│ │ +    CmaClass = Union[cmaes.CMA, cmaes.SepCMA, cmaes.CMAwM]
│ │ +else:
│ │ +    cmaes = _LazyImport("cmaes")
│ │ +
│ │  _logger = logging.get_logger(__name__)
│ │  
│ │  _EPS = 1e-10
│ │  # The value of system_attrs must be less than 2046 characters on RDBStorage.
│ │  _SYSTEM_ATTR_MAX_LENGTH = 2045
│ │  
│ │  
│ │ -CmaClass = Union[CMA, SepCMA, CMAwM]
│ │ -
│ │ -
│ │  class _CmaEsAttrKeys(NamedTuple):
│ │      optimizer: str
│ │      n_restarts: str
│ │      generation: Callable[[int], str]
│ │  
│ │  
│ │  class CmaEsSampler(BaseSampler):
│ │ @@ -358,76 +360,77 @@
│ │                      self._independent_sampler.__class__.__name__
│ │                  )
│ │              )
│ │              self._warn_independent_sampling = False
│ │              return {}
│ │  
│ │          # When `with_margin=True`, bounds in discrete dimensions are handled inside `CMAwM`.
│ │ -        trans = _SearchSpaceTransform(search_space, transform_step=not self._with_margin)
│ │ +        trans = _SearchSpaceTransform(
│ │ +            search_space, transform_step=not self._with_margin, transform_0_1=True
│ │ +        )
│ │  
│ │          optimizer, n_restarts = self._restore_optimizer(completed_trials)
│ │          if optimizer is None:
│ │              n_restarts = 0
│ │              optimizer = self._init_optimizer(trans, study.direction, population_size=self._popsize)
│ │  
│ │ -        generation_attr_key = self._attr_keys.generation(n_restarts)
│ │ -
│ │          if optimizer.dim != len(trans.bounds):
│ │              _logger.info(
│ │                  "`CmaEsSampler` does not support dynamic search space. "
│ │                  "`{}` is used instead of `CmaEsSampler`.".format(
│ │                      self._independent_sampler.__class__.__name__
│ │                  )
│ │              )
│ │              self._warn_independent_sampling = False
│ │              return {}
│ │  
│ │          # TODO(c-bata): Reduce the number of wasted trials during parallel optimization.
│ │          # See https://github.com/optuna/optuna/pull/920#discussion_r385114002 for details.
│ │ -        solution_trials = [
│ │ -            t
│ │ -            for t in completed_trials
│ │ -            if optimizer.generation == t.system_attrs.get(generation_attr_key, -1)
│ │ -        ]
│ │ +        solution_trials = self._get_solution_trials(
│ │ +            completed_trials, optimizer.generation, n_restarts
│ │ +        )
│ │ +
│ │          if len(solution_trials) >= optimizer.population_size:
│ │              solutions: List[Tuple[np.ndarray, float]] = []
│ │              for t in solution_trials[: optimizer.population_size]:
│ │                  assert t.value is not None, "completed trials must have a value"
│ │ -                if isinstance(optimizer, CMAwM):
│ │ -                    x = t.system_attrs["x_for_tell"]
│ │ +                if isinstance(optimizer, cmaes.CMAwM):
│ │ +                    x = np.array(t.system_attrs["x_for_tell"])
│ │                  else:
│ │                      x = trans.transform(t.params)
│ │                  y = t.value if study.direction == StudyDirection.MINIMIZE else -t.value
│ │                  solutions.append((x, y))
│ │  
│ │              optimizer.tell(solutions)
│ │  
│ │              if self._restart_strategy == "ipop" and optimizer.should_stop():
│ │                  n_restarts += 1
│ │ -                generation_attr_key = self._attr_keys.generation(n_restarts)
│ │                  popsize = optimizer.population_size * self._inc_popsize
│ │                  optimizer = self._init_optimizer(
│ │                      trans, study.direction, population_size=popsize, randomize_start_point=True
│ │                  )
│ │  
│ │              # Store optimizer.
│ │              optimizer_str = pickle.dumps(optimizer).hex()
│ │              optimizer_attrs = self._split_optimizer_str(optimizer_str)
│ │              for key in optimizer_attrs:
│ │                  study._storage.set_trial_system_attr(trial._trial_id, key, optimizer_attrs[key])
│ │  
│ │          # Caution: optimizer should update its seed value.
│ │          seed = self._cma_rng.randint(1, 2**16) + trial.number
│ │          optimizer._rng.seed(seed)
│ │ -        if isinstance(optimizer, CMAwM):
│ │ +        if isinstance(optimizer, cmaes.CMAwM):
│ │              params, x_for_tell = optimizer.ask()
│ │ -            study._storage.set_trial_system_attr(trial._trial_id, "x_for_tell", x_for_tell)
│ │ +            study._storage.set_trial_system_attr(
│ │ +                trial._trial_id, "x_for_tell", x_for_tell.tolist()
│ │ +            )
│ │          else:
│ │              params = optimizer.ask()
│ │  
│ │ +        generation_attr_key = self._attr_keys.generation(n_restarts)
│ │          study._storage.set_trial_system_attr(
│ │              trial._trial_id, generation_attr_key, optimizer.generation
│ │          )
│ │          study._storage.set_trial_system_attr(
│ │              trial._trial_id, self._attr_keys.n_restarts, n_restarts
│ │          )
│ │  
│ │ @@ -471,47 +474,37 @@
│ │              end = min((i + 1) * _SYSTEM_ATTR_MAX_LENGTH, optimizer_len)
│ │              attrs["{}:{}".format(self._attr_keys.optimizer, i)] = optimizer_str[start:end]
│ │          return attrs
│ │  
│ │      def _restore_optimizer(
│ │          self,
│ │          completed_trials: "List[optuna.trial.FrozenTrial]",
│ │ -    ) -> Tuple[Optional[CmaClass], int]:
│ │ -
│ │ +    ) -> Tuple[Optional["CmaClass"], int]:
│ │          # Restore a previous CMA object.
│ │          for trial in reversed(completed_trials):
│ │              optimizer_attrs = {
│ │                  key: value
│ │                  for key, value in trial.system_attrs.items()
│ │                  if key.startswith(self._attr_keys.optimizer)
│ │              }
│ │              if len(optimizer_attrs) == 0:
│ │                  continue
│ │  
│ │ -            if (
│ │ -                not self._use_separable_cma
│ │ -                and not self._with_margin
│ │ -                and "cma:optimizer" in optimizer_attrs
│ │ -            ):
│ │ -                # Check "cma:optimizer" key for backward compatibility.
│ │ -                optimizer_str = optimizer_attrs["cma:optimizer"]
│ │ -            else:
│ │ -                optimizer_str = self._concat_optimizer_attrs(optimizer_attrs)
│ │ -
│ │ +            optimizer_str = self._concat_optimizer_attrs(optimizer_attrs)
│ │              n_restarts: int = trial.system_attrs.get(self._attr_keys.n_restarts, 0)
│ │              return pickle.loads(bytes.fromhex(optimizer_str)), n_restarts
│ │          return None, 0
│ │  
│ │      def _init_optimizer(
│ │          self,
│ │          trans: _SearchSpaceTransform,
│ │          direction: StudyDirection,
│ │          population_size: Optional[int] = None,
│ │          randomize_start_point: bool = False,
│ │ -    ) -> CmaClass:
│ │ +    ) -> "CmaClass":
│ │          lower_bounds = trans.bounds[:, 0]
│ │          upper_bounds = trans.bounds[:, 1]
│ │          n_dimension = len(trans.bounds)
│ │  
│ │          if self._source_trials is None:
│ │              if randomize_start_point:
│ │                  mean = lower_bounds + (upper_bounds - lower_bounds) * self._cma_rng.rand(
│ │ @@ -542,21 +535,21 @@
│ │                  if t.state in expected_states
│ │                  and _is_compatible_search_space(trans, t.distributions)
│ │              ]
│ │              if len(source_solutions) == 0:
│ │                  raise ValueError("No compatible source_trials")
│ │  
│ │              # TODO(c-bata): Add options to change prior parameters (alpha and gamma).
│ │ -            mean, sigma0, cov = get_warm_start_mgd(source_solutions)
│ │ +            mean, sigma0, cov = cmaes.get_warm_start_mgd(source_solutions)
│ │  
│ │          # Avoid ZeroDivisionError in cmaes.
│ │          sigma0 = max(sigma0, _EPS)
│ │  
│ │          if self._use_separable_cma:
│ │ -            return SepCMA(
│ │ +            return cmaes.SepCMA(
│ │                  mean=mean,
│ │                  sigma=sigma0,
│ │                  bounds=trans.bounds,
│ │                  seed=self._cma_rng.randint(1, 2**31 - 2),
│ │                  n_max_resampling=10 * n_dimension,
│ │                  population_size=population_size,
│ │              )
│ │ @@ -564,28 +557,26 @@
│ │          if self._with_margin:
│ │              steps = np.empty(len(trans._search_space), dtype=float)
│ │              for i, dist in enumerate(trans._search_space.values()):
│ │                  assert isinstance(dist, (IntDistribution, FloatDistribution))
│ │                  # Set step 0.0 for continuous search space.
│ │                  steps[i] = dist.step or 0.0
│ │  
│ │ -            # If there is no discrete search space, we use `CMA` because CMAwM` throws an error.
│ │ -            if np.any(steps > 0.0):
│ │ -                return CMAwM(
│ │ -                    mean=mean,
│ │ -                    sigma=sigma0,
│ │ -                    bounds=trans.bounds,
│ │ -                    steps=steps,
│ │ -                    cov=cov,
│ │ -                    seed=self._cma_rng.randint(1, 2**31 - 2),
│ │ -                    n_max_resampling=10 * n_dimension,
│ │ -                    population_size=population_size,
│ │ -                )
│ │ +            return cmaes.CMAwM(
│ │ +                mean=mean,
│ │ +                sigma=sigma0,
│ │ +                bounds=trans.bounds,
│ │ +                steps=steps,
│ │ +                cov=cov,
│ │ +                seed=self._cma_rng.randint(1, 2**31 - 2),
│ │ +                n_max_resampling=10 * n_dimension,
│ │ +                population_size=population_size,
│ │ +            )
│ │  
│ │ -        return CMA(
│ │ +        return cmaes.CMA(
│ │              mean=mean,
│ │              sigma=sigma0,
│ │              cov=cov,
│ │              bounds=trans.bounds,
│ │              seed=self._cma_rng.randint(1, 2**31 - 2),
│ │              n_max_resampling=10 * n_dimension,
│ │              population_size=population_size,
│ │ @@ -638,14 +629,20 @@
│ │                      continue
│ │                  # We rewrite the value of the trial `t` for sampling, so we need a deepcopy.
│ │                  copied_t = copy.deepcopy(t)
│ │                  copied_t.value = value
│ │                  complete_trials.append(copied_t)
│ │          return complete_trials
│ │  
│ │ +    def _get_solution_trials(
│ │ +        self, trials: List[FrozenTrial], generation: int, n_restarts: int
│ │ +    ) -> List[FrozenTrial]:
│ │ +        generation_attr_key = self._attr_keys.generation(n_restarts)
│ │ +        return [t for t in trials if generation == t.system_attrs.get(generation_attr_key, -1)]
│ │ +
│ │      def after_trial(
│ │          self,
│ │          study: "optuna.Study",
│ │          trial: "optuna.trial.FrozenTrial",
│ │          state: TrialState,
│ │          values: Optional[Sequence[float]],
│ │      ) -> None:
│ │   --- optuna-3.1.0b0/optuna/samplers/_grid.py
│ ├── +++ optuna-3.1.1/optuna/samplers/_grid.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/_partial_fixed.py
│ ├── +++ optuna-3.1.1/optuna/samplers/_partial_fixed.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/_qmc.py
│ ├── +++ optuna-3.1.1/optuna/samplers/_qmc.py
│ │┄ Files 0% similar despite different names
│ │ @@ -24,15 +24,15 @@
│ │  
│ │  
│ │  @experimental_class("3.0.0")
│ │  class QMCSampler(BaseSampler):
│ │      """A Quasi Monte Carlo Sampler that generates low-discrepancy sequences.
│ │  
│ │      Quasi Monte Carlo (QMC) sequences are designed to have lower discrepancies than
│ │ -    standard random seqeunces. They are known to perform better than the standard
│ │ +    standard random sequences. They are known to perform better than the standard
│ │      randam sequences in hyperparameter optimization.
│ │  
│ │      For further information about the use of QMC sequences for hyperparameter optimization,
│ │      please refer to the following paper:
│ │  
│ │      - `Bergstra, James, and Yoshua Bengio. Random search for hyper-parameter optimization.
│ │        Journal of machine learning research 13.2, 2012.
│ │ @@ -46,15 +46,15 @@
│ │          If your search space contains categorical parameters, it samples the categorical
│ │          parameters by its `independent_sampler` without using QMC algorithm.
│ │  
│ │      .. note::
│ │          The search space of the sampler is determined by either previous trials in the study or
│ │          the first trial that this sampler samples.
│ │  
│ │ -        If there are previous trials in the study, :class:`~optuna.samplers.QMCSamper` infers its
│ │ +        If there are previous trials in the study, :class:`~optuna.samplers.QMCSampler` infers its
│ │          search space using the trial which was created first in the study.
│ │  
│ │          Otherwise (if the study has no previous trials), :class:`~optuna.samplers.QMCSampler`
│ │          samples the first trial using its `independent_sampler` and then infers the search space
│ │          in the second trial.
│ │  
│ │          As mentioned above, the search space of the :class:`~optuna.samplers.QMCSampler` is
│ │ @@ -73,15 +73,15 @@
│ │                  that the number of trials should be set as power of two.
│ │  
│ │          scramble:
│ │              If this option is :obj:`True`, scrambling (randomization) is applied to the QMC
│ │              sequences.
│ │  
│ │          seed:
│ │ -            A seed for `QMCSampler`. This argument is used only when `scramble` is :obj:`True`.
│ │ +            A seed for ``QMCSampler``. This argument is used only when ``scramble`` is :obj:`True`.
│ │              If this is :obj:`None`, the seed is initialized randomly. Default is :obj:`None`.
│ │  
│ │              .. note::
│ │                  When using multiple :class:`~optuna.samplers.QMCSampler`'s in parallel and/or
│ │                  distributed optimization, all the samplers must share the same seed when the
│ │                  `scrambling` is enabled. Otherwise, the low-discrepancy property of the samples
│ │                  will be degraded.
│ │   --- optuna-3.1.0b0/optuna/samplers/_random.py
│ ├── +++ optuna-3.1.1/optuna/samplers/_random.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/_search_space/group_decomposed.py
│ ├── +++ optuna-3.1.1/optuna/samplers/_search_space/group_decomposed.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/_search_space/intersection.py
│ ├── +++ optuna-3.1.1/optuna/samplers/_search_space/intersection.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/_tpe/multi_objective_sampler.py
│ ├── +++ optuna-3.1.1/optuna/samplers/_tpe/multi_objective_sampler.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/_tpe/parzen_estimator.py
│ ├── +++ optuna-3.1.1/optuna/samplers/_tpe/parzen_estimator.py
│ │┄ Files 3% similar despite different names
│ │ @@ -1,27 +1,18 @@
│ │  from typing import Callable
│ │  from typing import Dict
│ │  from typing import NamedTuple
│ │  from typing import Optional
│ │  from typing import Tuple
│ │ -from typing import TYPE_CHECKING
│ │  
│ │  import numpy as np
│ │  
│ │  from optuna import distributions
│ │ -from optuna._imports import _LazyImport
│ │  from optuna.distributions import BaseDistribution
│ │ -
│ │ -
│ │ -if TYPE_CHECKING:
│ │ -    import scipy.special as special
│ │ -    import scipy.stats as stats
│ │ -else:
│ │ -    special = _LazyImport("scipy.special")
│ │ -    stats = _LazyImport("scipy.stats")
│ │ +from optuna.samplers._tpe import _truncnorm as truncnorm
│ │  
│ │  
│ │  EPS = 1e-12
│ │  SIGMA0_MAGNITUDE = 0.2
│ │  
│ │  _DISTRIBUTION_CLASSES = (
│ │      distributions.CategoricalDistribution,
│ │ @@ -121,28 +112,22 @@
│ │                  assert high is not None
│ │                  assert mus is not None
│ │                  assert sigmas is not None
│ │  
│ │                  # We sample from truncnorm.
│ │                  trunc_low = (low - mus[active]) / sigmas[active]
│ │                  trunc_high = (high - mus[active]) / sigmas[active]
│ │ -                samples = np.full((), fill_value=high + 1.0, dtype=np.float64)
│ │ -                while (samples >= high).any():
│ │ -                    samples = np.where(
│ │ -                        samples < high,
│ │ -                        samples,
│ │ -                        stats.truncnorm.rvs(
│ │ -                            trunc_low,
│ │ -                            trunc_high,
│ │ -                            size=size,
│ │ -                            loc=mus[active],
│ │ -                            scale=sigmas[active],
│ │ -                            random_state=rng,
│ │ -                        ),
│ │ -                    )
│ │ +                samples = truncnorm.rvs(
│ │ +                    trunc_low,
│ │ +                    trunc_high,
│ │ +                    size=size,
│ │ +                    loc=mus[active],
│ │ +                    scale=sigmas[active],
│ │ +                    random_state=rng,
│ │ +                )
│ │              samples_dict[param_name] = samples
│ │          samples_dict = self._transform_from_uniform(samples_dict)
│ │          return samples_dict
│ │  
│ │      def log_pdf(self, samples_dict: Dict[str, np.ndarray]) -> np.ndarray:
│ │  
│ │          samples_dict = self._transform_to_uniform(samples_dict)
│ │ @@ -180,32 +165,37 @@
│ │                  mus = self._mus[param_name]
│ │                  sigmas = self._sigmas[param_name]
│ │                  assert low is not None
│ │                  assert high is not None
│ │                  assert mus is not None
│ │                  assert sigmas is not None
│ │  
│ │ -                cdf_func = _ParzenEstimator._normal_cdf
│ │ -                p_accept = cdf_func(high, mus, sigmas) - cdf_func(low, mus, sigmas)
│ │                  if q is None:
│ │ -                    distance = samples[:, None] - mus
│ │ -                    mahalanobis = distance / np.maximum(sigmas, EPS)
│ │ -                    z = np.sqrt(2 * np.pi) * sigmas
│ │ -                    coefficient = 1 / z / p_accept
│ │ -                    log_pdf = -0.5 * mahalanobis**2 + np.log(coefficient)
│ │ +                    log_pdf = truncnorm.logpdf(
│ │ +                        samples[:, None],
│ │ +                        (low - mus) / sigmas,
│ │ +                        (high - mus) / sigmas,
│ │ +                        loc=mus,
│ │ +                        scale=sigmas,
│ │ +                    )
│ │                  else:
│ │                      upper_bound = np.minimum(samples + q / 2.0, high)
│ │                      lower_bound = np.maximum(samples - q / 2.0, low)
│ │ -                    cdf = cdf_func(upper_bound[:, None], mus[None], sigmas[None]) - cdf_func(
│ │ -                        lower_bound[:, None], mus[None], sigmas[None]
│ │ +                    log_gauss_mass = truncnorm._log_gauss_mass(
│ │ +                        (lower_bound[:, None] - mus) / sigmas,
│ │ +                        (upper_bound[:, None] - mus) / sigmas,
│ │                      )
│ │ -                    log_pdf = np.log(cdf + EPS) - np.log(p_accept + EPS)
│ │ +                    log_p_accept = truncnorm._log_gauss_mass(
│ │ +                        (low - mus) / sigmas, (high - mus) / sigmas
│ │ +                    )
│ │ +                    log_pdf = log_gauss_mass - log_p_accept
│ │              component_log_pdf += log_pdf
│ │ -        ret = special.logsumexp(component_log_pdf + np.log(self._weights), axis=1)
│ │ -        return ret
│ │ +        weighted_log_pdf = component_log_pdf + np.log(self._weights)
│ │ +        max_ = weighted_log_pdf.max(axis=1)
│ │ +        return np.log(np.exp(weighted_log_pdf - max_[:, np.newaxis]).sum(axis=1)) + max_
│ │  
│ │      def _calculate_weights(self, predetermined_weights: Optional[np.ndarray]) -> np.ndarray:
│ │  
│ │          # We decide the weights.
│ │          consider_prior = self._parameters.consider_prior
│ │          prior_weight = self._parameters.prior_weight
│ │          weights_func = self._parameters.weights
│ │ @@ -454,23 +444,14 @@
│ │  
│ │          if consider_prior:
│ │              sigmas[n_observations] = prior_sigma
│ │  
│ │          return mus, sigmas
│ │  
│ │      @staticmethod
│ │ -    def _normal_cdf(x: np.ndarray, mu: np.ndarray, sigma: np.ndarray) -> np.ndarray:
│ │ -
│ │ -        mu, sigma = map(np.asarray, (mu, sigma))
│ │ -        denominator = x - mu
│ │ -        numerator = np.maximum(np.sqrt(2) * sigma, EPS)
│ │ -        z = denominator / numerator
│ │ -        return 0.5 * (1 + special.erf(z))
│ │ -
│ │ -    @staticmethod
│ │      def _sample_from_categorical_dist(
│ │          rng: np.random.RandomState, probabilities: np.ndarray
│ │      ) -> np.ndarray:
│ │  
│ │          n_samples = probabilities.shape[0]
│ │          rnd_quantile = rng.rand(n_samples)
│ │          cum_probs = np.cumsum(probabilities, axis=1)
│ │   --- optuna-3.1.0b0/optuna/samplers/_tpe/sampler.py
│ ├── +++ optuna-3.1.1/optuna/samplers/_tpe/sampler.py
│ │┄ Files 1% similar despite different names
│ │ @@ -396,18 +396,17 @@
│ │          if n < self._n_startup_trials:
│ │              return {}
│ │  
│ │          # We divide data into below and above.
│ │          indices_below, indices_above = _split_observation_pairs(scores, self._gamma(n), violations)
│ │          # `None` items are intentionally converted to `nan` and then filtered out.
│ │          # For `nan` conversion, the dtype must be float.
│ │ -        # `None` items appear only when `group=True`. We just use the first parameter because the
│ │ -        # masks are the same for all parameters in one group.
│ │ +        # `None` items appear when `group=True` or `constant_liar=True`.
│ │          config_values = {k: np.asarray(v, dtype=float) for k, v in values.items()}
│ │ -        param_mask = ~np.isnan(list(config_values.values())[0])
│ │ +        param_mask = np.all(~np.isnan(list(config_values.values())), axis=0)
│ │          param_mask_below, param_mask_above = param_mask[indices_below], param_mask[indices_above]
│ │          below = {k: v[indices_below[param_mask_below]] for k, v in config_values.items()}
│ │          above = {k: v[indices_above[param_mask_above]] for k, v in config_values.items()}
│ │  
│ │          # We then sample by maximizing log likelihood ratio.
│ │          if study._is_multi_objective():
│ │              weights_below = _calculate_weights_below_for_multi_objective(
│ │ @@ -668,25 +667,28 @@
│ │                  param_value = distribution.to_internal_repr(trial.params[param_name])
│ │              else:
│ │                  param_value = None
│ │              values[param_name].append(param_value)
│ │  
│ │          if constraints_enabled:
│ │              assert violations is not None
│ │ -            constraint = trial.system_attrs.get(_CONSTRAINTS_KEY)
│ │ -            if constraint is None:
│ │ -                warnings.warn(
│ │ -                    f"Trial {trial.number} does not have constraint values."
│ │ -                    " It will be treated as a lower priority than other trials."
│ │ -                )
│ │ -                violation = float("inf")
│ │ +            if trial.state != TrialState.RUNNING:
│ │ +                constraint = trial.system_attrs.get(_CONSTRAINTS_KEY)
│ │ +                if constraint is None:
│ │ +                    warnings.warn(
│ │ +                        f"Trial {trial.number} does not have constraint values."
│ │ +                        " It will be treated as a lower priority than other trials."
│ │ +                    )
│ │ +                    violation = float("inf")
│ │ +                else:
│ │ +                    # Violation values of infeasible dimensions are summed up.
│ │ +                    violation = sum(v for v in constraint if v > 0)
│ │ +                violations.append(violation)
│ │              else:
│ │ -                # Violation values of infeasible dimensions are summed up.
│ │ -                violation = sum(v for v in constraint if v > 0)
│ │ -            violations.append(violation)
│ │ +                violations.append(float("inf"))
│ │  
│ │      return values, scores, violations
│ │  
│ │  
│ │  def _split_observation_pairs(
│ │      loss_vals: List[Tuple[float, List[float]]],
│ │      n_below: int,
│ │   --- optuna-3.1.0b0/optuna/samplers/nsgaii/__init__.py
│ ├── +++ optuna-3.1.1/optuna/samplers/nsgaii/__init__.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/nsgaii/_crossover.py
│ ├── +++ optuna-3.1.1/optuna/samplers/nsgaii/_crossover.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_base.py
│ ├── +++ optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_base.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_blxalpha.py
│ ├── +++ optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_blxalpha.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_sbx.py
│ ├── +++ optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_sbx.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_spx.py
│ ├── +++ optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_spx.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_undx.py
│ ├── +++ optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_undx.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_uniform.py
│ ├── +++ optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_uniform.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/nsgaii/_crossovers/_vsbx.py
│ ├── +++ optuna-3.1.1/optuna/samplers/nsgaii/_crossovers/_vsbx.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/samplers/nsgaii/_sampler.py
│ ├── +++ optuna-3.1.1/optuna/samplers/nsgaii/_sampler.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/storages/__init__.py
│ ├── +++ optuna-3.1.1/optuna/storages/__init__.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/storages/_base.py
│ ├── +++ optuna-3.1.1/optuna/storages/_base.py
│ │┄ Files 1% similar despite different names
│ │ @@ -46,21 +46,27 @@
│ │      Trials in finished states are not allowed to be modified.
│ │      Trials in the WAITING state are not allowed to be modified except for the `state` field.
│ │      """
│ │  
│ │      # Basic study manipulation
│ │  
│ │      @abc.abstractmethod
│ │ -    def create_new_study(self, study_name: Optional[str] = None) -> int:
│ │ +    def create_new_study(
│ │ +        self, directions: Sequence[StudyDirection], study_name: Optional[str] = None
│ │ +    ) -> int:
│ │          """Create a new study from a name.
│ │  
│ │          If no name is specified, the storage class generates a name.
│ │          The returned study ID is unique among all current and deleted studies.
│ │  
│ │          Args:
│ │ +            directions:
│ │ +                 A sequence of direction whose element is either
│ │ +                 :obj:`~optuna.study.StudyDirection.MAXIMIZE` or
│ │ +                 :obj:`~optuna.study.StudyDirection.MINIMIZE`.
│ │              study_name:
│ │                  Name of the new study to create.
│ │  
│ │          Returns:
│ │              ID of the created study.
│ │  
│ │          Raises:
│ │ @@ -120,35 +126,14 @@
│ │  
│ │          Raises:
│ │              :exc:`KeyError`:
│ │                  If no study with the matching ``study_id`` exists.
│ │          """
│ │          raise NotImplementedError
│ │  
│ │ -    @abc.abstractmethod
│ │ -    def set_study_directions(self, study_id: int, directions: Sequence[StudyDirection]) -> None:
│ │ -        """Register optimization problem directions to a study.
│ │ -
│ │ -        Args:
│ │ -            study_id:
│ │ -                ID of the study.
│ │ -            directions:
│ │ -                A sequence of direction whose element is either
│ │ -                :obj:`~optuna.study.StudyDirection.MAXIMIZE` or
│ │ -                :obj:`~optuna.study.StudyDirection.MINIMIZE`.
│ │ -
│ │ -        Raises:
│ │ -            :exc:`KeyError`:
│ │ -                If no study with the matching ``study_id`` exists.
│ │ -            :exc:`ValueError`:
│ │ -                If the directions are already set and the each coordinate of passed ``directions``
│ │ -                is the opposite direction or :obj:`~optuna.study.StudyDirection.NOT_SET`.
│ │ -        """
│ │ -        raise NotImplementedError
│ │ -
│ │      # Basic study access
│ │  
│ │      @abc.abstractmethod
│ │      def get_study_id_from_name(self, study_name: str) -> int:
│ │          """Read the ID of a study.
│ │  
│ │          Args:
│ │ @@ -250,15 +235,15 @@
│ │  
│ │          The returned trial ID is unique among all current and deleted trials.
│ │  
│ │          Args:
│ │              study_id:
│ │                  ID of the study.
│ │              template_trial:
│ │ -                Template :class:`~optuna.trial.FronzenTrial` with default user-attributes,
│ │ +                Template :class:`~optuna.trial.FrozenTrial` with default user-attributes,
│ │                  system-attributes, intermediate-values, and a state.
│ │  
│ │          Returns:
│ │              ID of the created trial.
│ │  
│ │          Raises:
│ │              :exc:`KeyError`:
│ │   --- optuna-3.1.0b0/optuna/storages/_cached_storage.py
│ ├── +++ optuna-3.1.1/optuna/storages/_cached_storage.py
│ │┄ Files 3% similar despite different names
│ │ @@ -26,15 +26,15 @@
│ │      def __init__(self) -> None:
│ │          # Trial number to corresponding FrozenTrial.
│ │          self.trials: Dict[int, FrozenTrial] = {}
│ │          # A list of trials which do not require storage access to read latest attributes.
│ │          self.owned_or_finished_trial_ids: Set[int] = set()
│ │          # Cache distributions to avoid storage access on distribution consistency check.
│ │          self.param_distribution: Dict[str, distributions.BaseDistribution] = {}
│ │ -        self.directions: List[StudyDirection] = [StudyDirection.NOT_SET]
│ │ +        self.directions: Optional[List[StudyDirection]] = None
│ │          self.name: Optional[str] = None
│ │  
│ │  
│ │  class _CachedStorage(BaseStorage, BaseHeartbeat):
│ │      """A wrapper class of storage backends.
│ │  
│ │      This class is used in :func:`~optuna.get_storage` function and automatically
│ │ @@ -92,21 +92,25 @@
│ │          del state["_lock"]
│ │          return state
│ │  
│ │      def __setstate__(self, state: Dict[Any, Any]) -> None:
│ │          self.__dict__.update(state)
│ │          self._lock = threading.Lock()
│ │  
│ │ -    def create_new_study(self, study_name: Optional[str] = None) -> int:
│ │ +    def create_new_study(
│ │ +        self, directions: Sequence[StudyDirection], study_name: Optional[str] = None
│ │ +    ) -> int:
│ │  
│ │ -        study_id = self._backend.create_new_study(study_name)
│ │ +        study_id = self._backend.create_new_study(directions=directions, study_name=study_name)
│ │          with self._lock:
│ │              study = _StudyInfo()
│ │              study.name = study_name
│ │ +            study.directions = list(directions)
│ │              self._studies[study_id] = study
│ │ +
│ │          return study_id
│ │  
│ │      def delete_study(self, study_id: int) -> None:
│ │  
│ │          with self._lock:
│ │              if study_id in self._studies:
│ │                  for trial_id in self._studies[study_id].trials:
│ │ @@ -115,31 +119,14 @@
│ │                              self._trial_id_to_study_id_and_number[trial_id]
│ │                          ]
│ │                          del self._trial_id_to_study_id_and_number[trial_id]
│ │                  del self._studies[study_id]
│ │  
│ │          self._backend.delete_study(study_id)
│ │  
│ │ -    def set_study_directions(self, study_id: int, directions: Sequence[StudyDirection]) -> None:
│ │ -
│ │ -        with self._lock:
│ │ -            if study_id in self._studies:
│ │ -                current_directions = self._studies[study_id].directions
│ │ -                if directions == current_directions:
│ │ -                    return
│ │ -                elif (
│ │ -                    len(current_directions) == 1
│ │ -                    and current_directions[0] == StudyDirection.NOT_SET
│ │ -                ):
│ │ -                    self._studies[study_id].directions = list(directions)
│ │ -                    self._backend.set_study_directions(study_id, directions)
│ │ -                    return
│ │ -
│ │ -        self._backend.set_study_directions(study_id, directions)
│ │ -
│ │      def set_study_user_attr(self, study_id: int, key: str, value: Any) -> None:
│ │  
│ │          self._backend.set_study_user_attr(study_id, key, value)
│ │  
│ │      def set_study_system_attr(self, study_id: int, key: str, value: Any) -> None:
│ │  
│ │          self._backend.set_study_system_attr(study_id, key, value)
│ │ @@ -164,15 +151,15 @@
│ │          return name
│ │  
│ │      def get_study_directions(self, study_id: int) -> List[StudyDirection]:
│ │  
│ │          with self._lock:
│ │              if study_id in self._studies:
│ │                  directions = self._studies[study_id].directions
│ │ -                if len(directions) > 1 or directions[0] != StudyDirection.NOT_SET:
│ │ +                if directions is not None:
│ │                      return directions
│ │  
│ │          directions = self._backend.get_study_directions(study_id)
│ │          with self._lock:
│ │              if study_id not in self._studies:
│ │                  self._studies[study_id] = _StudyInfo()
│ │              self._studies[study_id].directions = directions
│ │   --- optuna-3.1.0b0/optuna/storages/_heartbeat.py
│ ├── +++ optuna-3.1.1/optuna/storages/_heartbeat.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/storages/_in_memory.py
│ ├── +++ optuna-3.1.1/optuna/storages/_in_memory.py
│ │┄ Files 2% similar despite different names
│ │ @@ -47,27 +47,30 @@
│ │          del state["_lock"]
│ │          return state
│ │  
│ │      def __setstate__(self, state: Dict[Any, Any]) -> None:
│ │          self.__dict__.update(state)
│ │          self._lock = threading.RLock()
│ │  
│ │ -    def create_new_study(self, study_name: Optional[str] = None) -> int:
│ │ +    def create_new_study(
│ │ +        self, directions: Sequence[StudyDirection], study_name: Optional[str] = None
│ │ +    ) -> int:
│ │  
│ │          with self._lock:
│ │              study_id = self._max_study_id + 1
│ │              self._max_study_id += 1
│ │  
│ │              if study_name is not None:
│ │                  if study_name in self._study_name_to_id:
│ │                      raise DuplicatedStudyError
│ │              else:
│ │                  study_uuid = str(uuid.uuid4())
│ │                  study_name = DEFAULT_STUDY_NAME_PREFIX + study_uuid
│ │ -            self._studies[study_id] = _StudyInfo(study_name)
│ │ +
│ │ +            self._studies[study_id] = _StudyInfo(study_name, list(directions))
│ │              self._study_name_to_id[study_name] = study_id
│ │  
│ │              _logger.info("A new study created in memory with name: {}".format(study_name))
│ │  
│ │              return study_id
│ │  
│ │      def delete_study(self, study_id: int) -> None:
│ │ @@ -77,30 +80,14 @@
│ │  
│ │              for trial in self._studies[study_id].trials:
│ │                  del self._trial_id_to_study_id_and_number[trial._trial_id]
│ │              study_name = self._studies[study_id].name
│ │              del self._study_name_to_id[study_name]
│ │              del self._studies[study_id]
│ │  
│ │ -    def set_study_directions(self, study_id: int, directions: Sequence[StudyDirection]) -> None:
│ │ -
│ │ -        with self._lock:
│ │ -            self._check_study_id(study_id)
│ │ -
│ │ -            study = self._studies[study_id]
│ │ -            if study.directions[0] != StudyDirection.NOT_SET and study.directions != list(
│ │ -                directions
│ │ -            ):
│ │ -                raise ValueError(
│ │ -                    "Cannot overwrite study direction from {} to {}.".format(
│ │ -                        study.directions, directions
│ │ -                    )
│ │ -                )
│ │ -            study.directions = list(directions)
│ │ -
│ │      def set_study_user_attr(self, study_id: int, key: str, value: Any) -> None:
│ │  
│ │          with self._lock:
│ │              self._check_study_id(study_id)
│ │  
│ │              self._studies[study_id].user_attrs[key] = value
│ │  
│ │ @@ -254,14 +241,15 @@
│ │  
│ │      def get_best_trial(self, study_id: int) -> FrozenTrial:
│ │  
│ │          with self._lock:
│ │              self._check_study_id(study_id)
│ │  
│ │              best_trial_id = self._studies[study_id].best_trial_id
│ │ +
│ │              if best_trial_id is None:
│ │                  raise ValueError("No trials are completed yet.")
│ │              elif len(self._studies[study_id].directions) > 1:
│ │                  raise RuntimeError(
│ │                      "Best trial can be obtained only for single-objective optimization."
│ │                  )
│ │              return self.get_trial(best_trial_id)
│ │ @@ -419,15 +407,15 @@
│ │      def _check_trial_id(self, trial_id: int) -> None:
│ │  
│ │          if trial_id not in self._trial_id_to_study_id_and_number:
│ │              raise KeyError("No trial with trial_id {} exists.".format(trial_id))
│ │  
│ │  
│ │  class _StudyInfo:
│ │ -    def __init__(self, name: str) -> None:
│ │ +    def __init__(self, name: str, directions: List[StudyDirection]) -> None:
│ │          self.trials: List[FrozenTrial] = []
│ │          self.param_distribution: Dict[str, distributions.BaseDistribution] = {}
│ │          self.user_attrs: Dict[str, Any] = {}
│ │          self.system_attrs: Dict[str, Any] = {}
│ │          self.name: str = name
│ │ -        self.directions: List[StudyDirection] = [StudyDirection.NOT_SET]
│ │ +        self.directions: List[StudyDirection] = directions
│ │          self.best_trial_id: Optional[int] = None
│ │   --- optuna-3.1.0b0/optuna/storages/_journal/base.py
│ ├── +++ optuna-3.1.1/optuna/storages/_journal/base.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/storages/_journal/file.py
│ ├── +++ optuna-3.1.1/optuna/storages/_journal/file.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/storages/_journal/redis.py
│ ├── +++ optuna-3.1.1/optuna/storages/_journal/redis.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/storages/_journal/storage.py
│ ├── +++ optuna-3.1.1/optuna/storages/_journal/storage.py
│ │┄ Files 2% similar despite different names
│ │ @@ -36,21 +36,20 @@
│ │  
│ │  
│ │  class JournalOperation(enum.IntEnum):
│ │      CREATE_STUDY = 0
│ │      DELETE_STUDY = 1
│ │      SET_STUDY_USER_ATTR = 2
│ │      SET_STUDY_SYSTEM_ATTR = 3
│ │ -    SET_STUDY_DIRECTIONS = 4
│ │ -    CREATE_TRIAL = 5
│ │ -    SET_TRIAL_PARAM = 6
│ │ -    SET_TRIAL_STATE_VALUES = 7
│ │ -    SET_TRIAL_INTERMEDIATE_VALUE = 8
│ │ -    SET_TRIAL_USER_ATTR = 9
│ │ -    SET_TRIAL_SYSTEM_ATTR = 10
│ │ +    CREATE_TRIAL = 4
│ │ +    SET_TRIAL_PARAM = 5
│ │ +    SET_TRIAL_STATE_VALUES = 6
│ │ +    SET_TRIAL_INTERMEDIATE_VALUE = 7
│ │ +    SET_TRIAL_USER_ATTR = 8
│ │ +    SET_TRIAL_SYSTEM_ATTR = 9
│ │  
│ │  
│ │  @experimental_class("3.1.0")
│ │  class JournalStorage(BaseStorage):
│ │      """Storage class for Journal storage backend.
│ │  
│ │      Note that library users can instantiate this class, but the attributes
│ │ @@ -58,14 +57,15 @@
│ │  
│ │      Journal storage writes a record of every operation to the database as it is executed and
│ │      at the same time, keeps a latest snapshot of the database in-memory. If the database crashes
│ │      for any reason, the storage can re-establish the contents in memory by replaying the
│ │      operations stored from the beginning.
│ │  
│ │      Journal storage has several benefits over the conventional value logging storages.
│ │ +
│ │      1. The number of IOs can be reduced because of larger granularity of logs.
│ │      2. Journal storage has simpler backend API than value logging storage.
│ │      3. Journal storage keeps a snapshot in-memory so no need to add more cache.
│ │  
│ │      Example:
│ │  
│ │          .. code::
│ │ @@ -74,19 +74,32 @@
│ │  
│ │  
│ │              def objective(trial):
│ │                  ...
│ │  
│ │  
│ │              storage = optuna.storages.JournalStorage(
│ │ -                optuna.storages.JournalFileStorage("./log_file"),
│ │ +                optuna.storages.JournalFileStorage("./journal.log"),
│ │              )
│ │  
│ │              study = optuna.create_study(storage=storage)
│ │              study.optimize(objective)
│ │ +
│ │ +    In a Windows environment, an error message "A required privilege is not held by the
│ │ +    client" may appear. In this case, you can solve the problem with creating storage
│ │ +    by specifying :class:`~optuna.storages.JournalFileOpenLock` as follows.
│ │ +
│ │ +    .. code::
│ │ +
│ │ +        file_path = "./journal.log"
│ │ +        lock_obj = optuna.storages.JournalFileOpenLock(file_path)
│ │ +
│ │ +        storage = optuna.storages.JournalStorage(
│ │ +            optuna.storages.JournalFileStorage(file_path, lock_obj=lock_obj),
│ │ +        )
│ │      """
│ │  
│ │      def __init__(self, log_storage: BaseJournalLogStorage) -> None:
│ │          self._worker_id_prefix = str(uuid.uuid4()) + "-"
│ │          self._backend = log_storage
│ │          self._thread_lock = threading.Lock()
│ │          self._replay_result = JournalStorageReplayResult(self._worker_id_prefix)
│ │ @@ -131,19 +144,23 @@
│ │          worker_id = self._replay_result.worker_id
│ │          self._backend.append_logs([{"op_code": op_code, "worker_id": worker_id, **extra_fields}])
│ │  
│ │      def _sync_with_backend(self) -> None:
│ │          logs = self._backend.read_logs(self._replay_result.log_number_read)
│ │          self._replay_result.apply_logs(logs)
│ │  
│ │ -    def create_new_study(self, study_name: Optional[str] = None) -> int:
│ │ +    def create_new_study(
│ │ +        self, directions: Sequence[StudyDirection], study_name: Optional[str] = None
│ │ +    ) -> int:
│ │          study_name = study_name or DEFAULT_STUDY_NAME_PREFIX + str(uuid.uuid4())
│ │  
│ │          with self._thread_lock:
│ │ -            self._write_log(JournalOperation.CREATE_STUDY, {"study_name": study_name})
│ │ +            self._write_log(
│ │ +                JournalOperation.CREATE_STUDY, {"study_name": study_name, "directions": directions}
│ │ +            )
│ │              self._sync_with_backend()
│ │  
│ │              for frozen_study in self._replay_result.get_all_studies():
│ │                  if frozen_study.study_name != study_name:
│ │                      continue
│ │  
│ │                  _logger.info("A new study created in Journal with name: {}".format(study_name))
│ │ @@ -152,14 +169,15 @@
│ │                  # Dump snapshot here.
│ │                  if (
│ │                      isinstance(self._backend, BaseJournalLogSnapshot)
│ │                      and study_id != 0
│ │                      and study_id % SNAPSHOT_INTERVAL == 0
│ │                  ):
│ │                      self._backend.save_snapshot(pickle.dumps(self._replay_result))
│ │ +
│ │                  return study_id
│ │              assert False, "Should not reach."
│ │  
│ │      def delete_study(self, study_id: int) -> None:
│ │          with self._thread_lock:
│ │              self._write_log(JournalOperation.DELETE_STUDY, {"study_id": study_id})
│ │              self._sync_with_backend()
│ │ @@ -172,20 +190,14 @@
│ │  
│ │      def set_study_system_attr(self, study_id: int, key: str, value: Any) -> None:
│ │          log: Dict[str, Any] = {"study_id": study_id, "system_attr": {key: value}}
│ │          with self._thread_lock:
│ │              self._write_log(JournalOperation.SET_STUDY_SYSTEM_ATTR, log)
│ │              self._sync_with_backend()
│ │  
│ │ -    def set_study_directions(self, study_id: int, directions: Sequence[StudyDirection]) -> None:
│ │ -        log: Dict[str, Any] = {"study_id": study_id, "directions": directions}
│ │ -        with self._thread_lock:
│ │ -            self._write_log(JournalOperation.SET_STUDY_DIRECTIONS, log)
│ │ -            self._sync_with_backend()
│ │ -
│ │      def get_study_id_from_name(self, study_name: str) -> int:
│ │          with self._thread_lock:
│ │              self._sync_with_backend()
│ │              for study in self._replay_result.get_all_studies():
│ │                  if study.study_name == study_name:
│ │                      return study._study_id
│ │              raise KeyError(NOT_FOUND_MSG)
│ │ @@ -390,16 +402,14 @@
│ │                  self._apply_create_study(log)
│ │              elif op == JournalOperation.DELETE_STUDY:
│ │                  self._apply_delete_study(log)
│ │              elif op == JournalOperation.SET_STUDY_USER_ATTR:
│ │                  self._apply_set_study_user_attr(log)
│ │              elif op == JournalOperation.SET_STUDY_SYSTEM_ATTR:
│ │                  self._apply_set_study_system_attr(log)
│ │ -            elif op == JournalOperation.SET_STUDY_DIRECTIONS:
│ │ -                self._apply_set_study_directions(log)
│ │              elif op == JournalOperation.CREATE_TRIAL:
│ │                  self._apply_create_trial(log)
│ │              elif op == JournalOperation.SET_TRIAL_PARAM:
│ │                  self._apply_set_trial_param(log)
│ │              elif op == JournalOperation.SET_TRIAL_STATE_VALUES:
│ │                  self._apply_set_trial_state_values(log)
│ │              elif op == JournalOperation.SET_TRIAL_INTERMEDIATE_VALUE:
│ │ @@ -453,14 +463,15 @@
│ │              return True
│ │          if self._is_issued_by_this_worker(log):
│ │              raise KeyError(NOT_FOUND_MSG)
│ │          return False
│ │  
│ │      def _apply_create_study(self, log: Dict[str, Any]) -> None:
│ │          study_name = log["study_name"]
│ │ +        directions = [StudyDirection(d) for d in log["directions"]]
│ │  
│ │          if study_name in [s.study_name for s in self._studies.values()]:
│ │              if self._is_issued_by_this_worker(log):
│ │                  raise DuplicatedStudyError(
│ │                      "Another study with name '{}' already exists. "
│ │                      "Please specify a different name, or reuse the existing one "
│ │                      "by setting `load_if_exists` (for Python API) or "
│ │ @@ -469,18 +480,19 @@
│ │              return
│ │  
│ │          study_id = self._next_study_id
│ │          self._next_study_id += 1
│ │  
│ │          self._studies[study_id] = FrozenStudy(
│ │              study_name=study_name,
│ │ -            direction=StudyDirection.NOT_SET,
│ │ +            direction=None,
│ │              user_attrs={},
│ │              system_attrs={},
│ │              study_id=study_id,
│ │ +            directions=directions,
│ │          )
│ │          self._study_id_to_trial_ids[study_id] = []
│ │  
│ │      def _apply_delete_study(self, log: Dict[str, Any]) -> None:
│ │          study_id = log["study_id"]
│ │  
│ │          if self._study_exists(study_id, log):
│ │ @@ -497,34 +509,14 @@
│ │      def _apply_set_study_system_attr(self, log: Dict[str, Any]) -> None:
│ │          study_id = log["study_id"]
│ │  
│ │          if self._study_exists(study_id, log):
│ │              assert len(log["system_attr"]) == 1
│ │              self._studies[study_id].system_attrs.update(log["system_attr"])
│ │  
│ │ -    def _apply_set_study_directions(self, log: Dict[str, Any]) -> None:
│ │ -        study_id = log["study_id"]
│ │ -
│ │ -        if not self._study_exists(study_id, log):
│ │ -            return
│ │ -
│ │ -        directions = [StudyDirection(d) for d in log["directions"]]
│ │ -
│ │ -        current_directions = self._studies[study_id]._directions
│ │ -        if current_directions[0] != StudyDirection.NOT_SET and current_directions != directions:
│ │ -            if self._is_issued_by_this_worker(log):
│ │ -                raise ValueError(
│ │ -                    "Cannot overwrite study direction from {} to {}.".format(
│ │ -                        current_directions, directions
│ │ -                    )
│ │ -                )
│ │ -            return
│ │ -
│ │ -        self._studies[study_id]._directions = [StudyDirection(d) for d in directions]
│ │ -
│ │      def _apply_create_trial(self, log: Dict[str, Any]) -> None:
│ │          study_id = log["study_id"]
│ │  
│ │          if not self._study_exists(study_id, log):
│ │              return
│ │  
│ │          trial_id = len(self._trials)
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/alembic/env.py
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/alembic/env.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v0.9.0.a.py
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v0.9.0.a.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v1.2.0.a.py
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v1.2.0.a.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v1.3.0.a.py
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v1.3.0.a.py
│ │┄ Files 19% similar despite different names
│ │ @@ -7,17 +7,22 @@
│ │  """
│ │  import json
│ │  
│ │  from alembic import op
│ │  import sqlalchemy as sa
│ │  
│ │  from sqlalchemy.exc import SQLAlchemyError
│ │ -from sqlalchemy.ext.declarative import declarative_base
│ │  from sqlalchemy import orm
│ │  
│ │ +try:
│ │ +    from sqlalchemy.orm import declarative_base
│ │ +except ImportError:
│ │ +    # TODO(c-bata): Remove this after dropping support for SQLAlchemy v1.3 or prior.
│ │ +    from sqlalchemy.ext.declarative import declarative_base
│ │ +
│ │  # revision identifiers, used by Alembic.
│ │  revision = "v1.3.0.a"
│ │  down_revision = "v1.2.0.a"
│ │  branch_labels = None
│ │  depends_on = None
│ │  
│ │  # Model definition
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v2.4.0.a.py
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v2.4.0.a.py
│ │┄ Files 2% similar despite different names
│ │ @@ -12,19 +12,24 @@
│ │  from sqlalchemy import Column
│ │  from sqlalchemy import Enum
│ │  from sqlalchemy import Float
│ │  from sqlalchemy import ForeignKey
│ │  from sqlalchemy import Integer
│ │  from sqlalchemy import UniqueConstraint
│ │  from sqlalchemy.exc import SQLAlchemyError
│ │ -from sqlalchemy.ext.declarative import declarative_base
│ │  from sqlalchemy import orm
│ │  
│ │  from optuna.study import StudyDirection
│ │  
│ │ +try:
│ │ +    from sqlalchemy.orm import declarative_base
│ │ +except ImportError:
│ │ +    # TODO(c-bata): Remove this after dropping support for SQLAlchemy v1.3 or prior.
│ │ +    from sqlalchemy.ext.declarative import declarative_base
│ │ +
│ │  
│ │  # revision identifiers, used by Alembic.
│ │  revision = "v2.4.0.a"
│ │  down_revision = "v1.3.0.a"
│ │  branch_labels = None
│ │  depends_on = None
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v2.6.0.a_.py
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v2.6.0.a_.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v3.0.0.a.py
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v3.0.0.a.py
│ │┄ Files 3% similar despite different names
│ │ @@ -17,29 +17,34 @@
│ │  from sqlalchemy import ForeignKey
│ │  from sqlalchemy import Integer
│ │  from sqlalchemy import orm
│ │  from sqlalchemy import String
│ │  from sqlalchemy import Text
│ │  from sqlalchemy import UniqueConstraint
│ │  from sqlalchemy.exc import SQLAlchemyError
│ │ -from sqlalchemy.ext.declarative import declarative_base
│ │  
│ │  from optuna.distributions import _convert_old_distribution_to_new_distribution
│ │  from optuna.distributions import BaseDistribution
│ │  from optuna.distributions import DiscreteUniformDistribution
│ │  from optuna.distributions import distribution_to_json
│ │  from optuna.distributions import FloatDistribution
│ │  from optuna.distributions import IntDistribution
│ │  from optuna.distributions import IntLogUniformDistribution
│ │  from optuna.distributions import IntUniformDistribution
│ │  from optuna.distributions import json_to_distribution
│ │  from optuna.distributions import LogUniformDistribution
│ │  from optuna.distributions import UniformDistribution
│ │  from optuna.trial import TrialState
│ │  
│ │ +try:
│ │ +    from sqlalchemy.orm import declarative_base
│ │ +except ImportError:
│ │ +    # TODO(c-bata): Remove this after dropping support for SQLAlchemy v1.3 or prior.
│ │ +    from sqlalchemy.ext.declarative import declarative_base
│ │ +
│ │  
│ │  # revision identifiers, used by Alembic.
│ │  revision = "v3.0.0.a"
│ │  down_revision = "v2.6.0.a"
│ │  branch_labels = None
│ │  depends_on = None
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v3.0.0.b.py
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v3.0.0.b.py
│ │┄ Files 3% similar despite different names
│ │ @@ -10,17 +10,22 @@
│ │  from alembic import op
│ │  from sqlalchemy import and_
│ │  from sqlalchemy import Column
│ │  from sqlalchemy import Enum
│ │  from sqlalchemy import Float
│ │  from sqlalchemy import ForeignKey
│ │  from sqlalchemy import Integer
│ │ -from sqlalchemy.ext.declarative import declarative_base
│ │  from sqlalchemy.orm import Session
│ │  
│ │ +try:
│ │ +    from sqlalchemy.orm import declarative_base
│ │ +except ImportError:
│ │ +    # TODO(c-bata): Remove this after dropping support for SQLAlchemy v1.3 or prior.
│ │ +    from sqlalchemy.ext.declarative import declarative_base
│ │ +
│ │  
│ │  # revision identifiers, used by Alembic.
│ │  revision = "v3.0.0.b"
│ │  down_revision = "v3.0.0.a"
│ │  branch_labels = None
│ │  depends_on = None
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v3.0.0.c.py
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v3.0.0.c.py
│ │┄ Files 3% similar despite different names
│ │ @@ -7,19 +7,24 @@
│ │  """
│ │  import enum
│ │  
│ │  import numpy as np
│ │  from alembic import op
│ │  import sqlalchemy as sa
│ │  from sqlalchemy.exc import SQLAlchemyError
│ │ -from sqlalchemy.ext.declarative import declarative_base
│ │  from sqlalchemy import orm
│ │  from typing import Optional
│ │  from typing import Tuple
│ │  
│ │ +try:
│ │ +    from sqlalchemy.orm import declarative_base
│ │ +except ImportError:
│ │ +    # TODO(c-bata): Remove this after dropping support for SQLAlchemy v1.3 or prior.
│ │ +    from sqlalchemy.ext.declarative import declarative_base
│ │ +
│ │  
│ │  # revision identifiers, used by Alembic.
│ │  revision = "v3.0.0.c"
│ │  down_revision = "v3.0.0.b"
│ │  branch_labels = None
│ │  depends_on = None
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/alembic/versions/v3.0.0.d.py
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/alembic/versions/v3.0.0.d.py
│ │┄ Files 3% similar despite different names
│ │ @@ -7,19 +7,24 @@
│ │  """
│ │  import enum
│ │  
│ │  import numpy as np
│ │  from alembic import op
│ │  import sqlalchemy as sa
│ │  from sqlalchemy.exc import SQLAlchemyError
│ │ -from sqlalchemy.ext.declarative import declarative_base
│ │  from sqlalchemy import orm
│ │  from typing import Optional
│ │  from typing import Tuple
│ │  
│ │ +try:
│ │ +    from sqlalchemy.orm import declarative_base
│ │ +except ImportError:
│ │ +    # TODO(c-bata): Remove this after dropping support for SQLAlchemy v1.3 or prior.
│ │ +    from sqlalchemy.ext.declarative import declarative_base
│ │ +
│ │  
│ │  # revision identifiers, used by Alembic.
│ │  revision = "v3.0.0.d"
│ │  down_revision = "v3.0.0.c"
│ │  branch_labels = None
│ │  depends_on = None
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/alembic.ini
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/alembic.ini
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/models.py
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/models.py
│ │┄ Files 2% similar despite different names
│ │ @@ -2,34 +2,41 @@
│ │  import math
│ │  from typing import Any
│ │  from typing import List
│ │  from typing import Optional
│ │  from typing import Tuple
│ │  
│ │  from sqlalchemy import asc
│ │ +from sqlalchemy import case
│ │  from sqlalchemy import CheckConstraint
│ │  from sqlalchemy import Column
│ │  from sqlalchemy import DateTime
│ │  from sqlalchemy import desc
│ │  from sqlalchemy import Enum
│ │  from sqlalchemy import Float
│ │  from sqlalchemy import ForeignKey
│ │  from sqlalchemy import func
│ │  from sqlalchemy import Integer
│ │  from sqlalchemy import orm
│ │  from sqlalchemy import String
│ │  from sqlalchemy import Text
│ │  from sqlalchemy import UniqueConstraint
│ │ -from sqlalchemy.ext.declarative import declarative_base
│ │  
│ │  from optuna import distributions
│ │  from optuna.study._study_direction import StudyDirection
│ │  from optuna.trial import TrialState
│ │  
│ │  
│ │ +try:
│ │ +    from sqlalchemy.orm import declarative_base
│ │ +except ImportError:
│ │ +    # TODO(c-bata): Remove this after dropping support for SQLAlchemy v1.3 or prior.
│ │ +    from sqlalchemy.ext.declarative import declarative_base
│ │ +
│ │ +
│ │  # Don't modify this version number anymore.
│ │  # The schema management functionality has been moved to alembic.
│ │  SCHEMA_VERSION = 12
│ │  
│ │  MAX_INDEXED_STRING_LENGTH = 512
│ │  MAX_VERSION_LENGTH = 256
│ │  
│ │ @@ -87,27 +94,14 @@
│ │      objective = Column(Integer, nullable=False)
│ │  
│ │      study = orm.relationship(
│ │          StudyModel, backref=orm.backref("directions", cascade="all, delete-orphan")
│ │      )
│ │  
│ │      @classmethod
│ │ -    def find_by_study_and_objective(
│ │ -        cls, study: StudyModel, objective: int, session: orm.Session
│ │ -    ) -> Optional["StudyDirectionModel"]:
│ │ -        study_direction = (
│ │ -            session.query(cls)
│ │ -            .filter(cls.study_id == study.study_id)
│ │ -            .filter(cls.objective == objective)
│ │ -            .one_or_none()
│ │ -        )
│ │ -
│ │ -        return study_direction
│ │ -
│ │ -    @classmethod
│ │      def where_study_id(cls, study_id: int, session: orm.Session) -> List["StudyDirectionModel"]:
│ │  
│ │          return session.query(cls).filter(cls.study_id == study_id).all()
│ │  
│ │  
│ │  class StudyUserAttributeModel(BaseModel):
│ │      __tablename__ = "study_user_attributes"
│ │ @@ -200,15 +194,23 @@
│ │  
│ │          trial = (
│ │              session.query(cls)
│ │              .filter(cls.study_id == study_id)
│ │              .filter(cls.state == TrialState.COMPLETE)
│ │              .join(TrialValueModel)
│ │              .filter(TrialValueModel.objective == objective)
│ │ -            .order_by(desc(TrialValueModel.value))
│ │ +            .order_by(
│ │ +                desc(
│ │ +                    case(
│ │ +                        {"INF_NEG": -1, "FINITE": 0, "INF_POS": 1},
│ │ +                        value=TrialValueModel.value_type,
│ │ +                    )
│ │ +                ),
│ │ +                desc(TrialValueModel.value),
│ │ +            )
│ │              .limit(1)
│ │              .one_or_none()
│ │          )
│ │          if trial is None:
│ │              raise ValueError(NOT_FOUND_MSG)
│ │          return trial
│ │  
│ │ @@ -219,15 +221,23 @@
│ │  
│ │          trial = (
│ │              session.query(cls)
│ │              .filter(cls.study_id == study_id)
│ │              .filter(cls.state == TrialState.COMPLETE)
│ │              .join(TrialValueModel)
│ │              .filter(TrialValueModel.objective == objective)
│ │ -            .order_by(asc(TrialValueModel.value))
│ │ +            .order_by(
│ │ +                asc(
│ │ +                    case(
│ │ +                        {"INF_NEG": -1, "FINITE": 0, "INF_POS": 1},
│ │ +                        value=TrialValueModel.value_type,
│ │ +                    )
│ │ +                ),
│ │ +                asc(TrialValueModel.value),
│ │ +            )
│ │              .limit(1)
│ │              .one_or_none()
│ │          )
│ │          if trial is None:
│ │              raise ValueError(NOT_FOUND_MSG)
│ │          return trial
│ │   --- optuna-3.1.0b0/optuna/storages/_rdb/storage.py
│ ├── +++ optuna-3.1.1/optuna/storages/_rdb/storage.py
│ │┄ Files 1% similar despite different names
│ │ @@ -189,15 +189,17 @@
│ │          self,
│ │          url: str,
│ │          engine_kwargs: Optional[Dict[str, Any]] = None,
│ │          skip_compatibility_check: bool = False,
│ │          *,
│ │          heartbeat_interval: Optional[int] = None,
│ │          grace_period: Optional[int] = None,
│ │ -        failed_trial_callback: Optional[Callable[["optuna.Study", FrozenTrial], None]] = None,
│ │ +        failed_trial_callback: Optional[
│ │ +            Callable[["optuna.study.Study", FrozenTrial], None]
│ │ +        ] = None,
│ │          skip_table_creation: bool = False,
│ │      ) -> None:
│ │  
│ │          self.engine_kwargs = engine_kwargs or {}
│ │          self.url = self._fill_storage_url_template(url)
│ │          self.skip_compatibility_check = skip_compatibility_check
│ │          if heartbeat_interval is not None and heartbeat_interval <= 0:
│ │ @@ -251,26 +253,30 @@
│ │              sqlalchemy_orm.sessionmaker(bind=self.engine)
│ │          )
│ │          models.BaseModel.metadata.create_all(self.engine)
│ │          self._version_manager = _VersionManager(self.url, self.engine, self.scoped_session)
│ │          if not self.skip_compatibility_check:
│ │              self._version_manager.check_table_schema_compatibility()
│ │  
│ │ -    def create_new_study(self, study_name: Optional[str] = None) -> int:
│ │ +    def create_new_study(
│ │ +        self, directions: Sequence[StudyDirection], study_name: Optional[str] = None
│ │ +    ) -> int:
│ │  
│ │          try:
│ │              with _create_scoped_session(self.scoped_session) as session:
│ │                  if study_name is None:
│ │                      study_name = self._create_unique_study_name(session)
│ │  
│ │ -                direction = models.StudyDirectionModel(
│ │ -                    direction=StudyDirection.NOT_SET, objective=0
│ │ -                )
│ │ -                study = models.StudyModel(study_name=study_name, directions=[direction])
│ │ -                session.add(study)
│ │ +                direction_models = [
│ │ +                    models.StudyDirectionModel(objective=objective, direction=d)
│ │ +                    for objective, d in enumerate(list(directions))
│ │ +                ]
│ │ +
│ │ +                session.add(models.StudyModel(study_name=study_name, directions=direction_models))
│ │ +
│ │          except sqlalchemy_exc.IntegrityError:
│ │              raise optuna.exceptions.DuplicatedStudyError(
│ │                  "Another study with name '{}' already exists. "
│ │                  "Please specify a different name, or reuse the existing one "
│ │                  "by setting `load_if_exists` (for Python API) or "
│ │                  "`--skip-if-exists` flag (for CLI).".format(study_name)
│ │              )
│ │ @@ -293,46 +299,14 @@
│ │              study_name = DEFAULT_STUDY_NAME_PREFIX + study_uuid
│ │              study = models.StudyModel.find_by_name(study_name, session)
│ │              if study is None:
│ │                  break
│ │  
│ │          return study_name
│ │  
│ │ -    # TODO(sano): Prevent simultaneously setting different direction in distributed environments.
│ │ -    def set_study_directions(self, study_id: int, directions: Sequence[StudyDirection]) -> None:
│ │ -
│ │ -        with _create_scoped_session(self.scoped_session) as session:
│ │ -            study = models.StudyModel.find_or_raise_by_id(study_id, session)
│ │ -            directions = list(directions)
│ │ -            current_directions = [
│ │ -                d.direction for d in models.StudyDirectionModel.where_study_id(study_id, session)
│ │ -            ]
│ │ -            if (
│ │ -                len(current_directions) > 0
│ │ -                and current_directions[0] != StudyDirection.NOT_SET
│ │ -                and current_directions != directions
│ │ -            ):
│ │ -                raise ValueError(
│ │ -                    "Cannot overwrite study direction from {} to {}.".format(
│ │ -                        current_directions, directions
│ │ -                    )
│ │ -                )
│ │ -
│ │ -            for objective, d in enumerate(directions):
│ │ -                direction_model = models.StudyDirectionModel.find_by_study_and_objective(
│ │ -                    study, objective, session
│ │ -                )
│ │ -                if direction_model is None:
│ │ -                    direction_model = models.StudyDirectionModel(
│ │ -                        study_id=study_id, objective=objective, direction=d
│ │ -                    )
│ │ -                    session.add(direction_model)
│ │ -                else:
│ │ -                    direction_model.direction = d
│ │ -
│ │      def set_study_user_attr(self, study_id: int, key: str, value: Any) -> None:
│ │  
│ │          with _create_scoped_session(self.scoped_session, True) as session:
│ │              study = models.StudyModel.find_or_raise_by_id(study_id, session)
│ │              attribute = models.StudyUserAttributeModel.find_by_study_and_key(study, key, session)
│ │              if attribute is None:
│ │                  attribute = models.StudyUserAttributeModel(
│ │ @@ -1065,15 +1039,17 @@
│ │  
│ │          return stale_trial_ids
│ │  
│ │      def get_heartbeat_interval(self) -> Optional[int]:
│ │  
│ │          return self.heartbeat_interval
│ │  
│ │ -    def get_failed_trial_callback(self) -> Optional[Callable[["optuna.Study", FrozenTrial], None]]:
│ │ +    def get_failed_trial_callback(
│ │ +        self,
│ │ +    ) -> Optional[Callable[["optuna.study.Study", FrozenTrial], None]]:
│ │  
│ │          return self.failed_trial_callback
│ │  
│ │  
│ │  class _VersionManager:
│ │      def __init__(
│ │          self,
│ │ @@ -1101,34 +1077,36 @@
│ │              )
│ │              session.add(version_info)
│ │  
│ │      def _init_alembic(self) -> None:
│ │  
│ │          logging.getLogger("alembic").setLevel(logging.WARN)
│ │  
│ │ -        context = alembic_migration.MigrationContext.configure(self.engine.connect())
│ │ -        is_initialized = context.get_current_revision() is not None
│ │ +        with self.engine.connect() as connection:
│ │ +            context = alembic_migration.MigrationContext.configure(connection)
│ │ +            is_initialized = context.get_current_revision() is not None
│ │  
│ │ -        if is_initialized:
│ │ -            # The `alembic_version` table already exists and is not empty.
│ │ -            return
│ │ +            if is_initialized:
│ │ +                # The `alembic_version` table already exists and is not empty.
│ │ +                return
│ │  
│ │ -        if self._is_alembic_supported():
│ │ -            revision = self.get_head_version()
│ │ -        else:
│ │ -            # The storage has been created before alembic is introduced.
│ │ -            revision = self._get_base_version()
│ │ +            if self._is_alembic_supported():
│ │ +                revision = self.get_head_version()
│ │ +            else:
│ │ +                # The storage has been created before alembic is introduced.
│ │ +                revision = self._get_base_version()
│ │  
│ │          self._set_alembic_revision(revision)
│ │  
│ │      def _set_alembic_revision(self, revision: str) -> None:
│ │ -
│ │ -        context = alembic_migration.MigrationContext.configure(self.engine.connect())
│ │ -        script = self._create_alembic_script()
│ │ -        context.stamp(script, revision)
│ │ +        with self.engine.connect() as connection:
│ │ +            context = alembic_migration.MigrationContext.configure(connection)
│ │ +            with connection.begin():
│ │ +                script = self._create_alembic_script()
│ │ +                context.stamp(script, revision)
│ │  
│ │      def check_table_schema_compatibility(self) -> None:
│ │  
│ │          with _create_scoped_session(self.scoped_session) as session:
│ │              # NOTE: After invocation of `_init_version_info_model` method,
│ │              #       it is ensured that a `VersionInfoModel` entry exists.
│ │              version_info = models.VersionInfoModel.find(session)
│ │ @@ -1155,17 +1133,17 @@
│ │              message += (
│ │                  "Please try updating optuna to the latest version by `$ pip install -U optuna`."
│ │              )
│ │  
│ │          raise RuntimeError(message)
│ │  
│ │      def get_current_version(self) -> str:
│ │ -
│ │ -        context = alembic_migration.MigrationContext.configure(self.engine.connect())
│ │ -        version = context.get_current_revision()
│ │ +        with self.engine.connect() as connection:
│ │ +            context = alembic_migration.MigrationContext.configure(connection)
│ │ +            version = context.get_current_revision()
│ │          assert version is not None
│ │  
│ │          return version
│ │  
│ │      def get_head_version(self) -> str:
│ │  
│ │          script = self._create_alembic_script()
│ │   --- optuna-3.1.0b0/optuna/study/__init__.py
│ ├── +++ optuna-3.1.1/optuna/study/__init__.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/study/_dataframe.py
│ ├── +++ optuna-3.1.1/optuna/study/_dataframe.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/study/_frozen.py
│ ├── +++ optuna-3.1.1/optuna/study/_frozen.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/study/_multi_objective.py
│ ├── +++ optuna-3.1.1/optuna/study/_multi_objective.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/study/_optimize.py
│ ├── +++ optuna-3.1.1/optuna/study/_optimize.py
│ │┄ Files 2% similar despite different names
│ │ @@ -159,26 +159,27 @@
│ │              if elapsed_seconds >= timeout:
│ │                  break
│ │  
│ │          try:
│ │              frozen_trial = _run_trial(study, func, catch)
│ │          finally:
│ │              # The following line mitigates memory problems that can be occurred in some
│ │ -            # environments (e.g., services that use computing containers such as CircleCI).
│ │ +            # environments (e.g., services that use computing containers such as GitHub Actions).
│ │              # Please refer to the following PR for further details:
│ │              # https://github.com/optuna/optuna/pull/325.
│ │              if gc_after_trial:
│ │                  gc.collect()
│ │  
│ │          if callbacks is not None:
│ │              for callback in callbacks:
│ │                  callback(study, frozen_trial)
│ │  
│ │          if progress_bar is not None:
│ │ -            progress_bar.update((datetime.datetime.now() - time_start).total_seconds())
│ │ +            elapsed_seconds = (datetime.datetime.now() - time_start).total_seconds()
│ │ +            progress_bar.update(elapsed_seconds, study)
│ │  
│ │      study._storage.remove_session()
│ │  
│ │  
│ │  def _run_trial(
│ │      study: "optuna.Study",
│ │      func: "optuna.study.study.ObjectiveFuncType",
│ │   --- optuna-3.1.0b0/optuna/study/_study_summary.py
│ ├── +++ optuna-3.1.1/optuna/study/_study_summary.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/study/_tell.py
│ ├── +++ optuna-3.1.1/optuna/study/_tell.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/study/study.py
│ ├── +++ optuna-3.1.1/optuna/study/study.py
│ │┄ Files 1% similar despite different names
│ │ @@ -335,15 +335,15 @@
│ │          range. Uses a sampler which implements the task of value suggestion based on a specified
│ │          distribution. The sampler is specified in :func:`~optuna.study.create_study` and the
│ │          default choice for the sampler is TPE.
│ │          See also :class:`~optuna.samplers.TPESampler` for more details on 'TPE'.
│ │  
│ │          Optimization will be stopped when receiving a termination signal such as SIGINT and
│ │          SIGTERM. Unlike other signals, a trial is automatically and cleanly failed when receiving
│ │ -        SIGINT (Ctrl+C). If :obj:`n_jobs` is greater than one or if another signal than SIGINT
│ │ +        SIGINT (Ctrl+C). If ``n_jobs`` is greater than one or if another signal than SIGINT
│ │          is used, the interrupted trial state won't be properly updated.
│ │  
│ │          Example:
│ │  
│ │              .. testcode::
│ │  
│ │                  import optuna
│ │ @@ -373,15 +373,15 @@
│ │              timeout:
│ │                  Stop study after the given number of second(s). :obj:`None` represents no limit in
│ │                  terms of elapsed time. The study continues to create trials until the number of
│ │                  trials reaches ``n_trials``, ``timeout`` period elapses,
│ │                  :func:`~optuna.study.Study.stop` is called or, a termination signal such as
│ │                  SIGTERM or Ctrl+C is received.
│ │              n_jobs:
│ │ -                The number of parallel jobs. If this argument is set to :obj:`-1`, the number is
│ │ +                The number of parallel jobs. If this argument is set to ``-1``, the number is
│ │                  set to CPU count.
│ │  
│ │                  .. note::
│ │                      ``n_jobs`` allows parallelization using :obj:`threading` and may suffer from
│ │                      `Python's GIL <https://wiki.python.org/moin/GlobalInterpreterLock>`_.
│ │                      It is recommended to use :ref:`process-based parallelization<distributed>`
│ │                      if ``func`` is CPU bound.
│ │ @@ -1151,15 +1151,15 @@
│ │  
│ │      direction_objects = [
│ │          d if isinstance(d, StudyDirection) else StudyDirection[d.upper()] for d in directions
│ │      ]
│ │  
│ │      storage = storages.get_storage(storage)
│ │      try:
│ │ -        study_id = storage.create_new_study(study_name)
│ │ +        study_id = storage.create_new_study(direction_objects, study_name)
│ │      except exceptions.DuplicatedStudyError:
│ │          if load_if_exists:
│ │              assert study_name is not None
│ │  
│ │              _logger.info(
│ │                  "Using an existing study with name '{}' instead of "
│ │                  "creating a new one.".format(study_name)
│ │ @@ -1168,15 +1168,14 @@
│ │          else:
│ │              raise
│ │  
│ │      if sampler is None and len(direction_objects) > 1:
│ │          sampler = samplers.NSGAIISampler()
│ │  
│ │      study_name = storage.get_study_name_from_id(study_id)
│ │ -    storage.set_study_directions(study_id, direction_objects)
│ │      study = Study(study_name=study_name, storage=storage, sampler=sampler, pruner=pruner)
│ │  
│ │      return study
│ │  
│ │  
│ │  @convert_positional_args(
│ │      previous_positional_arg_names=[
│ │   --- optuna-3.1.0b0/optuna/testing/samplers.py
│ ├── +++ optuna-3.1.1/optuna/testing/samplers.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/testing/storages.py
│ ├── +++ optuna-3.1.1/optuna/testing/storages.py
│ │┄ Files 3% similar despite different names
│ │ @@ -90,23 +90,23 @@
│ │                  "redis", fakeredis.FakeStrictRedis()
│ │              )
│ │              return optuna.storages.JournalStorage(journal_redis_storage)
│ │          elif "journal" in self.storage_specifier:
│ │              file_storage = JournalFileStorage(tempfile.NamedTemporaryFile().name)
│ │              return optuna.storages.JournalStorage(file_storage)
│ │          elif self.storage_specifier == "dask":
│ │ -            self.dask_client = distributed.Client()
│ │ +            self.dask_client = distributed.Client()  # type: ignore[no-untyped-call]
│ │  
│ │              return optuna.integration.DaskStorage(client=self.dask_client, **self.extra_args)
│ │          else:
│ │              assert False
│ │  
│ │      def __exit__(
│ │          self, exc_type: Type[BaseException], exc_val: BaseException, exc_tb: TracebackType
│ │      ) -> None:
│ │  
│ │          if self.tempfile:
│ │              self.tempfile.close()
│ │  
│ │          if self.dask_client:
│ │ -            self.dask_client.shutdown()
│ │ -            self.dask_client.close()
│ │ +            self.dask_client.shutdown()  # type: ignore[no-untyped-call]
│ │ +            self.dask_client.close()  # type: ignore[no-untyped-call]
│ │   --- optuna-3.1.0b0/optuna/testing/threading.py
│ ├── +++ optuna-3.1.1/optuna/testing/threading.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/testing/visualization.py
│ ├── +++ optuna-3.1.1/optuna/testing/visualization.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/trial/_fixed.py
│ ├── +++ optuna-3.1.1/optuna/trial/_fixed.py
│ │┄ Files 8% similar despite different names
│ │ @@ -1,11 +1,12 @@
│ │  import datetime
│ │  from typing import Any
│ │  from typing import Dict
│ │  from typing import Optional
│ │ +from typing import overload
│ │  from typing import Sequence
│ │  import warnings
│ │  
│ │  from optuna import distributions
│ │  from optuna._deprecated import deprecated_func
│ │  from optuna.distributions import BaseDistribution
│ │  from optuna.distributions import CategoricalChoiceType
│ │ @@ -92,14 +93,40 @@
│ │      def suggest_discrete_uniform(self, name: str, low: float, high: float, q: float) -> float:
│ │  
│ │          return self.suggest_float(name, low, high, step=q)
│ │  
│ │      def suggest_int(self, name: str, low: int, high: int, step: int = 1, log: bool = False) -> int:
│ │          return int(self._suggest(name, IntDistribution(low, high, log=log, step=step)))
│ │  
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[None]) -> None:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[bool]) -> bool:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[int]) -> int:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[float]) -> float:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[str]) -> str:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(
│ │ +        self, name: str, choices: Sequence[CategoricalChoiceType]
│ │ +    ) -> CategoricalChoiceType:
│ │ +        ...
│ │ +
│ │      def suggest_categorical(
│ │          self, name: str, choices: Sequence[CategoricalChoiceType]
│ │      ) -> CategoricalChoiceType:
│ │  
│ │          return self._suggest(name, CategoricalDistribution(choices=choices))
│ │  
│ │      def report(self, value: float, step: int) -> None:
│ │   --- optuna-3.1.0b0/optuna/trial/_frozen.py
│ ├── +++ optuna-3.1.1/optuna/trial/_frozen.py
│ │┄ Files 3% similar despite different names
│ │ @@ -1,12 +1,13 @@
│ │  import datetime
│ │  from typing import Any
│ │  from typing import Dict
│ │  from typing import List
│ │  from typing import Optional
│ │ +from typing import overload
│ │  from typing import Sequence
│ │  import warnings
│ │  
│ │  from optuna import distributions
│ │  from optuna import logging
│ │  from optuna._deprecated import deprecated_func
│ │  from optuna.distributions import _convert_old_distribution_to_new_distribution
│ │ @@ -230,14 +231,40 @@
│ │      def suggest_discrete_uniform(self, name: str, low: float, high: float, q: float) -> float:
│ │  
│ │          return self.suggest_float(name, low, high, step=q)
│ │  
│ │      def suggest_int(self, name: str, low: int, high: int, step: int = 1, log: bool = False) -> int:
│ │          return int(self._suggest(name, IntDistribution(low, high, log=log, step=step)))
│ │  
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[None]) -> None:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[bool]) -> bool:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[int]) -> int:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[float]) -> float:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[str]) -> str:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(
│ │ +        self, name: str, choices: Sequence[CategoricalChoiceType]
│ │ +    ) -> CategoricalChoiceType:
│ │ +        ...
│ │ +
│ │      def suggest_categorical(
│ │          self, name: str, choices: Sequence[CategoricalChoiceType]
│ │      ) -> CategoricalChoiceType:
│ │  
│ │          return self._suggest(name, CategoricalDistribution(choices=choices))
│ │  
│ │      def report(self, value: float, step: int) -> None:
│ │   --- optuna-3.1.0b0/optuna/trial/_state.py
│ ├── +++ optuna-3.1.1/optuna/trial/_state.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/trial/_trial.py
│ ├── +++ optuna-3.1.1/optuna/trial/_trial.py
│ │┄ Files 3% similar despite different names
│ │ @@ -1,12 +1,13 @@
│ │  import copy
│ │  import datetime
│ │  from typing import Any
│ │  from typing import Dict
│ │  from typing import Optional
│ │ +from typing import overload
│ │  from typing import Sequence
│ │  import warnings
│ │  
│ │  import optuna
│ │  from optuna import distributions
│ │  from optuna import logging
│ │  from optuna import pruners
│ │ @@ -313,14 +314,40 @@
│ │          """
│ │  
│ │          distribution = IntDistribution(low=low, high=high, log=log, step=step)
│ │          suggested_value = int(self._suggest(name, distribution))
│ │          self._check_distribution(name, distribution)
│ │          return suggested_value
│ │  
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[None]) -> None:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[bool]) -> bool:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[int]) -> int:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[float]) -> float:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(self, name: str, choices: Sequence[str]) -> str:
│ │ +        ...
│ │ +
│ │ +    @overload
│ │ +    def suggest_categorical(
│ │ +        self, name: str, choices: Sequence[CategoricalChoiceType]
│ │ +    ) -> CategoricalChoiceType:
│ │ +        ...
│ │ +
│ │      def suggest_categorical(
│ │          self, name: str, choices: Sequence[CategoricalChoiceType]
│ │      ) -> CategoricalChoiceType:
│ │          """Suggest a value for the categorical parameter.
│ │  
│ │          The value is sampled from ``choices``.
│ │  
│ │ @@ -695,14 +722,15 @@
│ │          Returns:
│ │              A dictionary containing all user attributes.
│ │          """
│ │  
│ │          return copy.deepcopy(self._cached_frozen_trial.user_attrs)
│ │  
│ │      @property
│ │ +    @deprecated_func("3.1.0", "6.0.0")
│ │      def system_attrs(self) -> Dict[str, Any]:
│ │          """Return system attributes.
│ │  
│ │          Returns:
│ │              A dictionary containing all system attributes.
│ │          """
│ │   --- optuna-3.1.0b0/optuna/visualization/__init__.py
│ ├── +++ optuna-3.1.1/optuna/visualization/__init__.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/_contour.py
│ ├── +++ optuna-3.1.1/optuna/visualization/_contour.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/_edf.py
│ ├── +++ optuna-3.1.1/optuna/visualization/_edf.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/_intermediate_values.py
│ ├── +++ optuna-3.1.1/optuna/visualization/_intermediate_values.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/_optimization_history.py
│ ├── +++ optuna-3.1.1/optuna/visualization/_optimization_history.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/_parallel_coordinate.py
│ ├── +++ optuna-3.1.1/optuna/visualization/_parallel_coordinate.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/_param_importances.py
│ ├── +++ optuna-3.1.1/optuna/visualization/_param_importances.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/_pareto_front.py
│ ├── +++ optuna-3.1.1/optuna/visualization/_pareto_front.py
│ │┄ Files 0% similar despite different names
│ │ @@ -96,15 +96,15 @@
│ │                  change. See https://github.com/optuna/optuna/releases/tag/v3.0.0.
│ │          constraints_func:
│ │              An optional function that computes the objective constraints. It must take a
│ │              :class:`~optuna.trial.FrozenTrial` and return the constraints. The return value must
│ │              be a sequence of :obj:`float` s. A value strictly larger than 0 means that a
│ │              constraint is violated. A value equal to or smaller than 0 is considered feasible.
│ │              This specification is the same as in, for example,
│ │ -            :class:`~optuna.integration.NSGAIISampler`.
│ │ +            :class:`~optuna.samplers.NSGAIISampler`.
│ │  
│ │              If given, trials are classified into three categories: feasible and best, feasible but
│ │              non-best, and infeasible. Categories are shown in different colors. Here, whether a
│ │              trial is best (on Pareto front) or not is determined ignoring all infeasible trials.
│ │  
│ │              .. note::
│ │                  Added in v3.0.0 as an experimental feature. The interface may change in newer
│ │   --- optuna-3.1.0b0/optuna/visualization/_plotly_imports.py
│ ├── +++ optuna-3.1.1/optuna/visualization/_plotly_imports.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/_slice.py
│ ├── +++ optuna-3.1.1/optuna/visualization/_slice.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/_utils.py
│ ├── +++ optuna-3.1.1/optuna/visualization/_utils.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/matplotlib/__init__.py
│ ├── +++ optuna-3.1.1/optuna/visualization/matplotlib/__init__.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/matplotlib/_contour.py
│ ├── +++ optuna-3.1.1/optuna/visualization/matplotlib/_contour.py
│ │┄ Files 1% similar despite different names
│ │ @@ -3,27 +3,30 @@
│ │  from typing import List
│ │  from typing import Optional
│ │  from typing import Sequence
│ │  from typing import Tuple
│ │  from typing import Union
│ │  
│ │  import numpy as np
│ │ -import scipy
│ │  
│ │  from optuna._experimental import experimental_func
│ │ +from optuna._imports import try_import
│ │  from optuna.logging import get_logger
│ │  from optuna.study import Study
│ │  from optuna.trial import FrozenTrial
│ │  from optuna.visualization._contour import _AxisInfo
│ │  from optuna.visualization._contour import _ContourInfo
│ │  from optuna.visualization._contour import _get_contour_info
│ │  from optuna.visualization._contour import _SubContourInfo
│ │  from optuna.visualization.matplotlib._matplotlib_imports import _imports
│ │  
│ │  
│ │ +with try_import() as _optuna_imports:
│ │ +    import scipy
│ │ +
│ │  if _imports.is_successful():
│ │      from optuna.visualization.matplotlib._matplotlib_imports import Axes
│ │      from optuna.visualization.matplotlib._matplotlib_imports import Colormap
│ │      from optuna.visualization.matplotlib._matplotlib_imports import ContourSet
│ │      from optuna.visualization.matplotlib._matplotlib_imports import plt
│ │  
│ │  _logger = get_logger(__name__)
│ │   --- optuna-3.1.0b0/optuna/visualization/matplotlib/_edf.py
│ ├── +++ optuna-3.1.1/optuna/visualization/matplotlib/_edf.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/matplotlib/_intermediate_values.py
│ ├── +++ optuna-3.1.1/optuna/visualization/matplotlib/_intermediate_values.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/matplotlib/_matplotlib_imports.py
│ ├── +++ optuna-3.1.1/optuna/visualization/matplotlib/_matplotlib_imports.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/matplotlib/_optimization_history.py
│ ├── +++ optuna-3.1.1/optuna/visualization/matplotlib/_optimization_history.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/matplotlib/_parallel_coordinate.py
│ ├── +++ optuna-3.1.1/optuna/visualization/matplotlib/_parallel_coordinate.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/matplotlib/_param_importances.py
│ ├── +++ optuna-3.1.1/optuna/visualization/matplotlib/_param_importances.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/matplotlib/_pareto_front.py
│ ├── +++ optuna-3.1.1/optuna/visualization/matplotlib/_pareto_front.py
│ │┄ Files 1% similar despite different names
│ │ @@ -75,15 +75,15 @@
│ │                  change. See https://github.com/optuna/optuna/releases/tag/v3.0.0.
│ │          constraints_func:
│ │              An optional function that computes the objective constraints. It must take a
│ │              :class:`~optuna.trial.FrozenTrial` and return the constraints. The return value must
│ │              be a sequence of :obj:`float` s. A value strictly larger than 0 means that a
│ │              constraint is violated. A value equal to or smaller than 0 is considered feasible.
│ │              This specification is the same as in, for example,
│ │ -            :class:`~optuna.integration.NSGAIISampler`.
│ │ +            :class:`~optuna.samplers.NSGAIISampler`.
│ │  
│ │              If given, trials are classified into three categories: feasible and best, feasible but
│ │              non-best, and infeasible. Categories are shown in different colors. Here, whether a
│ │              trial is best (on Pareto front) or not is determined ignoring all infeasible trials.
│ │          targets:
│ │              A function that returns a tuple of target values to display.
│ │              The argument to this function is :class:`~optuna.trial.FrozenTrial`.
│ │   --- optuna-3.1.0b0/optuna/visualization/matplotlib/_slice.py
│ ├── +++ optuna-3.1.1/optuna/visualization/matplotlib/_slice.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna/visualization/matplotlib/_utils.py
│ ├── +++ optuna-3.1.1/optuna/visualization/matplotlib/_utils.py
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/optuna.egg-info/PKG-INFO
│ ├── +++ optuna-3.1.1/optuna.egg-info/PKG-INFO
│ │┄ Files 4% similar despite different names
│ │ @@ -1,10 +1,10 @@
│ │  Metadata-Version: 2.1
│ │  Name: optuna
│ │ -Version: 3.1.0b0
│ │ +Version: 3.1.1
│ │  Summary: A hyperparameter optimization framework
│ │  Home-page: https://optuna.org/
│ │  Author: Takuya Akiba
│ │  Author-email: akiba@preferred.jp
│ │  License: UNKNOWN
│ │  Project-URL: Source, https://github.com/optuna/optuna
│ │  Project-URL: Documentation, https://optuna.readthedocs.io
│ │ @@ -13,22 +13,22 @@
│ │          
│ │          # Optuna: A hyperparameter optimization framework
│ │          
│ │          [![Python](https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9%20%7C%203.10%20%7C%203.11-blue)](https://www.python.org)
│ │          [![pypi](https://img.shields.io/pypi/v/optuna.svg)](https://pypi.python.org/pypi/optuna)
│ │          [![conda](https://img.shields.io/conda/vn/conda-forge/optuna.svg)](https://anaconda.org/conda-forge/optuna)
│ │          [![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/optuna/optuna)
│ │ -        [![CircleCI](https://circleci.com/gh/optuna/optuna.svg?style=svg)](https://circleci.com/gh/optuna/optuna)
│ │          [![Read the Docs](https://readthedocs.org/projects/optuna/badge/?version=stable)](https://optuna.readthedocs.io/en/stable/)
│ │          [![Codecov](https://codecov.io/gh/optuna/optuna/branch/master/graph/badge.svg)](https://codecov.io/gh/optuna/optuna/branch/master)
│ │          
│ │          [**Website**](https://optuna.org/)
│ │          | [**Docs**](https://optuna.readthedocs.io/en/stable/)
│ │          | [**Install Guide**](https://optuna.readthedocs.io/en/stable/installation.html)
│ │          | [**Tutorial**](https://optuna.readthedocs.io/en/stable/tutorial/index.html)
│ │ +        | [**Examples**](https://github.com/optuna/optuna-examples)
│ │          
│ │          *Optuna* is an automatic hyperparameter optimization software framework, particularly designed
│ │          for machine learning. It features an imperative, *define-by-run* style user API. Thanks to our
│ │          *define-by-run* API, the code written with Optuna enjoys high modularity, and the user of
│ │          Optuna can dynamically construct the search spaces for the hyperparameters.
│ │          
│ │          ## Key Features
│ │   --- optuna-3.1.0b0/optuna.egg-info/SOURCES.txt
│ ├── +++ optuna-3.1.1/optuna.egg-info/SOURCES.txt
│ │┄ Files 2% similar despite different names
│ │ @@ -106,14 +106,16 @@
│ │  optuna/samplers/_partial_fixed.py
│ │  optuna/samplers/_qmc.py
│ │  optuna/samplers/_random.py
│ │  optuna/samplers/_search_space/__init__.py
│ │  optuna/samplers/_search_space/group_decomposed.py
│ │  optuna/samplers/_search_space/intersection.py
│ │  optuna/samplers/_tpe/__init__.py
│ │ +optuna/samplers/_tpe/_erf.py
│ │ +optuna/samplers/_tpe/_truncnorm.py
│ │  optuna/samplers/_tpe/multi_objective_sampler.py
│ │  optuna/samplers/_tpe/parzen_estimator.py
│ │  optuna/samplers/_tpe/sampler.py
│ │  optuna/samplers/nsgaii/__init__.py
│ │  optuna/samplers/nsgaii/_crossover.py
│ │  optuna/samplers/nsgaii/_sampler.py
│ │  optuna/samplers/nsgaii/_crossovers/__init__.py
│ │   --- optuna-3.1.0b0/optuna.egg-info/requires.txt
│ ├── +++ optuna-3.1.1/optuna.egg-info/requires.txt
│ │┄ Files 8% similar despite different names
│ │ @@ -1,13 +1,12 @@
│ │  alembic>=1.5.0
│ │ -cmaes>=0.9.0
│ │ +cmaes>=0.9.1
│ │  colorlog
│ │  numpy
│ │  packaging>=20.0
│ │ -scipy>=1.7.0
│ │  sqlalchemy>=1.3.0
│ │  tqdm
│ │  PyYAML
│ │  
│ │  [benchmark]
│ │  asv>=0.5.0
│ │  botorch
│ │ @@ -34,57 +33,62 @@
│ │  matplotlib!=3.6.0
│ │  mlflow
│ │  pandas
│ │  pillow
│ │  plotly>=4.9.0
│ │  scikit-learn
│ │  scikit-optimize
│ │ -sphinx
│ │ +sphinx<6
│ │  sphinx-copybutton
│ │  sphinx-gallery
│ │  sphinx-plotly-directive
│ │  sphinx_rtd_theme
│ │  torch==1.11.0
│ │  torchaudio==0.11.0
│ │  torchvision==0.12.0
│ │  
│ │  [integration]
│ │ +chainer>=5.0.0
│ │ +cma
│ │ +distributed
│ │ +mpi4py
│ │ +pandas
│ │ +scikit-learn>=0.24.2
│ │ +wandb
│ │ +xgboost
│ │ +
│ │ +[integration:python_version < "3.11"]
│ │  allennlp>=2.2.0
│ │  cached-path<=1.1.2
│ │  botorch<0.8.0,>=0.4.0
│ │  catalyst>=21.3
│ │  catboost>=0.26
│ │ -chainer>=5.0.0
│ │ -cma
│ │ -distributed
│ │  fastai
│ │  lightgbm
│ │  mlflow
│ │ -mpi4py
│ │  mxnet
│ │ -pandas
│ │  pytorch-ignite
│ │  pytorch-lightning>=1.5.0
│ │ -scikit-learn>=0.24.2
│ │  scikit-optimize
│ │  shap
│ │  skorch
│ │  tensorflow
│ │  tensorflow-datasets
│ │  torch==1.11.0
│ │  torchaudio==0.11.0
│ │  torchvision==0.12.0
│ │ -wandb
│ │ -xgboost
│ │  
│ │  [optional]
│ │  matplotlib!=3.6.0
│ │  pandas
│ │  plotly>=4.9.0
│ │  redis
│ │  scikit-learn>=0.24.2
│ │  
│ │  [test]
│ │  codecov
│ │  fakeredis[lua]
│ │  kaleido
│ │  pytest
│ │ +
│ │ +[test:python_version >= "3.8"]
│ │ +scipy>=1.9.2
│ │   --- optuna-3.1.0b0/pyproject.toml
│ ├── +++ optuna-3.1.1/pyproject.toml
│ │┄ Files identical despite different names
│ │   --- optuna-3.1.0b0/setup.cfg
│ ├── +++ optuna-3.1.1/setup.cfg
│ │┄ Files identical despite different names
