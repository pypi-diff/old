# Comparing `tmp/gs_coordinator-0.9.0-py2.py3-none-macosx_10_9_x86_64.whl.zip` & `tmp/gs_coordinator-0.9.1-py2.py3-none-macosx_11_0_arm64.whl.zip`

## zipinfo {}

```diff
@@ -1,29 +1,30 @@
-Zip file size: 68074 bytes, number of entries: 27
--rw-r--r--  2.0 unx      711 b- defN 21-Dec-03 11:09 foo/__init__.py
--rw-rw-r--  2.0 unx     2345 b- defN 21-Dec-03 13:05 gs_coordinator-0.9.0.dist-info/RECORD
--rw-r--r--  2.0 unx      140 b- defN 21-Dec-03 13:05 gs_coordinator-0.9.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       18 b- defN 21-Dec-03 13:05 gs_coordinator-0.9.0.dist-info/top_level.txt
--rw-r--r--  2.0 unx    20393 b- defN 21-Dec-03 13:05 gs_coordinator-0.9.0.dist-info/METADATA
--rw-r--r--  2.0 unx    54753 b- defN 21-Dec-03 11:09 gscoordinator/coordinator.py
--rw-r--r--  2.0 unx     4451 b- defN 21-Dec-03 11:09 gscoordinator/dag_manager.py
--rw-r--r--  2.0 unx     1078 b- defN 21-Dec-03 13:05 gscoordinator/version.py
--rw-r--r--  2.0 unx     2653 b- defN 21-Dec-03 11:09 gscoordinator/object_manager.py
--rw-r--r--  2.0 unx      990 b- defN 21-Dec-03 11:09 gscoordinator/__init__.py
--rw-r--r--  2.0 unx    63521 b- defN 21-Dec-03 11:09 gscoordinator/utils.py
--rw-r--r--  2.0 unx    48851 b- defN 21-Dec-03 11:09 gscoordinator/cluster.py
--rw-r--r--  2.0 unx     3025 b- defN 21-Dec-03 11:09 gscoordinator/io_utils.py
--rw-r--r--  2.0 unx    20680 b- defN 21-Dec-03 11:09 gscoordinator/launcher.py
--rw-r--r--  2.0 unx     2412 b- defN 21-Dec-03 11:09 gscoordinator/learning.py
--rw-r--r--  2.0 unx       77 b- defN 21-Dec-03 11:09 gscoordinator/__main__.py
--rw-r--r--  2.0 unx      646 b- defN 21-Dec-03 11:09 gscoordinator/template/__init__.py
--rw-r--r--  2.0 unx     4340 b- defN 21-Dec-03 11:09 gscoordinator/template/pregel.pxd.template
--rw-r--r--  2.0 unx     4755 b- defN 21-Dec-03 11:09 gscoordinator/template/pie.pxd.template
--rw-r--r--  2.0 unx    16614 b- defN 21-Dec-03 11:09 gscoordinator/template/CMakeLists.template
--rw-r--r--  2.0 unx      646 b- defN 21-Dec-03 11:09 gscoordinator/builtin/__init__.py
--rw-r--r--  2.0 unx     7740 b- defN 21-Dec-03 11:09 gscoordinator/builtin/app/.gs_conf.yaml
--rw-r--r--  2.0 unx     3201 b- defN 21-Dec-03 13:05 gscoordinator/builtin/app/builtin_app.gar
--rw-r--r--  2.0 unx      646 b- defN 21-Dec-03 11:09 gscoordinator/builtin/app/__init__.py
--rw-r--r--  2.0 unx      646 b- defN 21-Dec-03 11:09 gscoordinator/hook/__init__.py
--rw-r--r--  2.0 unx      646 b- defN 21-Dec-03 11:09 gscoordinator/hook/prestop/__init__.py
--rw-r--r--  2.0 unx     1563 b- defN 21-Dec-03 11:09 gscoordinator/hook/prestop/__main__.py
-27 files, 267541 bytes uncompressed, 64268 bytes compressed:  76.0%
+Zip file size: 68431 bytes, number of entries: 28
+-rw-rw-r--  2.0 unx     2419 b- defN 21-Dec-23 08:11 gs_coordinator-0.9.1.dist-info/RECORD
+-rw-r--r--  2.0 unx      138 b- defN 21-Dec-23 08:11 gs_coordinator-0.9.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       18 b- defN 21-Dec-23 08:11 gs_coordinator-0.9.1.dist-info/top_level.txt
+-rw-r--r--  2.0 unx    20538 b- defN 21-Dec-23 08:11 gs_coordinator-0.9.1.dist-info/METADATA
+-rw-r--r--  2.0 unx      711 b- defN 21-Nov-14 07:52 foo/__init__.py
+-rw-r--r--  2.0 unx    55088 b- defN 21-Dec-22 02:29 gscoordinator/coordinator.py
+-rw-r--r--  2.0 unx     4451 b- defN 21-Nov-14 07:52 gscoordinator/dag_manager.py
+-rw-r--r--  2.0 unx      944 b- defN 21-Dec-20 03:54 gscoordinator/version.py
+-rw-r--r--  2.0 unx     2661 b- defN 21-Dec-20 03:54 gscoordinator/object_manager.py
+-rw-r--r--  2.0 unx      990 b- defN 21-Dec-03 12:25 gscoordinator/__init__.py
+-rw-r--r--  2.0 unx        6 b- defN 21-Dec-20 04:53 gscoordinator/VERSION
+-rw-r--r--  2.0 unx    62822 b- defN 21-Dec-23 06:52 gscoordinator/utils.py
+-rw-r--r--  2.0 unx    49323 b- defN 21-Dec-20 03:54 gscoordinator/cluster.py
+-rw-r--r--  2.0 unx     3383 b- defN 21-Dec-20 03:54 gscoordinator/io_utils.py
+-rw-r--r--  2.0 unx    20972 b- defN 21-Dec-20 03:54 gscoordinator/launcher.py
+-rw-r--r--  2.0 unx     2412 b- defN 21-Nov-14 07:52 gscoordinator/learning.py
+-rw-r--r--  2.0 unx       77 b- defN 21-Nov-14 07:52 gscoordinator/__main__.py
+-rw-r--r--  2.0 unx      646 b- defN 21-Nov-14 07:52 gscoordinator/template/__init__.py
+-rw-r--r--  2.0 unx     4340 b- defN 21-Nov-14 07:52 gscoordinator/template/pregel.pxd.template
+-rw-r--r--  2.0 unx     4755 b- defN 21-Nov-14 07:52 gscoordinator/template/pie.pxd.template
+-rw-r--r--  2.0 unx    16752 b- defN 21-Dec-23 06:52 gscoordinator/template/CMakeLists.template
+-rw-r--r--  2.0 unx      646 b- defN 21-Nov-14 07:52 gscoordinator/builtin/__init__.py
+-rw-r--r--  2.0 unx     7740 b- defN 21-Nov-14 07:52 gscoordinator/builtin/app/.gs_conf.yaml
+-rw-r--r--  2.0 unx     3201 b- defN 21-Dec-23 08:11 gscoordinator/builtin/app/builtin_app.gar
+-rw-r--r--  2.0 unx      646 b- defN 21-Nov-14 07:52 gscoordinator/builtin/app/__init__.py
+-rw-r--r--  2.0 unx      646 b- defN 21-Dec-03 12:25 gscoordinator/hook/__init__.py
+-rw-r--r--  2.0 unx      646 b- defN 21-Dec-03 12:25 gscoordinator/hook/prestop/__init__.py
+-rw-r--r--  2.0 unx     1563 b- defN 21-Dec-03 12:25 gscoordinator/hook/prestop/__main__.py
+28 files, 268534 bytes uncompressed, 64507 bytes compressed:  76.0%
```

## zipnote {}

```diff
@@ -1,20 +1,20 @@
-Filename: foo/__init__.py
+Filename: gs_coordinator-0.9.1.dist-info/RECORD
 Comment: 
 
-Filename: gs_coordinator-0.9.0.dist-info/RECORD
+Filename: gs_coordinator-0.9.1.dist-info/WHEEL
 Comment: 
 
-Filename: gs_coordinator-0.9.0.dist-info/WHEEL
+Filename: gs_coordinator-0.9.1.dist-info/top_level.txt
 Comment: 
 
-Filename: gs_coordinator-0.9.0.dist-info/top_level.txt
+Filename: gs_coordinator-0.9.1.dist-info/METADATA
 Comment: 
 
-Filename: gs_coordinator-0.9.0.dist-info/METADATA
+Filename: foo/__init__.py
 Comment: 
 
 Filename: gscoordinator/coordinator.py
 Comment: 
 
 Filename: gscoordinator/dag_manager.py
 Comment: 
@@ -24,14 +24,17 @@
 
 Filename: gscoordinator/object_manager.py
 Comment: 
 
 Filename: gscoordinator/__init__.py
 Comment: 
 
+Filename: gscoordinator/VERSION
+Comment: 
+
 Filename: gscoordinator/utils.py
 Comment: 
 
 Filename: gscoordinator/cluster.py
 Comment: 
 
 Filename: gscoordinator/io_utils.py
```

## gscoordinator/coordinator.py

```diff
@@ -28,15 +28,14 @@
 import queue
 import random
 import re
 import signal
 import string
 import sys
 import threading
-import time
 import traceback
 import urllib.parse
 import urllib.request
 from concurrent import futures
 
 import grpc
 from packaging import version
@@ -51,14 +50,15 @@
 from graphscope.framework.dag_utils import create_graph
 from graphscope.framework.dag_utils import create_loader
 from graphscope.framework.errors import AnalyticalEngineInternalError
 from graphscope.framework.graph_utils import normalize_parameter_edges
 from graphscope.framework.graph_utils import normalize_parameter_vertices
 from graphscope.framework.loader import Loader
 from graphscope.framework.utils import PipeMerger
+from graphscope.framework.utils import get_tempdir
 from graphscope.framework.utils import normalize_data_type_str
 from graphscope.proto import attr_value_pb2
 from graphscope.proto import coordinator_service_pb2_grpc
 from graphscope.proto import engine_service_pb2_grpc
 from graphscope.proto import error_codes_pb2
 from graphscope.proto import graph_def_pb2
 from graphscope.proto import message_pb2
@@ -212,20 +212,20 @@
                     session_id=self._session_id,
                     cluster_type=self._launcher.type(),
                     num_workers=self._launcher.num_workers,
                     engine_config=json.dumps(self._analytical_engine_config),
                     pod_name_list=self._engine_hosts.split(","),
                     namespace=self._k8s_namespace,
                 )
-            else:
-                context.set_code(grpc.StatusCode.ALREADY_EXISTS)
-                context.set_details(
-                    "Cannot setup more than one connection at the same time."
-                )
-                return message_pb2.ConnectSessionResponse()
+            # connect failed, more than one connection at the same time.
+            context.set_code(grpc.StatusCode.ALREADY_EXISTS)
+            context.set_details(
+                "Cannot setup more than one connection at the same time."
+            )
+            return message_pb2.ConnectSessionResponse()
         # Connect to serving coordinator.
         self._request = request
         try:
             self._analytical_engine_config = self._get_engine_config()
         except grpc.RpcError as e:
             logger.error(
                 "Get engine config failed, code: %s, details: %s",
@@ -341,14 +341,15 @@
                 or op.op == types_pb2.ADD_LABELS
             ):
                 op = self._maybe_register_graph(op, session_id)
 
         request = message_pb2.RunStepRequest(
             session_id=self._session_id, dag_def=dag_def
         )
+        error = None  # n.b.: avoid raising deep nested error stack to users
         try:
             response = self._analytical_engine_stub.RunStep(request)
         except grpc.RpcError as e:
             logger.error(
                 "Engine RunStep failed, code: %s, details: %s",
                 e.code().name,
                 e.details(),
@@ -356,17 +357,19 @@
             if e.code() == grpc.StatusCode.INTERNAL:
                 # TODO: make the stacktrace seperated from normal error messages
                 # Too verbose.
                 if len(e.details()) > 3072:  # 3k bytes
                     msg = f"{e.details()[:3072]} ... [truncated]"
                 else:
                     msg = e.details()
-                raise AnalyticalEngineInternalError(msg)
+                error = AnalyticalEngineInternalError(msg)
             else:
                 raise
+        if error is not None:
+            raise error
         op_results.extend(response.results)
         for r in response.results:
             op = self._key_to_op[r.key]
             if op.op not in (
                 types_pb2.CONTEXT_TO_NUMPY,
                 types_pb2.CONTEXT_TO_DATAFRAME,
                 types_pb2.REPORT_GRAPH,
@@ -378,15 +381,17 @@
             op = self._key_to_op[key]
             if op.op in (
                 types_pb2.CREATE_GRAPH,
                 types_pb2.PROJECT_GRAPH,
                 types_pb2.ADD_LABELS,
                 types_pb2.ADD_COLUMN,
             ):
-                schema_path = os.path.join("/tmp", op_result.graph_def.key + ".json")
+                schema_path = os.path.join(
+                    get_tempdir(), op_result.graph_def.key + ".json"
+                )
                 vy_info = graph_def_pb2.VineyardInfoPb()
                 op_result.graph_def.extension.Unpack(vy_info)
                 self._object_manager.put(
                     op_result.graph_def.key,
                     GraphMeta(
                         op_result.graph_def.key,
                         vy_info.vineyard_id,
@@ -588,28 +593,31 @@
                 )
             )
             dag_def = op_def_pb2.DagDef()
             dag_def.op.extend([op_def])
             register_request = message_pb2.RunStepRequest(
                 session_id=session_id, dag_def=dag_def
             )
+            error = None  # n.b.: avoid raising deep nested error stack to users
             try:
                 register_response = self._analytical_engine_stub.RunStep(
                     register_request
                 )
             except grpc.RpcError as e:
                 logger.error(
                     "Register graph failed, code: %s, details: %s",
                     e.code().name,
                     e.details(),
                 )
                 if e.code() == grpc.StatusCode.INTERNAL:
-                    raise AnalyticalEngineInternalError(e.details())
+                    error = AnalyticalEngineInternalError(e.details())
                 else:
                     raise
+            if error is not None:
+                raise error
             self._object_manager.put(
                 graph_sig,
                 LibMeta(
                     register_response.results[0].result,
                     "graph_frame",
                     graph_lib_path,
                 ),
@@ -618,25 +626,21 @@
             attr_value_pb2.AttrValue(s=graph_sig.encode("utf-8"))
         )
         return op
 
     def FetchLogs(self, request, context):
         while self._streaming_logs:
             try:
-                tag, message = self._pipe_merged.poll(timeout=2)
+                info_message, error_message = self._pipe_merged.poll(timeout=2)
             except queue.Empty:
-                tag, message = "", ""
+                info_message, error_message = "", ""
             except Exception as e:
-                tag, message = "out", "WARNING: failed to read log: %s" % e
+                info_message, error_message = "WARNING: failed to read log: %s" % e, ""
 
-            if tag and message:
-                if tag == "err":
-                    info_message, error_message = "", message
-                elif tag == "out":
-                    info_message, error_message = message, ""
+            if info_message and error_message:
                 if self._streaming_logs:
                     yield message_pb2.FetchLogsResponse(
                         info_message=info_message, error_message=error_message
                     )
 
     def CloseSession(self, request, context):
         """
@@ -707,18 +711,15 @@
                     code=error_codes_pb2.OK,
                     key=op.key,
                     result=maxgraph_external_endpoint.encode("utf-8")
                     if maxgraph_external_endpoint
                     else maxgraph_endpoint.encode("utf-8"),
                     extra_info=str(object_id).encode("utf-8"),
                 )
-            else:
-                raise RuntimeError(
-                    "Error code: {0}, message {1}".format(return_code, outs)
-                )
+            raise RuntimeError("Error code: {0}, message {1}".format(return_code, outs))
         except Exception as e:
             proc.kill()
             self._launcher.close_interactive_instance(object_id)
             raise RuntimeError("Create interactive instance failed.") from e
 
     def _execute_gremlin_query(self, op: op_def_pb2.OpDef):
         message = op.attr[types_pb2.GIE_GREMLIN_QUERY_MESSAGE].s.decode()
@@ -1054,26 +1055,29 @@
         return engine_service_pb2_grpc.EngineServiceStub(channel)
 
     def _get_engine_config(self):
         dag_def = create_single_op_dag(types_pb2.GET_ENGINE_CONFIG)
         request = message_pb2.RunStepRequest(
             session_id=self._session_id, dag_def=dag_def
         )
+        error = None  # n.b.: avoid raising deep nested error stack to users
         try:
             response = self._analytical_engine_stub.RunStep(request)
         except grpc.RpcError as e:
             logger.error(
                 "Get engine config failed, code: %s, details: %s",
                 e.code().name,
                 e.details(),
             )
             if e.code() == grpc.StatusCode.INTERNAL:
-                raise AnalyticalEngineInternalError(e.details())
+                error = AnalyticalEngineInternalError(e.details())
             else:
                 raise
+        if error is not None:
+            raise error
         config = json.loads(response.results[0].result.decode("utf-8"))
         config.update(self._launcher.get_engine_config())
         return config
 
     def _compile_lib_and_distribute(self, compile_func, lib_name, op):
         space = self._builtin_workspace
         if types_pb2.GAR in op.attr:
```

## gscoordinator/version.py

```diff
@@ -13,20 +13,14 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
-version_file_path = os.path.join(
-    os.path.dirname(os.path.abspath(__file__)), "..", "..", "VERSION"
-)
+version_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "VERSION")
 
-if os.path.isfile(version_file_path):
-    with open(version_file_path, "r", encoding="utf-8") as fp:
-        __version__ = fp.read().strip()
-    __version_tuple__ = (int(v) for v in __version__.split("."))
-else:
-    __version__ = "0.9.0"
-    __version_tuple__ = (0, 9, 0)
+with open(version_file_path, "r", encoding="utf-8") as fp:
+    __version__ = fp.read().strip()
+__version_tuple__ = (int(v) for v in __version__.split("."))
 
 del version_file_path
```

## gscoordinator/object_manager.py

```diff
@@ -16,17 +16,17 @@
 # limitations under the License.
 #
 
 from gremlin_python.driver.client import Client
 
 
 class LibMeta(object):
-    def __init__(self, key, type, lib_path):
+    def __init__(self, key, lib_type, lib_path):
         self.key = key
-        self.type = type
+        self.type = lib_type
         self.lib_path = lib_path
 
 
 class GraphMeta(object):
     def __init__(self, key, vineyard_id, graph_def, schema_path=None):
         self.key = key
         self.type = "graph"
```

## gscoordinator/utils.py

```diff
@@ -24,15 +24,14 @@
 import hashlib
 import inspect
 import json
 import logging
 import numbers
 import os
 import pickle
-import re
 import shutil
 import socket
 import subprocess
 import sys
 import threading
 import time
 import uuid
@@ -46,28 +45,30 @@
 
 import yaml
 from google.protobuf.any_pb2 import Any
 from graphscope.framework import utils
 from graphscope.framework.errors import CompilationError
 from graphscope.framework.graph_schema import GraphSchema
 from graphscope.framework.utils import PipeWatcher
+from graphscope.framework.utils import get_platform_info
+from graphscope.framework.utils import get_tempdir
 from graphscope.proto import attr_value_pb2
 from graphscope.proto import data_types_pb2
 from graphscope.proto import graph_def_pb2
 from graphscope.proto import op_def_pb2
 from graphscope.proto import types_pb2
 
 logger = logging.getLogger("graphscope")
 
 
 # runtime workspace
 try:
     WORKSPACE = os.environ["GRAPHSCOPE_RUNTIME"]
 except KeyError:
-    WORKSPACE = "/tmp/gs"
+    WORKSPACE = os.path.join(get_tempdir(), "gs")
 
 # COORDINATOR_HOME
 #   1) get from gscoordinator python module, if failed,
 #   2) infer from current directory
 try:
     import gscoordinator
 
@@ -159,32 +160,35 @@
         md_type,
         pregel_combine,
         java_jar_path,
         java_app_class,
     ) = _codegen_app_info(attr, DEFAULT_GS_CONFIG_FILE)
     graph_header, graph_type = _codegen_graph_info(attr)
     logger.info("Codegened graph type: %s, Graph header: %s", graph_type, graph_header)
+
+    app_sha256 = ""
     if app_type == "cpp_pie":
-        return hashlib.sha256(
+        app_sha256 = hashlib.sha256(
             f"{app_type}.{app_class}.{graph_type}".encode("utf-8")
         ).hexdigest()
     elif app_type == "java_pie":
         s = hashlib.sha256()
         # CAUTION!!!!!
         # We believe jar_path.java_app_class can uniquely define one java app
         s.update(f"{app_type}.{java_jar_path}.{java_app_class}".encode("utf-8"))
         if types_pb2.GAR in attr:
             s.update(attr[types_pb2.GAR].s)
-        return s.hexdigest()
+        app_sha256 = s.hexdigest()
     else:
         s = hashlib.sha256()
         s.update(f"{app_type}.{app_class}.{graph_type}".encode("utf-8"))
         if types_pb2.GAR in attr:
             s.update(attr[types_pb2.GAR].s)
-        return s.hexdigest()
+        app_sha256 = s.hexdigest()
+    return app_sha256
 
 
 def get_graph_sha256(attr):
     _, graph_class = _codegen_graph_info(attr)
     return hashlib.sha256(graph_class.encode("utf-8")).hexdigest()
 
 
@@ -236,19 +240,21 @@
     logger.info("Codegened graph type: %s, Graph header: %s", graph_type, graph_header)
 
     os.chdir(app_dir)
 
     module_name = ""
     # Output directory for java codegen
     java_codegen_out_dir = ""
+    # set OPAL_PREFIX in CMAKE_PREFIX_PATH
+    OPAL_PREFIX = os.environ.get("OPAL_PREFIX", "")
     cmake_commands = [
-        "cmake",
+        shutil.which("cmake"),
         ".",
         f"-DNETWORKX={engine_config['networkx']}",
-        f"-DCMAKE_PREFIX_PATH={GRAPHSCOPE_HOME}",
+        f"-DCMAKE_PREFIX_PATH='{GRAPHSCOPE_HOME};{OPAL_PREFIX}'",
     ]
     if app_type == "java_pie":
         if not os.path.isfile(GRAPE_PROCESSOR_JAR):
             raise RuntimeError("Grape runtime jar not found")
         # for java need to run preprocess
         java_codegen_out_dir = os.path.join(
             workspace, "{}-{}".format(JAVA_CODEGNE_OUTPUT_PREFIX, library_name)
@@ -291,15 +297,17 @@
             os.path.join(TEMPLATE_DIR, f"{pxd_name}.pxd.template"),
             os.path.join(app_dir, f"{pxd_name}.pxd"),
         )
         # Assume the gar will have and only have one .pyx file
         for pyx_file in glob.glob(app_dir + "/*.pyx"):
             module_name = os.path.splitext(os.path.basename(pyx_file))[0]
             cc_file = os.path.join(app_dir, module_name + ".cc")
-            subprocess.check_call(["cython", "-3", "--cplus", "-o", cc_file, pyx_file])
+            subprocess.check_call(
+                [shutil.which("cython"), "-3", "--cplus", "-o", cc_file, pyx_file]
+            )
         app_header = f"{module_name}.h"
 
     # replace and generate cmakelist
     cmakelists_file_tmp = os.path.join(TEMPLATE_DIR, "CMakeLists.template")
     cmakelists_file = os.path.join(app_dir, "CMakeLists.txt")
     with open(cmakelists_file_tmp, mode="r") as template:
         content = template.read()
@@ -325,34 +333,36 @@
         encoding="utf-8",
         errors="replace",
         stdout=subprocess.DEVNULL,
         stderr=subprocess.PIPE,
         universal_newlines=True,
         bufsize=1,
     )
-    cmake_stderr_watcher = PipeWatcher(cmake_process.stderr, sys.stdout)
+    cmake_stderr_watcher = PipeWatcher(cmake_process.stderr, sys.stderr)
     setattr(cmake_process, "stderr_watcher", cmake_stderr_watcher)
     cmake_process.wait()
 
     make_process = subprocess.Popen(
         [shutil.which("make"), "-j4"],
         env=os.environ.copy(),
         encoding="utf-8",
         errors="replace",
         stdout=subprocess.DEVNULL,
         stderr=subprocess.PIPE,
         universal_newlines=True,
         bufsize=1,
     )
-    make_stderr_watcher = PipeWatcher(make_process.stderr, sys.stdout)
+    make_stderr_watcher = PipeWatcher(make_process.stderr, sys.stderr)
     setattr(make_process, "stderr_watcher", make_stderr_watcher)
     make_process.wait()
     lib_path = get_lib_path(app_dir, library_name)
     if not os.path.isfile(lib_path):
-        raise CompilationError(f"Failed to compile app {app_class}")
+        raise CompilationError(
+            f"Failed to compile app {app_class} on platform {get_platform_info()}"
+        )
     return lib_path, java_jar_path, java_codegen_out_dir, app_type
 
 
 def compile_graph_frame(workspace: str, library_name, attr: dict, engine_config: dict):
     """Compile an application.
 
     Args:
@@ -378,19 +388,21 @@
     library_dir = os.path.join(workspace, library_name)
     os.makedirs(library_dir, exist_ok=True)
 
     os.chdir(library_dir)
 
     graph_type = attr[types_pb2.GRAPH_TYPE].graph_type
 
+    # set OPAL_PREFIX in CMAKE_PREFIX_PATH
+    OPAL_PREFIX = os.environ.get("OPAL_PREFIX", "")
     cmake_commands = [
-        "cmake",
+        shutil.which("cmake"),
         ".",
         f"-DNETWORKX={engine_config['networkx']}",
-        f"-DCMAKE_PREFIX_PATH={GRAPHSCOPE_HOME}",
+        f"-DCMAKE_PREFIX_PATH='{GRAPHSCOPE_HOME};{OPAL_PREFIX}'",
     ]
     if graph_type == graph_def_pb2.ARROW_PROPERTY:
         cmake_commands += ["-DPROPERTY_GRAPH_FRAME=True"]
     elif graph_type in (
         graph_def_pb2.ARROW_PROJECTED,
         graph_def_pb2.DYNAMIC_PROJECTED,
         graph_def_pb2.ARROW_FLATTENED,
@@ -419,34 +431,36 @@
         encoding="utf-8",
         errors="replace",
         stdout=subprocess.DEVNULL,
         stderr=subprocess.PIPE,
         universal_newlines=True,
         bufsize=1,
     )
-    cmake_stderr_watcher = PipeWatcher(cmake_process.stderr, sys.stdout)
+    cmake_stderr_watcher = PipeWatcher(cmake_process.stderr, sys.stderr)
     setattr(cmake_process, "stderr_watcher", cmake_stderr_watcher)
     cmake_process.wait()
 
     make_process = subprocess.Popen(
         [shutil.which("make"), "-j4"],
         env=os.environ.copy(),
         encoding="utf-8",
         errors="replace",
         stdout=subprocess.DEVNULL,
         stderr=subprocess.PIPE,
         universal_newlines=True,
         bufsize=1,
     )
-    make_stderr_watcher = PipeWatcher(make_process.stderr, sys.stdout)
+    make_stderr_watcher = PipeWatcher(make_process.stderr, sys.stderr)
     setattr(make_process, "stderr_watcher", make_stderr_watcher)
     make_process.wait()
     lib_path = get_lib_path(library_dir, library_name)
     if not os.path.isfile(lib_path):
-        raise CompilationError(f"Failed to compile graph {graph_class}")
+        raise CompilationError(
+            f"Failed to compile graph {graph_class} on platform {get_platform_info()}"
+        )
     return lib_path, None, None, None
 
 
 def op_pre_process(op, op_result_pool, key_to_op, **kwargs):  # noqa: C901
     if op.op == types_pb2.REPORT_GRAPH:
         return
     if op.op == types_pb2.CREATE_GRAPH:
@@ -583,15 +597,14 @@
 def _pre_process_for_create_learning_graph_op(op, op_result_pool, key_to_op, **kwargs):
     from graphscope.learning.graph import Graph as LearningGraph
 
     nodes = pickle.loads(op.attr[types_pb2.NODES].s)
     edges = pickle.loads(op.attr[types_pb2.EDGES].s)
     gen_labels = pickle.loads(op.attr[types_pb2.GLE_GEN_LABELS].s)
     # get graph schema
-    op, op_result_pool, key_to_op
     key_of_parent_op = op.parents[0]
     result = op_result_pool[key_of_parent_op]
     assert result.graph_def.extension.Is(graph_def_pb2.VineyardInfoPb.DESCRIPTOR)
     schema = GraphSchema()
     schema.from_graph_def(result.graph_def)
     # get graph vineyard id
     vy_info = graph_def_pb2.VineyardInfoPb()
@@ -940,58 +953,21 @@
         attr_value_pb2.AttrValue(s=graph_name.encode("utf-8"))
     )
     op.attr[types_pb2.ARROW_PROPERTY_DEFINITION].CopyFrom(attr)
     del op.attr[types_pb2.VERTEX_COLLECTIONS]
     del op.attr[types_pb2.EDGE_COLLECTIONS]
 
 
-def _tranform_numpy_selector(context_type, schema, selector):
-    if context_type == "tensor":
-        selector = None
-    if context_type == "vertex_data":
-        selector = transform_vertex_data_selector(selector)
-    if context_type == "labeled_vertex_data":
-        selector = transform_labeled_vertex_data_selector(schema, selector)
-    if context_type == "vertex_property":
-        selector = transform_vertex_property_data_selector(selector)
-    if context_type == "labeled_vertex_property":
-        selector = transform_labeled_vertex_property_data_selector(schema, selector)
-    return selector
-
-
-def _tranform_dataframe_selector(context_type, schema, selector):
-    selector = json.loads(selector)
-    if context_type == "tensor":
-        selector = {key: None for key, value in selector.items()}
-    if context_type == "vertex_data":
-        selector = {
-            key: transform_vertex_data_selector(value)
-            for key, value in selector.items()
-        }
-    if context_type == "labeled_vertex_data":
-        selector = {
-            key: transform_labeled_vertex_data_selector(schema, value)
-            for key, value in selector.items()
-        }
-    if context_type == "vertex_property":
-        selector = {
-            key: transform_vertex_property_data_selector(value)
-            for key, value in selector.items()
-        }
-    if context_type == "labeled_vertex_property":
-        selector = {
-            key: transform_labeled_vertex_property_data_selector(schema, value)
-            for key, value in selector.items()
-        }
-    return json.dumps(selector)
+# Below are selector transformation part, which will transform label / property
+# names to corresponding id.
 
 
 def _transform_vertex_data_v(selector):
-    if selector not in ("v.id", "v.data"):
-        raise SyntaxError("selector of v must be 'id' or 'data'")
+    if selector not in ("v.id", "v.data", "v.label_id"):
+        raise SyntaxError("selector of v must be 'id', 'data' or 'label_id'")
     return selector
 
 
 def _transform_vertex_data_e(selector):
     if selector not in ("e.src", "e.dst", "e.data"):
         raise SyntaxError("selector of e must be 'src', 'dst' or 'data'")
     return selector
@@ -1009,39 +985,37 @@
     return selector
 
 
 def _transform_labeled_vertex_data_v(schema, label, prop):
     label_id = schema.get_vertex_label_id(label)
     if prop == "id":
         return f"label{label_id}.{prop}"
-    else:
-        prop_id = schema.get_vertex_property_id(label, prop)
-        return f"label{label_id}.property{prop_id}"
+    prop_id = schema.get_vertex_property_id(label, prop)
+    return f"label{label_id}.property{prop_id}"
 
 
 def _transform_labeled_vertex_data_e(schema, label, prop):
     label_id = schema.get_edge_label_id(label)
     if prop in ("src", "dst"):
         return f"label{label_id}.{prop}"
-    else:
-        prop_id = schema.get_vertex_property_id(label, prop)
-        return f"label{label_id}.property{prop_id}"
+    prop_id = schema.get_vertex_property_id(label, prop)
+    return f"label{label_id}.property{prop_id}"
 
 
 def _transform_labeled_vertex_data_r(schema, label):
     label_id = schema.get_vertex_label_id(label)
     return f"label{label_id}"
 
 
 def _transform_labeled_vertex_property_data_r(schema, label, prop):
     label_id = schema.get_vertex_label_id(label)
     return f"label{label_id}.{prop}"
 
 
-def transform_vertex_data_selector(selector):
+def transform_vertex_data_selector(schema, selector):
     """Optional values:
     vertex selector: 'v.id', 'v.data'
     edge selector: 'e.src', 'e.dst', 'e.data'
     result selector: 'r'
     """
     if selector is None:
         raise RuntimeError("selector cannot be None")
@@ -1055,15 +1029,15 @@
     elif segments[0] == "r":
         selector = _transform_vertex_data_r(selector)
     else:
         raise SyntaxError(f"Invalid selector: {selector}, choose from v / e / r.")
     return selector
 
 
-def transform_vertex_property_data_selector(selector):
+def transform_vertex_property_data_selector(schema, selector):
     """Optional values:
     vertex selector: 'v.id', 'v.data'
     edge selector: 'e.src', 'e.dst', 'e.data'
     result selector format: 'r.y', y  denotes property name.
     """
     if selector is None:
         raise RuntimeError("selector cannot be None")
@@ -1122,14 +1096,34 @@
     elif ret_type == "e":
         ret = _transform_labeled_vertex_data_e(schema, *segments)
     elif ret_type == "r":
         ret = _transform_labeled_vertex_property_data_r(schema, *segments)
     return f"{ret_type}:{ret}"
 
 
+_transform_selector_func_map = {
+    "tensor": lambda _, _2: None,
+    "vertex_data": transform_vertex_data_selector,
+    "labeled_vertex_data": transform_labeled_vertex_data_selector,
+    "vertex_property": transform_vertex_property_data_selector,
+    "labeled_vertex_property": transform_labeled_vertex_property_data_selector,
+}
+
+
+def _tranform_numpy_selector(context_type, schema, selector):
+    return _transform_selector_func_map[context_type](schema, selector)
+
+
+def _tranform_dataframe_selector(context_type, schema, selector):
+    selector = json.loads(selector)
+    transform_func = _transform_selector_func_map[context_type]
+    selector = {key: transform_func(schema, value) for key, value in selector.items()}
+    return json.dumps(selector)
+
+
 def _extract_gar(app_dir: str, attr):
     """Extract gar to workspace
     Args:
         workspace (str): Working directory
         attr (`AttrValue`): Optionally it can contains the bytes of gar.
     """
     fp = BUILTIN_APP_RESOURCE_PATH  # default is builtin app resources.
@@ -1287,32 +1281,32 @@
     return dag
 
 
 def dump_as_json(schema, path):
     out = {}
     items = []
     idx = 0
-    for i in range(len(schema.vertex_labels)):
-        vertex = {"id": idx, "label": schema.vertex_labels[i], "type": "VERTEX"}
+    for i, vertex_label in enumerate(schema.vertex_labels):
+        vertex = {"id": idx, "label": vertex_label, "type": "VERTEX"}
         vertex["propertyDefList"] = []
-        for j in range(len(schema.vertex_property_names[i].s)):
+        for j, value in enumerate(schema.vertex_property_names[i].s):
             names = schema.vertex_property_names[i]
             types = schema.vertex_property_types[i]
             vertex["propertyDefList"].append(
                 {"id": j, "name": names.s[j], "data_type": types.s[j].upper()}
             )
         vertex["indexes"] = []
         vertex["indexes"].append({"propertyNames": [names.s[0]]})
         items.append(vertex)
         idx += 1
 
-    for i in range(len(schema.edge_labels)):
-        edge = {"id": idx, "label": schema.edge_labels[i], "type": "EDGE"}
+    for i, edge_label in enumerate(schema.edge_labels):
+        edge = {"id": idx, "label": edge_label, "type": "EDGE"}
         edge["propertyDefList"] = []
-        for j in range(len(schema.edge_property_names[i].s)):
+        for j, value in enumerate(schema.edge_property_names[i].s):
             names = schema.edge_property_names[i]
             types = schema.edge_property_types[i]
             edge["propertyDefList"].append(
                 {"id": j, "name": names.s[j], "data_type": types.s[j].upper()}
             )
         edge["rawRelationShips"] = []
         edge["rawRelationShips"].append(
@@ -1655,32 +1649,14 @@
 def check_argument(condition, message=None):
     if not condition:
         if message is None:
             message = "in '%s'" % inspect.stack()[1].code_context[0]
         raise ValueError(f"Check failed: {message}")
 
 
-def find_java():
-    java_exec = ""
-    if "JAVA_HOME" in os.environ:
-        java_exec = os.path.expandvars("$JAVA_HOME/bin/java")
-    if not java_exec:
-        java_exec = shutil.which("java")
-    if not java_exec:
-        raise RuntimeError("java command not found.")
-    return java_exec
-
-
-def get_java_version():
-    java_exec = find_java()
-    pattern = r'"(\d+\.\d+\.\d+).*"'
-    version = subprocess.check_output([java_exec, "-version"], stderr=subprocess.STDOUT)
-    return re.search(pattern, version.decode("utf-8")).groups()[0]
-
-
 def check_gremlin_server_ready(endpoint):
     def _check_task(endpoint):
         from gremlin_python.driver.client import Client
 
         if "MY_POD_NAME" in os.environ:
             # inner kubernetes env
             if endpoint == "localhost" or endpoint == "127.0.0.1":
```

## gscoordinator/cluster.py

```diff
@@ -52,14 +52,15 @@
 from graphscope.deploy.kubernetes.resource_builder import VolumeBuilder
 from graphscope.deploy.kubernetes.resource_builder import resolve_volume_builder
 from graphscope.deploy.kubernetes.utils import delete_kubernetes_object
 from graphscope.deploy.kubernetes.utils import get_kubernetes_object_info
 from graphscope.deploy.kubernetes.utils import get_service_endpoints
 from graphscope.deploy.kubernetes.utils import try_to_resolve_api_client
 from graphscope.framework.utils import PipeWatcher
+from graphscope.framework.utils import get_tempdir
 from graphscope.framework.utils import is_free_port
 from graphscope.proto import types_pb2
 
 from gscoordinator.launcher import Launcher
 from gscoordinator.utils import ANALYTICAL_ENGINE_PATH
 from gscoordinator.utils import GRAPHSCOPE_HOME
 from gscoordinator.utils import INTERACTIVE_ENGINE_SCRIPT
@@ -81,15 +82,15 @@
 
         {
             "my-deployment": "Deployment",
             "my-service": "Service"
         }
     """
 
-    _resource_object_path = "/tmp/resource_object"  # fixed
+    _resource_object_path = os.path.join(get_tempdir(), "resource_object")  # fixed
 
     def __init__(self, api_client):
         self._api_client = api_client
         self._resource_object = []
         self._meta_info = {}
 
     def append(self, target):
@@ -314,32 +315,32 @@
 
     @property
     def hosts(self):
         """String of a list of pod name, comma separated."""
         return ",".join(self._pod_name_list)
 
     def distribute_file(self, path):
-        dir = os.path.dirname(path)
+        d = os.path.dirname(path)
         for pod in self._pod_name_list:
             subprocess.check_call(
                 [
-                    "kubectl",
+                    shutil.which("kubectl"),
                     "exec",
                     pod,
                     "-c",
                     "engine",
                     "--",
                     "mkdir",
                     "-p",
-                    dir,
+                    d,
                 ]
             )
             subprocess.check_call(
                 [
-                    "kubectl",
+                    shutil.which("kubectl"),
                     "cp",
                     path,
                     "{}:{}".format(pod, path),
                     "-c",
                     "engine",
                 ]
             )
@@ -460,35 +461,37 @@
             }
         else:
             vineyard_socket_volume_type = "emptyDir"
             vineyard_socket_volume_fields = {}
         scheduler_builder.add_volume(
             VolumeBuilder(
                 name="vineyard-ipc-volume",
-                type=vineyard_socket_volume_type,
+                volume_type=vineyard_socket_volume_type,
                 field=vineyard_socket_volume_fields,
                 mounts_list=[
-                    {"mountPath": "/tmp/vineyard_workspace"},
+                    {"mountPath": os.path.join(get_tempdir(), "vineyard_workspace")},
                 ],
             )
         )
         # volume2 is for shared memory
         scheduler_builder.add_volume(
             VolumeBuilder(
                 name="host-shm",
-                type="emptyDir",
+                volume_type="emptyDir",
                 field={"medium": "Memory"},
                 mounts_list=[{"mountPath": "/dev/shm"}],
             )
         )
         # add env
         scheduler_builder.add_simple_envs(
             {
                 "GLOG_v": str(self._glog_level),
-                "VINEYARD_IPC_SOCKET": "/tmp/vineyard_workspace/vineyard.sock",
+                "VINEYARD_IPC_SOCKET": os.path.join(
+                    get_tempdir(), "vineyard_workspace", "vineyard.sock"
+                ),
                 "WITH_VINEYARD": "ON",
             }
         )
 
         # add vineyard container
         if not self._exists_vineyard_daemonset(
             self._saved_locals["vineyard_daemonset"]
@@ -558,26 +561,26 @@
             }
         else:
             vineyard_socket_volume_type = "emptyDir"
             vineyard_socket_volume_fields = {}
         engine_builder.add_volume(
             VolumeBuilder(
                 name="vineyard-ipc-volume",
-                type=vineyard_socket_volume_type,
+                volume_type=vineyard_socket_volume_type,
                 field=vineyard_socket_volume_fields,
                 mounts_list=[
                     {"mountPath": "/tmp/vineyard_workspace"},
                 ],
             )
         )
         # volume2 is for shared memory
         engine_builder.add_volume(
             VolumeBuilder(
                 name="host-shm",
-                type="emptyDir",
+                volume_type="emptyDir",
                 field={"medium": "Memory"},
                 mounts_list=[{"mountPath": "/dev/shm"}],
             )
         )
 
         # Mount aliyun demo dataset bucket
         if self._saved_locals["mount_dataset"] is not None:
@@ -596,15 +599,17 @@
             volume_builder = resolve_volume_builder(name, volume)
             if volume_builder is not None:
                 engine_builder.add_volume(volume_builder)
 
         # add env
         env = {
             "GLOG_v": str(self._glog_level),
-            "VINEYARD_IPC_SOCKET": "/tmp/vineyard_workspace/vineyard.sock",
+            "VINEYARD_IPC_SOCKET": os.path.join(
+                get_tempdir(), "vineyard_workspace", "vineyard.sock"
+            ),
             "WITH_VINEYARD": "ON",
             "PATH": os.environ["PATH"],
             "LD_LIBRARY_PATH": os.environ["LD_LIBRARY_PATH"],
             "DYLD_LIBRARY_PATH": os.environ["DYLD_LIBRARY_PATH"],
         }
         if "OPAL_PREFIX" in os.environ:
             env.update({"OPAL_PREFIX": os.environ["OPAL_PREFIX"]})
@@ -770,25 +775,25 @@
 
     def _get_vineyard_service_endpoint(self):
         # Always len(endpoints) >= 1
         endpoints = get_service_endpoints(
             api_client=self._api_client,
             namespace=self._saved_locals["namespace"],
             name=self._vineyard_service_name,
-            type=self._saved_locals["service_type"],
+            service_type=self._saved_locals["service_type"],
         )
         return endpoints[0]
 
     def _get_mars_scheduler_service_endpoint(self):
         # Always len(endpoints) >= 1
         endpoints = get_service_endpoints(
             api_client=self._api_client,
             namespace=self._saved_locals["namespace"],
             name=self._mars_service_name,
-            type=self._saved_locals["service_type"],
+            service_type=self._saved_locals["service_type"],
         )
         return endpoints[0]
 
     def _create_graphlearn_service(self, object_id, start_port, num_workers):
         targets = []
 
         labels = {
@@ -831,15 +836,15 @@
                     endpoints.sort(key=lambda ep: ep[1])
                     return [ep[0] for ep in endpoints]
         elif self._saved_locals["service_type"] == "LoadBalancer":
             endpoints = get_service_endpoints(
                 api_client=self._api_client,
                 namespace=self._saved_locals["namespace"],
                 name=self._gle_service_name_prefix + str(object_id),
-                type=self._saved_locals["service_type"],
+                service_type=self._saved_locals["service_type"],
             )
             return endpoints
         raise RuntimeError("Get graphlearn service endpoint failed.")
 
     def get_engine_config(self):
         config = {
             "vineyard_service_name": self.get_vineyard_service_name(),
@@ -1017,38 +1022,39 @@
 
     def _get_etcd_service_endpoint(self):
         # Always len(endpoints) >= 1
         endpoints = get_service_endpoints(
             api_client=self._api_client,
             namespace=self._saved_locals["namespace"],
             name=self._etcd_service_name,
-            type="ClusterIP",
+            service_type="ClusterIP",
         )
         return endpoints[0]
 
     def _launch_analytical_engine_locally(self):
         logger.info(
             "Starting GAE rpc service on {} ...".format(
                 str(self._analytical_engine_endpoint)
             )
         )
 
         # generate and distribute hostfile
-        with open("/tmp/kube_hosts", "w") as f:
-            for i in range(len(self._pod_ip_list)):
-                f.write("{} {}\n".format(self._pod_ip_list[i], self._pod_name_list[i]))
+        kube_hosts_path = os.path.join(get_tempdir(), "kube_hosts")
+        with open(kube_hosts_path, "w") as f:
+            for i, pod_ip in enumerate(self._pod_ip_list):
+                f.write("{} {}\n".format(pod_ip, self._pod_name_list[i]))
 
         for pod in self._pod_name_list:
             subprocess.check_call(
                 [
-                    "kubectl",
+                    shutil.which("kubectl"),
                     "-n",
                     self._saved_locals["namespace"],
                     "cp",
-                    "/tmp/kube_hosts",
+                    kube_hosts_path,
                     "{}:/tmp/hosts_of_nodes".format(pod),
                     "-c",
                     self._engine_container_name,
                 ]
             )
 
         # launch engine
@@ -1061,15 +1067,20 @@
         cmd.extend(["--vineyard_shared_mem", self._saved_locals["vineyard_shared_mem"]])
 
         if rmcp.openmpi():
             cmd.extend(["-v", str(self._glog_level)])
         else:
             mpi_env["GLOG_v"] = str(self._glog_level)
 
-        cmd.extend(["--vineyard_socket", "/tmp/vineyard_workspace/vineyard.sock"])
+        cmd.extend(
+            [
+                "--vineyard_socket",
+                os.path.join(get_tempdir(), "vineyard_workspace", "vineyard.sock"),
+            ]
+        )
         logger.info("Analytical engine launching command: {}".format(" ".join(cmd)))
 
         env = os.environ.copy()
         env.update(mpi_env)
 
         self._analytical_engine_process = subprocess.Popen(
             cmd,
```

## gscoordinator/io_utils.py

```diff
@@ -12,26 +12,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
-import sys
-import threading
 from queue import Queue
 
 from tqdm import tqdm
 
 
 class LoadingProgressTracker:
     progbar = None
     cur_stub = 0
 
     stubs = [
+        "PROGRESS--GRAPH-LOADING-DESCRIPTION-",
         "PROGRESS--GRAPH-LOADING-READ-VERTEX-0",
         "PROGRESS--GRAPH-LOADING-READ-VERTEX-100",
         "PROGRESS--GRAPH-LOADING-READ-EDGE-0",
         "PROGRESS--GRAPH-LOADING-READ-EDGE-100",
         "PROGRESS--GRAPH-LOADING-CONSTRUCT-VERTEX-0",
         "PROGRESS--GRAPH-LOADING-CONSTRUCT-VERTEX-100",
         "PROGRESS--GRAPH-LOADING-CONSTRUCT-EDGE-0",
@@ -72,27 +71,32 @@
 
     def flush(self):
         self._stream_backup.flush()
 
     def poll(self, block=True, timeout=None):
         return self._lines.get(block=block, timeout=timeout)
 
-    def _show_progress(self):
-        total = len(LoadingProgressTracker.stubs)
-        if LoadingProgressTracker.progbar is None:
+    def _show_progress(self, line):
+        total = len(LoadingProgressTracker.stubs) - 1
+        if "PROGRESS--GRAPH-LOADING-DESCRIPTION-" in line:
+            if LoadingProgressTracker.progbar is not None:
+                self._stream_backup.write("Error! Progress bar is not None!")
+                LoadingProgressTracker.progbar.close()
+            desc = line.split("-")[-1].strip()
             LoadingProgressTracker.progbar = tqdm(
-                desc="Loading Graph", total=total, file=sys.stderr
+                desc=desc, total=total, file=self._stream_backup
             )
-        LoadingProgressTracker.progbar.update(1)
-        LoadingProgressTracker.cur_stub += 1
-        if LoadingProgressTracker.cur_stub == total:
-            LoadingProgressTracker.cur_stub = 0
-            LoadingProgressTracker.progbar.close()
-            LoadingProgressTracker.progbar = None
-            sys.stderr.flush()
+        elif LoadingProgressTracker.progbar is not None:
+            LoadingProgressTracker.progbar.update(1)
+            LoadingProgressTracker.cur_stub += 1
+            if LoadingProgressTracker.cur_stub == total:
+                LoadingProgressTracker.cur_stub = 0
+                LoadingProgressTracker.progbar.close()
+                LoadingProgressTracker.progbar = None
+                self.flush()
 
     def _filter_progress(self, line):
         # print('show_progress: ', len(line), ", ", line)
         if "PROGRESS--GRAPH" not in line:
             return line
-        self._show_progress()
+        self._show_progress(line)
         return None
```

## gscoordinator/launcher.py

```diff
@@ -19,30 +19,30 @@
 import base64
 import json
 import logging
 import os
 import shutil
 import subprocess
 import sys
-import tempfile
 import time
 from abc import ABCMeta
 from abc import abstractmethod
 
 from graphscope.framework.utils import PipeWatcher
 from graphscope.framework.utils import get_free_port
+from graphscope.framework.utils import get_java_version
+from graphscope.framework.utils import get_tempdir
 from graphscope.framework.utils import is_free_port
 from graphscope.proto import types_pb2
 
 from gscoordinator.utils import ANALYTICAL_ENGINE_PATH
 from gscoordinator.utils import GRAPHSCOPE_HOME
 from gscoordinator.utils import INTERACTIVE_ENGINE_SCRIPT
 from gscoordinator.utils import WORKSPACE
 from gscoordinator.utils import ResolveMPICmdPrefix
-from gscoordinator.utils import get_java_version
 from gscoordinator.utils import get_timestamp
 from gscoordinator.utils import parse_as_glog_level
 
 logger = logging.getLogger("graphscope")
 
 
 class Launcher(metaclass=ABCMeta):
@@ -52,14 +52,17 @@
         self._analytical_engine_endpoint = None
 
         # add `${GRAPHSCOPE_HOME}/bin` to ${PATH}
         os.environ["PATH"] += os.pathsep + os.path.join(GRAPHSCOPE_HOME, "bin")
         # OPAL_PREFIX for openmpi
         if os.path.isdir(os.path.join(GRAPHSCOPE_HOME, "openmpi")):
             os.environ["OPAL_PREFIX"] = os.path.join(GRAPHSCOPE_HOME, "openmpi")
+        # Darwin is open-mpi
+        if os.path.isdir(os.path.join(GRAPHSCOPE_HOME, "open-mpi")):
+            os.environ["OPAL_PREFIX"] = os.path.join(GRAPHSCOPE_HOME, "open-mpi")
         # add '${GRAPHSCOPE_HOME}/lib' to ${LD_LIBRARY_PATH} to find libvineyard_internal_registry.so(dylib)
         if "LD_LIBRARY_PATH" in os.environ:
             os.environ["LD_LIBRARY_PATH"] = (
                 os.path.join(GRAPHSCOPE_HOME, "lib")
                 + os.pathsep
                 + os.environ["LD_LIBRARY_PATH"]
             )
@@ -129,17 +132,15 @@
         self._hosts = hosts
         self._vineyard_socket = vineyard_socket
         self._shared_mem = shared_mem
         self._glog_level = parse_as_glog_level(log_level)
         self._instance_id = instance_id
         self._timeout_seconds = timeout_seconds
 
-        self._vineyard_socket_prefix = os.path.join(
-            tempfile.gettempdir(), "vineyard.sock."
-        )
+        self._vineyard_socket_prefix = os.path.join(get_tempdir(), "vineyard.sock.")
 
         # A graphsope instance may has multiple session by reconnecting to coordinator
         self._instance_workspace = os.path.join(WORKSPACE, self._instance_id)
         os.makedirs(self._instance_workspace, exist_ok=True)
         # setting during client connect to coordinator
         self._session_workspace = None
 
@@ -181,19 +182,23 @@
             self._closed = True
 
     def set_session_workspace(self, session_id):
         self._session_workspace = os.path.join(self._instance_workspace, session_id)
         os.makedirs(self._session_workspace, exist_ok=True)
 
     def distribute_file(self, path):
-        dir = os.path.dirname(path)
+        d = os.path.dirname(path)
         for host in self._hosts.split(","):
             if host not in ("localhost", "127.0.0.1"):
-                subprocess.check_call(["ssh", host, "mkdir -p {}".format(dir)])
-                subprocess.check_call(["scp", "-r", path, "{}:{}".format(host, path)])
+                subprocess.check_call(
+                    [shutil.which("ssh"), host, "mkdir -p {}".format(d)]
+                )
+                subprocess.check_call(
+                    [shutil.which("scp"), "-r", path, "{}:{}".format(host, path)]
+                )
 
     def poll(self):
         if self._analytical_engine_process:
             return self._analytical_engine_process.poll()
         return -1
 
     @property
```

## gscoordinator/template/CMakeLists.template

```diff
@@ -119,18 +119,20 @@
 elseif("${GRAPHSCOPE_GCC_ABI_BACKWARDS_COMPATIBLE}" STREQUAL "1")
     add_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)
 endif()
 
 # try to find headers from package first----------------------------------------
 include_directories(${CMAKE_SOURCE_DIR})
 include_directories("${ANALYTICAL_ENGINE_HOME}/openmpi/include"
+                    "${ANALYTICAL_ENGINE_HOME}/open-mpi/include"
                     "${ANALYTICAL_ENGINE_HOME}/include"
                     "${ANALYTICAL_ENGINE_HOME}/include/vineyard"
                     "${ANALYTICAL_ENGINE_HOME}/include/graphscope"
                     "${ANALYTICAL_ENGINE_HOME}/include/graphscope/apps"
+                    "${ANALYTICAL_ENGINE_HOME}/include/graphscope/proto"
                     "${ANALYTICAL_ENGINE_HOME}/include/grape/analytical_apps")
 
 # find Threads------------------------------------------------------------------
 set(CMAKE_THREAD_PREFER_PTHREAD ON)
 find_package(Threads REQUIRED)
 
 # find Python-------------------------------------------------------------------
@@ -149,15 +151,15 @@
 find_package(libgrapelite QUIET)
 if (libgrapelite_FOUND)
   include_directories(AFTER SYSTEM "${LIBGRAPELITE_INCLUDE_DIRS}")
   include_directories(AFTER SYSTEM "${LIBGRAPELITE_INCLUDE_DIRS}/grape/analytical_apps")
 endif()
 
 # find vineyard-----------------------------------------
-find_package(vineyard 0.3.12 QUIET)
+find_package(vineyard 0.3.13 QUIET)
 if (vineyard_FOUND)
   include_directories(AFTER SYSTEM ${VINEYARD_INCLUDE_DIRS})
 endif()
 add_compile_options(-DENABLE_SELECTOR)
 
 # find Glog---------------------------------------------------------------------
 include("${ANALYTICAL_ENGINE_HOME}/${CMAKE_INSTALL_LIBDIR}/cmake/graphscope-analytical/cmake/FindGlog.cmake")
```

## gscoordinator/builtin/app/builtin_app.gar

### zipinfo {}

```diff
@@ -1,5 +1,5 @@
 Zip file size: 3201 bytes, number of entries: 3
--rw-r--r--  2.0 unx     7740 b- defN 21-Dec-03 11:09 .gs_conf.yaml
--rw-r--r--  2.0 unx      646 b- defN 21-Dec-03 11:09 __init__.py
--rw-r--r--  2.0 unx     1476 b- defN 21-Dec-03 13:05 builtin_app.zip
+-rw-r--r--  2.0 unx     7740 b- defN 21-Nov-14 15:52 .gs_conf.yaml
+-rw-r--r--  2.0 unx      646 b- defN 21-Nov-14 15:52 __init__.py
+-rw-r--r--  2.0 unx     1476 b- defN 21-Dec-23 16:11 builtin_app.zip
 3 files, 9862 bytes uncompressed, 2873 bytes compressed:  70.9%
```

### builtin_app.zip

 * *Command `'zipinfo {}'` failed with exit code 9. Standard output:*

 * *    Archive:  /tmp/diffoscope_4ilvgzot_090091/tmp3o30nvuc_ZipContainer/builtin_app.zip*

 * *    […]*

 * *Archive contents identical but files differ, possibly due to different compression levels. Falling back to binary comparison.*

```diff
@@ -1,8 +1,8 @@
-00000000: 504b 0304 1400 0000 0800 2f59 8353 9e96  PK......../Y.S..
+00000000: 504b 0304 1400 0000 0800 857e 6e53 9e96  PK.........~nS..
 00000010: a946 d603 0000 3c1e 0000 0d00 0000 2e67  .F....<........g
 00000020: 735f 636f 6e66 2e79 616d 6ccd 584d 8f9b  s_conf.yaml.XM..
 00000030: 3010 bdf7 57e4 0fb4 96f6 981b c9ee b651  0...W..........Q
 00000040: 572b 54b6 6a6f c821 5e70 e318 cb36 d94d  W+T.jo.!^p...6.M
 00000050: 7f7d 0702 d866 4962 4254 7180 44d8 f3e6  .}...fIbBTq.D...
 00000060: bdf1 d78c b110 f34f b3d9 e719 6669 3e9f  .......O....fi>.
 00000070: 099c 1289 f916 3ecd 66fa 20c8 7c96 0811  ......>.f. .|...
@@ -58,15 +58,15 @@
 00000390: ed4f 4dc2 337e 2738 ffa0 ba96 745d e8f1  .OM.3~'8....t]..
 000003a0: 8b35 6890 fcd7 eb09 e713 0c13 85d3 82ee  .5h.............
 000003b0: 0450 1970 6bb8 5251 65d3 776d 68c0 908b  .P.pk.RQe.wmh...
 000003c0: 3d41 f1e5 8969 31f4 4d15 e158 36fa bb65  =A...i1.M..X6..e
 000003d0: 9a1d 802e fe04 43b0 86b3 9610 7e5d deb5  ......C.....~]..
 000003e0: 30c6 3e99 97e5 0bf5 fbbd 6980 ae56 3ff4  0.>.......i..V?.
 000003f0: f6a7 2f0a fdf7 4183 8271 cdad d147 b1ff  ../...A..q...G..
-00000400: 0050 4b03 0414 0000 0008 002f 5983 537c  .PK......../Y.S|
+00000400: 0050 4b03 0414 0000 0008 0085 7e6e 537c  .PK.........~nS|
 00000410: bd2d f09a 0100 0086 0200 000b 0000 005f  .-............._
 00000420: 5f69 6e69 745f 5f2e 7079 6592 416f db30  _init__.pye.Ao.0
 00000430: 0c85 effd 156f c965 1b92 38c8 2e43 77f2  .....o.e..8..Cw.
 00000440: d26c 3516 3840 9cae e851 b669 9b80 2369  .l5.8@...Q.i..#i
 00000450: 925c 37ff 7e94 9b01 2da6 8b20 f1f1 e923  .\7.~...-.. ...#
 00000460: a9f9 0724 8377 49c9 3a21 fd0c 7b09 9dd1  ...$.wI.:!..{...
 00000470: 5f6e e658 7e5e a232 35eb f616 4368 965f  _n.X~^.25...Ch._
```

#### builtin_app.zip

```diff
@@ -1,8 +1,8 @@
-00000000: 504b 0304 1400 0000 0800 2f59 8353 9e96  PK......../Y.S..
+00000000: 504b 0304 1400 0000 0800 857e 6e53 9e96  PK.........~nS..
 00000010: a946 d603 0000 3c1e 0000 0d00 0000 2e67  .F....<........g
 00000020: 735f 636f 6e66 2e79 616d 6ccd 584d 8f9b  s_conf.yaml.XM..
 00000030: 3010 bdf7 57e4 0fb4 96f6 981b c9ee b651  0...W..........Q
 00000040: 572b 54b6 6a6f c821 5e70 e318 cb36 d94d  W+T.jo.!^p...6.M
 00000050: 7f7d 0702 d866 4962 4254 7180 44d8 f3e6  .}...fIbBTq.D...
 00000060: bdf1 d78c b110 f34f b3d9 e719 6669 3e9f  .......O....fi>.
 00000070: 099c 1289 f916 3ecd 66fa 20c8 7c96 0811  ......>.f. .|...
@@ -58,15 +58,15 @@
 00000390: ed4f 4dc2 337e 2738 ffa0 ba96 745d e8f1  .OM.3~'8....t]..
 000003a0: 8b35 6890 fcd7 eb09 e713 0c13 85d3 82ee  .5h.............
 000003b0: 0450 1970 6bb8 5251 65d3 776d 68c0 908b  .P.pk.RQe.wmh...
 000003c0: 3d41 f1e5 8969 31f4 4d15 e158 36fa bb65  =A...i1.M..X6..e
 000003d0: 9a1d 802e fe04 43b0 86b3 9610 7e5d deb5  ......C.....~]..
 000003e0: 30c6 3e99 97e5 0bf5 fbbd 6980 ae56 3ff4  0.>.......i..V?.
 000003f0: f6a7 2f0a fdf7 4183 8271 cdad d147 b1ff  ../...A..q...G..
-00000400: 0050 4b03 0414 0000 0008 002f 5983 537c  .PK......../Y.S|
+00000400: 0050 4b03 0414 0000 0008 0085 7e6e 537c  .PK.........~nS|
 00000410: bd2d f09a 0100 0086 0200 000b 0000 005f  .-............._
 00000420: 5f69 6e69 745f 5f2e 7079 6592 416f db30  _init__.pye.Ao.0
 00000430: 0c85 effd 156f c965 1b92 38c8 2e43 77f2  .....o.e..8..Cw.
 00000440: d26c 3516 3840 9cae e851 b669 9b80 2369  .l5.8@...Q.i..#i
 00000450: 925c 37ff 7e94 9b01 2da6 8b20 f1f1 e923  .\7.~...-.. ...#
 00000460: a9f9 0724 8377 49c9 3a21 fd0c 7b09 9dd1  ...$.wI.:!..{...
 00000470: 5f6e e658 7e5e a232 35eb f616 4368 965f  _n.X~^.25...Ch._
```

## Comparing `gs_coordinator-0.9.0.dist-info/RECORD` & `gs_coordinator-0.9.1.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 foo/__init__.py,sha256=nA2B1i17FCc8Rp5e9xU7jbkkxmpD5lY8r6FdGONxUFQ,711
+gscoordinator/VERSION,sha256=F5pTkJZuhcIHGofBsx3hPfZ0YGZRlvZSn4xJhoQvgeU,6
 gscoordinator/__init__.py,sha256=nUqENyB6dtqMgYfbJjblAdV2nemmlsaTMp_yTteBnOw,990
 gscoordinator/__main__.py,sha256=pfC-UJ0gz8i-p1lGAs2jBB8zALAz6PaBokLa9v6AYQ4,77
-gscoordinator/cluster.py,sha256=vomlAycxiCKGwLPdurHs4Pz4_ZpP_FOC6Kannm061i8,48851
-gscoordinator/coordinator.py,sha256=Xsc-tUe_uNh6j7B3yqsjT3UhiVyO11TBeNqUYCIB0-I,54753
+gscoordinator/cluster.py,sha256=0FMwrJ_DYAIe_qyLDjAmpjNbQslsrC3ULiKpHUfWyvo,49323
+gscoordinator/coordinator.py,sha256=XvdcbMcv19X00S4STaB27EkkdLlnuYXF5PkFS1eL1ds,55088
 gscoordinator/dag_manager.py,sha256=W9qQkdhz-9_hBrjiSz_sf2xNgZPGZkY2Rj7fIdKuSvE,4451
-gscoordinator/io_utils.py,sha256=OthAH21QH3wKjkYZB6eyhy9YSIT2HeUU0II927tky14,3025
-gscoordinator/launcher.py,sha256=Ih9lG1mV90Lus4b6a2xqZb4E69kR-5bVNlLV64_31RY,20680
+gscoordinator/io_utils.py,sha256=1II3IR6d8oOKQCuYOHONgRWNXZP97D9WFqbgPlsQe68,3383
+gscoordinator/launcher.py,sha256=8uuHkyPPybUyOlv4TPk5TaFldOjk2OVEkdF7-PKrULk,20972
 gscoordinator/learning.py,sha256=5-rhnMmc4w_gi9hzZ5sA1OqPKXGVuDgyNYGQMDjPaQ8,2412
-gscoordinator/object_manager.py,sha256=Pgv7pRZmcQdBQXf3yiQ4BRCAr3jf_rmsb2ZX9uYzVEo,2653
-gscoordinator/utils.py,sha256=q7YQSw4MTBgHNNphOqPyNDE1V-cQNI0krf11tx3rQmE,63521
-gscoordinator/version.py,sha256=HO9nnK_dkq9ZoZeOSqmHJ106P7lmZlqgtzW2w9BYN3g,1078
+gscoordinator/object_manager.py,sha256=0AtqAEfZG4kkfL7JWjE8rPjs0H7NsK1AYTwnV6qI_5A,2661
+gscoordinator/utils.py,sha256=3AXUQllrFhKdmoY6oOrrh87dIOJm6I6U3S8BXFsJRmk,62822
+gscoordinator/version.py,sha256=Ia5PwDdLuxZLJVMZj2t8P7irJXWpae3D-yC166BOLdI,944
 gscoordinator/builtin/__init__.py,sha256=TsAsMQRDcEJ9nRXDAeF-pJPapJrZPwVeb-TxxVp42aY,646
 gscoordinator/builtin/app/.gs_conf.yaml,sha256=4phIwyvTXU8C3SVr2vV7DmjslerniWLYEKn7YwYGGbI,7740
 gscoordinator/builtin/app/__init__.py,sha256=TsAsMQRDcEJ9nRXDAeF-pJPapJrZPwVeb-TxxVp42aY,646
-gscoordinator/builtin/app/builtin_app.gar,sha256=sC3kxAkPIGSVRrqxHdSXOdaZGheNXPv356bhTGjyRNU,3201
+gscoordinator/builtin/app/builtin_app.gar,sha256=el_aSXkm8Q8hVMRFflq84cwv37dMboYSFyxIPyzpKF0,3201
 gscoordinator/hook/__init__.py,sha256=jzzLJxC1gBgRLMb4xzX6vucYuNdosuWOtZKRj7Cal_o,646
 gscoordinator/hook/prestop/__init__.py,sha256=jzzLJxC1gBgRLMb4xzX6vucYuNdosuWOtZKRj7Cal_o,646
 gscoordinator/hook/prestop/__main__.py,sha256=FGaQBiN3E1pUiAvVp1enIbm0q2bk8ncRau576LnlrsI,1563
-gscoordinator/template/CMakeLists.template,sha256=ZF2A7X6oPIsyMFG3NMa6wr8-QbZCRa5eIuQApKpY3Qk,16614
+gscoordinator/template/CMakeLists.template,sha256=bBpCv65m46yszL_-qoWgz91lYcW6UZUb5Bd2DRJKy6o,16752
 gscoordinator/template/__init__.py,sha256=TsAsMQRDcEJ9nRXDAeF-pJPapJrZPwVeb-TxxVp42aY,646
 gscoordinator/template/pie.pxd.template,sha256=RmJe0mvHUyD8ggPvGii5ngv7qtiHfkoEm9tgmNrAOes,4755
 gscoordinator/template/pregel.pxd.template,sha256=Eh4tESR8KlmM30Mcw-eIUfiQNeadDj3WWGCUzkIsT2M,4340
-gs_coordinator-0.9.0.dist-info/METADATA,sha256=P2rmBW0PROrY4KTiBFi6v7-Y9a04lyU67rDPqvjRGVs,20393
-gs_coordinator-0.9.0.dist-info/WHEEL,sha256=G-EaRF3W8a-ylmBxjFKj9bXNl6EShznRuYtlcWCIQOk,140
-gs_coordinator-0.9.0.dist-info/top_level.txt,sha256=uL5TBKxpsoGmSgGkU8_1wOwBBx_PNItrWlTzW3Kz-Ec,18
-gs_coordinator-0.9.0.dist-info/RECORD,,
+gs_coordinator-0.9.1.dist-info/METADATA,sha256=p3mMi2DZCP1mybNnsvcngdX_H8m7L7s1cNaSZh78uXA,20538
+gs_coordinator-0.9.1.dist-info/WHEEL,sha256=-5Lw3o3Ue-I2Pf51UUpbu-ix_MFMgbqrgHANoNag9xU,138
+gs_coordinator-0.9.1.dist-info/top_level.txt,sha256=uL5TBKxpsoGmSgGkU8_1wOwBBx_PNItrWlTzW3Kz-Ec,18
+gs_coordinator-0.9.1.dist-info/RECORD,,
```

## Comparing `gs_coordinator-0.9.0.dist-info/METADATA` & `gs_coordinator-0.9.1.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: gs-coordinator
-Version: 0.9.0
+Version: 0.9.1
 Summary: UNKNOWN
 Home-page: https://github.com/alibaba/GraphScope
 Author: GraphScope Team, Damo Academy
 Author-email: graphscope@alibaba-inc.com
 License: Apache License 2.0
 Keywords: GraphScope,Graph Computations
 Platform: UNKNOWN
@@ -42,14 +42,15 @@
 <p align="center">
     A One-Stop Large-Scale Graph Computing System from Alibaba
 </p>
 
 [![GraphScope CI](https://github.com/alibaba/GraphScope/workflows/GraphScope%20CI/badge.svg)](https://github.com/alibaba/GraphScope/actions?workflow=GraphScope+CI)
 [![Coverage](https://codecov.io/gh/alibaba/GraphScope/branch/main/graph/badge.svg)](https://codecov.io/gh/alibaba/GraphScope)
 [![Playground](https://shields.io/badge/JupyterLab-Try%20GraphScope%20Now!-F37626?logo=jupyter)](https://try.graphscope.app)
+[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alibaba/GraphScope)
 [![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/graphscope)](https://artifacthub.io/packages/helm/graphscope/graphscope)
 [![Docs-en](https://shields.io/badge/Docs-English-blue?logo=Read%20The%20Docs)](https://graphscope.io/docs)
 [![FAQ-en](https://img.shields.io/badge/-FAQ-blue?logo=Read%20The%20Docs)](https://graphscope.io/docs/frequently_asked_questions.html)
 [![Docs-zh](https://shields.io/badge/Docs-%E4%B8%AD%E6%96%87-blue?logo=Read%20The%20Docs)](https://graphscope.io/docs/zh/)
 [![FAQ-zh](https://img.shields.io/badge/-FAQ%E4%B8%AD%E6%96%87-blue?logo=Read%20The%20Docs)](https://graphscope.io/docs/zh/frequently_asked_questions.html)
 [![README-zh](https://shields.io/badge/README-%E4%B8%AD%E6%96%87-blue)](README-zh.md)
 
@@ -402,15 +403,15 @@
 Documentation can be generated using Sphinx. Users can build the documentation using:
 
 ```bash
 # build the docs
 make graphscope-docs
 
 # to open preview on local
-open docs/_build/html/index.html
+open docs/_build/latest/html/index.html
 ```
 
 The latest version of online documentation can be found at https://graphscope.io/docs
 
 
 ## License
```

