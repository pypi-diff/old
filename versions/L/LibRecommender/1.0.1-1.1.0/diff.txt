--- tmp/LibRecommender-1.0.1.tar.gz
+++ tmp/LibRecommender-1.1.0.tar.gz
├── filetype from file(1)
│ @@ -1 +1 @@
│ -gzip compressed data, was "LibRecommender-1.0.1.tar", last modified: Mon Feb 20 14:10:50 2023, max compression
│ +gzip compressed data, was "LibRecommender-1.1.0.tar", last modified: Thu Apr  6 09:00:37 2023, max compression
│   --- LibRecommender-1.0.1.tar
├── +++ LibRecommender-1.1.0.tar
│ ├── file list
│ │ @@ -1,150 +1,163 @@
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.554699 LibRecommender-1.0.1/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     1083 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/LICENSE
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.514699 LibRecommender-1.0.1/LibRecommender.egg-info/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    28560 2023-02-20 14:10:50.000000 LibRecommender-1.0.1/LibRecommender.egg-info/PKG-INFO
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     3767 2023-02-20 14:10:50.000000 LibRecommender-1.0.1/LibRecommender.egg-info/SOURCES.txt
│ │ --rw-r--r--   0 runner    (1001) docker     (122)        1 2023-02-20 14:10:50.000000 LibRecommender-1.0.1/LibRecommender.egg-info/dependency_links.txt
│ │ --rw-r--r--   0 runner    (1001) docker     (122)       19 2023-02-20 14:10:50.000000 LibRecommender-1.0.1/LibRecommender.egg-info/requires.txt
│ │ --rw-r--r--   0 runner    (1001) docker     (122)       19 2023-02-20 14:10:50.000000 LibRecommender-1.0.1/LibRecommender.egg-info/top_level.txt
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      252 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/MANIFEST.in
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    28560 2023-02-20 14:10:50.558699 LibRecommender-1.0.1/PKG-INFO
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    27484 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/README.md
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.514699 LibRecommender-1.0.1/libreco/
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)       22 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/__init__.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.526699 LibRecommender-1.0.1/libreco/algorithms/
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     1108 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)   948314 2023-02-20 14:10:47.000000 LibRecommender-1.0.1/libreco/algorithms/_als.cpp
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     8810 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/_als.pyx
│ │ --rw-r--r--   0 runner    (1001) docker     (122)  1005286 2023-02-20 14:10:48.000000 LibRecommender-1.0.1/libreco/algorithms/_bpr.cpp
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    16314 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/_bpr.pyx
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    11096 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/als.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    10944 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/autoint.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    12608 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/bpr.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     8985 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/caser.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    11706 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/deepfm.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     3609 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/deepwalk.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    16904 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/din.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    11037 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/fm.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    10866 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/graphsage.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    12587 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/graphsage_dgl.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     2579 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/item2vec.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     6082 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/item_cf.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     4725 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/lightgcn.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     9209 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/ncf.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     5062 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/ngcf.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    10305 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/pinsage.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     7494 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/pinsage_dgl.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     9268 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/rnn4rec.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     5015 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/svd.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     6945 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/svdpp.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.526699 LibRecommender-1.0.1/libreco/algorithms/torch_modules/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      343 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/torch_modules/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     6585 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/torch_modules/graphsage_module.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     3580 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/torch_modules/lightgcn_module.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     5260 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/torch_modules/ngcf_module.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     7816 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/torch_modules/pinsage_module.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     6779 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/user_cf.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     8742 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/wave_net.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    11492 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/wide_deep.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    10623 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/youtube_ranking.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    12691 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/algorithms/youtube_retrieval.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.530699 LibRecommender-1.0.1/libreco/bases/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      284 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/bases/__init__.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     4607 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/bases/base.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    12659 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/bases/cf_base.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    17494 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/bases/embed_base.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     4618 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/bases/gensim_base.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      545 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/bases/meta.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    11750 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/bases/tf_base.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.534699 LibRecommender-1.0.1/libreco/data/
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)      599 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/data/__init__.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     6528 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/data/data_generator.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    27903 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/data/data_info.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    23292 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/data/dataset.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     4591 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/data/processing.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     4055 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/data/sequence.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    13797 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/data/split.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     4790 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/data/transformed.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.534699 LibRecommender-1.0.1/libreco/embedding/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)        0 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/embedding/__init__.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.534699 LibRecommender-1.0.1/libreco/evaluation/
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)       87 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/evaluation/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     4878 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/evaluation/computation.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     9636 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/evaluation/evaluate.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     2610 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/evaluation/metrics.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.534699 LibRecommender-1.0.1/libreco/feature/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     1073 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/feature/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    10016 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/feature/column.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     1993 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/feature/column_mapping.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    13477 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/feature/unique_features.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.538699 LibRecommender-1.0.1/libreco/graph/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      291 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/graph/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     3445 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/graph/from_dgl.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.538699 LibRecommender-1.0.1/libreco/prediction/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      256 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/prediction/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     4707 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/prediction/predict.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.538699 LibRecommender-1.0.1/libreco/recommendation/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      362 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/recommendation/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     1091 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/recommendation/cold_start.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     2428 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/recommendation/ranking.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     4216 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/recommendation/recommend.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.538699 LibRecommender-1.0.1/libreco/sampling/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      902 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/sampling/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     1420 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/sampling/batch_unit.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    19322 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/sampling/data_sampler.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     3817 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/sampling/negatives.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     5773 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/sampling/random_walks.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.542699 LibRecommender-1.0.1/libreco/tfops/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      742 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/tfops/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     1676 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/tfops/configs.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     3525 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/tfops/features.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     5560 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/tfops/layers.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     1603 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/tfops/loss.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     5128 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/tfops/rebuild.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     2713 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/tfops/variables.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)       99 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/tfops/version.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.546699 LibRecommender-1.0.1/libreco/torchops/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      696 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/torchops/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      693 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/torchops/configs.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     2258 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/torchops/features.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     3056 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/torchops/loss.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     5116 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/torchops/rebuild.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.546699 LibRecommender-1.0.1/libreco/training/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      453 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/training/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    18384 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/training/tf_trainer.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)    16534 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/training/torch_trainer.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     3690 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/training/trainer.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.550699 LibRecommender-1.0.1/libreco/utils/
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/utils/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)  1112943 2023-02-20 14:10:49.000000 LibRecommender-1.0.1/libreco/utils/_similarities.cpp
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    16885 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/utils/_similarities.pyx
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     1165 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/utils/constants.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)      319 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/utils/exception.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     1456 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/utils/initializers.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     2583 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/utils/misc.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)    14334 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/utils/sampling.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     4024 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/utils/save_load.py
│ │ --rwxr-xr-x   0 runner    (1001) docker     (122)     7383 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/utils/similarities.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     4238 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libreco/utils/validate.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.550699 LibRecommender-1.0.1/libserving/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)       22 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     2612 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/benchmark.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      981 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/request.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.554699 LibRecommender-1.0.1/libserving/sanic_serving/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)        0 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/sanic_serving/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      728 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/sanic_serving/common.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     2799 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/sanic_serving/embed_deploy.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     2876 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/sanic_serving/knn_deploy.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     7895 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/sanic_serving/tf_deploy.py
│ │ -drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-02-20 14:10:50.554699 LibRecommender-1.0.1/libserving/serialization/
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      295 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/serialization/__init__.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     1298 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/serialization/common.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     1322 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/serialization/embed.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      977 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/serialization/knn.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     4339 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/serialization/redis.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     5034 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/libserving/serialization/tfmodel.py
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     2664 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/pyproject.toml
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      134 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/requirements-serving.txt
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      217 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/requirements.txt
│ │ --rw-r--r--   0 runner    (1001) docker     (122)      158 2023-02-20 14:10:50.558699 LibRecommender-1.0.1/setup.cfg
│ │ --rw-r--r--   0 runner    (1001) docker     (122)     3394 2023-02-20 14:09:31.000000 LibRecommender-1.0.1/setup.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.182230 LibRecommender-1.1.0/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     1083 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/LICENSE
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.114229 LibRecommender-1.1.0/LibRecommender.egg-info/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)    28129 2023-04-06 09:00:37.000000 LibRecommender-1.1.0/LibRecommender.egg-info/PKG-INFO
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     4103 2023-04-06 09:00:37.000000 LibRecommender-1.1.0/LibRecommender.egg-info/SOURCES.txt
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-04-06 09:00:37.000000 LibRecommender-1.1.0/LibRecommender.egg-info/dependency_links.txt
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)       19 2023-04-06 09:00:37.000000 LibRecommender-1.1.0/LibRecommender.egg-info/requires.txt
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)       19 2023-04-06 09:00:37.000000 LibRecommender-1.1.0/LibRecommender.egg-info/top_level.txt
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      252 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/MANIFEST.in
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)    28129 2023-04-06 09:00:37.182230 LibRecommender-1.1.0/PKG-INFO
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)    27012 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/README.md
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.114229 LibRecommender-1.1.0/libreco/
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)       22 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/__init__.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.126229 LibRecommender-1.1.0/libreco/algorithms/
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     1108 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)   948377 2023-04-06 09:00:33.000000 LibRecommender-1.1.0/libreco/algorithms/_als.cpp
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     8810 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/_als.pyx
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)  1006940 2023-04-06 09:00:34.000000 LibRecommender-1.1.0/libreco/algorithms/_bpr.cpp
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    16314 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/_bpr.pyx
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    11324 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/als.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    11397 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/autoint.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    13580 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/bpr.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     8247 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/caser.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    12159 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/deepfm.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     3609 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/deepwalk.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    16879 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/din.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    11490 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/fm.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     6364 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/graphsage.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     7535 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/graphsage_dgl.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     2579 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/item2vec.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     6082 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/item_cf.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     4725 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/lightgcn.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     6148 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/ncf.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5062 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/ngcf.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     6749 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/pinsage.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     7210 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/pinsage_dgl.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     8733 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/rnn4rec.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     5438 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/svd.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     7620 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/svdpp.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.130229 LibRecommender-1.1.0/libreco/algorithms/torch_modules/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      343 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/torch_modules/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     6726 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/torch_modules/graphsage_module.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     3560 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/torch_modules/lightgcn_module.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5240 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/torch_modules/ngcf_module.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     7945 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/torch_modules/pinsage_module.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     6799 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/user_cf.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     7960 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/wave_net.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    11945 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/wide_deep.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    11077 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/youtube_ranking.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    12797 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/algorithms/youtube_retrieval.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.134230 LibRecommender-1.1.0/libreco/bases/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      387 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/bases/__init__.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     4731 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/bases/base.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)    13085 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/bases/cf_base.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)    19006 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/bases/embed_base.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     4865 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/bases/gensim_base.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      545 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/bases/meta.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5920 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/bases/sage_base.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     2853 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/bases/seq_base.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)    13317 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/bases/tf_base.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.138229 LibRecommender-1.1.0/libreco/batch/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      169 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/batch/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     3940 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/batch/batch_data.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5495 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/batch/batch_unit.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)    17782 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/batch/collators.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      153 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/batch/enums.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     3475 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/batch/sequence.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     2642 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/batch/tf_feed_dicts.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.142230 LibRecommender-1.1.0/libreco/data/
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)      599 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/data/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     2432 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/data/consumed.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    20843 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/data/data_info.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    26422 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/data/dataset.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     5947 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/data/processing.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    13829 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/data/split.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     4785 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/data/transformed.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.142230 LibRecommender-1.1.0/libreco/embedding/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/embedding/__init__.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.146230 LibRecommender-1.1.0/libreco/evaluation/
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)       87 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/evaluation/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     1896 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/evaluation/computation.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     5974 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/evaluation/evaluate.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     2259 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/evaluation/metrics.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.146230 LibRecommender-1.1.0/libreco/feature/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/feature/__init__.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     1979 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/feature/column_mapping.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5187 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/feature/multi_sparse.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     7384 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/feature/sparse.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     2167 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/feature/unique.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     8111 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/feature/update.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.150230 LibRecommender-1.1.0/libreco/graph/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      511 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/graph/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     4594 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/graph/from_dgl.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     4371 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/graph/message.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     6505 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/graph/neighbor_walk.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.150230 LibRecommender-1.1.0/libreco/prediction/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      256 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/prediction/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     4016 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/prediction/predict.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5571 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/prediction/preprocess.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.154230 LibRecommender-1.1.0/libreco/recommendation/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      362 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/recommendation/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     1091 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/recommendation/cold_start.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     4309 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/recommendation/preprocess.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     2428 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/recommendation/ranking.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     2261 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/recommendation/recommend.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.154230 LibRecommender-1.1.0/libreco/sampling/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      624 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/sampling/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     3921 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/sampling/negatives.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5773 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/sampling/random_walks.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.158230 LibRecommender-1.1.0/libreco/tfops/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      742 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/tfops/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     1676 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/tfops/configs.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     3525 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/tfops/features.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5644 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/tfops/layers.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     1603 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/tfops/loss.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5114 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/tfops/rebuild.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     2713 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/tfops/variables.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)       99 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/tfops/version.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.162230 LibRecommender-1.1.0/libreco/torchops/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      533 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/torchops/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      693 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/torchops/configs.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     3327 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/torchops/loss.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5559 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/torchops/rebuild.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.166230 LibRecommender-1.1.0/libreco/training/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/training/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     1467 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/training/dispatch.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)    10043 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/training/tf_trainer.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     9466 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/training/torch_trainer.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     1291 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/training/trainer.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.174230 LibRecommender-1.1.0/libreco/utils/
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/utils/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)  1114597 2023-04-06 09:00:36.000000 LibRecommender-1.1.0/libreco/utils/_similarities.cpp
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    16885 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/utils/_similarities.pyx
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     1481 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/utils/constants.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)      319 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/utils/exception.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     1456 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/utils/initializers.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     2583 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/utils/misc.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)    14284 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/utils/sampling.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     4024 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/utils/save_load.py
│ │ +-rwxr-xr-x   0 runner    (1001) docker     (122)     7383 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/utils/similarities.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5408 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libreco/utils/validate.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.174230 LibRecommender-1.1.0/libserving/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)       22 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      981 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/request.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.178230 LibRecommender-1.1.0/libserving/sanic_serving/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/sanic_serving/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     2642 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/sanic_serving/benchmark.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      728 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/sanic_serving/common.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     2799 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/sanic_serving/embed_deploy.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     2876 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/sanic_serving/knn_deploy.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     7895 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/sanic_serving/tf_deploy.py
│ │ +drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-06 09:00:37.182230 LibRecommender-1.1.0/libserving/serialization/
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      295 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/serialization/__init__.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     1298 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/serialization/common.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     1481 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/serialization/embed.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     1190 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/serialization/knn.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5144 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/serialization/redis.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     5288 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/libserving/serialization/tfmodel.py
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     2874 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/pyproject.toml
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      134 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/requirements-serving.txt
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      217 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/requirements.txt
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)      158 2023-04-06 09:00:37.182230 LibRecommender-1.1.0/setup.cfg
│ │ +-rw-r--r--   0 runner    (1001) docker     (122)     3394 2023-04-06 08:59:10.000000 LibRecommender-1.1.0/setup.py
│ │   --- LibRecommender-1.0.1/LICENSE
│ ├── +++ LibRecommender-1.1.0/LICENSE
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/LibRecommender.egg-info/PKG-INFO
│ ├── +++ LibRecommender-1.1.0/LibRecommender.egg-info/PKG-INFO
│ │┄ Files 4% similar despite different names
│ │ @@ -1,63 +1,63 @@
│ │  Metadata-Version: 2.1
│ │  Name: LibRecommender
│ │ -Version: 1.0.1
│ │ +Version: 1.1.0
│ │  Summary: Versatile end-to-end recommender system.
│ │  Home-page: https://github.com/massquantity/LibRecommender
│ │  Author: massquantity
│ │  Author-email: massquantity <jinxin_madie@163.com>
│ │  License: MIT
│ │ -Project-URL: documentation, https://librecommender.readthedocs.io/en/stable/
│ │ +Project-URL: documentation, https://librecommender.readthedocs.io/en/latest/
│ │  Project-URL: repository, https://github.com/massquantity/LibRecommender
│ │  Keywords: Collaborative Filtering,Recommender System
│ │  Classifier: Development Status :: 5 - Production/Stable
│ │  Classifier: Intended Audience :: Developers
│ │  Classifier: Intended Audience :: Education
│ │  Classifier: Intended Audience :: Science/Research
│ │  Classifier: License :: OSI Approved :: MIT License
│ │  Classifier: Programming Language :: Python :: 3.6
│ │  Classifier: Programming Language :: Python :: 3.7
│ │  Classifier: Programming Language :: Python :: 3.8
│ │  Classifier: Programming Language :: Python :: 3.9
│ │  Classifier: Programming Language :: Python :: 3.10
│ │  Classifier: Programming Language :: Cython
│ │ +Classifier: Programming Language :: Rust
│ │  Requires-Python: >=3.6
│ │  Description-Content-Type: text/markdown
│ │  License-File: LICENSE
│ │  
│ │  # LibRecommender
│ │  
│ │ -[![Build](https://img.shields.io/github/actions/workflow/status/massquantity/LibRecommender/wheels.yml?branch=master)](https://github.com/massquantity/LibRecommender/actions/workflows/wheels.yml)
│ │ +[![Build](https://img.shields.io/github/actions/workflow/status/massquantity/LibRecommender/wheels.yml?branch=master&logo=github)](https://github.com/massquantity/LibRecommender/actions/workflows/wheels.yml)
│ │  [![CI](https://github.com/massquantity/LibRecommender/actions/workflows/ci.yml/badge.svg)](https://github.com/massquantity/LibRecommender/actions/workflows/ci.yml)
│ │ -[![codecov](https://codecov.io/gh/massquantity/LibRecommender/branch/master/graph/badge.svg?token=BYOYFBUJRL)](https://codecov.io/gh/massquantity/LibRecommender)
│ │ +[![Codecov](https://img.shields.io/codecov/c/github/massquantity/LibRecommender?color=ffdfba&logo=codecov&logoColor=%2300FC87CD)](https://app.codecov.io/gh/massquantity/LibRecommender)
│ │  [![pypi](https://img.shields.io/pypi/v/LibRecommender?color=blue)](https://pypi.org/project/LibRecommender/)
│ │  [![Downloads](https://static.pepy.tech/personalized-badge/librecommender?period=total&units=international_system&left_color=grey&right_color=lightgrey&left_text=Downloads)](https://pepy.tech/project/librecommender)
│ │ -[![python versions](https://img.shields.io/pypi/pyversions/LibRecommender?logo=python&logoColor=ffffba)](https://pypi.org/project/LibRecommender/)
│ │  [![Codacy Badge](https://app.codacy.com/project/badge/Grade/860f0cb5339c41fba9bee5770d09be47)](https://www.codacy.com/gh/massquantity/LibRecommender/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=massquantity/LibRecommender&amp;utm_campaign=Badge_Grade)
│ │  [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
│ │  [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v1.json)](https://github.com/charliermarsh/ruff)
│ │ -[![platform](https://img.shields.io/badge/platform-linux%20%7C%20macos%20%7C%20windows-%23ffdfba)](https://img.shields.io/badge/platform-linux%20%7C%20macos%20%7C%20windows-%23ffdfba)
│ │ -[![Documentation Status](https://readthedocs.org/projects/librecommender/badge/?version=stable)](https://librecommender.readthedocs.io/en/stable/?badge=stable)
│ │ +[![Documentation Status](https://readthedocs.org/projects/librecommender/badge/?version=latest)](https://librecommender.readthedocs.io/en/latest/?badge=latest)
│ │  ![visitors](https://visitor-badge.glitch.me/badge?page_id=massquantity.LibRecommender&right_color=rgb(91,194,231))
│ │  [![License](https://img.shields.io/github/license/massquantity/LibRecommender?color=ff69b4)](https://github.com/massquantity/LibRecommender/blob/master/LICENSE)
│ │  
│ │  
│ │  ## Overview
│ │  
│ │  **LibRecommender** is an easy-to-use recommender system focused on end-to-end recommendation process. It contains a training([libreco](https://github.com/massquantity/LibRecommender/tree/master/libreco)) and serving([libserving](https://github.com/massquantity/LibRecommender/tree/master/libserving)) module to let users quickly train and deploy different kinds of recommendation models.
│ │  
│ │  **The main features are:**
│ │  
│ │  + Implements a number of popular recommendation algorithms such as FM, DIN, LightGCN etc. See [full algorithm list](#references).
│ │  + A hybrid recommender system, which allows user to use either collaborative-filtering or content-based features. New features can be added on the fly.
│ │ -+ Low memory usage, automatically convert categorical and multi-value categorical features to sparse representation.
│ │ -+ Support training for both explicit and implicit datasets, as well as negative sampling on implicit data.
│ │ -+ Provide end-to-end workflow, i.e. data handling / preprocessing -> model training -> evaluate -> save/load -> serving.
│ │ -+ Support cold-start prediction and recommendation.
│ │ -+ Provide unified and friendly API for all algorithms. 
│ │ ++ Low memory usage, automatically converts categorical and multi-value categorical features to sparse representation.
│ │ ++ Supports training for both explicit and implicit datasets, as well as negative sampling on implicit data.
│ │ ++ Provides end-to-end workflow, i.e. data handling / preprocessing -> model training -> evaluate -> save/load -> serving.
│ │ ++ Supports cold-start prediction and recommendation.
│ │ ++ Supports dynamic feature and sequence recommendation.
│ │ ++ Provides unified and friendly API for all algorithms. 
│ │  + Easy to retrain model with new users/items from new data.
│ │  
│ │  
│ │  
│ │  ## Usage
│ │  
│ │  #### _pure collaborative-filtering example_ : 
│ │ @@ -74,19 +74,14 @@
│ │  
│ │  # split whole data into three folds for training, evaluating and testing
│ │  train_data, eval_data, test_data = random_split(data, multi_ratios=[0.8, 0.1, 0.1])
│ │  
│ │  train_data, data_info = DatasetPure.build_trainset(train_data)
│ │  eval_data = DatasetPure.build_evalset(eval_data)
│ │  test_data = DatasetPure.build_testset(test_data)
│ │ -
│ │ -# sample negative items for each record
│ │ -train_data.build_negative_samples(data_info)
│ │ -eval_data.build_negative_samples(data_info)
│ │ -test_data.build_negative_samples(data_info)
│ │  print(data_info)  # n_users: 5894, n_items: 3253, data sparsity: 0.4172 %
│ │  
│ │  lightgcn = LightGCN(
│ │      task="ranking",
│ │      data_info=data_info,
│ │      loss_type="bpr",
│ │      embed_size=16,
│ │ @@ -95,23 +90,25 @@
│ │      batch_size=2048,
│ │      num_neg=1,
│ │      device="cuda",
│ │  )
│ │  # monitor metrics on eval data during training
│ │  lightgcn.fit(
│ │      train_data,
│ │ +    neg_sampling=True,
│ │      verbose=2,
│ │      eval_data=eval_data,
│ │      metrics=["loss", "roc_auc", "precision", "recall", "ndcg"],
│ │  )
│ │  
│ │  # do final evaluation on test data
│ │  evaluate(
│ │      model=lightgcn,
│ │      data=test_data,
│ │ +    neg_sampling=True,
│ │      metrics=["loss", "roc_auc", "precision", "recall", "ndcg"],
│ │  )
│ │  
│ │  # predict preference of user 2211 to item 110
│ │  lightgcn.predict(user=2211, item=110)
│ │  # recommend 7 items for user 2211
│ │  lightgcn.recommend_user(user=2211, n_rec=7)
│ │ @@ -140,32 +137,29 @@
│ │  user_col = ["sex", "age", "occupation"]
│ │  item_col = ["genre1", "genre2", "genre3"]
│ │  
│ │  train_data, data_info = DatasetFeat.build_trainset(
│ │      train_data, user_col, item_col, sparse_col, dense_col
│ │  )
│ │  test_data = DatasetFeat.build_testset(test_data)
│ │ -
│ │ -# sample negative items for each record
│ │ -train_data.build_negative_samples(data_info)  
│ │ -test_data.build_negative_samples(data_info)
│ │  print(data_info)  # n_users: 5962, n_items: 3226, data sparsity: 0.4185 %
│ │  
│ │  ytb_ranking = YouTubeRanking(
│ │      task="ranking",
│ │      data_info=data_info,
│ │      embed_size=16,
│ │      n_epochs=3,
│ │      lr=1e-4,
│ │      batch_size=512,
│ │      use_bn=True,
│ │      hidden_units=(128, 64, 32),
│ │  )
│ │  ytb_ranking.fit(
│ │      train_data,
│ │ +    neg_sampling=True,
│ │      verbose=2,
│ │      shuffle=True,
│ │      eval_data=test_data,
│ │      metrics=["loss", "roc_auc", "precision", "recall", "map", "ndcg"],
│ │  )
│ │  
│ │  # predict preference of user 2211 to item 110
│ │ @@ -192,15 +186,15 @@
│ │  
│ │  **Also note that your data should not contain missing values.**
│ │  
│ │  
│ │  
│ │  ## Documentation
│ │  
│ │ -The tutorials and API documentation are hosted on [librecommender.readthedocs.io](https://librecommender.readthedocs.io/en/stable/).
│ │ +The tutorials and API documentation are hosted on [librecommender.readthedocs.io](https://librecommender.readthedocs.io/en/latest/).
│ │  
│ │  The example scripts are under [examples/](https://github.com/massquantity/LibRecommender/tree/master/examples) folder.
│ │  
│ │  
│ │  
│ │  ## Installation & Dependencies 
│ │  
│ │ @@ -226,16 +220,16 @@
│ │  - PyTorch >= 1.10
│ │  - Numpy >= 1.19.5
│ │  - Pandas >= 1.0.0
│ │  - Scipy >= 1.2.1
│ │  - scikit-learn >= 0.20.0
│ │  - gensim >= 4.0.0
│ │  - tqdm
│ │ -- [nmslib](https://github.com/nmslib/nmslib) (optional, used in approximate similarity searching. See [Embedding](https://librecommender.readthedocs.io/en/stable/user_guide/embedding.html))
│ │ -- [DGL](https://github.com/dmlc/dgl) (optional, used in GraphSage and PinSage. See [Implementation Details](https://librecommender.readthedocs.io/en/stable/internal/implementation_details.html#pinsage))
│ │ +- [nmslib](https://github.com/nmslib/nmslib) (optional, used in approximate similarity searching. See [Embedding](https://librecommender.readthedocs.io/en/latest/user_guide/embedding.html))
│ │ +- [DGL](https://github.com/dmlc/dgl) (optional, used in GraphSage and PinSage. See [Implementation Details](https://librecommender.readthedocs.io/en/latest/internal/implementation_details.html#pinsage))
│ │  
│ │  If you are using Python 3.6, you also need to install [dataclasses](https://github.com/ericvsmith/dataclasses), which was first introduced in Python 3.7.
│ │  
│ │  LibRecommender is tested under TensorFlow 1.15, 2.5, 2.8 and 2.10. If you encounter any problem during running, feel free to open an issue.
│ │  
│ │  **Known issue**: Sometimes one may encounter errors like `ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject`. In this case try upgrading numpy, and version 1.22.0 or higher is probably a safe option.
│ │   --- LibRecommender-1.0.1/LibRecommender.egg-info/SOURCES.txt
│ ├── +++ LibRecommender-1.1.0/LibRecommender.egg-info/SOURCES.txt
│ │┄ Files 7% similar despite different names
│ │ @@ -49,59 +49,71 @@
│ │  libreco/algorithms/torch_modules/pinsage_module.py
│ │  libreco/bases/__init__.py
│ │  libreco/bases/base.py
│ │  libreco/bases/cf_base.py
│ │  libreco/bases/embed_base.py
│ │  libreco/bases/gensim_base.py
│ │  libreco/bases/meta.py
│ │ +libreco/bases/sage_base.py
│ │ +libreco/bases/seq_base.py
│ │  libreco/bases/tf_base.py
│ │ +libreco/batch/__init__.py
│ │ +libreco/batch/batch_data.py
│ │ +libreco/batch/batch_unit.py
│ │ +libreco/batch/collators.py
│ │ +libreco/batch/enums.py
│ │ +libreco/batch/sequence.py
│ │ +libreco/batch/tf_feed_dicts.py
│ │  libreco/data/__init__.py
│ │ -libreco/data/data_generator.py
│ │ +libreco/data/consumed.py
│ │  libreco/data/data_info.py
│ │  libreco/data/dataset.py
│ │  libreco/data/processing.py
│ │ -libreco/data/sequence.py
│ │  libreco/data/split.py
│ │  libreco/data/transformed.py
│ │  libreco/embedding/__init__.py
│ │  libreco/evaluation/__init__.py
│ │  libreco/evaluation/computation.py
│ │  libreco/evaluation/evaluate.py
│ │  libreco/evaluation/metrics.py
│ │  libreco/feature/__init__.py
│ │ -libreco/feature/column.py
│ │  libreco/feature/column_mapping.py
│ │ -libreco/feature/unique_features.py
│ │ +libreco/feature/multi_sparse.py
│ │ +libreco/feature/sparse.py
│ │ +libreco/feature/unique.py
│ │ +libreco/feature/update.py
│ │  libreco/graph/__init__.py
│ │  libreco/graph/from_dgl.py
│ │ +libreco/graph/message.py
│ │ +libreco/graph/neighbor_walk.py
│ │  libreco/prediction/__init__.py
│ │  libreco/prediction/predict.py
│ │ +libreco/prediction/preprocess.py
│ │  libreco/recommendation/__init__.py
│ │  libreco/recommendation/cold_start.py
│ │ +libreco/recommendation/preprocess.py
│ │  libreco/recommendation/ranking.py
│ │  libreco/recommendation/recommend.py
│ │  libreco/sampling/__init__.py
│ │ -libreco/sampling/batch_unit.py
│ │ -libreco/sampling/data_sampler.py
│ │  libreco/sampling/negatives.py
│ │  libreco/sampling/random_walks.py
│ │  libreco/tfops/__init__.py
│ │  libreco/tfops/configs.py
│ │  libreco/tfops/features.py
│ │  libreco/tfops/layers.py
│ │  libreco/tfops/loss.py
│ │  libreco/tfops/rebuild.py
│ │  libreco/tfops/variables.py
│ │  libreco/tfops/version.py
│ │  libreco/torchops/__init__.py
│ │  libreco/torchops/configs.py
│ │ -libreco/torchops/features.py
│ │  libreco/torchops/loss.py
│ │  libreco/torchops/rebuild.py
│ │  libreco/training/__init__.py
│ │ +libreco/training/dispatch.py
│ │  libreco/training/tf_trainer.py
│ │  libreco/training/torch_trainer.py
│ │  libreco/training/trainer.py
│ │  libreco/utils/__init__.py
│ │  libreco/utils/_similarities.cpp
│ │  libreco/utils/_similarities.pyx
│ │  libreco/utils/constants.py
│ │ @@ -109,17 +121,17 @@
│ │  libreco/utils/initializers.py
│ │  libreco/utils/misc.py
│ │  libreco/utils/sampling.py
│ │  libreco/utils/save_load.py
│ │  libreco/utils/similarities.py
│ │  libreco/utils/validate.py
│ │  libserving/__init__.py
│ │ -libserving/benchmark.py
│ │  libserving/request.py
│ │  libserving/sanic_serving/__init__.py
│ │ +libserving/sanic_serving/benchmark.py
│ │  libserving/sanic_serving/common.py
│ │  libserving/sanic_serving/embed_deploy.py
│ │  libserving/sanic_serving/knn_deploy.py
│ │  libserving/sanic_serving/tf_deploy.py
│ │  libserving/serialization/__init__.py
│ │  libserving/serialization/common.py
│ │  libserving/serialization/embed.py
│ │   --- LibRecommender-1.0.1/PKG-INFO
│ ├── +++ LibRecommender-1.1.0/PKG-INFO
│ │┄ Files 4% similar despite different names
│ │ @@ -1,63 +1,63 @@
│ │  Metadata-Version: 2.1
│ │  Name: LibRecommender
│ │ -Version: 1.0.1
│ │ +Version: 1.1.0
│ │  Summary: Versatile end-to-end recommender system.
│ │  Home-page: https://github.com/massquantity/LibRecommender
│ │  Author: massquantity
│ │  Author-email: massquantity <jinxin_madie@163.com>
│ │  License: MIT
│ │ -Project-URL: documentation, https://librecommender.readthedocs.io/en/stable/
│ │ +Project-URL: documentation, https://librecommender.readthedocs.io/en/latest/
│ │  Project-URL: repository, https://github.com/massquantity/LibRecommender
│ │  Keywords: Collaborative Filtering,Recommender System
│ │  Classifier: Development Status :: 5 - Production/Stable
│ │  Classifier: Intended Audience :: Developers
│ │  Classifier: Intended Audience :: Education
│ │  Classifier: Intended Audience :: Science/Research
│ │  Classifier: License :: OSI Approved :: MIT License
│ │  Classifier: Programming Language :: Python :: 3.6
│ │  Classifier: Programming Language :: Python :: 3.7
│ │  Classifier: Programming Language :: Python :: 3.8
│ │  Classifier: Programming Language :: Python :: 3.9
│ │  Classifier: Programming Language :: Python :: 3.10
│ │  Classifier: Programming Language :: Cython
│ │ +Classifier: Programming Language :: Rust
│ │  Requires-Python: >=3.6
│ │  Description-Content-Type: text/markdown
│ │  License-File: LICENSE
│ │  
│ │  # LibRecommender
│ │  
│ │ -[![Build](https://img.shields.io/github/actions/workflow/status/massquantity/LibRecommender/wheels.yml?branch=master)](https://github.com/massquantity/LibRecommender/actions/workflows/wheels.yml)
│ │ +[![Build](https://img.shields.io/github/actions/workflow/status/massquantity/LibRecommender/wheels.yml?branch=master&logo=github)](https://github.com/massquantity/LibRecommender/actions/workflows/wheels.yml)
│ │  [![CI](https://github.com/massquantity/LibRecommender/actions/workflows/ci.yml/badge.svg)](https://github.com/massquantity/LibRecommender/actions/workflows/ci.yml)
│ │ -[![codecov](https://codecov.io/gh/massquantity/LibRecommender/branch/master/graph/badge.svg?token=BYOYFBUJRL)](https://codecov.io/gh/massquantity/LibRecommender)
│ │ +[![Codecov](https://img.shields.io/codecov/c/github/massquantity/LibRecommender?color=ffdfba&logo=codecov&logoColor=%2300FC87CD)](https://app.codecov.io/gh/massquantity/LibRecommender)
│ │  [![pypi](https://img.shields.io/pypi/v/LibRecommender?color=blue)](https://pypi.org/project/LibRecommender/)
│ │  [![Downloads](https://static.pepy.tech/personalized-badge/librecommender?period=total&units=international_system&left_color=grey&right_color=lightgrey&left_text=Downloads)](https://pepy.tech/project/librecommender)
│ │ -[![python versions](https://img.shields.io/pypi/pyversions/LibRecommender?logo=python&logoColor=ffffba)](https://pypi.org/project/LibRecommender/)
│ │  [![Codacy Badge](https://app.codacy.com/project/badge/Grade/860f0cb5339c41fba9bee5770d09be47)](https://www.codacy.com/gh/massquantity/LibRecommender/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=massquantity/LibRecommender&amp;utm_campaign=Badge_Grade)
│ │  [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
│ │  [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v1.json)](https://github.com/charliermarsh/ruff)
│ │ -[![platform](https://img.shields.io/badge/platform-linux%20%7C%20macos%20%7C%20windows-%23ffdfba)](https://img.shields.io/badge/platform-linux%20%7C%20macos%20%7C%20windows-%23ffdfba)
│ │ -[![Documentation Status](https://readthedocs.org/projects/librecommender/badge/?version=stable)](https://librecommender.readthedocs.io/en/stable/?badge=stable)
│ │ +[![Documentation Status](https://readthedocs.org/projects/librecommender/badge/?version=latest)](https://librecommender.readthedocs.io/en/latest/?badge=latest)
│ │  ![visitors](https://visitor-badge.glitch.me/badge?page_id=massquantity.LibRecommender&right_color=rgb(91,194,231))
│ │  [![License](https://img.shields.io/github/license/massquantity/LibRecommender?color=ff69b4)](https://github.com/massquantity/LibRecommender/blob/master/LICENSE)
│ │  
│ │  
│ │  ## Overview
│ │  
│ │  **LibRecommender** is an easy-to-use recommender system focused on end-to-end recommendation process. It contains a training([libreco](https://github.com/massquantity/LibRecommender/tree/master/libreco)) and serving([libserving](https://github.com/massquantity/LibRecommender/tree/master/libserving)) module to let users quickly train and deploy different kinds of recommendation models.
│ │  
│ │  **The main features are:**
│ │  
│ │  + Implements a number of popular recommendation algorithms such as FM, DIN, LightGCN etc. See [full algorithm list](#references).
│ │  + A hybrid recommender system, which allows user to use either collaborative-filtering or content-based features. New features can be added on the fly.
│ │ -+ Low memory usage, automatically convert categorical and multi-value categorical features to sparse representation.
│ │ -+ Support training for both explicit and implicit datasets, as well as negative sampling on implicit data.
│ │ -+ Provide end-to-end workflow, i.e. data handling / preprocessing -> model training -> evaluate -> save/load -> serving.
│ │ -+ Support cold-start prediction and recommendation.
│ │ -+ Provide unified and friendly API for all algorithms. 
│ │ ++ Low memory usage, automatically converts categorical and multi-value categorical features to sparse representation.
│ │ ++ Supports training for both explicit and implicit datasets, as well as negative sampling on implicit data.
│ │ ++ Provides end-to-end workflow, i.e. data handling / preprocessing -> model training -> evaluate -> save/load -> serving.
│ │ ++ Supports cold-start prediction and recommendation.
│ │ ++ Supports dynamic feature and sequence recommendation.
│ │ ++ Provides unified and friendly API for all algorithms. 
│ │  + Easy to retrain model with new users/items from new data.
│ │  
│ │  
│ │  
│ │  ## Usage
│ │  
│ │  #### _pure collaborative-filtering example_ : 
│ │ @@ -74,19 +74,14 @@
│ │  
│ │  # split whole data into three folds for training, evaluating and testing
│ │  train_data, eval_data, test_data = random_split(data, multi_ratios=[0.8, 0.1, 0.1])
│ │  
│ │  train_data, data_info = DatasetPure.build_trainset(train_data)
│ │  eval_data = DatasetPure.build_evalset(eval_data)
│ │  test_data = DatasetPure.build_testset(test_data)
│ │ -
│ │ -# sample negative items for each record
│ │ -train_data.build_negative_samples(data_info)
│ │ -eval_data.build_negative_samples(data_info)
│ │ -test_data.build_negative_samples(data_info)
│ │  print(data_info)  # n_users: 5894, n_items: 3253, data sparsity: 0.4172 %
│ │  
│ │  lightgcn = LightGCN(
│ │      task="ranking",
│ │      data_info=data_info,
│ │      loss_type="bpr",
│ │      embed_size=16,
│ │ @@ -95,23 +90,25 @@
│ │      batch_size=2048,
│ │      num_neg=1,
│ │      device="cuda",
│ │  )
│ │  # monitor metrics on eval data during training
│ │  lightgcn.fit(
│ │      train_data,
│ │ +    neg_sampling=True,
│ │      verbose=2,
│ │      eval_data=eval_data,
│ │      metrics=["loss", "roc_auc", "precision", "recall", "ndcg"],
│ │  )
│ │  
│ │  # do final evaluation on test data
│ │  evaluate(
│ │      model=lightgcn,
│ │      data=test_data,
│ │ +    neg_sampling=True,
│ │      metrics=["loss", "roc_auc", "precision", "recall", "ndcg"],
│ │  )
│ │  
│ │  # predict preference of user 2211 to item 110
│ │  lightgcn.predict(user=2211, item=110)
│ │  # recommend 7 items for user 2211
│ │  lightgcn.recommend_user(user=2211, n_rec=7)
│ │ @@ -140,32 +137,29 @@
│ │  user_col = ["sex", "age", "occupation"]
│ │  item_col = ["genre1", "genre2", "genre3"]
│ │  
│ │  train_data, data_info = DatasetFeat.build_trainset(
│ │      train_data, user_col, item_col, sparse_col, dense_col
│ │  )
│ │  test_data = DatasetFeat.build_testset(test_data)
│ │ -
│ │ -# sample negative items for each record
│ │ -train_data.build_negative_samples(data_info)  
│ │ -test_data.build_negative_samples(data_info)
│ │  print(data_info)  # n_users: 5962, n_items: 3226, data sparsity: 0.4185 %
│ │  
│ │  ytb_ranking = YouTubeRanking(
│ │      task="ranking",
│ │      data_info=data_info,
│ │      embed_size=16,
│ │      n_epochs=3,
│ │      lr=1e-4,
│ │      batch_size=512,
│ │      use_bn=True,
│ │      hidden_units=(128, 64, 32),
│ │  )
│ │  ytb_ranking.fit(
│ │      train_data,
│ │ +    neg_sampling=True,
│ │      verbose=2,
│ │      shuffle=True,
│ │      eval_data=test_data,
│ │      metrics=["loss", "roc_auc", "precision", "recall", "map", "ndcg"],
│ │  )
│ │  
│ │  # predict preference of user 2211 to item 110
│ │ @@ -192,15 +186,15 @@
│ │  
│ │  **Also note that your data should not contain missing values.**
│ │  
│ │  
│ │  
│ │  ## Documentation
│ │  
│ │ -The tutorials and API documentation are hosted on [librecommender.readthedocs.io](https://librecommender.readthedocs.io/en/stable/).
│ │ +The tutorials and API documentation are hosted on [librecommender.readthedocs.io](https://librecommender.readthedocs.io/en/latest/).
│ │  
│ │  The example scripts are under [examples/](https://github.com/massquantity/LibRecommender/tree/master/examples) folder.
│ │  
│ │  
│ │  
│ │  ## Installation & Dependencies 
│ │  
│ │ @@ -226,16 +220,16 @@
│ │  - PyTorch >= 1.10
│ │  - Numpy >= 1.19.5
│ │  - Pandas >= 1.0.0
│ │  - Scipy >= 1.2.1
│ │  - scikit-learn >= 0.20.0
│ │  - gensim >= 4.0.0
│ │  - tqdm
│ │ -- [nmslib](https://github.com/nmslib/nmslib) (optional, used in approximate similarity searching. See [Embedding](https://librecommender.readthedocs.io/en/stable/user_guide/embedding.html))
│ │ -- [DGL](https://github.com/dmlc/dgl) (optional, used in GraphSage and PinSage. See [Implementation Details](https://librecommender.readthedocs.io/en/stable/internal/implementation_details.html#pinsage))
│ │ +- [nmslib](https://github.com/nmslib/nmslib) (optional, used in approximate similarity searching. See [Embedding](https://librecommender.readthedocs.io/en/latest/user_guide/embedding.html))
│ │ +- [DGL](https://github.com/dmlc/dgl) (optional, used in GraphSage and PinSage. See [Implementation Details](https://librecommender.readthedocs.io/en/latest/internal/implementation_details.html#pinsage))
│ │  
│ │  If you are using Python 3.6, you also need to install [dataclasses](https://github.com/ericvsmith/dataclasses), which was first introduced in Python 3.7.
│ │  
│ │  LibRecommender is tested under TensorFlow 1.15, 2.5, 2.8 and 2.10. If you encounter any problem during running, feel free to open an issue.
│ │  
│ │  **Known issue**: Sometimes one may encounter errors like `ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject`. In this case try upgrading numpy, and version 1.22.0 or higher is probably a safe option.
│ │   --- LibRecommender-1.0.1/README.md
│ ├── +++ LibRecommender-1.1.0/README.md
│ │┄ Files 2% similar despite different names
│ │ @@ -1,37 +1,36 @@
│ │  # LibRecommender
│ │  
│ │ -[![Build](https://img.shields.io/github/actions/workflow/status/massquantity/LibRecommender/wheels.yml?branch=master)](https://github.com/massquantity/LibRecommender/actions/workflows/wheels.yml)
│ │ +[![Build](https://img.shields.io/github/actions/workflow/status/massquantity/LibRecommender/wheels.yml?branch=master&logo=github)](https://github.com/massquantity/LibRecommender/actions/workflows/wheels.yml)
│ │  [![CI](https://github.com/massquantity/LibRecommender/actions/workflows/ci.yml/badge.svg)](https://github.com/massquantity/LibRecommender/actions/workflows/ci.yml)
│ │ -[![codecov](https://codecov.io/gh/massquantity/LibRecommender/branch/master/graph/badge.svg?token=BYOYFBUJRL)](https://codecov.io/gh/massquantity/LibRecommender)
│ │ +[![Codecov](https://img.shields.io/codecov/c/github/massquantity/LibRecommender?color=ffdfba&logo=codecov&logoColor=%2300FC87CD)](https://app.codecov.io/gh/massquantity/LibRecommender)
│ │  [![pypi](https://img.shields.io/pypi/v/LibRecommender?color=blue)](https://pypi.org/project/LibRecommender/)
│ │  [![Downloads](https://static.pepy.tech/personalized-badge/librecommender?period=total&units=international_system&left_color=grey&right_color=lightgrey&left_text=Downloads)](https://pepy.tech/project/librecommender)
│ │ -[![python versions](https://img.shields.io/pypi/pyversions/LibRecommender?logo=python&logoColor=ffffba)](https://pypi.org/project/LibRecommender/)
│ │  [![Codacy Badge](https://app.codacy.com/project/badge/Grade/860f0cb5339c41fba9bee5770d09be47)](https://www.codacy.com/gh/massquantity/LibRecommender/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=massquantity/LibRecommender&amp;utm_campaign=Badge_Grade)
│ │  [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
│ │  [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v1.json)](https://github.com/charliermarsh/ruff)
│ │ -[![platform](https://img.shields.io/badge/platform-linux%20%7C%20macos%20%7C%20windows-%23ffdfba)](https://img.shields.io/badge/platform-linux%20%7C%20macos%20%7C%20windows-%23ffdfba)
│ │ -[![Documentation Status](https://readthedocs.org/projects/librecommender/badge/?version=stable)](https://librecommender.readthedocs.io/en/stable/?badge=stable)
│ │ +[![Documentation Status](https://readthedocs.org/projects/librecommender/badge/?version=latest)](https://librecommender.readthedocs.io/en/latest/?badge=latest)
│ │  ![visitors](https://visitor-badge.glitch.me/badge?page_id=massquantity.LibRecommender&right_color=rgb(91,194,231))
│ │  [![License](https://img.shields.io/github/license/massquantity/LibRecommender?color=ff69b4)](https://github.com/massquantity/LibRecommender/blob/master/LICENSE)
│ │  
│ │  
│ │  ## Overview
│ │  
│ │  **LibRecommender** is an easy-to-use recommender system focused on end-to-end recommendation process. It contains a training([libreco](https://github.com/massquantity/LibRecommender/tree/master/libreco)) and serving([libserving](https://github.com/massquantity/LibRecommender/tree/master/libserving)) module to let users quickly train and deploy different kinds of recommendation models.
│ │  
│ │  **The main features are:**
│ │  
│ │  + Implements a number of popular recommendation algorithms such as FM, DIN, LightGCN etc. See [full algorithm list](#references).
│ │  + A hybrid recommender system, which allows user to use either collaborative-filtering or content-based features. New features can be added on the fly.
│ │ -+ Low memory usage, automatically convert categorical and multi-value categorical features to sparse representation.
│ │ -+ Support training for both explicit and implicit datasets, as well as negative sampling on implicit data.
│ │ -+ Provide end-to-end workflow, i.e. data handling / preprocessing -> model training -> evaluate -> save/load -> serving.
│ │ -+ Support cold-start prediction and recommendation.
│ │ -+ Provide unified and friendly API for all algorithms. 
│ │ ++ Low memory usage, automatically converts categorical and multi-value categorical features to sparse representation.
│ │ ++ Supports training for both explicit and implicit datasets, as well as negative sampling on implicit data.
│ │ ++ Provides end-to-end workflow, i.e. data handling / preprocessing -> model training -> evaluate -> save/load -> serving.
│ │ ++ Supports cold-start prediction and recommendation.
│ │ ++ Supports dynamic feature and sequence recommendation.
│ │ ++ Provides unified and friendly API for all algorithms. 
│ │  + Easy to retrain model with new users/items from new data.
│ │  
│ │  
│ │  
│ │  ## Usage
│ │  
│ │  #### _pure collaborative-filtering example_ : 
│ │ @@ -48,19 +47,14 @@
│ │  
│ │  # split whole data into three folds for training, evaluating and testing
│ │  train_data, eval_data, test_data = random_split(data, multi_ratios=[0.8, 0.1, 0.1])
│ │  
│ │  train_data, data_info = DatasetPure.build_trainset(train_data)
│ │  eval_data = DatasetPure.build_evalset(eval_data)
│ │  test_data = DatasetPure.build_testset(test_data)
│ │ -
│ │ -# sample negative items for each record
│ │ -train_data.build_negative_samples(data_info)
│ │ -eval_data.build_negative_samples(data_info)
│ │ -test_data.build_negative_samples(data_info)
│ │  print(data_info)  # n_users: 5894, n_items: 3253, data sparsity: 0.4172 %
│ │  
│ │  lightgcn = LightGCN(
│ │      task="ranking",
│ │      data_info=data_info,
│ │      loss_type="bpr",
│ │      embed_size=16,
│ │ @@ -69,23 +63,25 @@
│ │      batch_size=2048,
│ │      num_neg=1,
│ │      device="cuda",
│ │  )
│ │  # monitor metrics on eval data during training
│ │  lightgcn.fit(
│ │      train_data,
│ │ +    neg_sampling=True,
│ │      verbose=2,
│ │      eval_data=eval_data,
│ │      metrics=["loss", "roc_auc", "precision", "recall", "ndcg"],
│ │  )
│ │  
│ │  # do final evaluation on test data
│ │  evaluate(
│ │      model=lightgcn,
│ │      data=test_data,
│ │ +    neg_sampling=True,
│ │      metrics=["loss", "roc_auc", "precision", "recall", "ndcg"],
│ │  )
│ │  
│ │  # predict preference of user 2211 to item 110
│ │  lightgcn.predict(user=2211, item=110)
│ │  # recommend 7 items for user 2211
│ │  lightgcn.recommend_user(user=2211, n_rec=7)
│ │ @@ -114,32 +110,29 @@
│ │  user_col = ["sex", "age", "occupation"]
│ │  item_col = ["genre1", "genre2", "genre3"]
│ │  
│ │  train_data, data_info = DatasetFeat.build_trainset(
│ │      train_data, user_col, item_col, sparse_col, dense_col
│ │  )
│ │  test_data = DatasetFeat.build_testset(test_data)
│ │ -
│ │ -# sample negative items for each record
│ │ -train_data.build_negative_samples(data_info)  
│ │ -test_data.build_negative_samples(data_info)
│ │  print(data_info)  # n_users: 5962, n_items: 3226, data sparsity: 0.4185 %
│ │  
│ │  ytb_ranking = YouTubeRanking(
│ │      task="ranking",
│ │      data_info=data_info,
│ │      embed_size=16,
│ │      n_epochs=3,
│ │      lr=1e-4,
│ │      batch_size=512,
│ │      use_bn=True,
│ │      hidden_units=(128, 64, 32),
│ │  )
│ │  ytb_ranking.fit(
│ │      train_data,
│ │ +    neg_sampling=True,
│ │      verbose=2,
│ │      shuffle=True,
│ │      eval_data=test_data,
│ │      metrics=["loss", "roc_auc", "precision", "recall", "map", "ndcg"],
│ │  )
│ │  
│ │  # predict preference of user 2211 to item 110
│ │ @@ -166,15 +159,15 @@
│ │  
│ │  **Also note that your data should not contain missing values.**
│ │  
│ │  
│ │  
│ │  ## Documentation
│ │  
│ │ -The tutorials and API documentation are hosted on [librecommender.readthedocs.io](https://librecommender.readthedocs.io/en/stable/).
│ │ +The tutorials and API documentation are hosted on [librecommender.readthedocs.io](https://librecommender.readthedocs.io/en/latest/).
│ │  
│ │  The example scripts are under [examples/](https://github.com/massquantity/LibRecommender/tree/master/examples) folder.
│ │  
│ │  
│ │  
│ │  ## Installation & Dependencies 
│ │  
│ │ @@ -200,16 +193,16 @@
│ │  - PyTorch >= 1.10
│ │  - Numpy >= 1.19.5
│ │  - Pandas >= 1.0.0
│ │  - Scipy >= 1.2.1
│ │  - scikit-learn >= 0.20.0
│ │  - gensim >= 4.0.0
│ │  - tqdm
│ │ -- [nmslib](https://github.com/nmslib/nmslib) (optional, used in approximate similarity searching. See [Embedding](https://librecommender.readthedocs.io/en/stable/user_guide/embedding.html))
│ │ -- [DGL](https://github.com/dmlc/dgl) (optional, used in GraphSage and PinSage. See [Implementation Details](https://librecommender.readthedocs.io/en/stable/internal/implementation_details.html#pinsage))
│ │ +- [nmslib](https://github.com/nmslib/nmslib) (optional, used in approximate similarity searching. See [Embedding](https://librecommender.readthedocs.io/en/latest/user_guide/embedding.html))
│ │ +- [DGL](https://github.com/dmlc/dgl) (optional, used in GraphSage and PinSage. See [Implementation Details](https://librecommender.readthedocs.io/en/latest/internal/implementation_details.html#pinsage))
│ │  
│ │  If you are using Python 3.6, you also need to install [dataclasses](https://github.com/ericvsmith/dataclasses), which was first introduced in Python 3.7.
│ │  
│ │  LibRecommender is tested under TensorFlow 1.15, 2.5, 2.8 and 2.10. If you encounter any problem during running, feel free to open an issue.
│ │  
│ │  **Known issue**: Sometimes one may encounter errors like `ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject`. In this case try upgrading numpy, and version 1.22.0 or higher is probably a safe option.
│ │   --- LibRecommender-1.0.1/libreco/algorithms/__init__.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/__init__.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/algorithms/_als.cpp
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/_als.cpp
│ │┄ Files 0% similar despite different names
│ │ @@ -1,8 +1,8 @@
│ │ -/* Generated by Cython 0.29.33 */
│ │ +/* Generated by Cython 0.29.34 */
│ │  
│ │  /* BEGIN: Cython Metadata
│ │  {
│ │      "distutils": {
│ │          "depends": [],
│ │          "extra_compile_args": [
│ │              "-Wno-unused-function",
│ │ @@ -13,15 +13,15 @@
│ │              "-std=c++11"
│ │          ],
│ │          "extra_link_args": [
│ │              "-fopenmp",
│ │              "-std=c++11"
│ │          ],
│ │          "include_dirs": [
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include"
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include"
│ │          ],
│ │          "language": "c++",
│ │          "name": "libreco.algorithms._als",
│ │          "sources": [
│ │              "libreco/algorithms/_als.pyx"
│ │          ]
│ │      },
│ │ @@ -34,16 +34,16 @@
│ │  #endif /* PY_SSIZE_T_CLEAN */
│ │  #include "Python.h"
│ │  #ifndef Py_PYTHON_H
│ │      #error Python headers needed to compile C extensions, please install development version of Python.
│ │  #elif PY_VERSION_HEX < 0x02060000 || (0x03000000 <= PY_VERSION_HEX && PY_VERSION_HEX < 0x03030000)
│ │      #error Cython requires Python 2.6+ or Python 3.3+.
│ │  #else
│ │ -#define CYTHON_ABI "0_29_33"
│ │ -#define CYTHON_HEX_VERSION 0x001D21F0
│ │ +#define CYTHON_ABI "0_29_34"
│ │ +#define CYTHON_HEX_VERSION 0x001D22F0
│ │  #define CYTHON_FUTURE_DIVISION 0
│ │  #include <stddef.h>
│ │  #ifndef offsetof
│ │    #define offsetof(type, member) ( (size_t) & ((type*)0) -> member )
│ │  #endif
│ │  #if !defined(WIN32) && !defined(MS_WINDOWS)
│ │    #ifndef __stdcall
│ │ @@ -228,15 +228,15 @@
│ │    #elif !defined(CYTHON_USE_ASYNC_SLOTS)
│ │      #define CYTHON_USE_ASYNC_SLOTS 1
│ │    #endif
│ │    #if PY_VERSION_HEX < 0x02070000
│ │      #undef CYTHON_USE_PYLONG_INTERNALS
│ │      #define CYTHON_USE_PYLONG_INTERNALS 0
│ │    #elif !defined(CYTHON_USE_PYLONG_INTERNALS)
│ │ -    #define CYTHON_USE_PYLONG_INTERNALS 1
│ │ +    #define CYTHON_USE_PYLONG_INTERNALS (PY_VERSION_HEX < 0x030C00A5)
│ │    #endif
│ │    #ifndef CYTHON_USE_PYLIST_INTERNALS
│ │      #define CYTHON_USE_PYLIST_INTERNALS 1
│ │    #endif
│ │    #ifndef CYTHON_USE_UNICODE_INTERNALS
│ │      #define CYTHON_USE_UNICODE_INTERNALS 1
│ │    #endif
│ │ @@ -267,15 +267,15 @@
│ │    #ifndef CYTHON_PEP489_MULTI_PHASE_INIT
│ │      #define CYTHON_PEP489_MULTI_PHASE_INIT (PY_VERSION_HEX >= 0x03050000)
│ │    #endif
│ │    #ifndef CYTHON_USE_TP_FINALIZE
│ │      #define CYTHON_USE_TP_FINALIZE (PY_VERSION_HEX >= 0x030400a1)
│ │    #endif
│ │    #ifndef CYTHON_USE_DICT_VERSIONS
│ │ -    #define CYTHON_USE_DICT_VERSIONS (PY_VERSION_HEX >= 0x030600B1)
│ │ +    #define CYTHON_USE_DICT_VERSIONS ((PY_VERSION_HEX >= 0x030600B1) && (PY_VERSION_HEX < 0x030C00A5))
│ │    #endif
│ │    #if PY_VERSION_HEX >= 0x030B00A4
│ │      #undef CYTHON_USE_EXC_INFO_STACK
│ │      #define CYTHON_USE_EXC_INFO_STACK 0
│ │    #elif !defined(CYTHON_USE_EXC_INFO_STACK)
│ │      #define CYTHON_USE_EXC_INFO_STACK (PY_VERSION_HEX >= 0x030700A3)
│ │    #endif
│ │ @@ -21204,28 +21204,28 @@
│ │                              "BaseException");
│ │              goto bad;
│ │          }
│ │          PyException_SetCause(value, fixed_cause);
│ │      }
│ │      PyErr_SetObject(type, value);
│ │      if (tb) {
│ │ -#if CYTHON_COMPILING_IN_PYPY
│ │ -        PyObject *tmp_type, *tmp_value, *tmp_tb;
│ │ -        PyErr_Fetch(&tmp_type, &tmp_value, &tmp_tb);
│ │ -        Py_INCREF(tb);
│ │ -        PyErr_Restore(tmp_type, tmp_value, tb);
│ │ -        Py_XDECREF(tmp_tb);
│ │ -#else
│ │ +#if CYTHON_FAST_THREAD_STATE
│ │          PyThreadState *tstate = __Pyx_PyThreadState_Current;
│ │          PyObject* tmp_tb = tstate->curexc_traceback;
│ │          if (tb != tmp_tb) {
│ │              Py_INCREF(tb);
│ │              tstate->curexc_traceback = tb;
│ │              Py_XDECREF(tmp_tb);
│ │          }
│ │ +#else
│ │ +        PyObject *tmp_type, *tmp_value, *tmp_tb;
│ │ +        PyErr_Fetch(&tmp_type, &tmp_value, &tmp_tb);
│ │ +        Py_INCREF(tb);
│ │ +        PyErr_Restore(tmp_type, tmp_value, tb);
│ │ +        Py_XDECREF(tmp_tb);
│ │  #endif
│ │      }
│ │  bad:
│ │      Py_XDECREF(owned_instance);
│ │      return;
│ │  }
│ │  #endif
│ │   --- LibRecommender-1.0.1/libreco/algorithms/_als.pyx
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/_als.pyx
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/algorithms/_bpr.cpp
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/_bpr.cpp
│ │┄ Files 1% similar despite different names
│ │ @@ -1,33 +1,33 @@
│ │ -/* Generated by Cython 0.29.33 */
│ │ +/* Generated by Cython 0.29.34 */
│ │  
│ │  /* BEGIN: Cython Metadata
│ │  {
│ │      "distutils": {
│ │          "depends": [
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include/numpy/arrayobject.h",
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include/numpy/arrayscalars.h",
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include/numpy/ndarrayobject.h",
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include/numpy/ndarraytypes.h",
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include/numpy/ufuncobject.h"
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include/numpy/arrayobject.h",
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include/numpy/arrayscalars.h",
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include/numpy/ndarrayobject.h",
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include/numpy/ndarraytypes.h",
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include/numpy/ufuncobject.h"
│ │          ],
│ │          "extra_compile_args": [
│ │              "-Wno-unused-function",
│ │              "-Wno-maybe-uninitialized",
│ │              "-O3",
│ │              "-ffast-math",
│ │              "-fopenmp",
│ │              "-std=c++11"
│ │          ],
│ │          "extra_link_args": [
│ │              "-fopenmp",
│ │              "-std=c++11"
│ │          ],
│ │          "include_dirs": [
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include"
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include"
│ │          ],
│ │          "language": "c++",
│ │          "name": "libreco.algorithms._bpr",
│ │          "sources": [
│ │              "libreco/algorithms/_bpr.pyx"
│ │          ]
│ │      },
│ │ @@ -40,16 +40,16 @@
│ │  #endif /* PY_SSIZE_T_CLEAN */
│ │  #include "Python.h"
│ │  #ifndef Py_PYTHON_H
│ │      #error Python headers needed to compile C extensions, please install development version of Python.
│ │  #elif PY_VERSION_HEX < 0x02060000 || (0x03000000 <= PY_VERSION_HEX && PY_VERSION_HEX < 0x03030000)
│ │      #error Cython requires Python 2.6+ or Python 3.3+.
│ │  #else
│ │ -#define CYTHON_ABI "0_29_33"
│ │ -#define CYTHON_HEX_VERSION 0x001D21F0
│ │ +#define CYTHON_ABI "0_29_34"
│ │ +#define CYTHON_HEX_VERSION 0x001D22F0
│ │  #define CYTHON_FUTURE_DIVISION 0
│ │  #include <stddef.h>
│ │  #ifndef offsetof
│ │    #define offsetof(type, member) ( (size_t) & ((type*)0) -> member )
│ │  #endif
│ │  #if !defined(WIN32) && !defined(MS_WINDOWS)
│ │    #ifndef __stdcall
│ │ @@ -234,15 +234,15 @@
│ │    #elif !defined(CYTHON_USE_ASYNC_SLOTS)
│ │      #define CYTHON_USE_ASYNC_SLOTS 1
│ │    #endif
│ │    #if PY_VERSION_HEX < 0x02070000
│ │      #undef CYTHON_USE_PYLONG_INTERNALS
│ │      #define CYTHON_USE_PYLONG_INTERNALS 0
│ │    #elif !defined(CYTHON_USE_PYLONG_INTERNALS)
│ │ -    #define CYTHON_USE_PYLONG_INTERNALS 1
│ │ +    #define CYTHON_USE_PYLONG_INTERNALS (PY_VERSION_HEX < 0x030C00A5)
│ │    #endif
│ │    #ifndef CYTHON_USE_PYLIST_INTERNALS
│ │      #define CYTHON_USE_PYLIST_INTERNALS 1
│ │    #endif
│ │    #ifndef CYTHON_USE_UNICODE_INTERNALS
│ │      #define CYTHON_USE_UNICODE_INTERNALS 1
│ │    #endif
│ │ @@ -273,15 +273,15 @@
│ │    #ifndef CYTHON_PEP489_MULTI_PHASE_INIT
│ │      #define CYTHON_PEP489_MULTI_PHASE_INIT (PY_VERSION_HEX >= 0x03050000)
│ │    #endif
│ │    #ifndef CYTHON_USE_TP_FINALIZE
│ │      #define CYTHON_USE_TP_FINALIZE (PY_VERSION_HEX >= 0x030400a1)
│ │    #endif
│ │    #ifndef CYTHON_USE_DICT_VERSIONS
│ │ -    #define CYTHON_USE_DICT_VERSIONS (PY_VERSION_HEX >= 0x030600B1)
│ │ +    #define CYTHON_USE_DICT_VERSIONS ((PY_VERSION_HEX >= 0x030600B1) && (PY_VERSION_HEX < 0x030C00A5))
│ │    #endif
│ │    #if PY_VERSION_HEX >= 0x030B00A4
│ │      #undef CYTHON_USE_EXC_INFO_STACK
│ │      #define CYTHON_USE_EXC_INFO_STACK 0
│ │    #elif !defined(CYTHON_USE_EXC_INFO_STACK)
│ │      #define CYTHON_USE_EXC_INFO_STACK (PY_VERSION_HEX >= 0x030700A3)
│ │    #endif
│ │ @@ -1148,195 +1148,195 @@
│ │    char enc_type;
│ │    char new_packmode;
│ │    char enc_packmode;
│ │    char is_valid_array;
│ │  } __Pyx_BufFmt_Context;
│ │  
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":689
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":689
│ │   * # in Cython to enable them only on the right systems.
│ │   * 
│ │   * ctypedef npy_int8       int8_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_int16      int16_t
│ │   * ctypedef npy_int32      int32_t
│ │   */
│ │  typedef npy_int8 __pyx_t_5numpy_int8_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":690
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":690
│ │   * 
│ │   * ctypedef npy_int8       int8_t
│ │   * ctypedef npy_int16      int16_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_int32      int32_t
│ │   * ctypedef npy_int64      int64_t
│ │   */
│ │  typedef npy_int16 __pyx_t_5numpy_int16_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":691
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":691
│ │   * ctypedef npy_int8       int8_t
│ │   * ctypedef npy_int16      int16_t
│ │   * ctypedef npy_int32      int32_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_int64      int64_t
│ │   * #ctypedef npy_int96      int96_t
│ │   */
│ │  typedef npy_int32 __pyx_t_5numpy_int32_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":692
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":692
│ │   * ctypedef npy_int16      int16_t
│ │   * ctypedef npy_int32      int32_t
│ │   * ctypedef npy_int64      int64_t             # <<<<<<<<<<<<<<
│ │   * #ctypedef npy_int96      int96_t
│ │   * #ctypedef npy_int128     int128_t
│ │   */
│ │  typedef npy_int64 __pyx_t_5numpy_int64_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":696
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":696
│ │   * #ctypedef npy_int128     int128_t
│ │   * 
│ │   * ctypedef npy_uint8      uint8_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_uint16     uint16_t
│ │   * ctypedef npy_uint32     uint32_t
│ │   */
│ │  typedef npy_uint8 __pyx_t_5numpy_uint8_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":697
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":697
│ │   * 
│ │   * ctypedef npy_uint8      uint8_t
│ │   * ctypedef npy_uint16     uint16_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_uint32     uint32_t
│ │   * ctypedef npy_uint64     uint64_t
│ │   */
│ │  typedef npy_uint16 __pyx_t_5numpy_uint16_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":698
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":698
│ │   * ctypedef npy_uint8      uint8_t
│ │   * ctypedef npy_uint16     uint16_t
│ │   * ctypedef npy_uint32     uint32_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_uint64     uint64_t
│ │   * #ctypedef npy_uint96     uint96_t
│ │   */
│ │  typedef npy_uint32 __pyx_t_5numpy_uint32_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":699
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":699
│ │   * ctypedef npy_uint16     uint16_t
│ │   * ctypedef npy_uint32     uint32_t
│ │   * ctypedef npy_uint64     uint64_t             # <<<<<<<<<<<<<<
│ │   * #ctypedef npy_uint96     uint96_t
│ │   * #ctypedef npy_uint128    uint128_t
│ │   */
│ │  typedef npy_uint64 __pyx_t_5numpy_uint64_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":703
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":703
│ │   * #ctypedef npy_uint128    uint128_t
│ │   * 
│ │   * ctypedef npy_float32    float32_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_float64    float64_t
│ │   * #ctypedef npy_float80    float80_t
│ │   */
│ │  typedef npy_float32 __pyx_t_5numpy_float32_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":704
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":704
│ │   * 
│ │   * ctypedef npy_float32    float32_t
│ │   * ctypedef npy_float64    float64_t             # <<<<<<<<<<<<<<
│ │   * #ctypedef npy_float80    float80_t
│ │   * #ctypedef npy_float128   float128_t
│ │   */
│ │  typedef npy_float64 __pyx_t_5numpy_float64_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":713
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":713
│ │   * # The int types are mapped a bit surprising --
│ │   * # numpy.int corresponds to 'l' and numpy.long to 'q'
│ │   * ctypedef npy_long       int_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_longlong   long_t
│ │   * ctypedef npy_longlong   longlong_t
│ │   */
│ │  typedef npy_long __pyx_t_5numpy_int_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":714
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":714
│ │   * # numpy.int corresponds to 'l' and numpy.long to 'q'
│ │   * ctypedef npy_long       int_t
│ │   * ctypedef npy_longlong   long_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_longlong   longlong_t
│ │   * 
│ │   */
│ │  typedef npy_longlong __pyx_t_5numpy_long_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":715
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":715
│ │   * ctypedef npy_long       int_t
│ │   * ctypedef npy_longlong   long_t
│ │   * ctypedef npy_longlong   longlong_t             # <<<<<<<<<<<<<<
│ │   * 
│ │   * ctypedef npy_ulong      uint_t
│ │   */
│ │  typedef npy_longlong __pyx_t_5numpy_longlong_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":717
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":717
│ │   * ctypedef npy_longlong   longlong_t
│ │   * 
│ │   * ctypedef npy_ulong      uint_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_ulonglong  ulong_t
│ │   * ctypedef npy_ulonglong  ulonglong_t
│ │   */
│ │  typedef npy_ulong __pyx_t_5numpy_uint_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":718
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":718
│ │   * 
│ │   * ctypedef npy_ulong      uint_t
│ │   * ctypedef npy_ulonglong  ulong_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_ulonglong  ulonglong_t
│ │   * 
│ │   */
│ │  typedef npy_ulonglong __pyx_t_5numpy_ulong_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":719
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":719
│ │   * ctypedef npy_ulong      uint_t
│ │   * ctypedef npy_ulonglong  ulong_t
│ │   * ctypedef npy_ulonglong  ulonglong_t             # <<<<<<<<<<<<<<
│ │   * 
│ │   * ctypedef npy_intp       intp_t
│ │   */
│ │  typedef npy_ulonglong __pyx_t_5numpy_ulonglong_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":721
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":721
│ │   * ctypedef npy_ulonglong  ulonglong_t
│ │   * 
│ │   * ctypedef npy_intp       intp_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_uintp      uintp_t
│ │   * 
│ │   */
│ │  typedef npy_intp __pyx_t_5numpy_intp_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":722
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":722
│ │   * 
│ │   * ctypedef npy_intp       intp_t
│ │   * ctypedef npy_uintp      uintp_t             # <<<<<<<<<<<<<<
│ │   * 
│ │   * ctypedef npy_double     float_t
│ │   */
│ │  typedef npy_uintp __pyx_t_5numpy_uintp_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":724
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":724
│ │   * ctypedef npy_uintp      uintp_t
│ │   * 
│ │   * ctypedef npy_double     float_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_double     double_t
│ │   * ctypedef npy_longdouble longdouble_t
│ │   */
│ │  typedef npy_double __pyx_t_5numpy_float_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":725
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":725
│ │   * 
│ │   * ctypedef npy_double     float_t
│ │   * ctypedef npy_double     double_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_longdouble longdouble_t
│ │   * 
│ │   */
│ │  typedef npy_double __pyx_t_5numpy_double_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":726
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":726
│ │   * ctypedef npy_double     float_t
│ │   * ctypedef npy_double     double_t
│ │   * ctypedef npy_longdouble longdouble_t             # <<<<<<<<<<<<<<
│ │   * 
│ │   * ctypedef npy_cfloat      cfloat_t
│ │   */
│ │  typedef npy_longdouble __pyx_t_5numpy_longdouble_t;
│ │ @@ -1367,42 +1367,42 @@
│ │  
│ │  /*--- Type declarations ---*/
│ │  struct __pyx_array_obj;
│ │  struct __pyx_MemviewEnum_obj;
│ │  struct __pyx_memoryview_obj;
│ │  struct __pyx_memoryviewslice_obj;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":728
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":728
│ │   * ctypedef npy_longdouble longdouble_t
│ │   * 
│ │   * ctypedef npy_cfloat      cfloat_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_cdouble     cdouble_t
│ │   * ctypedef npy_clongdouble clongdouble_t
│ │   */
│ │  typedef npy_cfloat __pyx_t_5numpy_cfloat_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":729
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":729
│ │   * 
│ │   * ctypedef npy_cfloat      cfloat_t
│ │   * ctypedef npy_cdouble     cdouble_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_clongdouble clongdouble_t
│ │   * 
│ │   */
│ │  typedef npy_cdouble __pyx_t_5numpy_cdouble_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":730
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":730
│ │   * ctypedef npy_cfloat      cfloat_t
│ │   * ctypedef npy_cdouble     cdouble_t
│ │   * ctypedef npy_clongdouble clongdouble_t             # <<<<<<<<<<<<<<
│ │   * 
│ │   * ctypedef npy_cdouble     complex_t
│ │   */
│ │  typedef npy_clongdouble __pyx_t_5numpy_clongdouble_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":732
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":732
│ │   * ctypedef npy_clongdouble clongdouble_t
│ │   * 
│ │   * ctypedef npy_cdouble     complex_t             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew1(a):
│ │   */
│ │  typedef npy_cdouble __pyx_t_5numpy_complex_t;
│ │ @@ -2068,20 +2068,28 @@
│ │  
│ │  /* SetupReduce.proto */
│ │  static int __Pyx_setup_reduce(PyObject* type_obj);
│ │  
│ │  /* TypeImport.proto */
│ │  #ifndef __PYX_HAVE_RT_ImportType_proto
│ │  #define __PYX_HAVE_RT_ImportType_proto
│ │ +#if __STDC_VERSION__ >= 201112L
│ │ +#include <stdalign.h>
│ │ +#endif
│ │ +#if __STDC_VERSION__ >= 201112L || __cplusplus >= 201103L
│ │ +#define __PYX_GET_STRUCT_ALIGNMENT(s) alignof(s)
│ │ +#else
│ │ +#define __PYX_GET_STRUCT_ALIGNMENT(s) sizeof(void*)
│ │ +#endif
│ │  enum __Pyx_ImportType_CheckSize {
│ │     __Pyx_ImportType_CheckSize_Error = 0,
│ │     __Pyx_ImportType_CheckSize_Warn = 1,
│ │     __Pyx_ImportType_CheckSize_Ignore = 2
│ │  };
│ │ -static PyTypeObject *__Pyx_ImportType(PyObject* module, const char *module_name, const char *class_name, size_t size, enum __Pyx_ImportType_CheckSize check_size);
│ │ +static PyTypeObject *__Pyx_ImportType(PyObject* module, const char *module_name, const char *class_name, size_t size, size_t alignment, enum __Pyx_ImportType_CheckSize check_size);
│ │  #endif
│ │  
│ │  /* CLineInTraceback.proto */
│ │  #ifdef CYTHON_CLINE_IN_TRACEBACK
│ │  #define __Pyx_CLineForTraceback(tstate, c_line)  (((CYTHON_CLINE_IN_TRACEBACK)) ? c_line : 0)
│ │  #else
│ │  static int __Pyx_CLineForTraceback(PyThreadState *tstate, int c_line);
│ │ @@ -6035,15 +6043,15 @@
│ │    goto __pyx_L0;
│ │    __pyx_L1_error:;
│ │    __Pyx_WriteUnraisable("libreco.algorithms._bpr._bpr_update_adam", __pyx_clineno, __pyx_lineno, __pyx_filename, 1, 0);
│ │    __pyx_L0:;
│ │    __Pyx_RefNannyFinishContext();
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":734
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":734
│ │   * ctypedef npy_cdouble     complex_t
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew1(a):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(1, <void*>a)
│ │   * 
│ │   */
│ │  
│ │ @@ -6052,29 +6060,29 @@
│ │    __Pyx_RefNannyDeclarations
│ │    PyObject *__pyx_t_1 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("PyArray_MultiIterNew1", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":735
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":735
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew1(a):
│ │   *     return PyArray_MultiIterNew(1, <void*>a)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew2(a, b):
│ │   */
│ │    __Pyx_XDECREF(__pyx_r);
│ │    __pyx_t_1 = PyArray_MultiIterNew(1, ((void *)__pyx_v_a)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 735, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │    __pyx_r = __pyx_t_1;
│ │    __pyx_t_1 = 0;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":734
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":734
│ │   * ctypedef npy_cdouble     complex_t
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew1(a):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(1, <void*>a)
│ │   * 
│ │   */
│ │  
│ │ @@ -6085,15 +6093,15 @@
│ │    __pyx_r = 0;
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":737
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":737
│ │   *     return PyArray_MultiIterNew(1, <void*>a)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew2(a, b):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
│ │   * 
│ │   */
│ │  
│ │ @@ -6102,29 +6110,29 @@
│ │    __Pyx_RefNannyDeclarations
│ │    PyObject *__pyx_t_1 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("PyArray_MultiIterNew2", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":738
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":738
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew2(a, b):
│ │   *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew3(a, b, c):
│ │   */
│ │    __Pyx_XDECREF(__pyx_r);
│ │    __pyx_t_1 = PyArray_MultiIterNew(2, ((void *)__pyx_v_a), ((void *)__pyx_v_b)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 738, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │    __pyx_r = __pyx_t_1;
│ │    __pyx_t_1 = 0;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":737
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":737
│ │   *     return PyArray_MultiIterNew(1, <void*>a)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew2(a, b):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
│ │   * 
│ │   */
│ │  
│ │ @@ -6135,15 +6143,15 @@
│ │    __pyx_r = 0;
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":740
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":740
│ │   *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew3(a, b, c):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
│ │   * 
│ │   */
│ │  
│ │ @@ -6152,29 +6160,29 @@
│ │    __Pyx_RefNannyDeclarations
│ │    PyObject *__pyx_t_1 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("PyArray_MultiIterNew3", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":741
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":741
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew3(a, b, c):
│ │   *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew4(a, b, c, d):
│ │   */
│ │    __Pyx_XDECREF(__pyx_r);
│ │    __pyx_t_1 = PyArray_MultiIterNew(3, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 741, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │    __pyx_r = __pyx_t_1;
│ │    __pyx_t_1 = 0;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":740
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":740
│ │   *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew3(a, b, c):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
│ │   * 
│ │   */
│ │  
│ │ @@ -6185,15 +6193,15 @@
│ │    __pyx_r = 0;
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":743
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":743
│ │   *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew4(a, b, c, d):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
│ │   * 
│ │   */
│ │  
│ │ @@ -6202,29 +6210,29 @@
│ │    __Pyx_RefNannyDeclarations
│ │    PyObject *__pyx_t_1 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("PyArray_MultiIterNew4", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":744
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":744
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew4(a, b, c, d):
│ │   *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):
│ │   */
│ │    __Pyx_XDECREF(__pyx_r);
│ │    __pyx_t_1 = PyArray_MultiIterNew(4, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c), ((void *)__pyx_v_d)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 744, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │    __pyx_r = __pyx_t_1;
│ │    __pyx_t_1 = 0;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":743
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":743
│ │   *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew4(a, b, c, d):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
│ │   * 
│ │   */
│ │  
│ │ @@ -6235,15 +6243,15 @@
│ │    __pyx_r = 0;
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":746
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":746
│ │   *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
│ │   * 
│ │   */
│ │  
│ │ @@ -6252,29 +6260,29 @@
│ │    __Pyx_RefNannyDeclarations
│ │    PyObject *__pyx_t_1 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("PyArray_MultiIterNew5", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":747
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":747
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):
│ │   *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline tuple PyDataType_SHAPE(dtype d):
│ │   */
│ │    __Pyx_XDECREF(__pyx_r);
│ │    __pyx_t_1 = PyArray_MultiIterNew(5, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c), ((void *)__pyx_v_d), ((void *)__pyx_v_e)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 747, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │    __pyx_r = __pyx_t_1;
│ │    __pyx_t_1 = 0;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":746
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":746
│ │   *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
│ │   * 
│ │   */
│ │  
│ │ @@ -6285,212 +6293,212 @@
│ │    __pyx_r = 0;
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":749
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":749
│ │   *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
│ │   * 
│ │   * cdef inline tuple PyDataType_SHAPE(dtype d):             # <<<<<<<<<<<<<<
│ │   *     if PyDataType_HASSUBARRAY(d):
│ │   *         return <tuple>d.subarray.shape
│ │   */
│ │  
│ │  static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyDataType_SHAPE(PyArray_Descr *__pyx_v_d) {
│ │    PyObject *__pyx_r = NULL;
│ │    __Pyx_RefNannyDeclarations
│ │    int __pyx_t_1;
│ │    __Pyx_RefNannySetupContext("PyDataType_SHAPE", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":750
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":750
│ │   * 
│ │   * cdef inline tuple PyDataType_SHAPE(dtype d):
│ │   *     if PyDataType_HASSUBARRAY(d):             # <<<<<<<<<<<<<<
│ │   *         return <tuple>d.subarray.shape
│ │   *     else:
│ │   */
│ │    __pyx_t_1 = (PyDataType_HASSUBARRAY(__pyx_v_d) != 0);
│ │    if (__pyx_t_1) {
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":751
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":751
│ │   * cdef inline tuple PyDataType_SHAPE(dtype d):
│ │   *     if PyDataType_HASSUBARRAY(d):
│ │   *         return <tuple>d.subarray.shape             # <<<<<<<<<<<<<<
│ │   *     else:
│ │   *         return ()
│ │   */
│ │      __Pyx_XDECREF(__pyx_r);
│ │      __Pyx_INCREF(((PyObject*)__pyx_v_d->subarray->shape));
│ │      __pyx_r = ((PyObject*)__pyx_v_d->subarray->shape);
│ │      goto __pyx_L0;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":750
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":750
│ │   * 
│ │   * cdef inline tuple PyDataType_SHAPE(dtype d):
│ │   *     if PyDataType_HASSUBARRAY(d):             # <<<<<<<<<<<<<<
│ │   *         return <tuple>d.subarray.shape
│ │   *     else:
│ │   */
│ │    }
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":753
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":753
│ │   *         return <tuple>d.subarray.shape
│ │   *     else:
│ │   *         return ()             # <<<<<<<<<<<<<<
│ │   * 
│ │   * 
│ │   */
│ │    /*else*/ {
│ │      __Pyx_XDECREF(__pyx_r);
│ │      __Pyx_INCREF(__pyx_empty_tuple);
│ │      __pyx_r = __pyx_empty_tuple;
│ │      goto __pyx_L0;
│ │    }
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":749
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":749
│ │   *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
│ │   * 
│ │   * cdef inline tuple PyDataType_SHAPE(dtype d):             # <<<<<<<<<<<<<<
│ │   *     if PyDataType_HASSUBARRAY(d):
│ │   *         return <tuple>d.subarray.shape
│ │   */
│ │  
│ │    /* function exit code */
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":928
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":928
│ │   *     int _import_umath() except -1
│ │   * 
│ │   * cdef inline void set_array_base(ndarray arr, object base):             # <<<<<<<<<<<<<<
│ │   *     Py_INCREF(base) # important to do this before stealing the reference below!
│ │   *     PyArray_SetBaseObject(arr, base)
│ │   */
│ │  
│ │  static CYTHON_INLINE void __pyx_f_5numpy_set_array_base(PyArrayObject *__pyx_v_arr, PyObject *__pyx_v_base) {
│ │    __Pyx_RefNannyDeclarations
│ │    __Pyx_RefNannySetupContext("set_array_base", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":929
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":929
│ │   * 
│ │   * cdef inline void set_array_base(ndarray arr, object base):
│ │   *     Py_INCREF(base) # important to do this before stealing the reference below!             # <<<<<<<<<<<<<<
│ │   *     PyArray_SetBaseObject(arr, base)
│ │   * 
│ │   */
│ │    Py_INCREF(__pyx_v_base);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":930
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":930
│ │   * cdef inline void set_array_base(ndarray arr, object base):
│ │   *     Py_INCREF(base) # important to do this before stealing the reference below!
│ │   *     PyArray_SetBaseObject(arr, base)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline object get_array_base(ndarray arr):
│ │   */
│ │    (void)(PyArray_SetBaseObject(__pyx_v_arr, __pyx_v_base));
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":928
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":928
│ │   *     int _import_umath() except -1
│ │   * 
│ │   * cdef inline void set_array_base(ndarray arr, object base):             # <<<<<<<<<<<<<<
│ │   *     Py_INCREF(base) # important to do this before stealing the reference below!
│ │   *     PyArray_SetBaseObject(arr, base)
│ │   */
│ │  
│ │    /* function exit code */
│ │    __Pyx_RefNannyFinishContext();
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":932
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":932
│ │   *     PyArray_SetBaseObject(arr, base)
│ │   * 
│ │   * cdef inline object get_array_base(ndarray arr):             # <<<<<<<<<<<<<<
│ │   *     base = PyArray_BASE(arr)
│ │   *     if base is NULL:
│ │   */
│ │  
│ │  static CYTHON_INLINE PyObject *__pyx_f_5numpy_get_array_base(PyArrayObject *__pyx_v_arr) {
│ │    PyObject *__pyx_v_base;
│ │    PyObject *__pyx_r = NULL;
│ │    __Pyx_RefNannyDeclarations
│ │    int __pyx_t_1;
│ │    __Pyx_RefNannySetupContext("get_array_base", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":933
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":933
│ │   * 
│ │   * cdef inline object get_array_base(ndarray arr):
│ │   *     base = PyArray_BASE(arr)             # <<<<<<<<<<<<<<
│ │   *     if base is NULL:
│ │   *         return None
│ │   */
│ │    __pyx_v_base = PyArray_BASE(__pyx_v_arr);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":934
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":934
│ │   * cdef inline object get_array_base(ndarray arr):
│ │   *     base = PyArray_BASE(arr)
│ │   *     if base is NULL:             # <<<<<<<<<<<<<<
│ │   *         return None
│ │   *     return <object>base
│ │   */
│ │    __pyx_t_1 = ((__pyx_v_base == NULL) != 0);
│ │    if (__pyx_t_1) {
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":935
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":935
│ │   *     base = PyArray_BASE(arr)
│ │   *     if base is NULL:
│ │   *         return None             # <<<<<<<<<<<<<<
│ │   *     return <object>base
│ │   * 
│ │   */
│ │      __Pyx_XDECREF(__pyx_r);
│ │      __pyx_r = Py_None; __Pyx_INCREF(Py_None);
│ │      goto __pyx_L0;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":934
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":934
│ │   * cdef inline object get_array_base(ndarray arr):
│ │   *     base = PyArray_BASE(arr)
│ │   *     if base is NULL:             # <<<<<<<<<<<<<<
│ │   *         return None
│ │   *     return <object>base
│ │   */
│ │    }
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":936
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":936
│ │   *     if base is NULL:
│ │   *         return None
│ │   *     return <object>base             # <<<<<<<<<<<<<<
│ │   * 
│ │   * # Versions of the import_* functions which are more suitable for
│ │   */
│ │    __Pyx_XDECREF(__pyx_r);
│ │    __Pyx_INCREF(((PyObject *)__pyx_v_base));
│ │    __pyx_r = ((PyObject *)__pyx_v_base);
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":932
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":932
│ │   *     PyArray_SetBaseObject(arr, base)
│ │   * 
│ │   * cdef inline object get_array_base(ndarray arr):             # <<<<<<<<<<<<<<
│ │   *     base = PyArray_BASE(arr)
│ │   *     if base is NULL:
│ │   */
│ │  
│ │    /* function exit code */
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":940
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":940
│ │   * # Versions of the import_* functions which are more suitable for
│ │   * # Cython code.
│ │   * cdef inline int import_array() except -1:             # <<<<<<<<<<<<<<
│ │   *     try:
│ │   *         __pyx_import_array()
│ │   */
│ │  
│ │ @@ -6506,15 +6514,15 @@
│ │    PyObject *__pyx_t_7 = NULL;
│ │    PyObject *__pyx_t_8 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("import_array", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":941
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":941
│ │   * # Cython code.
│ │   * cdef inline int import_array() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         __pyx_import_array()
│ │   *     except Exception:
│ │   */
│ │    {
│ │ @@ -6522,53 +6530,53 @@
│ │      __Pyx_PyThreadState_assign
│ │      __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
│ │      __Pyx_XGOTREF(__pyx_t_1);
│ │      __Pyx_XGOTREF(__pyx_t_2);
│ │      __Pyx_XGOTREF(__pyx_t_3);
│ │      /*try:*/ {
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":942
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":942
│ │   * cdef inline int import_array() except -1:
│ │   *     try:
│ │   *         __pyx_import_array()             # <<<<<<<<<<<<<<
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.multiarray failed to import")
│ │   */
│ │        __pyx_t_4 = _import_array(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(1, 942, __pyx_L3_error)
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":941
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":941
│ │   * # Cython code.
│ │   * cdef inline int import_array() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         __pyx_import_array()
│ │   *     except Exception:
│ │   */
│ │      }
│ │      __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
│ │      __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
│ │      __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
│ │      goto __pyx_L8_try_end;
│ │      __pyx_L3_error:;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":943
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":943
│ │   *     try:
│ │   *         __pyx_import_array()
│ │   *     except Exception:             # <<<<<<<<<<<<<<
│ │   *         raise ImportError("numpy.core.multiarray failed to import")
│ │   * 
│ │   */
│ │      __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
│ │      if (__pyx_t_4) {
│ │        __Pyx_AddTraceback("numpy.import_array", __pyx_clineno, __pyx_lineno, __pyx_filename);
│ │        if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(1, 943, __pyx_L5_except_error)
│ │        __Pyx_GOTREF(__pyx_t_5);
│ │        __Pyx_GOTREF(__pyx_t_6);
│ │        __Pyx_GOTREF(__pyx_t_7);
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":944
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":944
│ │   *         __pyx_import_array()
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.multiarray failed to import")             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline int import_umath() except -1:
│ │   */
│ │        __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple_, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(1, 944, __pyx_L5_except_error)
│ │ @@ -6576,30 +6584,30 @@
│ │        __Pyx_Raise(__pyx_t_8, 0, 0, 0);
│ │        __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
│ │        __PYX_ERR(1, 944, __pyx_L5_except_error)
│ │      }
│ │      goto __pyx_L5_except_error;
│ │      __pyx_L5_except_error:;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":941
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":941
│ │   * # Cython code.
│ │   * cdef inline int import_array() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         __pyx_import_array()
│ │   *     except Exception:
│ │   */
│ │      __Pyx_XGIVEREF(__pyx_t_1);
│ │      __Pyx_XGIVEREF(__pyx_t_2);
│ │      __Pyx_XGIVEREF(__pyx_t_3);
│ │      __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
│ │      goto __pyx_L1_error;
│ │      __pyx_L8_try_end:;
│ │    }
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":940
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":940
│ │   * # Versions of the import_* functions which are more suitable for
│ │   * # Cython code.
│ │   * cdef inline int import_array() except -1:             # <<<<<<<<<<<<<<
│ │   *     try:
│ │   *         __pyx_import_array()
│ │   */
│ │  
│ │ @@ -6614,15 +6622,15 @@
│ │    __Pyx_AddTraceback("numpy.import_array", __pyx_clineno, __pyx_lineno, __pyx_filename);
│ │    __pyx_r = -1;
│ │    __pyx_L0:;
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":946
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":946
│ │   *         raise ImportError("numpy.core.multiarray failed to import")
│ │   * 
│ │   * cdef inline int import_umath() except -1:             # <<<<<<<<<<<<<<
│ │   *     try:
│ │   *         _import_umath()
│ │   */
│ │  
│ │ @@ -6638,15 +6646,15 @@
│ │    PyObject *__pyx_t_7 = NULL;
│ │    PyObject *__pyx_t_8 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("import_umath", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":947
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":947
│ │   * 
│ │   * cdef inline int import_umath() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   */
│ │    {
│ │ @@ -6654,53 +6662,53 @@
│ │      __Pyx_PyThreadState_assign
│ │      __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
│ │      __Pyx_XGOTREF(__pyx_t_1);
│ │      __Pyx_XGOTREF(__pyx_t_2);
│ │      __Pyx_XGOTREF(__pyx_t_3);
│ │      /*try:*/ {
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":948
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":948
│ │   * cdef inline int import_umath() except -1:
│ │   *     try:
│ │   *         _import_umath()             # <<<<<<<<<<<<<<
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.umath failed to import")
│ │   */
│ │        __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(1, 948, __pyx_L3_error)
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":947
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":947
│ │   * 
│ │   * cdef inline int import_umath() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   */
│ │      }
│ │      __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
│ │      __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
│ │      __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
│ │      goto __pyx_L8_try_end;
│ │      __pyx_L3_error:;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":949
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":949
│ │   *     try:
│ │   *         _import_umath()
│ │   *     except Exception:             # <<<<<<<<<<<<<<
│ │   *         raise ImportError("numpy.core.umath failed to import")
│ │   * 
│ │   */
│ │      __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
│ │      if (__pyx_t_4) {
│ │        __Pyx_AddTraceback("numpy.import_umath", __pyx_clineno, __pyx_lineno, __pyx_filename);
│ │        if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(1, 949, __pyx_L5_except_error)
│ │        __Pyx_GOTREF(__pyx_t_5);
│ │        __Pyx_GOTREF(__pyx_t_6);
│ │        __Pyx_GOTREF(__pyx_t_7);
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":950
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":950
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:
│ │   */
│ │        __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__2, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(1, 950, __pyx_L5_except_error)
│ │ @@ -6708,30 +6716,30 @@
│ │        __Pyx_Raise(__pyx_t_8, 0, 0, 0);
│ │        __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
│ │        __PYX_ERR(1, 950, __pyx_L5_except_error)
│ │      }
│ │      goto __pyx_L5_except_error;
│ │      __pyx_L5_except_error:;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":947
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":947
│ │   * 
│ │   * cdef inline int import_umath() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   */
│ │      __Pyx_XGIVEREF(__pyx_t_1);
│ │      __Pyx_XGIVEREF(__pyx_t_2);
│ │      __Pyx_XGIVEREF(__pyx_t_3);
│ │      __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
│ │      goto __pyx_L1_error;
│ │      __pyx_L8_try_end:;
│ │    }
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":946
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":946
│ │   *         raise ImportError("numpy.core.multiarray failed to import")
│ │   * 
│ │   * cdef inline int import_umath() except -1:             # <<<<<<<<<<<<<<
│ │   *     try:
│ │   *         _import_umath()
│ │   */
│ │  
│ │ @@ -6746,15 +6754,15 @@
│ │    __Pyx_AddTraceback("numpy.import_umath", __pyx_clineno, __pyx_lineno, __pyx_filename);
│ │    __pyx_r = -1;
│ │    __pyx_L0:;
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":952
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":952
│ │   *         raise ImportError("numpy.core.umath failed to import")
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:             # <<<<<<<<<<<<<<
│ │   *     try:
│ │   *         _import_umath()
│ │   */
│ │  
│ │ @@ -6770,15 +6778,15 @@
│ │    PyObject *__pyx_t_7 = NULL;
│ │    PyObject *__pyx_t_8 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("import_ufunc", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":953
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":953
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   */
│ │    {
│ │ @@ -6786,53 +6794,53 @@
│ │      __Pyx_PyThreadState_assign
│ │      __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
│ │      __Pyx_XGOTREF(__pyx_t_1);
│ │      __Pyx_XGOTREF(__pyx_t_2);
│ │      __Pyx_XGOTREF(__pyx_t_3);
│ │      /*try:*/ {
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":954
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":954
│ │   * cdef inline int import_ufunc() except -1:
│ │   *     try:
│ │   *         _import_umath()             # <<<<<<<<<<<<<<
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.umath failed to import")
│ │   */
│ │        __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(1, 954, __pyx_L3_error)
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":953
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":953
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   */
│ │      }
│ │      __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
│ │      __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
│ │      __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
│ │      goto __pyx_L8_try_end;
│ │      __pyx_L3_error:;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":955
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":955
│ │   *     try:
│ │   *         _import_umath()
│ │   *     except Exception:             # <<<<<<<<<<<<<<
│ │   *         raise ImportError("numpy.core.umath failed to import")
│ │   * 
│ │   */
│ │      __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
│ │      if (__pyx_t_4) {
│ │        __Pyx_AddTraceback("numpy.import_ufunc", __pyx_clineno, __pyx_lineno, __pyx_filename);
│ │        if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(1, 955, __pyx_L5_except_error)
│ │        __Pyx_GOTREF(__pyx_t_5);
│ │        __Pyx_GOTREF(__pyx_t_6);
│ │        __Pyx_GOTREF(__pyx_t_7);
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":956
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":956
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef extern from *:
│ │   */
│ │        __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__2, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(1, 956, __pyx_L5_except_error)
│ │ @@ -6840,30 +6848,30 @@
│ │        __Pyx_Raise(__pyx_t_8, 0, 0, 0);
│ │        __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
│ │        __PYX_ERR(1, 956, __pyx_L5_except_error)
│ │      }
│ │      goto __pyx_L5_except_error;
│ │      __pyx_L5_except_error:;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":953
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":953
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   */
│ │      __Pyx_XGIVEREF(__pyx_t_1);
│ │      __Pyx_XGIVEREF(__pyx_t_2);
│ │      __Pyx_XGIVEREF(__pyx_t_3);
│ │      __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
│ │      goto __pyx_L1_error;
│ │      __pyx_L8_try_end:;
│ │    }
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":952
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":952
│ │   *         raise ImportError("numpy.core.umath failed to import")
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:             # <<<<<<<<<<<<<<
│ │   *     try:
│ │   *         _import_umath()
│ │   */
│ │  
│ │ @@ -6878,176 +6886,176 @@
│ │    __Pyx_AddTraceback("numpy.import_ufunc", __pyx_clineno, __pyx_lineno, __pyx_filename);
│ │    __pyx_r = -1;
│ │    __pyx_L0:;
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":966
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":966
│ │   * 
│ │   * 
│ │   * cdef inline bint is_timedelta64_object(object obj):             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     Cython equivalent of `isinstance(obj, np.timedelta64)`
│ │   */
│ │  
│ │  static CYTHON_INLINE int __pyx_f_5numpy_is_timedelta64_object(PyObject *__pyx_v_obj) {
│ │    int __pyx_r;
│ │    __Pyx_RefNannyDeclarations
│ │    __Pyx_RefNannySetupContext("is_timedelta64_object", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":978
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":978
│ │   *     bool
│ │   *     """
│ │   *     return PyObject_TypeCheck(obj, &PyTimedeltaArrType_Type)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * 
│ │   */
│ │    __pyx_r = PyObject_TypeCheck(__pyx_v_obj, (&PyTimedeltaArrType_Type));
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":966
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":966
│ │   * 
│ │   * 
│ │   * cdef inline bint is_timedelta64_object(object obj):             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     Cython equivalent of `isinstance(obj, np.timedelta64)`
│ │   */
│ │  
│ │    /* function exit code */
│ │    __pyx_L0:;
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":981
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":981
│ │   * 
│ │   * 
│ │   * cdef inline bint is_datetime64_object(object obj):             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     Cython equivalent of `isinstance(obj, np.datetime64)`
│ │   */
│ │  
│ │  static CYTHON_INLINE int __pyx_f_5numpy_is_datetime64_object(PyObject *__pyx_v_obj) {
│ │    int __pyx_r;
│ │    __Pyx_RefNannyDeclarations
│ │    __Pyx_RefNannySetupContext("is_datetime64_object", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":993
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":993
│ │   *     bool
│ │   *     """
│ │   *     return PyObject_TypeCheck(obj, &PyDatetimeArrType_Type)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * 
│ │   */
│ │    __pyx_r = PyObject_TypeCheck(__pyx_v_obj, (&PyDatetimeArrType_Type));
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":981
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":981
│ │   * 
│ │   * 
│ │   * cdef inline bint is_datetime64_object(object obj):             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     Cython equivalent of `isinstance(obj, np.datetime64)`
│ │   */
│ │  
│ │    /* function exit code */
│ │    __pyx_L0:;
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":996
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":996
│ │   * 
│ │   * 
│ │   * cdef inline npy_datetime get_datetime64_value(object obj) nogil:             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     returns the int64 value underlying scalar numpy datetime64 object
│ │   */
│ │  
│ │  static CYTHON_INLINE npy_datetime __pyx_f_5numpy_get_datetime64_value(PyObject *__pyx_v_obj) {
│ │    npy_datetime __pyx_r;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1003
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1003
│ │   *     also needed.  That can be found using `get_datetime64_unit`.
│ │   *     """
│ │   *     return (<PyDatetimeScalarObject*>obj).obval             # <<<<<<<<<<<<<<
│ │   * 
│ │   * 
│ │   */
│ │    __pyx_r = ((PyDatetimeScalarObject *)__pyx_v_obj)->obval;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":996
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":996
│ │   * 
│ │   * 
│ │   * cdef inline npy_datetime get_datetime64_value(object obj) nogil:             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     returns the int64 value underlying scalar numpy datetime64 object
│ │   */
│ │  
│ │    /* function exit code */
│ │    __pyx_L0:;
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1006
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1006
│ │   * 
│ │   * 
│ │   * cdef inline npy_timedelta get_timedelta64_value(object obj) nogil:             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     returns the int64 value underlying scalar numpy timedelta64 object
│ │   */
│ │  
│ │  static CYTHON_INLINE npy_timedelta __pyx_f_5numpy_get_timedelta64_value(PyObject *__pyx_v_obj) {
│ │    npy_timedelta __pyx_r;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1010
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1010
│ │   *     returns the int64 value underlying scalar numpy timedelta64 object
│ │   *     """
│ │   *     return (<PyTimedeltaScalarObject*>obj).obval             # <<<<<<<<<<<<<<
│ │   * 
│ │   * 
│ │   */
│ │    __pyx_r = ((PyTimedeltaScalarObject *)__pyx_v_obj)->obval;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1006
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1006
│ │   * 
│ │   * 
│ │   * cdef inline npy_timedelta get_timedelta64_value(object obj) nogil:             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     returns the int64 value underlying scalar numpy timedelta64 object
│ │   */
│ │  
│ │    /* function exit code */
│ │    __pyx_L0:;
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1013
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1013
│ │   * 
│ │   * 
│ │   * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     returns the unit part of the dtype for a numpy datetime64 object.
│ │   */
│ │  
│ │  static CYTHON_INLINE NPY_DATETIMEUNIT __pyx_f_5numpy_get_datetime64_unit(PyObject *__pyx_v_obj) {
│ │    NPY_DATETIMEUNIT __pyx_r;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1017
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1017
│ │   *     returns the unit part of the dtype for a numpy datetime64 object.
│ │   *     """
│ │   *     return <NPY_DATETIMEUNIT>(<PyDatetimeScalarObject*>obj).obmeta.base             # <<<<<<<<<<<<<<
│ │   */
│ │    __pyx_r = ((NPY_DATETIMEUNIT)((PyDatetimeScalarObject *)__pyx_v_obj)->obmeta.base);
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1013
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1013
│ │   * 
│ │   * 
│ │   * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     returns the unit part of the dtype for a numpy datetime64 object.
│ │   */
│ │  
│ │ @@ -21014,26 +21022,26 @@
│ │    return -1;
│ │  }
│ │  
│ │  static CYTHON_SMALL_CODE int __Pyx_InitCachedConstants(void) {
│ │    __Pyx_RefNannyDeclarations
│ │    __Pyx_RefNannySetupContext("__Pyx_InitCachedConstants", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":944
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":944
│ │   *         __pyx_import_array()
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.multiarray failed to import")             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline int import_umath() except -1:
│ │   */
│ │    __pyx_tuple_ = PyTuple_Pack(1, __pyx_kp_s_numpy_core_multiarray_failed_to); if (unlikely(!__pyx_tuple_)) __PYX_ERR(1, 944, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_tuple_);
│ │    __Pyx_GIVEREF(__pyx_tuple_);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":950
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":950
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:
│ │   */
│ │    __pyx_tuple__2 = PyTuple_Pack(1, __pyx_kp_s_numpy_core_umath_failed_to_impor); if (unlikely(!__pyx_tuple__2)) __PYX_ERR(1, 950, __pyx_L1_error)
│ │ @@ -21439,52 +21447,67 @@
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("__Pyx_modinit_type_import_code", 0);
│ │    /*--- Type import code ---*/
│ │    __pyx_t_1 = PyImport_ImportModule(__Pyx_BUILTIN_MODULE_NAME); if (unlikely(!__pyx_t_1)) __PYX_ERR(3, 9, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │    __pyx_ptype_7cpython_4type_type = __Pyx_ImportType(__pyx_t_1, __Pyx_BUILTIN_MODULE_NAME, "type", 
│ │    #if defined(PYPY_VERSION_NUM) && PYPY_VERSION_NUM < 0x050B0000
│ │ -  sizeof(PyTypeObject),
│ │ +  sizeof(PyTypeObject), __PYX_GET_STRUCT_ALIGNMENT(PyTypeObject),
│ │    #else
│ │ -  sizeof(PyHeapTypeObject),
│ │ +  sizeof(PyHeapTypeObject), __PYX_GET_STRUCT_ALIGNMENT(PyHeapTypeObject),
│ │    #endif
│ │    __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_7cpython_4type_type) __PYX_ERR(3, 9, __pyx_L1_error)
│ │    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
│ │    __pyx_t_1 = PyImport_ImportModule("numpy"); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 199, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │ -  __pyx_ptype_5numpy_dtype = __Pyx_ImportType(__pyx_t_1, "numpy", "dtype", sizeof(PyArray_Descr), __Pyx_ImportType_CheckSize_Ignore);
│ │ +  __pyx_ptype_5numpy_dtype = __Pyx_ImportType(__pyx_t_1, "numpy", "dtype", sizeof(PyArray_Descr), __PYX_GET_STRUCT_ALIGNMENT(PyArray_Descr),
│ │ +  __Pyx_ImportType_CheckSize_Ignore);
│ │     if (!__pyx_ptype_5numpy_dtype) __PYX_ERR(1, 199, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_flatiter = __Pyx_ImportType(__pyx_t_1, "numpy", "flatiter", sizeof(PyArrayIterObject), __Pyx_ImportType_CheckSize_Ignore);
│ │ +  __pyx_ptype_5numpy_flatiter = __Pyx_ImportType(__pyx_t_1, "numpy", "flatiter", sizeof(PyArrayIterObject), __PYX_GET_STRUCT_ALIGNMENT(PyArrayIterObject),
│ │ +  __Pyx_ImportType_CheckSize_Ignore);
│ │     if (!__pyx_ptype_5numpy_flatiter) __PYX_ERR(1, 222, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_broadcast = __Pyx_ImportType(__pyx_t_1, "numpy", "broadcast", sizeof(PyArrayMultiIterObject), __Pyx_ImportType_CheckSize_Ignore);
│ │ +  __pyx_ptype_5numpy_broadcast = __Pyx_ImportType(__pyx_t_1, "numpy", "broadcast", sizeof(PyArrayMultiIterObject), __PYX_GET_STRUCT_ALIGNMENT(PyArrayMultiIterObject),
│ │ +  __Pyx_ImportType_CheckSize_Ignore);
│ │     if (!__pyx_ptype_5numpy_broadcast) __PYX_ERR(1, 226, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_ndarray = __Pyx_ImportType(__pyx_t_1, "numpy", "ndarray", sizeof(PyArrayObject), __Pyx_ImportType_CheckSize_Ignore);
│ │ +  __pyx_ptype_5numpy_ndarray = __Pyx_ImportType(__pyx_t_1, "numpy", "ndarray", sizeof(PyArrayObject), __PYX_GET_STRUCT_ALIGNMENT(PyArrayObject),
│ │ +  __Pyx_ImportType_CheckSize_Ignore);
│ │     if (!__pyx_ptype_5numpy_ndarray) __PYX_ERR(1, 238, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_generic = __Pyx_ImportType(__pyx_t_1, "numpy", "generic", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_generic = __Pyx_ImportType(__pyx_t_1, "numpy", "generic", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_generic) __PYX_ERR(1, 770, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_number = __Pyx_ImportType(__pyx_t_1, "numpy", "number", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_number = __Pyx_ImportType(__pyx_t_1, "numpy", "number", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_number) __PYX_ERR(1, 772, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_integer = __Pyx_ImportType(__pyx_t_1, "numpy", "integer", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_integer = __Pyx_ImportType(__pyx_t_1, "numpy", "integer", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_integer) __PYX_ERR(1, 774, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_signedinteger = __Pyx_ImportType(__pyx_t_1, "numpy", "signedinteger", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_signedinteger = __Pyx_ImportType(__pyx_t_1, "numpy", "signedinteger", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_signedinteger) __PYX_ERR(1, 776, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_unsignedinteger = __Pyx_ImportType(__pyx_t_1, "numpy", "unsignedinteger", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_unsignedinteger = __Pyx_ImportType(__pyx_t_1, "numpy", "unsignedinteger", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_unsignedinteger) __PYX_ERR(1, 778, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_inexact = __Pyx_ImportType(__pyx_t_1, "numpy", "inexact", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_inexact = __Pyx_ImportType(__pyx_t_1, "numpy", "inexact", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_inexact) __PYX_ERR(1, 780, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_floating = __Pyx_ImportType(__pyx_t_1, "numpy", "floating", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_floating = __Pyx_ImportType(__pyx_t_1, "numpy", "floating", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_floating) __PYX_ERR(1, 782, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_complexfloating = __Pyx_ImportType(__pyx_t_1, "numpy", "complexfloating", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_complexfloating = __Pyx_ImportType(__pyx_t_1, "numpy", "complexfloating", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_complexfloating) __PYX_ERR(1, 784, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_flexible = __Pyx_ImportType(__pyx_t_1, "numpy", "flexible", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_flexible = __Pyx_ImportType(__pyx_t_1, "numpy", "flexible", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_flexible) __PYX_ERR(1, 786, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_character = __Pyx_ImportType(__pyx_t_1, "numpy", "character", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_character = __Pyx_ImportType(__pyx_t_1, "numpy", "character", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_character) __PYX_ERR(1, 788, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_ufunc = __Pyx_ImportType(__pyx_t_1, "numpy", "ufunc", sizeof(PyUFuncObject), __Pyx_ImportType_CheckSize_Ignore);
│ │ +  __pyx_ptype_5numpy_ufunc = __Pyx_ImportType(__pyx_t_1, "numpy", "ufunc", sizeof(PyUFuncObject), __PYX_GET_STRUCT_ALIGNMENT(PyUFuncObject),
│ │ +  __Pyx_ImportType_CheckSize_Ignore);
│ │     if (!__pyx_ptype_5numpy_ufunc) __PYX_ERR(1, 826, __pyx_L1_error)
│ │    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
│ │    __Pyx_RefNannyFinishContext();
│ │    return 0;
│ │    __pyx_L1_error:;
│ │    __Pyx_XDECREF(__pyx_t_1);
│ │    __Pyx_RefNannyFinishContext();
│ │ @@ -23119,28 +23142,28 @@
│ │                              "BaseException");
│ │              goto bad;
│ │          }
│ │          PyException_SetCause(value, fixed_cause);
│ │      }
│ │      PyErr_SetObject(type, value);
│ │      if (tb) {
│ │ -#if CYTHON_COMPILING_IN_PYPY
│ │ -        PyObject *tmp_type, *tmp_value, *tmp_tb;
│ │ -        PyErr_Fetch(&tmp_type, &tmp_value, &tmp_tb);
│ │ -        Py_INCREF(tb);
│ │ -        PyErr_Restore(tmp_type, tmp_value, tb);
│ │ -        Py_XDECREF(tmp_tb);
│ │ -#else
│ │ +#if CYTHON_FAST_THREAD_STATE
│ │          PyThreadState *tstate = __Pyx_PyThreadState_Current;
│ │          PyObject* tmp_tb = tstate->curexc_traceback;
│ │          if (tb != tmp_tb) {
│ │              Py_INCREF(tb);
│ │              tstate->curexc_traceback = tb;
│ │              Py_XDECREF(tmp_tb);
│ │          }
│ │ +#else
│ │ +        PyObject *tmp_type, *tmp_value, *tmp_tb;
│ │ +        PyErr_Fetch(&tmp_type, &tmp_value, &tmp_tb);
│ │ +        Py_INCREF(tb);
│ │ +        PyErr_Restore(tmp_type, tmp_value, tb);
│ │ +        Py_XDECREF(tmp_tb);
│ │  #endif
│ │      }
│ │  bad:
│ │      Py_XDECREF(owned_instance);
│ │      return;
│ │  }
│ │  #endif
│ │ @@ -23932,44 +23955,62 @@
│ │      return ret;
│ │  }
│ │  
│ │  /* TypeImport */
│ │  #ifndef __PYX_HAVE_RT_ImportType
│ │  #define __PYX_HAVE_RT_ImportType
│ │  static PyTypeObject *__Pyx_ImportType(PyObject *module, const char *module_name, const char *class_name,
│ │ -    size_t size, enum __Pyx_ImportType_CheckSize check_size)
│ │ +    size_t size, size_t alignment, enum __Pyx_ImportType_CheckSize check_size)
│ │  {
│ │      PyObject *result = 0;
│ │      char warning[200];
│ │      Py_ssize_t basicsize;
│ │ +    Py_ssize_t itemsize;
│ │  #ifdef Py_LIMITED_API
│ │      PyObject *py_basicsize;
│ │ +    PyObject *py_itemsize;
│ │  #endif
│ │      result = PyObject_GetAttrString(module, class_name);
│ │      if (!result)
│ │          goto bad;
│ │      if (!PyType_Check(result)) {
│ │          PyErr_Format(PyExc_TypeError,
│ │              "%.200s.%.200s is not a type object",
│ │              module_name, class_name);
│ │          goto bad;
│ │      }
│ │  #ifndef Py_LIMITED_API
│ │      basicsize = ((PyTypeObject *)result)->tp_basicsize;
│ │ +    itemsize = ((PyTypeObject *)result)->tp_itemsize;
│ │  #else
│ │      py_basicsize = PyObject_GetAttrString(result, "__basicsize__");
│ │      if (!py_basicsize)
│ │          goto bad;
│ │      basicsize = PyLong_AsSsize_t(py_basicsize);
│ │      Py_DECREF(py_basicsize);
│ │      py_basicsize = 0;
│ │      if (basicsize == (Py_ssize_t)-1 && PyErr_Occurred())
│ │          goto bad;
│ │ +    py_itemsize = PyObject_GetAttrString(result, "__itemsize__");
│ │ +    if (!py_itemsize)
│ │ +        goto bad;
│ │ +    itemsize = PyLong_AsSsize_t(py_itemsize);
│ │ +    Py_DECREF(py_itemsize);
│ │ +    py_itemsize = 0;
│ │ +    if (itemsize == (Py_ssize_t)-1 && PyErr_Occurred())
│ │ +        goto bad;
│ │  #endif
│ │ -    if ((size_t)basicsize < size) {
│ │ +    if (itemsize) {
│ │ +        if (size % alignment) {
│ │ +            alignment = size % alignment;
│ │ +        }
│ │ +        if (itemsize < (Py_ssize_t)alignment)
│ │ +            itemsize = (Py_ssize_t)alignment;
│ │ +    }
│ │ +    if ((size_t)(basicsize + itemsize) < size) {
│ │          PyErr_Format(PyExc_ValueError,
│ │              "%.200s.%.200s size changed, may indicate binary incompatibility. "
│ │              "Expected %zd from C header, got %zd from PyObject",
│ │              module_name, class_name, size, basicsize);
│ │          goto bad;
│ │      }
│ │      if (check_size == __Pyx_ImportType_CheckSize_Error && (size_t)basicsize != size) {
│ │   --- LibRecommender-1.0.1/libreco/algorithms/_bpr.pyx
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/_bpr.pyx
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/algorithms/als.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/als.py
│ │┄ Files 2% similar despite different names
│ │ @@ -7,14 +7,15 @@
│ │  
│ │  from ..bases import EmbedBase
│ │  from ..evaluation import print_metrics
│ │  from ..recommendation import recommend_from_embedding
│ │  from ..utils.initializers import truncated_normal
│ │  from ..utils.misc import time_block
│ │  from ..utils.save_load import save_default_recs, save_params
│ │ +from ..utils.validate import check_fitting
│ │  
│ │  LOG_FORMAT = "%(asctime)s - %(levelname)s - %(message)s"
│ │  logging.basicConfig(format=LOG_FORMAT)
│ │  
│ │  
│ │  class ALS(EmbedBase):
│ │      """*Alternating Least Squares* algorithm.
│ │ @@ -88,28 +89,35 @@
│ │          self.item_embed = truncated_normal(
│ │              shape=[self.n_items, self.embed_size], mean=0.0, scale=0.03
│ │          )
│ │  
│ │      def fit(
│ │          self,
│ │          train_data,
│ │ +        neg_sampling,
│ │          verbose=1,
│ │          shuffle=True,
│ │          eval_data=None,
│ │          metrics=None,
│ │          k=10,
│ │          eval_batch_size=8192,
│ │          eval_user_num=None,
│ │ +        **kwargs,
│ │      ):
│ │          """Fit ALS model on the training data.
│ │  
│ │          Parameters
│ │          ----------
│ │          train_data : :class:`~libreco.data.TransformedSet` object
│ │              Data object used for training.
│ │ +        neg_sampling : bool
│ │ +            Whether to perform negative sampling for evaluating data.
│ │ +
│ │ +            .. versionadded:: 1.1.0
│ │ +
│ │          verbose : int, default: 1
│ │              Print verbosity. If `eval_data` is provided, setting it to higher than 1
│ │              will print evaluation metrics during training.
│ │          shuffle : bool, default: True
│ │              Whether to shuffle the training data.
│ │          eval_data : :class:`~libreco.data.TransformedSet` object, default: None
│ │              Data object used for evaluating.
│ │ @@ -125,21 +133,20 @@
│ │          """
│ │          try:
│ │              from ._als import als_update
│ │          except (ImportError, ModuleNotFoundError):
│ │              logging.warning("Als cython version is not available")
│ │              raise
│ │  
│ │ -        self.check_attribute(eval_data, k)
│ │ +        check_fitting(self, train_data, eval_data, neg_sampling, k)
│ │          self.show_start_time()
│ │          self.build_model()
│ │          user_interaction = train_data.sparse_interaction  # sparse.csr_matrix
│ │          item_interaction = user_interaction.T.tocsr()
│ │          if self.task == "ranking":
│ │ -            # check_has_sampled(train_data, verbose)
│ │              user_interaction.data = user_interaction.data * self.alpha + 1
│ │              item_interaction.data = item_interaction.data * self.alpha + 1
│ │  
│ │          trainer = partial(als_update, task=self.task, use_cg=self.use_cg)
│ │          for epoch in range(1, self.n_epochs + 1):
│ │              with time_block(f"Epoch {epoch}", verbose):
│ │                  trainer(
│ │ @@ -156,31 +163,32 @@
│ │                      reg=self.reg,
│ │                      num_threads=self.n_threads,
│ │                  )
│ │  
│ │              if verbose > 1:
│ │                  print_metrics(
│ │                      model=self,
│ │ +                    neg_sampling=neg_sampling,
│ │                      train_data=train_data,
│ │                      eval_data=eval_data,
│ │                      metrics=metrics,
│ │                      eval_batch_size=eval_batch_size,
│ │                      k=k,
│ │                      sample_user_num=eval_user_num,
│ │                      seed=self.seed,
│ │                  )
│ │                  print("=" * 30)
│ │          self.assign_embedding_oov()
│ │          self.default_recs = recommend_from_embedding(
│ │ -            task=self.task,
│ │ +            model=self,
│ │              user_ids=[self.n_users],
│ │              n_rec=min(2000, self.n_items),
│ │ -            data_info=self.data_info,
│ │ -            user_embed=self.user_embed,
│ │ -            item_embed=self.item_embed,
│ │ +            user_embeddings=self.user_embed,
│ │ +            item_embeddings=self.item_embed,
│ │ +            seq=None,
│ │              filter_consumed=False,
│ │              random_rec=False,
│ │          ).flatten()
│ │  
│ │      @staticmethod
│ │      def _check_reg(reg):
│ │          if not isinstance(reg, float) or reg <= 0.0:
│ │   --- LibRecommender-1.0.1/libreco/algorithms/autoint.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/autoint.py
│ │┄ Files 2% similar despite different names
│ │ @@ -1,24 +1,24 @@
│ │  """Implementation of AutoInt."""
│ │  from ..bases import ModelMeta, TfBase
│ │ +from ..feature.multi_sparse import true_sparse_field_size
│ │  from ..tfops import (
│ │      dropout_config,
│ │      multi_sparse_combine_embedding,
│ │      reg_config,
│ │      tf,
│ │      tf_dense,
│ │  )
│ │  from ..utils.validate import (
│ │      check_dense_values,
│ │      check_multi_sparse,
│ │      check_sparse_indices,
│ │      dense_field_size,
│ │      sparse_feat_size,
│ │      sparse_field_size,
│ │ -    true_sparse_field_size,
│ │  )
│ │  
│ │  
│ │  class AutoInt(TfBase, metaclass=ModelMeta):
│ │      """AutoInt algorithm.
│ │  
│ │      Parameters
│ │ @@ -43,14 +43,23 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ +    sampler : {'random', 'unconsumed', 'popular'}, default: 'random'
│ │ +        Negative sampling strategy.
│ │ +
│ │ +        - ``'random'`` means random sampling.
│ │ +        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ +        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ +
│ │ +        .. versionadded:: 1.1.0
│ │ +
│ │      num_neg : int, default: 1
│ │          Number of negative samples for each positive sample, only used in `ranking` task.
│ │      use_bn : bool, default: True
│ │          Whether to use batch normalization.
│ │      dropout_rate : float or None, default: None
│ │          Probability of an element to be zeroed. If it is None, dropout is not used.
│ │      att_embed_size : int, list of int or tuple of (int,), default: (8, 8, 8)
│ │ @@ -88,14 +97,15 @@
│ │          embed_size=16,
│ │          n_epochs=10,
│ │          lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ +        sampler="random",
│ │          num_neg=1,
│ │          use_bn=True,
│ │          dropout_rate=None,
│ │          att_embed_size=(8, 8, 8),
│ │          num_heads=2,
│ │          use_residual=True,
│ │          multi_sparse_combiner="sqrtn",
│ │ @@ -110,14 +120,15 @@
│ │          self.embed_size = embed_size
│ │          self.n_epochs = n_epochs
│ │          self.lr = lr
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │          self.reg = reg_config(reg)
│ │          self.batch_size = batch_size
│ │ +        self.sampler = sampler
│ │          self.num_neg = num_neg
│ │          self.use_bn = use_bn
│ │          self.dropout_rate = dropout_config(dropout_rate)
│ │          # `att_embed_size` also decides the num of attention layer
│ │          self.att_embed_size, self.att_layer_num = self._att_config(att_embed_size)
│ │          self.num_heads = num_heads
│ │          self.use_residual = use_residual
│ │   --- LibRecommender-1.0.1/libreco/algorithms/bpr.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/bpr.py
│ │┄ Files 10% similar despite different names
│ │ @@ -4,17 +4,18 @@
│ │  
│ │  import numpy as np
│ │  
│ │  from ..bases import EmbedBase, ModelMeta
│ │  from ..evaluation import print_metrics
│ │  from ..recommendation import recommend_from_embedding
│ │  from ..tfops import reg_config, sess_config, tf
│ │ -from ..training import get_trainer
│ │ +from ..training.dispatch import get_trainer
│ │  from ..utils.initializers import truncated_normal
│ │  from ..utils.misc import time_block
│ │ +from ..utils.validate import check_fitting
│ │  
│ │  LOG_FORMAT = "%(asctime)s - %(levelname)s - %(message)s"
│ │  logging.basicConfig(format=LOG_FORMAT)
│ │  
│ │  
│ │  class BPR(EmbedBase, metaclass=ModelMeta, backend="tensorflow"):
│ │      """*Bayesian Personalized Ranking* algorithm.
│ │ @@ -47,14 +48,23 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ +    sampler : {'random', 'unconsumed', 'popular'}, default: 'random'
│ │ +        Negative sampling strategy.
│ │ +
│ │ +        - ``'random'`` means random sampling.
│ │ +        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ +        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ +
│ │ +        .. versionadded:: 1.1.0
│ │ +
│ │      num_neg : int, default: 1
│ │          Number of negative samples for each positive sample, only used in `ranking` task.
│ │      use_tf : bool, default: True
│ │          Whether to use TensorFlow or Cython version. The TensorFlow version is more
│ │          accurate, whereas the Cython version is faster.
│ │      seed : int, default: 42
│ │          Random seed.
│ │ @@ -85,14 +95,15 @@
│ │          embed_size=16,
│ │          n_epochs=20,
│ │          lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ +        sampler="random",
│ │          num_neg=1,
│ │          use_tf=True,
│ │          seed=42,
│ │          lower_upper_bound=None,
│ │          tf_sess_config=None,
│ │          optimizer="adam",
│ │          num_threads=1,
│ │ @@ -105,14 +116,15 @@
│ │          self.loss_type = loss_type
│ │          self.n_epochs = n_epochs
│ │          self.lr = lr
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │          self.reg = reg_config(reg) if use_tf else reg
│ │          self.batch_size = batch_size
│ │ +        self.sampler = sampler
│ │          self.num_neg = num_neg
│ │          self.use_tf = use_tf
│ │          self.seed = seed
│ │          self.optimizer = optimizer
│ │          self.num_threads = num_threads
│ │          if use_tf:
│ │              self.sess = sess_config(tf_sess_config)
│ │ @@ -177,28 +189,35 @@
│ │              tf.multiply(embed_user, tf.subtract(embed_item_pos, embed_item_neg)), axis=1
│ │          )
│ │          self.bpr_loss = tf.log_sigmoid(item_diff)
│ │  
│ │      def fit(
│ │          self,
│ │          train_data,
│ │ +        neg_sampling,
│ │          verbose=1,
│ │          shuffle=True,
│ │          eval_data=None,
│ │          metrics=None,
│ │          k=10,
│ │          eval_batch_size=8192,
│ │          eval_user_num=None,
│ │ +        num_workers=0,
│ │      ):
│ │          """Fit BPR model on the training data.
│ │  
│ │          Parameters
│ │          ----------
│ │          train_data : :class:`~libreco.data.TransformedSet` object
│ │              Data object used for training.
│ │ +        neg_sampling : bool
│ │ +            Whether to perform negative sampling for training or evaluating data.
│ │ +
│ │ +            .. versionadded:: 1.1.0
│ │ +
│ │          verbose : int, default: 1
│ │              Print verbosity. If `eval_data` is provided, setting it to higher than 1
│ │              will print evaluation metrics during training.
│ │          shuffle : bool, default: True
│ │              Whether to shuffle the training data.
│ │          eval_data : :class:`~libreco.data.TransformedSet` object, default: None
│ │              Data object used for evaluating.
│ │ @@ -207,61 +226,67 @@
│ │          k : int, default: 10
│ │              Parameter of metrics, e.g. recall at k, ndcg at k
│ │          eval_batch_size : int, default: 8192
│ │              Batch size for evaluating.
│ │          eval_user_num : int or None, default: None
│ │              Number of users for evaluating. Setting it to a positive number will sample
│ │              users randomly from eval data.
│ │ +        num_workers : int, default: 0
│ │ +            How many subprocesses to use for data loading.
│ │ +            0 means that the data will be loaded in the main process.
│ │          """
│ │ -        self.check_attribute(eval_data, k)
│ │ +        check_fitting(self, train_data, eval_data, neg_sampling, k)
│ │          self.show_start_time()
│ │ -        # check_has_sampled(train_data, verbose)
│ │          if not self.model_built:
│ │              self.build_model()
│ │              self.model_built = True
│ │          if self.use_tf:
│ │              if self.trainer is None:
│ │                  self.trainer = get_trainer(self)
│ │              self.trainer.run(
│ │                  train_data,
│ │ +                neg_sampling,
│ │                  verbose,
│ │                  shuffle,
│ │                  eval_data,
│ │                  metrics,
│ │                  k,
│ │                  eval_batch_size,
│ │                  eval_user_num,
│ │ +                num_workers,
│ │              )
│ │              self.set_embeddings()
│ │          else:
│ │              self._fit_cython(
│ │                  train_data=train_data,
│ │ +                neg_sampling=neg_sampling,
│ │                  verbose=verbose,
│ │                  shuffle=shuffle,
│ │                  eval_data=eval_data,
│ │                  metrics=metrics,
│ │                  k=k,
│ │                  eval_batch_size=eval_batch_size,
│ │                  eval_user_num=eval_user_num,
│ │              )
│ │          self.assign_embedding_oov()
│ │          self.default_recs = recommend_from_embedding(
│ │ -            task=self.task,
│ │ +            model=self,
│ │              user_ids=[self.n_users],
│ │              n_rec=min(2000, self.n_items),
│ │ -            data_info=self.data_info,
│ │ -            user_embed=self.user_embed,
│ │ -            item_embed=self.item_embed,
│ │ +            user_embeddings=self.user_embed,
│ │ +            item_embeddings=self.item_embed,
│ │ +            seq=None,
│ │              filter_consumed=False,
│ │              random_rec=False,
│ │          ).flatten()
│ │  
│ │      def _fit_cython(
│ │          self,
│ │          train_data,
│ │ +        neg_sampling,
│ │          verbose=1,
│ │          shuffle=True,
│ │          eval_data=None,
│ │          metrics=None,
│ │          k=None,
│ │          eval_batch_size=None,
│ │          eval_user_num=None,
│ │ @@ -325,14 +350,15 @@
│ │                      seed=self.seed,
│ │                      epoch=epoch,
│ │                  )
│ │  
│ │              if verbose > 1:
│ │                  print_metrics(
│ │                      model=self,
│ │ +                    neg_sampling=neg_sampling,
│ │                      eval_data=eval_data,
│ │                      metrics=metrics,
│ │                      eval_batch_size=eval_batch_size,
│ │                      k=k,
│ │                      sample_user_num=eval_user_num,
│ │                      seed=self.seed,
│ │                  )
│ │   --- LibRecommender-1.0.1/libreco/algorithms/caser.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/caser.py
│ │┄ Files 8% similar despite different names
│ │ @@ -1,26 +1,14 @@
│ │  """Implementation of Caser."""
│ │ -import numpy as np
│ │ -
│ │ -from ..bases import EmbedBase, ModelMeta
│ │ -from ..data.sequence import get_user_last_interacted
│ │ -from ..tfops import (
│ │ -    conv_nn,
│ │ -    dropout_config,
│ │ -    max_pool,
│ │ -    reg_config,
│ │ -    sess_config,
│ │ -    tf,
│ │ -    tf_dense,
│ │ -)
│ │ +from ..bases import ModelMeta, SeqEmbedBase
│ │ +from ..tfops import conv_nn, dropout_config, max_pool, reg_config, tf, tf_dense
│ │  from ..utils.misc import count_params
│ │ -from ..utils.validate import check_seq_mode
│ │  
│ │  
│ │ -class Caser(EmbedBase, metaclass=ModelMeta, backend="tensorflow"):
│ │ +class Caser(SeqEmbedBase, metaclass=ModelMeta, backend="tensorflow"):
│ │      """*Caser* algorithm.
│ │  
│ │      Parameters
│ │      ----------
│ │      task : {'rating', 'ranking'}
│ │          Recommendation task. See :ref:`Task`.
│ │      data_info : :class:`~libreco.data.DataInfo` object
│ │ @@ -41,14 +29,23 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ +    sampler : {'random', 'unconsumed', 'popular'}, default: 'random'
│ │ +        Negative sampling strategy.
│ │ +
│ │ +        - ``'random'`` means random sampling.
│ │ +        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ +        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ +
│ │ +        .. versionadded:: 1.1.0
│ │ +
│ │      num_neg : int, default: 1
│ │          Number of negative samples for each positive sample, only used in `ranking` task.
│ │      use_bn : bool, default: True
│ │          Whether to use batch normalization.
│ │      dropout_rate : float or None, default: None
│ │          Probability of an element to be zeroed. If it is None, dropout is not used.
│ │      nh_filters : int, default: 2
│ │ @@ -85,44 +82,50 @@
│ │          embed_size=16,
│ │          n_epochs=20,
│ │          lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ +        sampler="random",
│ │          num_neg=1,
│ │          use_bn=False,
│ │          dropout_rate=None,
│ │          nh_filters=2,
│ │          nv_filters=4,
│ │          recent_num=10,
│ │          random_num=None,
│ │          seed=42,
│ │          lower_upper_bound=None,
│ │          tf_sess_config=None,
│ │      ):
│ │ -        super().__init__(task, data_info, embed_size, lower_upper_bound)
│ │ -
│ │ +        super().__init__(
│ │ +            task,
│ │ +            data_info,
│ │ +            embed_size,
│ │ +            recent_num,
│ │ +            random_num,
│ │ +            lower_upper_bound,
│ │ +            tf_sess_config,
│ │ +        )
│ │          self.all_args = locals()
│ │ -        self.sess = sess_config(tf_sess_config)
│ │          self.loss_type = loss_type
│ │          self.n_epochs = n_epochs
│ │          self.lr = lr
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │          self.reg = reg_config(reg)
│ │          self.batch_size = batch_size
│ │ +        self.sampler = sampler
│ │          self.num_neg = num_neg
│ │          self.dropout_rate = dropout_config(dropout_rate)
│ │          self.use_bn = use_bn
│ │          self.nh_filters = nh_filters
│ │          self.nv_filters = nv_filters
│ │          self.seed = seed
│ │ -        self.seq_mode, self.max_seq_len = check_seq_mode(recent_num, random_num)
│ │ -        self.recent_seqs, self.recent_seq_lens = self._set_recent_seqs()
│ │  
│ │      def build_model(self):
│ │          tf.set_random_seed(self.seed)
│ │          self._build_placeholders()
│ │          self._build_variables()
│ │          self._build_user_embeddings()
│ │  
│ │ @@ -197,17 +200,15 @@
│ │                  strides=1,
│ │                  padding="valid",
│ │                  activation="relu",
│ │              )(inputs=seq_item_embed)
│ │  
│ │              # h_conv = tf.reduce_max(h_conv, axis=1)
│ │              h_size = h_conv.get_shape().as_list()[1]
│ │ -            h_conv = max_pool(pool_size=h_size, strides=1, padding="valid")(
│ │ -                inputs=h_conv
│ │ -            )
│ │ +            h_conv = max_pool(pool_size=h_size, strides=1, padding="valid")(h_conv)
│ │              h_conv = tf.squeeze(h_conv, axis=1)
│ │              convs_out.append(h_conv)
│ │  
│ │          v_conv = conv_nn(
│ │              filters=self.nv_filters,
│ │              kernel_size=1,
│ │              strides=1,
│ │ @@ -215,28 +216,7 @@
│ │              activation="relu",
│ │          )(inputs=tf.transpose(seq_item_embed, [0, 2, 1]))
│ │          convs_out.append(tf.keras.layers.Flatten()(v_conv))
│ │  
│ │          convs_out = tf.concat(convs_out, axis=1)
│ │          convs_out = tf_dense(units=self.embed_size, activation=tf.nn.relu)(convs_out)
│ │          self.user_vector = tf.concat([user_repr, convs_out], axis=1)
│ │ -
│ │ -    def _set_recent_seqs(self):
│ │ -        recent_seqs, recent_seq_lens = get_user_last_interacted(
│ │ -            self.n_users, self.user_consumed, self.n_items, self.max_seq_len
│ │ -        )
│ │ -        return recent_seqs, recent_seq_lens.astype(np.int64)
│ │ -
│ │ -    def set_embeddings(self):
│ │ -        feed_dict = {
│ │ -            self.user_indices: np.arange(self.n_users),
│ │ -            self.user_interacted_seq: self.recent_seqs,
│ │ -            self.user_interacted_len: self.recent_seq_lens,
│ │ -        }
│ │ -        user_vector = self.sess.run(self.user_vector, feed_dict)
│ │ -        item_weights = self.sess.run(self.item_weights)
│ │ -        item_biases = self.sess.run(self.item_biases)
│ │ -
│ │ -        user_bias = np.ones([len(user_vector), 1], dtype=user_vector.dtype)
│ │ -        item_bias = item_biases[:, None]
│ │ -        self.user_embed = np.hstack([user_vector, user_bias])
│ │ -        self.item_embed = np.hstack([item_weights, item_bias])
│ │   --- LibRecommender-1.0.1/libreco/algorithms/deepfm.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/deepfm.py
│ │┄ Files 5% similar despite different names
│ │ @@ -1,9 +1,10 @@
│ │  """Implementation of DeepFM."""
│ │  from ..bases import ModelMeta, TfBase
│ │ +from ..feature.multi_sparse import true_sparse_field_size
│ │  from ..tfops import (
│ │      dense_nn,
│ │      dropout_config,
│ │      multi_sparse_combine_embedding,
│ │      reg_config,
│ │      tf,
│ │      tf_dense,
│ │ @@ -13,15 +14,14 @@
│ │  from ..utils.validate import (
│ │      check_dense_values,
│ │      check_multi_sparse,
│ │      check_sparse_indices,
│ │      dense_field_size,
│ │      sparse_feat_size,
│ │      sparse_field_size,
│ │ -    true_sparse_field_size,
│ │  )
│ │  
│ │  
│ │  class DeepFM(TfBase, metaclass=ModelMeta):
│ │      """*DeepFM* algorithm.
│ │  
│ │      Parameters
│ │ @@ -46,14 +46,23 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ +    sampler : {'random', 'unconsumed', 'popular'}, default: 'random'
│ │ +        Negative sampling strategy.
│ │ +
│ │ +        - ``'random'`` means random sampling.
│ │ +        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ +        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ +
│ │ +        .. versionadded:: 1.1.0
│ │ +
│ │      num_neg : int, default: 1
│ │          Number of negative samples for each positive sample, only used in `ranking` task.
│ │      use_bn : bool, default: True
│ │          Whether to use batch normalization.
│ │      dropout_rate : float or None, default: None
│ │          Probability of an element to be zeroed. If it is None, dropout is not used.
│ │      hidden_units : int, list of int or tuple of (int,), default: (128, 64, 32)
│ │ @@ -91,14 +100,15 @@
│ │          embed_size=16,
│ │          n_epochs=20,
│ │          lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ +        sampler="random",
│ │          num_neg=1,
│ │          use_bn=True,
│ │          dropout_rate=None,
│ │          hidden_units=(128, 64, 32),
│ │          multi_sparse_combiner="sqrtn",
│ │          seed=42,
│ │          lower_upper_bound=None,
│ │ @@ -111,14 +121,15 @@
│ │          self.embed_size = embed_size
│ │          self.n_epochs = n_epochs
│ │          self.lr = lr
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │          self.reg = reg_config(reg)
│ │          self.batch_size = batch_size
│ │ +        self.sampler = sampler
│ │          self.num_neg = num_neg
│ │          self.use_bn = use_bn
│ │          self.dropout_rate = dropout_config(dropout_rate)
│ │          self.hidden_units = hidden_units_config(hidden_units)
│ │          self.seed = seed
│ │          self.sparse = check_sparse_indices(data_info)
│ │          self.dense = check_dense_values(data_info)
│ │   --- LibRecommender-1.0.1/libreco/algorithms/deepwalk.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/deepwalk.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/algorithms/din.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/din.py
│ │┄ Files 7% similar despite different names
│ │ @@ -1,31 +1,31 @@
│ │  """Implementation of DIN."""
│ │  import numpy as np
│ │  
│ │  from ..bases import ModelMeta, TfBase
│ │ -from ..data.sequence import get_user_last_interacted
│ │ +from ..batch.sequence import get_user_last_interacted
│ │ +from ..feature.multi_sparse import true_sparse_field_size
│ │  from ..tfops import (
│ │      dense_nn,
│ │      dropout_config,
│ │      multi_sparse_combine_embedding,
│ │      reg_config,
│ │      tf,
│ │      tf_dense,
│ │  )
│ │  from ..torchops import hidden_units_config
│ │  from ..utils.misc import count_params
│ │  from ..utils.validate import (
│ │      check_dense_values,
│ │ -    check_seq_mode,
│ │      check_multi_sparse,
│ │ +    check_seq_mode,
│ │      check_sparse_indices,
│ │      dense_field_size,
│ │      sparse_feat_size,
│ │      sparse_field_size,
│ │ -    true_sparse_field_size,
│ │  )
│ │  
│ │  
│ │  class DIN(TfBase, metaclass=ModelMeta):
│ │      """*Deep Interest Network* algorithm.
│ │  
│ │      Parameters
│ │ @@ -50,14 +50,23 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ +    sampler : {'random', 'unconsumed', 'popular'}, default: 'random'
│ │ +        Negative sampling strategy.
│ │ +
│ │ +        - ``'random'`` means random sampling.
│ │ +        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ +        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ +
│ │ +        .. versionadded:: 1.1.0
│ │ +
│ │      num_neg : int, default: 1
│ │          Number of negative samples for each positive sample, only used in `ranking` task.
│ │      use_bn : bool, default: True
│ │          Whether to use batch normalization.
│ │      dropout_rate : float or None, default: None
│ │          Probability of an element to be zeroed. If it is None, dropout is not used.
│ │      hidden_units : int, list of int or tuple of (int,), default: (128, 64, 32)
│ │ @@ -104,14 +113,15 @@
│ │          embed_size=16,
│ │          n_epochs=20,
│ │          lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ +        sampler="random",
│ │          num_neg=1,
│ │          use_bn=True,
│ │          dropout_rate=None,
│ │          hidden_units=(128, 64, 32),
│ │          recent_num=10,
│ │          random_num=None,
│ │          use_tf_attention=False,
│ │ @@ -127,14 +137,15 @@
│ │          self.embed_size = embed_size
│ │          self.n_epochs = n_epochs
│ │          self.lr = lr
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │          self.reg = reg_config(reg)
│ │          self.batch_size = batch_size
│ │ +        self.sampler = sampler
│ │          self.num_neg = num_neg
│ │          self.use_bn = use_bn
│ │          self.dropout_rate = dropout_config(dropout_rate)
│ │          self.hidden_units = hidden_units_config(hidden_units)
│ │          self.use_tf_attention = use_tf_attention
│ │          self.seq_mode, self.max_seq_len = check_seq_mode(recent_num, random_num)
│ │          self.recent_seqs, self.recent_seq_lens = self._set_recent_seqs()
│ │ @@ -186,15 +197,15 @@
│ │          count_params()
│ │  
│ │      def _build_placeholders(self):
│ │          self.user_indices = tf.placeholder(tf.int32, shape=[None])
│ │          self.item_indices = tf.placeholder(tf.int32, shape=[None])
│ │          self.user_interacted_seq = tf.placeholder(
│ │              tf.int32, shape=[None, self.max_seq_len]
│ │ -        )  # B * seq
│ │ +        )
│ │          self.user_interacted_len = tf.placeholder(tf.float32, shape=[None])
│ │          self.labels = tf.placeholder(tf.float32, shape=[None])
│ │          self.is_training = tf.placeholder_with_default(False, shape=[])
│ │  
│ │          if self.sparse:
│ │              self.sparse_indices = tf.placeholder(
│ │                  tf.int32, shape=[None, self.sparse_field_size]
│ │ @@ -266,25 +277,14 @@
│ │  
│ │          if self.item_sparse:
│ │              item_sparse_embed = tf.keras.layers.Flatten()(
│ │                  tf.gather(sparse_embed, self.item_sparse_col_indices, axis=1)
│ │              )
│ │              self.item_embed.append(item_sparse_embed)
│ │  
│ │ -        # sparse_embed = tf.nn.embedding_lookup(
│ │ -        #    self.sparse_feat, self.sparse_indices)
│ │ -        # self.concat_embed.append(tf.reshape(
│ │ -        #    sparse_embed, [-1, self.sparse_field_size * self.embed_size])
│ │ -        # )
│ │ -        # if self.item_sparse:
│ │ -        #    item_sparse_embed = tf.layers.flatten(
│ │ -        #        tf.gather(sparse_embed, self.item_sparse_col_indices, axis=1)
│ │ -        #    )
│ │ -        #    self.item_embed.append(item_sparse_embed)
│ │ -
│ │      def _build_dense(self):
│ │          batch_size = tf.shape(self.dense_values)[0]
│ │          # 1 * F_dense * K
│ │          dense_embed = tf.expand_dims(self.dense_feat, axis=0)
│ │          # B * F_dense * K
│ │          dense_embed = tf.tile(dense_embed, [batch_size, 1, 1])
│ │          dense_values_reshape = tf.reshape(
│ │   --- LibRecommender-1.0.1/libreco/algorithms/fm.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/fm.py
│ │┄ Files 8% similar despite different names
│ │ @@ -1,9 +1,10 @@
│ │  """Implementation of FM."""
│ │  from ..bases import ModelMeta, TfBase
│ │ +from ..feature.multi_sparse import true_sparse_field_size
│ │  from ..tfops import (
│ │      dropout_config,
│ │      multi_sparse_combine_embedding,
│ │      reg_config,
│ │      tf,
│ │      tf_dense,
│ │  )
│ │ @@ -11,15 +12,14 @@
│ │  from ..utils.validate import (
│ │      check_dense_values,
│ │      check_multi_sparse,
│ │      check_sparse_indices,
│ │      dense_field_size,
│ │      sparse_feat_size,
│ │      sparse_field_size,
│ │ -    true_sparse_field_size,
│ │  )
│ │  
│ │  
│ │  class FM(TfBase, metaclass=ModelMeta):
│ │      """*Factorization Machines* algorithm.
│ │  
│ │      Note this implementation is actually a mixture of FM and NFM, since it uses one
│ │ @@ -47,14 +47,23 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ +    sampler : {'random', 'unconsumed', 'popular'}, default: 'random'
│ │ +        Negative sampling strategy.
│ │ +
│ │ +        - ``'random'`` means random sampling.
│ │ +        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ +        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ +
│ │ +        .. versionadded:: 1.1.0
│ │ +
│ │      num_neg : int, default: 1
│ │          Number of negative samples for each positive sample, only used in `ranking` task.
│ │      use_bn : bool, default: True
│ │          Whether to use batch normalization.
│ │      dropout_rate : float or None, default: None
│ │          Probability of an element to be zeroed. If it is None, dropout is not used.
│ │      multi_sparse_combiner : {'normal', 'mean', 'sum', 'sqrtn'}, default: 'sqrtn'
│ │ @@ -89,14 +98,15 @@
│ │          embed_size=16,
│ │          n_epochs=20,
│ │          lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ +        sampler="random",
│ │          num_neg=1,
│ │          use_bn=True,
│ │          dropout_rate=None,
│ │          multi_sparse_combiner="sqrtn",
│ │          seed=42,
│ │          lower_upper_bound=None,
│ │          tf_sess_config=None,
│ │ @@ -108,14 +118,15 @@
│ │          self.embed_size = embed_size
│ │          self.n_epochs = n_epochs
│ │          self.lr = lr
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │          self.reg = reg_config(reg)
│ │          self.batch_size = batch_size
│ │ +        self.sampler = sampler
│ │          self.num_neg = num_neg
│ │          self.use_bn = use_bn
│ │          self.dropout_rate = dropout_config(dropout_rate)
│ │          self.seed = seed
│ │          self.sparse = check_sparse_indices(data_info)
│ │          self.dense = check_dense_values(data_info)
│ │          if self.sparse:
│ │   --- LibRecommender-1.0.1/libreco/algorithms/graphsage.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/graphsage_dgl.py
│ │┄ Files 25% similar despite different names
│ │ @@ -1,31 +1,29 @@
│ │ -"""Implementation of GraphSage."""
│ │ -import numpy as np
│ │ -import torch
│ │ -from tqdm import tqdm
│ │ -
│ │ -from ..bases import EmbedBase, ModelMeta
│ │ -from ..sampling import bipartite_neighbors
│ │ -from ..torchops import (
│ │ -    device_config,
│ │ -    feat_to_tensor,
│ │ -    item_unique_to_tensor,
│ │ -    user_unique_to_tensor,
│ │ +"""Implementation of GraphSageDGL."""
│ │ +import importlib
│ │ +
│ │ +from .torch_modules import GraphSageDGLModel
│ │ +from ..bases import ModelMeta, SageBase
│ │ +from ..graph import (
│ │ +    NeighborWalkerDGL,
│ │ +    build_i2i_homo_graph,
│ │ +    build_u2i_hetero_graph,
│ │ +    check_dgl,
│ │  )
│ │ -from .torch_modules import GraphSageModel
│ │  
│ │  
│ │ -class GraphSage(EmbedBase, metaclass=ModelMeta, backend="torch"):
│ │ -    """*GraphSage* algorithm.
│ │ +@check_dgl
│ │ +class GraphSageDGL(SageBase, metaclass=ModelMeta, backend="torch"):
│ │ +    """*GraphSageDGL* algorithm.
│ │  
│ │      .. NOTE::
│ │ -        This algorithm is implemented in PyTorch.
│ │ +        This algorithm is implemented in `DGL <https://github.com/dmlc/dgl>`_.
│ │  
│ │      .. CAUTION::
│ │ -        GraphSage can only be used in ``ranking`` task.
│ │ +        GraphSageDGL can only be used in ``ranking`` task.
│ │  
│ │      .. versionadded:: 0.12.0
│ │  
│ │      Parameters
│ │      ----------
│ │      task : {'ranking'}
│ │          Recommendation task. See :ref:`Task`.
│ │ @@ -35,14 +33,17 @@
│ │          Loss for model training.
│ │      paradigm : {'u2i', 'i2i'}, default: 'i2i'
│ │          Choice for features in model.
│ │  
│ │          - ``'u2i'`` will combine user features and item features.
│ │          - ``'i2i'`` will only use item features, this is the setting in the original paper.
│ │  
│ │ +    aggregator_type : {'mean', 'gcn', 'pool', 'lstm'}, default: 'mean'
│ │ +        Aggregator type to use in GraphSage. Refer to `SAGEConv
│ │ +        <https://docs.dgl.ai/en/latest/generated/dgl.nn.pytorch.conv.SAGEConv.html>`_ in DGL.
│ │      embed_size: int, default: 16
│ │          Vector size of embeddings.
│ │      n_epochs: int, default: 10
│ │          Number of epochs for training.
│ │      lr : float, default 0.001
│ │          Learning rate for training.
│ │      lr_decay : bool, default: False
│ │ @@ -105,28 +106,35 @@
│ │             Accept str type ``'cpu'`` or ``'cuda'``, instead of ``torch.device(...)``.
│ │  
│ │      lower_upper_bound : tuple or None, default: None
│ │          Lower and upper score bound for `rating` task.
│ │  
│ │      See Also
│ │      --------
│ │ -    GraphSageDGL
│ │ +    GraphSage
│ │  
│ │      References
│ │      ----------
│ │      *William L. Hamilton et al.* `Inductive Representation Learning on Large Graphs
│ │      <https://arxiv.org/abs/1706.02216>`_.
│ │      """
│ │  
│ │ +    def __new__(cls, *args, **kwargs):
│ │ +        if cls.dgl_error is not None:
│ │ +            raise cls.dgl_error
│ │ +        cls._dgl = importlib.import_module("dgl")
│ │ +        return super().__new__(cls)
│ │ +
│ │      def __init__(
│ │          self,
│ │          task,
│ │          data_info,
│ │          loss_type="cross_entropy",
│ │          paradigm="i2i",
│ │ +        aggregator_type="mean",
│ │          embed_size=16,
│ │          n_epochs=20,
│ │          lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-8,
│ │          amsgrad=False,
│ │          reg=None,
│ │ @@ -142,142 +150,60 @@
│ │          sampler="random",
│ │          start_node="random",
│ │          focus_start=False,
│ │          seed=42,
│ │          device="cuda",
│ │          lower_upper_bound=None,
│ │      ):
│ │ -        super().__init__(task, data_info, embed_size, lower_upper_bound)
│ │ -
│ │ +        super().__init__(
│ │ +            task,
│ │ +            data_info,
│ │ +            loss_type,
│ │ +            paradigm,
│ │ +            embed_size,
│ │ +            n_epochs,
│ │ +            lr,
│ │ +            lr_decay,
│ │ +            epsilon,
│ │ +            amsgrad,
│ │ +            reg,
│ │ +            batch_size,
│ │ +            num_neg,
│ │ +            dropout_rate,
│ │ +            remove_edges,
│ │ +            num_layers,
│ │ +            num_neighbors,
│ │ +            num_walks,
│ │ +            sample_walk_len,
│ │ +            margin,
│ │ +            sampler,
│ │ +            start_node,
│ │ +            focus_start,
│ │ +            seed,
│ │ +            device,
│ │ +            lower_upper_bound,
│ │ +        )
│ │          self.all_args = locals()
│ │ -        self.loss_type = loss_type
│ │ -        self.paradigm = paradigm
│ │ -        self.n_epochs = n_epochs
│ │ -        self.lr = lr
│ │ -        self.lr_decay = lr_decay
│ │ -        self.epsilon = epsilon
│ │ -        self.amsgrad = amsgrad
│ │ -        self.reg = reg
│ │ -        self.batch_size = batch_size
│ │ -        self.num_neg = num_neg
│ │ -        self.dropout_rate = dropout_rate
│ │ -        self.remove_edges = remove_edges
│ │ -        self.num_layers = num_layers
│ │ -        self.num_neighbors = num_neighbors
│ │ -        self.num_walks = num_walks
│ │ -        self.sample_walk_len = sample_walk_len
│ │ -        self.margin = margin
│ │ -        self.sampler = sampler
│ │ -        self.start_node = start_node
│ │ -        self.focus_start = focus_start
│ │ -        self.seed = seed
│ │ -        self.device = device_config(device)
│ │ -        self._check_params()
│ │ -
│ │ -    def _check_params(self):
│ │ -        if self.task != "ranking":
│ │ -            raise ValueError(f"{self.model_name} is only suitable for ranking")
│ │ -        if self.paradigm not in ("u2i", "i2i"):
│ │ -            raise ValueError("paradigm must either be `u2i` or `i2i`")
│ │ -        if self.loss_type not in ("cross_entropy", "focal", "bpr", "max_margin"):
│ │ -            raise ValueError(f"unsupported `loss_type`: {self.loss_type}")
│ │ +        self.aggregator_type = aggregator_type
│ │ +        if aggregator_type not in ("mean", "gcn", "pool", "lstm"):
│ │ +            raise ValueError(
│ │ +                f"Unsupported `aggregator_type`: {aggregator_type} for GraphSageDGL"
│ │ +            )
│ │  
│ │      def build_model(self):
│ │ -        self.torch_model = GraphSageModel(
│ │ +        self._dgl.seed(self.seed)
│ │ +        self.homo_g = build_i2i_homo_graph(
│ │ +            self.n_items, self.user_consumed, self.data_info.item_consumed
│ │ +        )
│ │ +        self.hetero_g = build_u2i_hetero_graph(
│ │ +            self.n_users, self.n_items, self.user_consumed
│ │ +        )
│ │ +        self.neighbor_walker = NeighborWalkerDGL(self, self.data_info)
│ │ +        self.torch_model = GraphSageDGLModel(
│ │              self.paradigm,
│ │              self.data_info,
│ │              self.embed_size,
│ │              self.batch_size,
│ │              self.num_layers,
│ │              self.dropout_rate,
│ │ +            self.aggregator_type,
│ │          ).to(self.device)
│ │ -
│ │ -    def get_user_repr(self, users, sparse_indices, dense_values):
│ │ -        user_feats = feat_to_tensor(users, sparse_indices, dense_values, self.device)
│ │ -        return self.torch_model.user_repr(*user_feats)
│ │ -
│ │ -    def sample_neighbors(self, items):
│ │ -        nodes = items
│ │ -        tensor_neighbors, tensor_offsets = [], []
│ │ -        tensor_neighbor_sparse_indices, tensor_neighbor_dense_values = [], []
│ │ -        for _ in range(self.num_layers):
│ │ -            neighbors, offsets = bipartite_neighbors(
│ │ -                nodes,
│ │ -                self.data_info.user_consumed,
│ │ -                self.data_info.item_consumed,
│ │ -                self.num_neighbors,
│ │ -            )
│ │ -
│ │ -            (
│ │ -                neighbor_tensor,
│ │ -                neighbor_sparse_indices,
│ │ -                neighbor_dense_values,
│ │ -            ) = item_unique_to_tensor(neighbors, self.data_info, self.device)
│ │ -            tensor_neighbors.append(neighbor_tensor)
│ │ -            tensor_neighbor_sparse_indices.append(neighbor_sparse_indices)
│ │ -            tensor_neighbor_dense_values.append(neighbor_dense_values)
│ │ -            tensor_offsets.append(
│ │ -                torch.tensor(offsets, dtype=torch.long, device=self.device)
│ │ -            )
│ │ -            nodes = neighbors
│ │ -        return (
│ │ -            tensor_neighbors,
│ │ -            tensor_neighbor_sparse_indices,
│ │ -            tensor_neighbor_dense_values,
│ │ -            tensor_offsets,
│ │ -        )
│ │ -
│ │ -    def get_item_repr(self, items, sparse_indices=None, dense_values=None, **_):
│ │ -        (
│ │ -            tensor_neighbors,
│ │ -            tensor_neighbor_sparse_indices,
│ │ -            tensor_neighbor_dense_values,
│ │ -            tensor_offsets,
│ │ -        ) = self.sample_neighbors(items)
│ │ -
│ │ -        if sparse_indices is not None or dense_values is not None:
│ │ -            item_tensor, item_sparse_indices, item_dense_values = feat_to_tensor(
│ │ -                items, sparse_indices, dense_values, self.device
│ │ -            )
│ │ -        else:
│ │ -            item_tensor, item_sparse_indices, item_dense_values = item_unique_to_tensor(
│ │ -                items, self.data_info, self.device
│ │ -            )
│ │ -        return self.torch_model(
│ │ -            item_tensor,
│ │ -            item_sparse_indices,
│ │ -            item_dense_values,
│ │ -            tensor_neighbors,
│ │ -            tensor_neighbor_sparse_indices,
│ │ -            tensor_neighbor_dense_values,
│ │ -            tensor_offsets,
│ │ -        )
│ │ -
│ │ -    @torch.no_grad()
│ │ -    def set_embeddings(self):
│ │ -        self.torch_model.eval()
│ │ -        all_items = list(range(self.n_items))
│ │ -        item_embed = []
│ │ -        for i in tqdm(range(0, self.n_items, self.batch_size), desc="item embedding"):
│ │ -            batch_items = all_items[i : i + self.batch_size]
│ │ -            item_reprs = self.get_item_repr(batch_items)
│ │ -            item_embed.append(item_reprs.detach().cpu().numpy())
│ │ -        self.item_embed = np.concatenate(item_embed, axis=0)
│ │ -        self.user_embed = self.get_user_embeddings()
│ │ -
│ │ -    @torch.no_grad()
│ │ -    def get_user_embeddings(self):
│ │ -        self.torch_model.eval()
│ │ -        user_embed = []
│ │ -        if self.paradigm == "u2i":
│ │ -            for i in range(0, self.n_users, self.batch_size):
│ │ -                users = np.arange(i, min(i + self.batch_size, self.n_users))
│ │ -                user_tensors = user_unique_to_tensor(users, self.data_info, self.device)
│ │ -                user_reprs = self.torch_model.user_repr(*user_tensors)
│ │ -                user_embed.append(user_reprs.detach().cpu().numpy())
│ │ -            return np.concatenate(user_embed, axis=0)
│ │ -        else:
│ │ -            for u in range(self.n_users):
│ │ -                items = self.user_consumed[u]
│ │ -                user_embed.append(np.mean(self.item_embed[items], axis=0))
│ │ -                # user_embed.append(self.item_embed[items[-1]])
│ │ -            return np.array(user_embed)
│ │   --- LibRecommender-1.0.1/libreco/algorithms/graphsage_dgl.py
│ ├── +++ LibRecommender-1.1.0/libreco/bases/tf_base.py
│ │┄ Files 24% similar despite different names
│ │ @@ -1,324 +1,370 @@
│ │ -"""Implementation of GraphSageDGL."""
│ │ -import importlib
│ │ -import itertools
│ │ +"""TF model base class."""
│ │ +import abc
│ │ +import os
│ │  
│ │  import numpy as np
│ │ -import torch
│ │ -from tqdm import tqdm
│ │  
│ │ -from ..bases import EmbedBase, ModelMeta
│ │ -from ..graph import check_dgl
│ │ -from ..torchops import device_config, item_unique_to_tensor, user_unique_to_tensor
│ │ -from .torch_modules import GraphSageDGLModel
│ │ -
│ │ -
│ │ -@check_dgl
│ │ -class GraphSageDGL(EmbedBase, metaclass=ModelMeta, backend="torch"):
│ │ -    """*GraphSageDGL* algorithm.
│ │ -
│ │ -    .. NOTE::
│ │ -        This algorithm is implemented in `DGL <https://github.com/dmlc/dgl>`_.
│ │ -
│ │ -    .. CAUTION::
│ │ -        GraphSageDGL can only be used in ``ranking`` task.
│ │ -
│ │ -    .. versionadded:: 0.12.0
│ │ +from .base import Base
│ │ +from ..prediction import predict_tf_feat
│ │ +from ..recommendation import cold_start_rec, construct_rec, recommend_tf_feat
│ │ +from ..tfops import modify_variable_names, sess_config, tf
│ │ +from ..training.dispatch import get_trainer
│ │ +from ..utils.constants import SEQUENCE_RECOMMEND_MODELS
│ │ +from ..utils.save_load import (
│ │ +    load_tf_model,
│ │ +    load_tf_variables,
│ │ +    save_default_recs,
│ │ +    save_params,
│ │ +    save_tf_model,
│ │ +    save_tf_variables,
│ │ +)
│ │ +from ..utils.validate import check_fitting, check_unknown_user
│ │ +
│ │ +
│ │ +class TfBase(Base):
│ │ +    """Base class for TF models.
│ │ +
│ │ +    Models that relies on TensorFlow graph for inference. Although some models such as
│ │ +    `RNN4Rec`, `SVD` etc., are trained using TensorFlow, they don't inherit from this
│ │ +    base class since their inference only uses embeddings.
│ │  
│ │      Parameters
│ │      ----------
│ │ -    task : {'ranking'}
│ │ +    task : {'rating', 'ranking'}
│ │          Recommendation task. See :ref:`Task`.
│ │      data_info : :class:`~libreco.data.DataInfo` object
│ │          Object that contains useful information for training and inference.
│ │ -    loss_type : {'cross_entropy', 'focal', 'bpr', 'max_margin'}, default: 'cross_entropy'
│ │ -        Loss for model training.
│ │ -    paradigm : {'u2i', 'i2i'}, default: 'i2i'
│ │ -        Choice for features in model.
│ │ -
│ │ -        - ``'u2i'`` will combine user features and item features.
│ │ -        - ``'i2i'`` will only use item features, this is the setting in the original paper.
│ │ -
│ │ -    aggregator_type : {'mean', 'gcn', 'pool', 'lstm'}, default: 'mean'
│ │ -        Aggregator type to use in GraphSage. Refer to `SAGEConv
│ │ -        <https://docs.dgl.ai/en/latest/generated/dgl.nn.pytorch.conv.SAGEConv.html>`_ in DGL.
│ │ -    embed_size: int, default: 16
│ │ -        Vector size of embeddings.
│ │ -    n_epochs: int, default: 10
│ │ -        Number of epochs for training.
│ │ -    lr : float, default 0.001
│ │ -        Learning rate for training.
│ │ -    lr_decay : bool, default: False
│ │ -        Whether to use learning rate decay.
│ │ -    epsilon : float, default: 1e-8
│ │ -        A small constant added to the denominator to improve numerical stability in
│ │ -        Adam optimizer.
│ │ -    amsgrad : bool, default: False
│ │ -        Whether to use the AMSGrad variant from the paper
│ │ -        `On the Convergence of Adam and Beyond <https://openreview.net/forum?id=ryQu7f-RZ>`_.
│ │ -    reg : float or None, default: None
│ │ -        Regularization parameter, must be non-negative or None.
│ │ -    batch_size : int, default: 256
│ │ -        Batch size for training.
│ │ -    num_neg : int, default: 1
│ │ -        Number of negative samples for each positive sample.
│ │ -    dropout_rate : float, default: 0.0
│ │ -        Probability of a node being dropped. 0.0 means dropout is not used.
│ │ -    remove_edges : bool, default: False
│ │ -        Whether to remove edges between target node and its positive pair nodes
│ │ -        when target node's sampled neighbor nodes contain positive pair nodes.
│ │ -        This only applies in 'i2i' paradigm.
│ │ -    num_layers : int, default: 2
│ │ -        Number of GCN layers.
│ │ -    num_neighbors : int, default: 3
│ │ -        Number of sampled neighbors in each layer
│ │ -    num_walks : int, default: 10
│ │ -        Number of random walks to sample positive item pairs. This only applies in
│ │ -        'i2i' paradigm.
│ │ -    sample_walk_len : int, default: 5
│ │ -        Length of each random walk to sample positive item pairs.
│ │ -    margin : float, default: 1.0
│ │ -        Margin used in `max_margin` loss.
│ │ -    sampler : {'random', 'unconsumed', 'popular', 'out-batch'}, default: 'random'
│ │ -        Negative sampling strategy. The ``'u2i'`` paradigm can use ``'random'``, ``'unconsumed'``,
│ │ -        ``'popular'``, and the ``'i2i'`` paradigm can use ``'random'``, ``'out-batch'``, ``'popular'``.
│ │ -
│ │ -        - ``'random'`` means random sampling.
│ │ -        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ -          This can't be used in ``'i2i'`` since it has no users.
│ │ -        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ -        - ``'out-batch'`` samples items that didn't appear in the batch.
│ │ -          This can only be used in ``'i2i'`` paradigm.
│ │ -
│ │ -    start_node : {'random', 'unpopular'}, default: 'random'
│ │ -        Strategy for choosing start nodes in random walks. ``'unpopular'`` will place a higher
│ │ -        probability on unpopular items, which may increase diversity but hurt metrics.
│ │ -        This only applies in ``'i2i'`` paradigm.
│ │ -    focus_start : bool, default: False
│ │ -        Whether to keep the start nodes in random walk sampling. The purpose of the
│ │ -        parameter ``start_node`` and ``focus_start`` is oversampling unpopular items.
│ │ -        If you set ``start_node='popular'`` and ``focus_start=True``, unpopular items will
│ │ -        be kept in positive samples, which may increase diversity.
│ │ -    seed : int, default: 42
│ │ -        Random seed.
│ │ -    device : {'cpu', 'cuda'}, default: 'cuda'
│ │ -        Refer to `torch.device <https://pytorch.org/docs/stable/tensor_attributes.html#torch.device>`_.
│ │ -
│ │ -        .. versionchanged:: 1.0.0
│ │ -           Accept str type ``'cpu'`` or ``'cuda'``, instead of ``torch.device(...)``.
│ │ -
│ │ -    lower_upper_bound : tuple or None, default: None
│ │ +    lower_upper_bound : tuple or None
│ │          Lower and upper score bound for `rating` task.
│ │ +    tf_sess_config : dict or None
│ │ +        Optional TensorFlow session config, see `ConfigProto options
│ │ +        <https://github.com/tensorflow/tensorflow/blob/v2.10.0/tensorflow/core/protobuf/config.proto#L431>`_.
│ │ +    """
│ │  
│ │ -    See Also
│ │ -    --------
│ │ -    GraphSage
│ │ +    def __init__(self, task, data_info, lower_upper_bound=None, tf_sess_config=None):
│ │ +        super().__init__(task, data_info, lower_upper_bound)
│ │ +        self.sess = sess_config(tf_sess_config)
│ │ +        self.model_built = False
│ │ +        self.trainer = None
│ │ +        self.loaded = False
│ │  
│ │ -    References
│ │ -    ----------
│ │ -    *William L. Hamilton et al.* `Inductive Representation Learning on Large Graphs
│ │ -    <https://arxiv.org/abs/1706.02216>`_.
│ │ -    """
│ │ +    @abc.abstractmethod
│ │ +    def build_model(self):
│ │ +        raise NotImplementedError
│ │ +
│ │ +    def fit(
│ │ +        self,
│ │ +        train_data,
│ │ +        neg_sampling,
│ │ +        verbose=1,
│ │ +        shuffle=True,
│ │ +        eval_data=None,
│ │ +        metrics=None,
│ │ +        k=10,
│ │ +        eval_batch_size=8192,
│ │ +        eval_user_num=None,
│ │ +        num_workers=0,
│ │ +    ):
│ │ +        """Fit TF model on the training data.
│ │  
│ │ -    def __new__(cls, *args, **kwargs):
│ │ -        if cls.dgl_error is not None:
│ │ -            raise cls.dgl_error
│ │ -        cls._dgl = importlib.import_module("dgl")
│ │ -        return super().__new__(cls)
│ │ +        Parameters
│ │ +        ----------
│ │ +        train_data : :class:`~libreco.data.TransformedSet` object
│ │ +            Data object used for training.
│ │ +        neg_sampling : bool
│ │ +            Whether to perform negative sampling for training or evaluating data.
│ │ +
│ │ +            .. versionadded:: 1.1.0
│ │ +
│ │ +            .. NOTE::
│ │ +               Negative sampling is needed if your data is implicit(i.e., `task` is ranking)
│ │ +               and ONLY contains positive labels. Otherwise, it should be False.
│ │ +
│ │ +        verbose : int, default: 1
│ │ +            Print verbosity. If `eval_data` is provided, setting it to higher than 1
│ │ +            will print evaluation metrics during training.
│ │ +        shuffle : bool, default: True
│ │ +            Whether to shuffle the training data.
│ │ +        eval_data : :class:`~libreco.data.TransformedSet` object, default: None
│ │ +            Data object used for evaluating.
│ │ +        metrics : list or None, default: None
│ │ +            List of metrics for evaluating.
│ │ +        k : int, default: 10
│ │ +            Parameter of metrics, e.g. recall at k, ndcg at k
│ │ +        eval_batch_size : int, default: 8192
│ │ +            Batch size for evaluating.
│ │ +        eval_user_num : int or None, default: None
│ │ +            Number of users for evaluating. Setting it to a positive number will sample
│ │ +            users randomly from eval data.
│ │ +        num_workers : int, default: 0
│ │ +            How many subprocesses to use for training data loading.
│ │ +            0 means that the data will be loaded in the main process,
│ │ +            which is slower than multiprocessing.
│ │ +
│ │ +            .. versionadded:: 1.1.0
│ │ +
│ │ +            .. CAUTION::
│ │ +               Using multiprocessing(``num_workers`` > 0) may consume more memory than
│ │ +               single processing. See `Multi-process data loading <https://pytorch.org/docs/stable/data.html#multi-process-data-loading>`_.
│ │ +
│ │ +        Raises
│ │ +        ------
│ │ +        RuntimeError
│ │ +            If :py:func:`fit` is called from a loaded model(:py:func:`load`).
│ │ +        AssertionError
│ │ +            If ``neg_sampling`` parameter is not bool type.
│ │ +        """
│ │ +        check_fitting(self, train_data, eval_data, neg_sampling, k)
│ │ +        self.show_start_time()
│ │ +        if not self.model_built:
│ │ +            self.build_model()
│ │ +            self.model_built = True
│ │ +        if self.trainer is None:
│ │ +            self.trainer = get_trainer(self)
│ │ +        self.trainer.run(
│ │ +            train_data,
│ │ +            neg_sampling,
│ │ +            verbose,
│ │ +            shuffle,
│ │ +            eval_data,
│ │ +            metrics,
│ │ +            k,
│ │ +            eval_batch_size,
│ │ +            eval_user_num,
│ │ +            num_workers,
│ │ +        )
│ │ +        self.assign_tf_variables_oov()
│ │ +        self.default_recs = recommend_tf_feat(
│ │ +            model=self,
│ │ +            user_ids=[self.n_users],
│ │ +            n_rec=min(2000, self.n_items),
│ │ +            user_feats=None,
│ │ +            seq=None,
│ │ +            filter_consumed=False,
│ │ +            random_rec=False,
│ │ +        ).flatten()
│ │ +
│ │ +    def predict(self, user, item, feats=None, cold_start="average", inner_id=False):
│ │ +        """Make prediction(s) on given user(s) and item(s).
│ │ +
│ │ +        Parameters
│ │ +        ----------
│ │ +        user : int or str or array_like
│ │ +            User id or batch of user ids.
│ │ +        item : int or str or array_like
│ │ +            Item id or batch of item ids.
│ │ +        feats : dict or pandas.Series or None, default: None
│ │ +            Extra features used in prediction.
│ │ +        cold_start : {'popular', 'average'}, default: 'average'
│ │ +            Cold start strategy.
│ │ +
│ │ +            - 'popular' will sample from popular items.
│ │ +            - 'average' will use the average of all the user/item embeddings as the
│ │ +              representation of the cold-start user/item.
│ │ +
│ │ +        inner_id : bool, default: False
│ │ +            Whether to use inner_id defined in `libreco`. For library users inner_id
│ │ +            may never be used.
│ │ +
│ │ +        Returns
│ │ +        -------
│ │ +        prediction : float or numpy.ndarray
│ │ +            Predicted scores for each user-item pair.
│ │ +        """
│ │ +        if self.model_name == "NCF" and feats is not None:
│ │ +            raise ValueError("NCF can't use features.")
│ │ +        return predict_tf_feat(self, user, item, feats, cold_start, inner_id)
│ │  
│ │ -    def __init__(
│ │ +    def recommend_user(
│ │          self,
│ │ -        task,
│ │ -        data_info,
│ │ -        loss_type="cross_entropy",
│ │ -        paradigm="i2i",
│ │ -        aggregator_type="mean",
│ │ -        embed_size=16,
│ │ -        n_epochs=20,
│ │ -        lr=0.001,
│ │ -        lr_decay=False,
│ │ -        epsilon=1e-8,
│ │ -        amsgrad=False,
│ │ -        reg=None,
│ │ -        batch_size=256,
│ │ -        num_neg=1,
│ │ -        dropout_rate=0.0,
│ │ -        remove_edges=False,
│ │ -        num_layers=2,
│ │ -        num_neighbors=3,
│ │ -        num_walks=10,
│ │ -        sample_walk_len=5,
│ │ -        margin=1.0,
│ │ -        sampler="random",
│ │ -        start_node="random",
│ │ -        focus_start=False,
│ │ -        seed=42,
│ │ -        device="cuda",
│ │ -        lower_upper_bound=None,
│ │ +        user,
│ │ +        n_rec,
│ │ +        user_feats=None,
│ │ +        seq=None,
│ │ +        cold_start="average",
│ │ +        inner_id=False,
│ │ +        filter_consumed=True,
│ │ +        random_rec=False,
│ │      ):
│ │ -        super().__init__(task, data_info, embed_size, lower_upper_bound)
│ │ +        """Recommend a list of items for given user(s).
│ │  
│ │ -        self.all_args = locals()
│ │ -        self.loss_type = loss_type
│ │ -        self.paradigm = paradigm
│ │ -        self.aggregator_type = aggregator_type
│ │ -        self.n_epochs = n_epochs
│ │ -        self.lr = lr
│ │ -        self.lr_decay = lr_decay
│ │ -        self.epsilon = epsilon
│ │ -        self.amsgrad = amsgrad
│ │ -        self.reg = reg
│ │ -        self.batch_size = batch_size
│ │ -        self.num_neg = num_neg
│ │ -        self.dropout_rate = dropout_rate
│ │ -        self.remove_edges = remove_edges
│ │ -        self.num_layers = num_layers
│ │ -        self.num_neighbors = num_neighbors
│ │ -        self.num_walks = num_walks
│ │ -        self.sample_walk_len = sample_walk_len
│ │ -        self.margin = margin
│ │ -        self.sampler = sampler
│ │ -        self.start_node = start_node
│ │ -        self.focus_start = focus_start
│ │ -        self.seed = seed
│ │ -        self.device = device_config(device)
│ │ -        self._check_params()
│ │ -
│ │ -    def _check_params(self):
│ │ -        if self.task != "ranking":
│ │ -            raise ValueError(f"{self.model_name} is only suitable for ranking")
│ │ -        if self.paradigm not in ("u2i", "i2i"):
│ │ -            raise ValueError("paradigm must either be `u2i` or `i2i`")
│ │ -        if self.loss_type not in ("cross_entropy", "focal", "bpr", "max_margin"):
│ │ -            raise ValueError(f"unsupported `loss_type`: {self.loss_type}")
│ │ -        if self.model_name == "GraphSageDGL" and self.aggregator_type not in (
│ │ -            "mean",
│ │ -            "gcn",
│ │ -            "pool",
│ │ -            "lstm",
│ │ -        ):
│ │ +        Parameters
│ │ +        ----------
│ │ +        user : int or str or array_like
│ │ +            User id or batch of user ids to recommend.
│ │ +        n_rec : int
│ │ +            Number of recommendations to return.
│ │ +        user_feats : dict or None, default: None
│ │ +            Extra user features for recommendation.
│ │ +        seq : list or numpy.ndarray
│ │ +            Extra item sequence for recommendation. If the sequence length is larger than
│ │ +            `recent_num` hyperparameter specified in the model, it will be truncated.
│ │ +            If it is smaller, it will be padded.
│ │ +
│ │ +            .. versionadded:: 1.1.0
│ │ +
│ │ +        cold_start : {'popular', 'average'}, default: 'average'
│ │ +            Cold start strategy.
│ │ +
│ │ +            - 'popular' will sample from popular items.
│ │ +            - 'average' will use the average of all the user/item embeddings as the
│ │ +              representation of the cold-start user/item.
│ │ +
│ │ +        inner_id : bool, default: False
│ │ +            Whether to use inner_id defined in `libreco`. For library users inner_id
│ │ +            may never be used.
│ │ +        filter_consumed : bool, default: True
│ │ +            Whether to filter out items that a user has previously consumed.
│ │ +        random_rec : bool, default: False
│ │ +            Whether to choose items for recommendation based on their prediction scores.
│ │ +
│ │ +        Returns
│ │ +        -------
│ │ +        recommendation : dict of {Union[int, str, array_like] : numpy.ndarray}
│ │ +            Recommendation result with user ids as keys and array_like recommended items as values.
│ │ +        """
│ │ +        if seq is not None and self.model_name not in SEQUENCE_RECOMMEND_MODELS:
│ │              raise ValueError(
│ │ -                f"unsupported `aggregator_type`: {self.aggregator_type} for GraphSageDGL"
│ │ +                f"`{self.model_name}` doesn't support arbitrary seq recommendation."
│ │              )
│ │ +        if not np.isscalar(user) and len(user) > 1:
│ │ +            if user_feats is not None:
│ │ +                raise ValueError(
│ │ +                    f"Batch recommend doesn't support assigning arbitrary features: {user}"
│ │ +                )
│ │ +            if seq is not None:
│ │ +                raise ValueError(
│ │ +                    f"Batch recommend doesn't support arbitrary item sequence: {user}"
│ │ +                )
│ │ +        if self.model_name == "NCF" and user_feats is not None:
│ │ +            raise ValueError("`NCF` can't use features.")
│ │ +
│ │ +        result_recs = dict()
│ │ +        user_ids, unknown_users = check_unknown_user(self.data_info, user, inner_id)
│ │ +        if unknown_users:
│ │ +            cold_recs = cold_start_rec(
│ │ +                self.data_info,
│ │ +                self.default_recs,
│ │ +                cold_start,
│ │ +                unknown_users,
│ │ +                n_rec,
│ │ +                inner_id,
│ │ +            )
│ │ +            result_recs.update(cold_recs)
│ │ +        if user_ids:
│ │ +            computed_recs = recommend_tf_feat(
│ │ +                self,
│ │ +                user_ids,
│ │ +                n_rec,
│ │ +                user_feats,
│ │ +                seq,
│ │ +                filter_consumed,
│ │ +                random_rec,
│ │ +                inner_id,
│ │ +            )
│ │ +            user_recs = construct_rec(self.data_info, user_ids, computed_recs, inner_id)
│ │ +            result_recs.update(user_recs)
│ │ +        return result_recs
│ │ +
│ │ +    def assign_tf_variables_oov(self):
│ │ +        (
│ │ +            user_variables,
│ │ +            item_variables,
│ │ +            sparse_variables,
│ │ +            dense_variables,
│ │ +            _,
│ │ +        ) = modify_variable_names(self, trainable=True)
│ │ +
│ │ +        update_ops = []
│ │ +        for v in tf.trainable_variables():
│ │ +            if user_variables is not None and v.name in user_variables:
│ │ +                # size = v.get_shape().as_list()[1]
│ │ +                mean_op = tf.IndexedSlices(
│ │ +                    tf.reduce_mean(
│ │ +                        tf.gather(v, tf.range(self.n_users)), axis=0, keepdims=True
│ │ +                    ),
│ │ +                    [self.n_users],
│ │ +                )
│ │ +                update_ops.append(v.scatter_update(mean_op))
│ │ +
│ │ +            if item_variables is not None and v.name in item_variables:
│ │ +                mean_op = tf.IndexedSlices(
│ │ +                    tf.reduce_mean(
│ │ +                        tf.gather(v, tf.range(self.n_items)), axis=0, keepdims=True
│ │ +                    ),
│ │ +                    [self.n_items],
│ │ +                )
│ │ +                update_ops.append(v.scatter_update(mean_op))
│ │ +
│ │ +            if sparse_variables is not None and v.name in sparse_variables:
│ │ +                sparse_oovs = self.data_info.sparse_oov
│ │ +                start = 0
│ │ +                for oov in sparse_oovs:
│ │ +                    # multi_sparse case
│ │ +                    if start >= oov:
│ │ +                        continue
│ │ +                    mean_tensor = tf.reduce_mean(
│ │ +                        tf.gather(v, tf.range(start, oov)), axis=0, keepdims=True
│ │ +                    )
│ │ +                    update_ops.append(v.scatter_nd_update([[oov]], mean_tensor))
│ │ +                    start = oov + 1
│ │ +
│ │ +        self.sess.run(update_ops)
│ │ +
│ │ +    def save(self, path, model_name, manual=True, inference_only=False):
│ │ +        """Save TF model for inference or retraining.
│ │ +
│ │ +        Parameters
│ │ +        ----------
│ │ +        path : str
│ │ +            File folder path to save model.
│ │ +        model_name : str
│ │ +            Name of the saved model file.
│ │ +        manual : bool, default: True
│ │ +            Whether to save model variables using numpy.
│ │ +        inference_only : bool, default: False
│ │ +            Whether to save model variables only for inference.
│ │ +
│ │ +        See Also
│ │ +        --------
│ │ +        load
│ │ +        """
│ │ +        if not os.path.isdir(path):
│ │ +            print(f"file folder {path} doesn't exists, creating a new one...")
│ │ +            os.makedirs(path)
│ │ +        save_params(self, path, model_name)
│ │ +        save_default_recs(self, path, model_name)
│ │ +        if manual:
│ │ +            save_tf_variables(self.sess, path, model_name, inference_only)
│ │ +        else:
│ │ +            save_tf_model(self.sess, path, model_name)
│ │  
│ │ -    def build_homo_graph(self):
│ │ -        src_items, dst_items = [], []
│ │ -        for i in range(self.n_items):
│ │ -            neighbors = set()
│ │ -            for u in self.data_info.item_consumed[i]:
│ │ -                neighbors.update(self.user_consumed[u])
│ │ -            src_items.extend(neighbors)
│ │ -            dst_items.extend([i] * len(neighbors))
│ │ -        src = torch.tensor(src_items, dtype=torch.long)
│ │ -        dst = torch.tensor(dst_items, dtype=torch.long)
│ │ -        g = self._dgl.graph((src, dst), num_nodes=self.n_items)
│ │ -        return g
│ │ -
│ │ -    def build_hetero_graph(self):
│ │ -        items = [list(self.user_consumed[u]) for u in range(self.n_users)]
│ │ -        counts = [len(i) for i in items]
│ │ -        users = torch.arange(self.n_users).repeat_interleave(torch.tensor(counts))
│ │ -        items = list(itertools.chain.from_iterable(items))
│ │ -        items = torch.tensor(items, dtype=torch.long)
│ │ -        graph_data = {
│ │ -            ("user", "consumed", "item"): (users, items),
│ │ -            ("item", "consumed-by", "user"): (items, users),
│ │ -        }
│ │ -        num_nodes = {"user": self.n_users, "item": self.n_items}
│ │ -        return self._dgl.heterograph(graph_data, num_nodes)
│ │ -
│ │ -    def build_model(self):
│ │ -        self.homo_g = self.build_homo_graph()
│ │ -        self.hetero_g = self.build_hetero_graph()
│ │ -        self.torch_model = GraphSageDGLModel(
│ │ -            self.paradigm,
│ │ -            self.data_info,
│ │ -            self.embed_size,
│ │ -            self.batch_size,
│ │ -            self.num_layers,
│ │ -            self.dropout_rate,
│ │ -            self.aggregator_type,
│ │ -        ).to(self.device)
│ │ -
│ │ -    def sample_frontier(self, nodes):
│ │ -        return self._dgl.sampling.sample_neighbors(
│ │ -            g=self.homo_g,
│ │ -            nodes=nodes,
│ │ -            fanout=self.num_neighbors,
│ │ -            edge_dir="in",
│ │ -        )
│ │ -
│ │ -    def transform_blocks(self, nodes, target_nodes=None):  # noqa: D400, D415
│ │ -        r"""Bipartite graph block: items(nodes) -> sampled neighbor nodes
│ │ -
│ │ -        -------------
│ │ -        |     / ... |
│ │ -        |    /  src |
│ │ -        |dst -  src |
│ │ -        |    \  src |
│ │ -        |     \ ... |
│ │ -        -------------
│ │ +    @classmethod
│ │ +    def load(cls, path, model_name, data_info, manual=True):
│ │ +        """Load saved TF model for inference.
│ │ +
│ │ +        Parameters
│ │ +        ----------
│ │ +        path : str
│ │ +            File folder path to save model.
│ │ +        model_name : str
│ │ +            Name of the saved model file.
│ │ +        data_info : :class:`~libreco.data.DataInfo` object
│ │ +            Object that contains some useful information.
│ │ +        manual : bool, default: True
│ │ +            Whether to load model variables using numpy. If you save the model using
│ │ +            `manual`, you should also load the mode using `manual`.
│ │ +
│ │ +        Returns
│ │ +        -------
│ │ +        model : type(cls)
│ │ +            Loaded TF model.
│ │ +
│ │ +        See Also
│ │ +        --------
│ │ +        save
│ │          """
│ │ -        dgl = self._dgl
│ │ -        blocks = []
│ │ -        for _ in range(self.num_layers):
│ │ -            frontier = self.sample_frontier(nodes)
│ │ -            if (
│ │ -                self.paradigm == "i2i"
│ │ -                and self.remove_edges
│ │ -                and target_nodes is not None
│ │ -            ):
│ │ -                heads_pos, heads_neg, tails_pos, tails_neg = target_nodes
│ │ -                eids = frontier.edge_ids(
│ │ -                    torch.cat([heads_pos, heads_neg]),
│ │ -                    torch.cat([tails_pos, tails_neg]),
│ │ -                    return_uv=True,
│ │ -                )[2]
│ │ -                if len(eids) > 0:
│ │ -                    frontier = dgl.remove_edges(frontier, eids)
│ │ -            block = dgl.to_block(frontier, dst_nodes=nodes)
│ │ -            nodes = block.srcdata[dgl.NID]
│ │ -            blocks.append(block)
│ │ -        blocks.reverse()
│ │ -        return blocks
│ │ -
│ │ -    def get_user_repr(self, users):
│ │ -        user_feat_tensors = user_unique_to_tensor(users, self.data_info, self.device)
│ │ -        return self.torch_model.user_repr(*user_feat_tensors)
│ │ -
│ │ -    def get_item_repr(self, nodes, target_nodes=None):
│ │ -        blocks = self.transform_blocks(nodes, target_nodes)
│ │ -        start_neighbor_nodes = blocks[0].srcdata[self._dgl.NID]
│ │ -        start_nodes, sparse_indices, dense_values = item_unique_to_tensor(
│ │ -            start_neighbor_nodes, self.data_info, self.device
│ │ -        )
│ │ -        for i in range(len(blocks)):
│ │ -            blocks[i] = blocks[i].to(self.device)
│ │ -        return self.torch_model(blocks, start_nodes, sparse_indices, dense_values)
│ │ -
│ │ -    @torch.no_grad()
│ │ -    def set_embeddings(self):
│ │ -        self.torch_model.eval()
│ │ -        all_items = list(range(self.n_items))
│ │ -        item_embed = []
│ │ -        for i in tqdm(range(0, self.n_items, self.batch_size), desc="item embedding"):
│ │ -            items = torch.tensor(all_items[i : i + self.batch_size], dtype=torch.long)
│ │ -            item_reprs = self.get_item_repr(items)
│ │ -            item_embed.append(item_reprs.detach().cpu().numpy())
│ │ -        self.item_embed = np.concatenate(item_embed, axis=0)
│ │ -        self.user_embed = self.get_user_embeddings()
│ │ -
│ │ -    @torch.no_grad()
│ │ -    def get_user_embeddings(self):
│ │ -        self.torch_model.eval()
│ │ -        user_embed = []
│ │ -        if self.paradigm == "u2i":
│ │ -            for i in range(0, self.n_users, self.batch_size):
│ │ -                users = np.arange(i, min(i + self.batch_size, self.n_users))
│ │ -                user_reprs = self.get_user_repr(users).detach().cpu().numpy()
│ │ -                user_embed.append(user_reprs)
│ │ -            return np.concatenate(user_embed, axis=0)
│ │ +        if manual:
│ │ +            return load_tf_variables(cls, path, model_name, data_info)
│ │          else:
│ │ -            for u in range(self.n_users):
│ │ -                items = self.user_consumed[u]
│ │ -                user_embed.append(np.mean(self.item_embed[items], axis=0))
│ │ -            return np.array(user_embed)
│ │ +            return load_tf_model(cls, path, model_name, data_info)
│ │   --- LibRecommender-1.0.1/libreco/algorithms/item2vec.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/item2vec.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/algorithms/item_cf.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/item_cf.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/algorithms/lightgcn.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/lightgcn.py
│ │┄ Ordering differences only
│ │┄ Files 1% similar despite different names
│ │ @@ -1,13 +1,13 @@
│ │  """Implementation of LightGCN."""
│ │  import torch
│ │  
│ │ +from .torch_modules import LightGCNModel
│ │  from ..bases import EmbedBase, ModelMeta
│ │  from ..torchops import device_config
│ │ -from .torch_modules import LightGCNModel
│ │  
│ │  
│ │  class LightGCN(EmbedBase, metaclass=ModelMeta, backend="torch"):
│ │      """*LightGCN* algorithm.
│ │  
│ │      .. CAUTION::
│ │          LightGCN can only be used in ``ranking`` task.
│ │   --- LibRecommender-1.0.1/libreco/algorithms/ncf.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/rnn4rec.py
│ │┄ Files 20% similar despite different names
│ │ @@ -1,26 +1,30 @@
│ │ -"""Implementation of NCF."""
│ │ -from ..bases import ModelMeta, TfBase
│ │ -from ..prediction import normalize_prediction
│ │ -from ..tfops import dense_nn, dropout_config, reg_config, tf, tf_dense
│ │ +"""Implementation of RNN4Rec model."""
│ │ +from ..bases import ModelMeta, SeqEmbedBase
│ │ +from ..tfops import dropout_config, reg_config, tf, tf_dense, tf_rnn
│ │  from ..torchops import hidden_units_config
│ │ -from ..utils.validate import check_unknown, convert_id
│ │ +from ..utils.misc import count_params
│ │  
│ │  
│ │ -class NCF(TfBase, metaclass=ModelMeta):
│ │ -    """*Neural Collaborative Filtering* algorithm.
│ │ +class RNN4Rec(SeqEmbedBase, metaclass=ModelMeta, backend="tensorflow"):
│ │ +    """*RNN4Rec* algorithm.
│ │ +
│ │ +    .. NOTE::
│ │ +        The original paper used GRU, but in this implementation we can also use LSTM.
│ │  
│ │      Parameters
│ │      ----------
│ │      task : {'rating', 'ranking'}
│ │          Recommendation task. See :ref:`Task`.
│ │      data_info : :class:`~libreco.data.DataInfo` object
│ │          Object that contains useful information for training and inference.
│ │ -    loss_type : {'cross_entropy', 'focal'}, default: 'cross_entropy'
│ │ +    loss_type : {'cross_entropy', 'focal', 'bpr'}, default: 'cross_entropy'
│ │          Loss for model training.
│ │ +    rnn_type : {'lstm', 'gru'}, default: 'gru'
│ │ +        RNN for modeling.
│ │      embed_size: int, default: 16
│ │          Vector size of embeddings.
│ │      n_epochs: int, default: 10
│ │          Number of epochs for training.
│ │      lr : float, default 0.001
│ │          Learning rate for training.
│ │      lr_decay : bool, default: False
│ │ @@ -31,216 +35,191 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ +    sampler : {'random', 'unconsumed', 'popular'}, default: 'random'
│ │ +        Negative sampling strategy.
│ │ +
│ │ +        - ``'random'`` means random sampling.
│ │ +        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ +        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ +
│ │ +        .. versionadded:: 1.1.0
│ │ +
│ │      num_neg : int, default: 1
│ │          Number of negative samples for each positive sample, only used in `ranking` task.
│ │ -    use_bn : bool, default: True
│ │ -        Whether to use batch normalization.
│ │      dropout_rate : float or None, default: None
│ │          Probability of an element to be zeroed. If it is None, dropout is not used.
│ │ -    hidden_units : int, list of int or tuple of (int,), default: (128, 64, 32)
│ │ -        Number of layers and corresponding layer size in MLP.
│ │ +    hidden_units : int, list of int or tuple of (int,), default: 16
│ │ +        Number of layers and corresponding layer size in RNN.
│ │  
│ │          .. versionchanged:: 1.0.0
│ │             Accept type of ``int``, ``list`` or ``tuple``, instead of ``str``.
│ │  
│ │ +    use_layer_norm : bool, default: False
│ │ +        Whether to use layer normalization.
│ │ +    recent_num : int or None, default: 10
│ │ +        Number of recent items to use in user behavior sequence.
│ │ +    random_num : int or None, default: None
│ │ +        Number of random sampled items to use in user behavior sequence.
│ │ +        If `recent_num` is not None, `random_num` is not considered.
│ │      seed : int, default: 42
│ │          Random seed.
│ │      lower_upper_bound : tuple or None, default: None
│ │          Lower and upper score bound for `rating` task.
│ │      tf_sess_config : dict or None, default: None
│ │          Optional TensorFlow session config, see `ConfigProto options
│ │          <https://github.com/tensorflow/tensorflow/blob/v2.10.0/tensorflow/core/protobuf/config.proto#L431>`_.
│ │  
│ │      References
│ │      ----------
│ │ -    *Xiangnan He et al.* `Neural Collaborative Filtering
│ │ -    <https://arxiv.org/pdf/1708.05031.pdf>`_.
│ │ +    *Balazs Hidasi et al.* `Session-based Recommendations with Recurrent Neural Networks
│ │ +    <https://arxiv.org/pdf/1511.06939.pdf>`_.
│ │      """
│ │  
│ │ -    user_variables = ["user_gmf", "user_mlp"]
│ │ -    item_variables = ["item_gmf", "item_mlp"]
│ │ +    item_variables = ["item_weights", "item_biases", "input_embed"]
│ │  
│ │      def __init__(
│ │          self,
│ │          task,
│ │ -        data_info,
│ │ +        data_info=None,
│ │          loss_type="cross_entropy",
│ │ +        rnn_type="gru",
│ │          embed_size=16,
│ │          n_epochs=20,
│ │ -        lr=0.01,
│ │ +        lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ +        sampler="random",
│ │          num_neg=1,
│ │ -        use_bn=True,
│ │          dropout_rate=None,
│ │ -        hidden_units=(128, 64, 32),
│ │ +        hidden_units=16,
│ │ +        use_layer_norm=False,
│ │ +        recent_num=10,
│ │ +        random_num=None,
│ │          seed=42,
│ │          lower_upper_bound=None,
│ │          tf_sess_config=None,
│ │      ):
│ │ -        super().__init__(task, data_info, lower_upper_bound, tf_sess_config)
│ │ -
│ │ +        super().__init__(
│ │ +            task,
│ │ +            data_info,
│ │ +            embed_size,
│ │ +            recent_num,
│ │ +            random_num,
│ │ +            lower_upper_bound,
│ │ +            tf_sess_config,
│ │ +        )
│ │          self.all_args = locals()
│ │          self.loss_type = loss_type
│ │ -        self.embed_size = embed_size
│ │ +        self.rnn_type = rnn_type.lower()
│ │          self.n_epochs = n_epochs
│ │          self.lr = lr
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │ +        self.hidden_units = hidden_units_config(hidden_units)
│ │          self.reg = reg_config(reg)
│ │          self.batch_size = batch_size
│ │ +        self.sampler = sampler
│ │          self.num_neg = num_neg
│ │ -        self.use_bn = use_bn
│ │          self.dropout_rate = dropout_config(dropout_rate)
│ │ -        self.hidden_units = hidden_units_config(hidden_units)
│ │ +        self.use_ln = use_layer_norm
│ │          self.seed = seed
│ │ +        self._check_params()
│ │ +
│ │ +    def _check_params(self):
│ │ +        if self.rnn_type not in ("lstm", "gru"):
│ │ +            raise ValueError("`rnn_type` must either be `lstm` or `gru`")
│ │ +        if self.loss_type not in ("cross_entropy", "bpr", "focal"):
│ │ +            raise ValueError(
│ │ +                "`loss_type` must be one of (`cross_entropy`, `focal`, `bpr`)"
│ │ +            )
│ │  
│ │      def build_model(self):
│ │ -        self.user_indices = tf.placeholder(tf.int32, shape=[None])
│ │ -        self.item_indices = tf.placeholder(tf.int32, shape=[None])
│ │ +        tf.set_random_seed(self.seed)
│ │          self.labels = tf.placeholder(tf.float32, shape=[None])
│ │          self.is_training = tf.placeholder_with_default(False, shape=[])
│ │ -
│ │ -        user_gmf = tf.get_variable(
│ │ -            name="user_gmf",
│ │ -            shape=[self.n_users + 1, self.embed_size],
│ │ -            initializer=tf.glorot_uniform_initializer(),
│ │ +        self._build_variables()
│ │ +        self._build_user_embeddings()
│ │ +        if self.task == "rating" or self.loss_type in ("cross_entropy", "focal"):
│ │ +            self.user_indices = tf.placeholder(tf.int32, shape=[None])
│ │ +            self.item_indices = tf.placeholder(tf.int32, shape=[None])
│ │ +
│ │ +            item_vector = tf.nn.embedding_lookup(self.item_weights, self.item_indices)
│ │ +            item_bias = tf.nn.embedding_lookup(self.item_biases, self.item_indices)
│ │ +            self.output = (
│ │ +                tf.reduce_sum(tf.multiply(self.user_vector, item_vector), axis=1)
│ │ +                + item_bias
│ │ +            )
│ │ +        elif self.loss_type == "bpr":
│ │ +            self.item_indices_pos = tf.placeholder(tf.int32, shape=[None])
│ │ +            self.item_indices_neg = tf.placeholder(tf.int32, shape=[None])
│ │ +            item_embed_pos = tf.nn.embedding_lookup(
│ │ +                self.item_weights, self.item_indices_pos
│ │ +            )
│ │ +            item_embed_neg = tf.nn.embedding_lookup(
│ │ +                self.item_weights, self.item_indices_neg
│ │ +            )
│ │ +            item_bias_pos = tf.nn.embedding_lookup(
│ │ +                self.item_biases, self.item_indices_pos
│ │ +            )
│ │ +            item_bias_neg = tf.nn.embedding_lookup(
│ │ +                self.item_biases, self.item_indices_neg
│ │ +            )
│ │ +
│ │ +            item_diff = tf.subtract(item_bias_pos, item_bias_neg) + tf.reduce_sum(
│ │ +                tf.multiply(
│ │ +                    self.user_vector, tf.subtract(item_embed_pos, item_embed_neg)
│ │ +                ),
│ │ +                axis=1,
│ │ +            )
│ │ +            self.bpr_loss = tf.log_sigmoid(item_diff)
│ │ +
│ │ +        count_params()
│ │ +
│ │ +    def _build_variables(self):
│ │ +        # weight and bias parameters for last fc_layer
│ │ +        self.item_biases = tf.get_variable(
│ │ +            name="item_biases",
│ │ +            shape=[self.n_items],
│ │ +            initializer=tf.zeros_initializer(),
│ │              regularizer=self.reg,
│ │          )
│ │ -        item_gmf = tf.get_variable(
│ │ -            name="item_gmf",
│ │ -            shape=[self.n_items + 1, self.embed_size],
│ │ +        self.item_weights = tf.get_variable(
│ │ +            name="item_weights",
│ │ +            shape=[self.n_items, self.embed_size],
│ │              initializer=tf.glorot_uniform_initializer(),
│ │              regularizer=self.reg,
│ │          )
│ │ -        user_mlp = tf.get_variable(
│ │ -            name="user_mlp",
│ │ -            shape=[self.n_users + 1, self.embed_size],
│ │ -            initializer=tf.glorot_uniform_initializer(),
│ │ -            regularizer=self.reg,
│ │ -        )
│ │ -        item_mlp = tf.get_variable(
│ │ -            name="item_mlp",
│ │ -            shape=[self.n_items + 1, self.embed_size],
│ │ +
│ │ +        # input_embed for rnn_layer, include padding value
│ │ +        self.input_embed = tf.get_variable(
│ │ +            name="input_embed",
│ │ +            shape=[self.n_items + 1, self.hidden_units[0]],
│ │              initializer=tf.glorot_uniform_initializer(),
│ │              regularizer=self.reg,
│ │          )
│ │  
│ │ -        user_gmf_embed = tf.nn.embedding_lookup(user_gmf, self.user_indices)
│ │ -        item_gmf_embed = tf.nn.embedding_lookup(item_gmf, self.item_indices)
│ │ -        user_mlp_embed = tf.nn.embedding_lookup(user_mlp, self.user_indices)
│ │ -        item_mlp_embed = tf.nn.embedding_lookup(item_mlp, self.item_indices)
│ │ -
│ │ -        gmf_layer = tf.multiply(user_gmf_embed, item_gmf_embed)
│ │ -        mlp_input = tf.concat([user_mlp_embed, item_mlp_embed], axis=1)
│ │ -        mlp_layer = dense_nn(
│ │ -            mlp_input,
│ │ -            self.hidden_units,
│ │ -            use_bn=self.use_bn,
│ │ +    def _build_user_embeddings(self):
│ │ +        self.user_interacted_seq = tf.placeholder(
│ │ +            tf.int32, shape=[None, self.max_seq_len]
│ │ +        )
│ │ +        self.user_interacted_len = tf.placeholder(tf.int64, shape=[None])
│ │ +        seq_item_embed = tf.nn.embedding_lookup(
│ │ +            self.input_embed, self.user_interacted_seq
│ │ +        )
│ │ +        rnn_output = tf_rnn(
│ │ +            inputs=seq_item_embed,
│ │ +            rnn_type=self.rnn_type,
│ │ +            lengths=self.user_interacted_len,
│ │ +            maxlen=self.max_seq_len,
│ │ +            hidden_units=self.hidden_units,
│ │              dropout_rate=self.dropout_rate,
│ │ +            use_ln=self.use_ln,
│ │              is_training=self.is_training,
│ │          )
│ │ -        concat_layer = tf.concat([gmf_layer, mlp_layer], axis=1)
│ │ -        self.output = tf.reshape(tf_dense(units=1)(concat_layer), [-1])
│ │ -
│ │ -    def predict(self, user, item, feats=None, cold_start="average", inner_id=False):
│ │ -        """Make prediction(s) on given user(s) and item(s).
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        user : int or str or array_like
│ │ -            User id or batch of user ids.
│ │ -        item : int or str or array_like
│ │ -            Item id or batch of item ids.
│ │ -        feats : None, default: None
│ │ -            NCF can't use features.
│ │ -        cold_start : {'popular', 'average'}, default: 'average'
│ │ -            Cold start strategy.
│ │ -
│ │ -            - 'popular' will sample from popular items.
│ │ -            - 'average' will use the average of all the user/item embeddings as the
│ │ -              representation of the cold-start user/item.
│ │ -
│ │ -        inner_id : bool, default: False
│ │ -            Whether to use inner_id defined in `libreco`. For library users inner_id
│ │ -            may never be used.
│ │ -
│ │ -        Returns
│ │ -        -------
│ │ -        prediction : float or array_like
│ │ -            Predicted scores for each user-item pair.
│ │ -        """
│ │ -        assert feats is None, "NCF can't use features."
│ │ -        user, item = convert_id(self, user, item, inner_id)
│ │ -        unknown_num, unknown_index, user, item = check_unknown(self, user, item)
│ │ -        preds = self.sess.run(
│ │ -            self.output,
│ │ -            feed_dict={
│ │ -                self.user_indices: user,
│ │ -                self.item_indices: item,
│ │ -                self.is_training: False,
│ │ -            },
│ │ -        )
│ │ -        return normalize_prediction(preds, self, cold_start, unknown_num, unknown_index)
│ │ -
│ │ -    def recommend_user(
│ │ -        self,
│ │ -        user,
│ │ -        n_rec,
│ │ -        user_feats=None,
│ │ -        item_data=None,
│ │ -        cold_start="average",
│ │ -        inner_id=False,
│ │ -        filter_consumed=True,
│ │ -        random_rec=False,
│ │ -    ):
│ │ -        """Recommend a list of items for given user(s).
│ │ -
│ │ -        Parameters
│ │ -        ----------
│ │ -        user : int or str or array_like
│ │ -            User id or batch of user ids to recommend.
│ │ -        n_rec : int
│ │ -            Number of recommendations to return.
│ │ -        user_feats : None, default: None
│ │ -            NCF can't use features.
│ │ -        item_data : None, default: None
│ │ -            NCF can't use features.
│ │ -        cold_start : {'popular', 'average'}, default: 'average'
│ │ -            Cold start strategy.
│ │ -
│ │ -            - 'popular' will sample from popular items.
│ │ -            - 'average' will use the average of all the user/item embeddings as the
│ │ -              representation of the cold-start user/item.
│ │ -
│ │ -        inner_id : bool, default: False
│ │ -            Whether to use inner_id defined in `libreco`. For library users inner_id
│ │ -            may never be used.
│ │ -        filter_consumed : bool, default: True
│ │ -            Whether to filter out items that a user has previously consumed.
│ │ -        random_rec : bool, default: False
│ │ -            Whether to choose items for recommendation based on their prediction scores.
│ │ -
│ │ -        Returns
│ │ -        -------
│ │ -        recommendation : dict
│ │ -            Recommendation result with user ids as keys
│ │ -            and array_like recommended items as values.
│ │ -        """
│ │ -        assert user_feats is None and item_data is None, "NCF can't use features."
│ │ -        return super().recommend_user(
│ │ -            user,
│ │ -            n_rec,
│ │ -            user_feats,
│ │ -            item_data,
│ │ -            cold_start,
│ │ -            inner_id,
│ │ -            filter_consumed,
│ │ -            random_rec,
│ │ -        )
│ │ +        self.user_vector = tf_dense(units=self.embed_size, activation=None)(rnn_output)
│ │   --- LibRecommender-1.0.1/libreco/algorithms/ngcf.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/ngcf.py
│ │┄ Ordering differences only
│ │┄ Files 1% similar despite different names
│ │ @@ -1,13 +1,13 @@
│ │  """Implementation of NGCF."""
│ │  import torch
│ │  
│ │ +from .torch_modules import NGCFModel
│ │  from ..bases import EmbedBase, ModelMeta
│ │  from ..torchops import device_config, hidden_units_config
│ │ -from .torch_modules import NGCFModel
│ │  
│ │  
│ │  class NGCF(EmbedBase, metaclass=ModelMeta, backend="torch"):
│ │      """*Neural Graph Collaborative Filtering* algorithm.
│ │  
│ │      .. WARNING::
│ │          NGCF can only be used in ``ranking`` task.
│ │   --- LibRecommender-1.0.1/libreco/algorithms/pinsage_dgl.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/pinsage_dgl.py
│ │┄ Files 6% similar despite different names
│ │ @@ -1,18 +1,17 @@
│ │  """Implementation of PinSageDGL."""
│ │  import importlib
│ │  
│ │ -from ..bases import ModelMeta
│ │ -from ..graph import check_dgl
│ │ -from .graphsage_dgl import GraphSageDGL
│ │  from .torch_modules import PinSageDGLModel
│ │ +from ..bases import ModelMeta, SageBase
│ │ +from ..graph import NeighborWalkerDGL, build_u2i_hetero_graph, check_dgl
│ │  
│ │  
│ │  @check_dgl
│ │ -class PinSageDGL(GraphSageDGL, metaclass=ModelMeta, backend="torch"):
│ │ +class PinSageDGL(SageBase, metaclass=ModelMeta, backend="torch"):
│ │      """*PinSageDGL* algorithm.
│ │  
│ │      .. NOTE::
│ │          This algorithm is implemented in `DGL <https://github.com/dmlc/dgl>`_.
│ │  
│ │      .. CAUTION::
│ │          PinSageDGL can only be used in ``ranking`` task.
│ │ @@ -153,15 +152,14 @@
│ │          lower_upper_bound=None,
│ │      ):
│ │          super().__init__(
│ │              task,
│ │              data_info,
│ │              loss_type,
│ │              paradigm,
│ │ -            "mean",
│ │              embed_size,
│ │              n_epochs,
│ │              lr,
│ │              lr_decay,
│ │              epsilon,
│ │              amsgrad,
│ │              reg,
│ │ @@ -178,33 +176,24 @@
│ │              start_node,
│ │              focus_start,
│ │              seed,
│ │              device,
│ │              lower_upper_bound,
│ │          )
│ │          self.all_args = locals()
│ │ -        self.num_walks = num_walks
│ │          self.neighbor_walk_len = neighbor_walk_len
│ │          self.termination_prob = termination_prob
│ │  
│ │      def build_model(self):
│ │ -        self.hetero_g = self.build_hetero_graph()
│ │ +        self._dgl.seed(self.seed)
│ │ +        self.hetero_g = build_u2i_hetero_graph(
│ │ +            self.n_users, self.n_items, self.user_consumed
│ │ +        )
│ │ +        self.neighbor_walker = NeighborWalkerDGL(self, self.data_info)
│ │          self.torch_model = PinSageDGLModel(
│ │              self.paradigm,
│ │              self.data_info,
│ │              self.embed_size,
│ │              self.batch_size,
│ │              self.num_layers,
│ │              self.dropout_rate,
│ │          ).to(self.device)
│ │ -
│ │ -    def sample_frontier(self, nodes):
│ │ -        sampler = self._dgl.sampling.PinSAGESampler(
│ │ -            self.hetero_g,
│ │ -            ntype="item",
│ │ -            other_type="user",
│ │ -            num_traversals=self.neighbor_walk_len,
│ │ -            termination_prob=self.termination_prob,
│ │ -            num_random_walks=self.num_walks,
│ │ -            num_neighbors=self.num_neighbors,
│ │ -        )
│ │ -        return sampler(nodes)
│ │   --- LibRecommender-1.0.1/libreco/algorithms/rnn4rec.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/youtube_ranking.py
│ │┄ Files 20% similar despite different names
│ │ @@ -1,34 +1,52 @@
│ │ -"""Implementation of RNN4Rec model."""
│ │ +"""Implementation of YouTubeRanking."""
│ │  import numpy as np
│ │  
│ │ -from ..bases import EmbedBase, ModelMeta
│ │ -from ..data.sequence import get_user_last_interacted
│ │ -from ..tfops import dropout_config, reg_config, sess_config, tf, tf_dense, tf_rnn
│ │ +from ..bases import ModelMeta, TfBase
│ │ +from ..batch.sequence import get_user_last_interacted
│ │ +from ..feature.multi_sparse import true_sparse_field_size
│ │ +from ..tfops import (
│ │ +    dense_nn,
│ │ +    dropout_config,
│ │ +    multi_sparse_combine_embedding,
│ │ +    reg_config,
│ │ +    tf,
│ │ +    tf_dense,
│ │ +)
│ │  from ..torchops import hidden_units_config
│ │  from ..utils.misc import count_params
│ │ -from ..utils.validate import check_seq_mode
│ │ +from ..utils.validate import (
│ │ +    check_dense_values,
│ │ +    check_multi_sparse,
│ │ +    check_seq_mode,
│ │ +    check_sparse_indices,
│ │ +    dense_field_size,
│ │ +    sparse_feat_size,
│ │ +    sparse_field_size,
│ │ +)
│ │  
│ │  
│ │ -class RNN4Rec(EmbedBase, metaclass=ModelMeta, backend="tensorflow"):
│ │ -    """*RNN4Rec* algorithm.
│ │ +class YouTubeRanking(TfBase, metaclass=ModelMeta):
│ │ +    """*YouTubeRanking* algorithm.
│ │  
│ │      .. NOTE::
│ │ -        The original paper used GRU, but in this implementation we can also use LSTM.
│ │ +        The algorithm implemented mainly corresponds to the ranking phase
│ │ +        based on the original paper.
│ │ +
│ │ +    .. WARNING::
│ │ +        YouTubeRanking can only be used in `ranking` task.
│ │  
│ │      Parameters
│ │      ----------
│ │ -    task : {'rating', 'ranking'}
│ │ +    task : {'ranking'}
│ │          Recommendation task. See :ref:`Task`.
│ │      data_info : :class:`~libreco.data.DataInfo` object
│ │          Object that contains useful information for training and inference.
│ │ -    loss_type : {'cross_entropy', 'focal', 'bpr'}, default: 'cross_entropy'
│ │ +    loss_type : {'cross_entropy', 'focal'}, default: 'cross_entropy'
│ │          Loss for model training.
│ │ -    rnn_type : {'lstm', 'gru'}, default: 'gru'
│ │ -        RNN for modeling.
│ │      embed_size: int, default: 16
│ │          Vector size of embeddings.
│ │      n_epochs: int, default: 10
│ │          Number of epochs for training.
│ │      lr : float, default 0.001
│ │          Learning rate for training.
│ │      lr_decay : bool, default: False
│ │ @@ -39,195 +57,235 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ +    sampler : {'random', 'unconsumed', 'popular'}, default: 'random'
│ │ +        Negative sampling strategy.
│ │ +
│ │ +        - ``'random'`` means random sampling.
│ │ +        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ +        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ +
│ │ +        .. versionadded:: 1.1.0
│ │ +
│ │      num_neg : int, default: 1
│ │          Number of negative samples for each positive sample, only used in `ranking` task.
│ │ +    use_bn : bool, default: True
│ │ +        Whether to use batch normalization.
│ │      dropout_rate : float or None, default: None
│ │          Probability of an element to be zeroed. If it is None, dropout is not used.
│ │ -    hidden_units : int, list of int or tuple of (int,), default: 16
│ │ -        Number of layers and corresponding layer size in RNN.
│ │ +    hidden_units : int, list of int or tuple of (int,), default: (128, 64, 32)
│ │ +        Number of layers and corresponding layer size in MLP.
│ │  
│ │          .. versionchanged:: 1.0.0
│ │             Accept type of ``int``, ``list`` or ``tuple``, instead of ``str``.
│ │  
│ │ -    use_layer_norm : bool, default: False
│ │ -        Whether to use layer normalization.
│ │      recent_num : int or None, default: 10
│ │          Number of recent items to use in user behavior sequence.
│ │      random_num : int or None, default: None
│ │          Number of random sampled items to use in user behavior sequence.
│ │          If `recent_num` is not None, `random_num` is not considered.
│ │ +    multi_sparse_combiner : {'normal', 'mean', 'sum', 'sqrtn'}, default: 'sqrtn'
│ │ +        Options for combining `multi_sparse` features.
│ │      seed : int, default: 42
│ │          Random seed.
│ │      lower_upper_bound : tuple or None, default: None
│ │          Lower and upper score bound for `rating` task.
│ │      tf_sess_config : dict or None, default: None
│ │          Optional TensorFlow session config, see `ConfigProto options
│ │          <https://github.com/tensorflow/tensorflow/blob/v2.10.0/tensorflow/core/protobuf/config.proto#L431>`_.
│ │  
│ │      References
│ │      ----------
│ │ -    *Balazs Hidasi et al.* `Session-based Recommendations with Recurrent Neural Networks
│ │ -    <https://arxiv.org/pdf/1511.06939.pdf>`_.
│ │ +    *Paul Covington et al.* `Deep Neural Networks for YouTube Recommendations
│ │ +    <https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf>`_.
│ │      """
│ │  
│ │ -    item_variables = ["item_weights", "item_biases", "input_embed"]
│ │ +    user_variables = ["user_features"]
│ │ +    item_variables = ["item_features"]
│ │ +    sparse_variables = ["sparse_features"]
│ │ +    dense_variables = ["dense_features"]
│ │  
│ │      def __init__(
│ │          self,
│ │ -        task,
│ │ +        task="ranking",
│ │          data_info=None,
│ │          loss_type="cross_entropy",
│ │ -        rnn_type="gru",
│ │          embed_size=16,
│ │          n_epochs=20,
│ │          lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ +        sampler="random",
│ │          num_neg=1,
│ │ +        use_bn=True,
│ │          dropout_rate=None,
│ │ -        hidden_units=16,
│ │ -        use_layer_norm=False,
│ │ +        hidden_units=(128, 64, 32),
│ │          recent_num=10,
│ │          random_num=None,
│ │ +        multi_sparse_combiner="sqrtn",
│ │          seed=42,
│ │          lower_upper_bound=None,
│ │          tf_sess_config=None,
│ │      ):
│ │ -        super().__init__(task, data_info, embed_size, lower_upper_bound)
│ │ +        super().__init__(task, data_info, lower_upper_bound, tf_sess_config)
│ │  
│ │ +        assert task == "ranking", "YouTube models is only suitable for ranking"
│ │          self.all_args = locals()
│ │ -        self.sess = sess_config(tf_sess_config)
│ │          self.loss_type = loss_type
│ │ -        self.rnn_type = rnn_type.lower()
│ │ +        self.embed_size = embed_size
│ │          self.n_epochs = n_epochs
│ │          self.lr = lr
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │ -        self.hidden_units = hidden_units_config(hidden_units)
│ │          self.reg = reg_config(reg)
│ │          self.batch_size = batch_size
│ │ +        self.sampler = sampler
│ │          self.num_neg = num_neg
│ │ +        self.use_bn = use_bn
│ │          self.dropout_rate = dropout_config(dropout_rate)
│ │ -        self.use_ln = use_layer_norm
│ │ -        self.seed = seed
│ │ +        self.hidden_units = hidden_units_config(hidden_units)
│ │          self.seq_mode, self.max_seq_len = check_seq_mode(recent_num, random_num)
│ │          self.recent_seqs, self.recent_seq_lens = self._set_recent_seqs()
│ │ -        self._check_params()
│ │ +        self.seed = seed
│ │ +        self.sparse = check_sparse_indices(data_info)
│ │ +        self.dense = check_dense_values(data_info)
│ │ +        if self.sparse:
│ │ +            self.sparse_feature_size = sparse_feat_size(data_info)
│ │ +            self.sparse_field_size = sparse_field_size(data_info)
│ │ +            self.multi_sparse_combiner = check_multi_sparse(
│ │ +                data_info, multi_sparse_combiner
│ │ +            )
│ │ +            self.true_sparse_field_size = true_sparse_field_size(
│ │ +                data_info, self.sparse_field_size, self.multi_sparse_combiner
│ │ +            )
│ │ +        if self.dense:
│ │ +            self.dense_field_size = dense_field_size(data_info)
│ │  
│ │      def build_model(self):
│ │          tf.set_random_seed(self.seed)
│ │ +        self.user_indices = tf.placeholder(tf.int32, shape=[None])
│ │ +        self.item_indices = tf.placeholder(tf.int32, shape=[None])
│ │ +        self.user_interacted_seq = tf.placeholder(
│ │ +            tf.int32, shape=[None, self.max_seq_len]
│ │ +        )
│ │ +        self.user_interacted_len = tf.placeholder(tf.float32, shape=[None])
│ │          self.labels = tf.placeholder(tf.float32, shape=[None])
│ │          self.is_training = tf.placeholder_with_default(False, shape=[])
│ │ -        self._build_variables()
│ │ -        self._build_user_embeddings()
│ │ -        if self.task == "rating" or self.loss_type in ("cross_entropy", "focal"):
│ │ -            self.user_indices = tf.placeholder(tf.int32, shape=[None])
│ │ -            self.item_indices = tf.placeholder(tf.int32, shape=[None])
│ │ -
│ │ -            item_vector = tf.nn.embedding_lookup(self.item_weights, self.item_indices)
│ │ -            item_bias = tf.nn.embedding_lookup(self.item_biases, self.item_indices)
│ │ -            self.output = (
│ │ -                tf.reduce_sum(tf.multiply(self.user_vector, item_vector), axis=1)
│ │ -                + item_bias
│ │ -            )
│ │ +        self.concat_embed = []
│ │  
│ │ -        elif self.loss_type == "bpr":
│ │ -            self.item_indices_pos = tf.placeholder(tf.int32, shape=[None])
│ │ -            self.item_indices_neg = tf.placeholder(tf.int32, shape=[None])
│ │ -            item_embed_pos = tf.nn.embedding_lookup(
│ │ -                self.item_weights, self.item_indices_pos
│ │ -            )
│ │ -            item_embed_neg = tf.nn.embedding_lookup(
│ │ -                self.item_weights, self.item_indices_neg
│ │ -            )
│ │ -            item_bias_pos = tf.nn.embedding_lookup(
│ │ -                self.item_biases, self.item_indices_pos
│ │ -            )
│ │ -            item_bias_neg = tf.nn.embedding_lookup(
│ │ -                self.item_biases, self.item_indices_neg
│ │ -            )
│ │ +        user_features = tf.get_variable(
│ │ +            name="user_features",
│ │ +            shape=[self.n_users + 1, self.embed_size],
│ │ +            initializer=tf.glorot_uniform_initializer(),
│ │ +            regularizer=self.reg,
│ │ +        )
│ │ +        item_features = tf.get_variable(
│ │ +            name="item_features",
│ │ +            shape=[self.n_items + 1, self.embed_size],
│ │ +            initializer=tf.glorot_uniform_initializer(),
│ │ +            regularizer=self.reg,
│ │ +        )
│ │ +        user_embed = tf.nn.embedding_lookup(user_features, self.user_indices)
│ │ +        item_embed = tf.nn.embedding_lookup(item_features, self.item_indices)
│ │  
│ │ -            item_diff = tf.subtract(item_bias_pos, item_bias_neg) + tf.reduce_sum(
│ │ -                tf.multiply(
│ │ -                    self.user_vector, tf.subtract(item_embed_pos, item_embed_neg)
│ │ -                ),
│ │ -                axis=1,
│ │ +        # unknown items are padded to 0-vector
│ │ +        zero_padding_op = tf.scatter_update(
│ │ +            item_features, self.n_items, tf.zeros([self.embed_size], dtype=tf.float32)
│ │ +        )
│ │ +        with tf.control_dependencies([zero_padding_op]):
│ │ +            # B * seq * K
│ │ +            multi_item_embed = tf.nn.embedding_lookup(
│ │ +                item_features, self.user_interacted_seq
│ │              )
│ │ -            self.bpr_loss = tf.log_sigmoid(item_diff)
│ │ +        pooled_embed = tf.div_no_nan(
│ │ +            tf.reduce_sum(multi_item_embed, axis=1),
│ │ +            tf.expand_dims(tf.sqrt(self.user_interacted_len), axis=1),
│ │ +        )
│ │ +        self.concat_embed.extend([user_embed, item_embed, pooled_embed])
│ │  
│ │ +        if self.sparse:
│ │ +            self._build_sparse()
│ │ +        if self.dense:
│ │ +            self._build_dense()
│ │ +
│ │ +        concat_embed = tf.concat(self.concat_embed, axis=1)
│ │ +        mlp_layer = dense_nn(
│ │ +            concat_embed,
│ │ +            self.hidden_units,
│ │ +            use_bn=self.use_bn,
│ │ +            dropout_rate=self.dropout_rate,
│ │ +            is_training=self.is_training,
│ │ +        )
│ │ +        self.output = tf.reshape(tf_dense(units=1)(mlp_layer), [-1])
│ │          count_params()
│ │  
│ │ -    def _build_variables(self):
│ │ -        # weight and bias parameters for last fc_layer
│ │ -        self.item_biases = tf.get_variable(
│ │ -            name="item_biases",
│ │ -            shape=[self.n_items],
│ │ -            initializer=tf.zeros_initializer(),
│ │ -            regularizer=self.reg,
│ │ +    def _build_sparse(self):
│ │ +        self.sparse_indices = tf.placeholder(
│ │ +            tf.int32, shape=[None, self.sparse_field_size]
│ │          )
│ │ -        self.item_weights = tf.get_variable(
│ │ -            name="item_weights",
│ │ -            shape=[self.n_items, self.embed_size],
│ │ +        sparse_features = tf.get_variable(
│ │ +            name="sparse_features",
│ │ +            shape=[self.sparse_feature_size, self.embed_size],
│ │              initializer=tf.glorot_uniform_initializer(),
│ │              regularizer=self.reg,
│ │          )
│ │  
│ │ -        # input_embed for rnn_layer, include padding value
│ │ -        self.input_embed = tf.get_variable(
│ │ -            name="input_embed",
│ │ -            shape=[self.n_items + 1, self.hidden_units[0]],
│ │ +        if self.data_info.multi_sparse_combine_info and self.multi_sparse_combiner in (
│ │ +            "sum",
│ │ +            "mean",
│ │ +            "sqrtn",
│ │ +        ):
│ │ +            sparse_embed = multi_sparse_combine_embedding(
│ │ +                self.data_info,
│ │ +                sparse_features,
│ │ +                self.sparse_indices,
│ │ +                self.multi_sparse_combiner,
│ │ +                self.embed_size,
│ │ +            )
│ │ +        else:
│ │ +            sparse_embed = tf.nn.embedding_lookup(sparse_features, self.sparse_indices)
│ │ +
│ │ +        sparse_embed = tf.reshape(
│ │ +            sparse_embed, [-1, self.true_sparse_field_size * self.embed_size]
│ │ +        )
│ │ +        self.concat_embed.append(sparse_embed)
│ │ +
│ │ +    def _build_dense(self):
│ │ +        self.dense_values = tf.placeholder(
│ │ +            tf.float32, shape=[None, self.dense_field_size]
│ │ +        )
│ │ +        dense_values_reshape = tf.reshape(
│ │ +            self.dense_values, [-1, self.dense_field_size, 1]
│ │ +        )
│ │ +        batch_size = tf.shape(self.dense_values)[0]
│ │ +
│ │ +        dense_features = tf.get_variable(
│ │ +            name="dense_features",
│ │ +            shape=[self.dense_field_size, self.embed_size],
│ │              initializer=tf.glorot_uniform_initializer(),
│ │              regularizer=self.reg,
│ │          )
│ │  
│ │ -    def _build_user_embeddings(self):
│ │ -        self.user_interacted_seq = tf.placeholder(
│ │ -            tf.int32, shape=[None, self.max_seq_len]
│ │ +        dense_embed = tf.tile(dense_features, [batch_size, 1])
│ │ +        dense_embed = tf.reshape(
│ │ +            dense_embed, [-1, self.dense_field_size, self.embed_size]
│ │          )
│ │ -        self.user_interacted_len = tf.placeholder(tf.int64, shape=[None])
│ │ -        seq_item_embed = tf.nn.embedding_lookup(
│ │ -            self.input_embed, self.user_interacted_seq
│ │ -        )
│ │ -        rnn_output = tf_rnn(
│ │ -            inputs=seq_item_embed,
│ │ -            rnn_type=self.rnn_type,
│ │ -            lengths=self.user_interacted_len,
│ │ -            maxlen=self.max_seq_len,
│ │ -            hidden_units=self.hidden_units,
│ │ -            dropout_rate=self.dropout_rate,
│ │ -            use_ln=self.use_ln,
│ │ -            is_training=self.is_training,
│ │ +        dense_embed = tf.multiply(dense_embed, dense_values_reshape)
│ │ +        dense_embed = tf.reshape(
│ │ +            dense_embed, [-1, self.dense_field_size * self.embed_size]
│ │          )
│ │ -        self.user_vector = tf_dense(units=self.embed_size, activation=None)(rnn_output)
│ │ +        self.concat_embed.append(dense_embed)
│ │  
│ │      def _set_recent_seqs(self):
│ │          recent_seqs, recent_seq_lens = get_user_last_interacted(
│ │              self.n_users, self.user_consumed, self.n_items, self.max_seq_len
│ │          )
│ │ -        return recent_seqs, recent_seq_lens.astype(np.int64)
│ │ -
│ │ -    def set_embeddings(self):
│ │ -        feed_dict = {
│ │ -            self.user_interacted_seq: self.recent_seqs,
│ │ -            self.user_interacted_len: self.recent_seq_lens,
│ │ -        }
│ │ -        user_vector = self.sess.run(self.user_vector, feed_dict)
│ │ -        item_weights = self.sess.run(self.item_weights)
│ │ -        item_biases = self.sess.run(self.item_biases)
│ │ -
│ │ -        user_bias = np.ones([len(user_vector), 1], dtype=user_vector.dtype)
│ │ -        item_bias = item_biases[:, None]
│ │ -        self.user_embed = np.hstack([user_vector, user_bias])
│ │ -        self.item_embed = np.hstack([item_weights, item_bias])
│ │ -
│ │ -    def _check_params(self):
│ │ -        # assert self.hidden_units[-1] == self.embed_size, (
│ │ -        #    "dimension of last rnn hidden unit should equal to embed_size"
│ │ -        # )
│ │ -        assert self.rnn_type in ("lstm", "gru"), "rnn_type must be either lstm or gru"
│ │ +        oov = np.full(self.max_seq_len, self.n_items, dtype=np.int32)
│ │ +        recent_seqs = np.vstack([recent_seqs, oov])
│ │ +        recent_seq_lens = np.append(recent_seq_lens, [1])
│ │ +        return recent_seqs, recent_seq_lens
│ │   --- LibRecommender-1.0.1/libreco/algorithms/svd.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/svd.py
│ │┄ Files 7% similar despite different names
│ │ @@ -30,14 +30,23 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ +    sampler : {'random', 'unconsumed', 'popular'}, default: 'random'
│ │ +        Negative sampling strategy.
│ │ +
│ │ +        - ``'random'`` means random sampling.
│ │ +        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ +        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ +
│ │ +        .. versionadded:: 1.1.0
│ │ +
│ │      num_neg : int, default: 1
│ │          Number of negative samples for each positive sample, only used in `ranking` task.
│ │      seed : int, default: 42
│ │          Random seed.
│ │      lower_upper_bound : tuple or None, default: None
│ │          Lower and upper score bound for `rating` task.
│ │      tf_sess_config : dict or None, default: None
│ │ @@ -61,14 +70,15 @@
│ │          embed_size=16,
│ │          n_epochs=20,
│ │          lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ +        sampler="random",
│ │          num_neg=1,
│ │          seed=42,
│ │          lower_upper_bound=None,
│ │          tf_sess_config=None,
│ │      ):
│ │          super().__init__(task, data_info, embed_size, lower_upper_bound)
│ │  
│ │ @@ -77,14 +87,15 @@
│ │          self.loss_type = loss_type
│ │          self.n_epochs = n_epochs
│ │          self.lr = lr
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │          self.reg = reg_config(reg)
│ │          self.batch_size = batch_size
│ │ +        self.sampler = sampler
│ │          self.num_neg = num_neg
│ │          self.seed = seed
│ │  
│ │      def build_model(self):
│ │          self.user_indices = tf.placeholder(tf.int32, shape=[None])
│ │          self.item_indices = tf.placeholder(tf.int32, shape=[None])
│ │          self.labels = tf.placeholder(tf.float32, shape=[None])
│ │   --- LibRecommender-1.0.1/libreco/algorithms/svdpp.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/svdpp.py
│ │┄ Files 4% similar despite different names
│ │ @@ -30,14 +30,23 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ +    sampler : {'random', 'unconsumed', 'popular'}, default: 'random'
│ │ +        Negative sampling strategy.
│ │ +
│ │ +        - ``'random'`` means random sampling.
│ │ +        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ +        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ +
│ │ +        .. versionadded:: 1.1.0
│ │ +
│ │      num_neg : int, default: 1
│ │          Number of negative samples for each positive sample, only used in `ranking` task.
│ │      seed : int, default: 42
│ │          Random seed.
│ │      lower_upper_bound : tuple or None, default: None
│ │          Lower and upper score bound for `rating` task.
│ │      tf_sess_config : dict or None, default: None
│ │ @@ -61,14 +70,15 @@
│ │          embed_size=16,
│ │          n_epochs=20,
│ │          lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ +        sampler="random",
│ │          num_neg=1,
│ │          seed=42,
│ │          recent_num=30,
│ │          lower_upper_bound=None,
│ │          tf_sess_config=None,
│ │      ):
│ │          super().__init__(task, data_info, embed_size, lower_upper_bound)
│ │ @@ -78,14 +88,15 @@
│ │          self.loss_type = loss_type
│ │          self.n_epochs = n_epochs
│ │          self.lr = lr
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │          self.reg = reg_config(reg)
│ │          self.batch_size = batch_size
│ │ +        self.sampler = sampler
│ │          self.num_neg = num_neg
│ │          self.recent_num = recent_num
│ │          self.seed = seed
│ │          self.sparse_interaction = None
│ │  
│ │      def build_model(self):
│ │          tf.set_random_seed(self.seed)
│ │ @@ -142,23 +153,37 @@
│ │              + bias_item
│ │              + tf.reduce_sum(tf.multiply(embed_user, embed_item), axis=1)
│ │          )
│ │  
│ │      def fit(
│ │          self,
│ │          train_data,
│ │ +        neg_sampling,
│ │          verbose=1,
│ │          shuffle=True,
│ │          eval_data=None,
│ │          metrics=None,
│ │ +        k=10,
│ │ +        eval_batch_size=8192,
│ │ +        eval_user_num=None,
│ │          **kwargs,
│ │      ):
│ │          if self.sparse_interaction is None:
│ │              self.sparse_interaction = self._set_sparse_interaction()
│ │ -        super().fit(train_data, verbose, shuffle, eval_data, metrics, **kwargs)
│ │ +        super().fit(
│ │ +            train_data,
│ │ +            neg_sampling,
│ │ +            verbose,
│ │ +            shuffle,
│ │ +            eval_data,
│ │ +            metrics,
│ │ +            k,
│ │ +            eval_batch_size,
│ │ +            eval_user_num,
│ │ +        )
│ │  
│ │      def set_embeddings(self):
│ │          bu, bi, puj, qi = self.sess.run(
│ │              [self.bu_var, self.bi_var, self.puj_var, self.qi_var]
│ │          )
│ │          user_bias = np.ones([len(puj), 2], dtype=puj.dtype)
│ │          user_bias[:, 0] = bu
│ │   --- LibRecommender-1.0.1/libreco/algorithms/torch_modules/graphsage_module.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/torch_modules/graphsage_module.py
│ │┄ Files 1% similar despite different names
│ │ @@ -73,15 +73,17 @@
│ │          items,
│ │          sparse_indices,
│ │          dense_values,
│ │          neighbors,
│ │          neighbor_sparse_indices,
│ │          neighbor_dense_values,
│ │          offsets,
│ │ +        weights=None,
│ │      ):
│ │ +        # paper author's implementation: https://github.com/williamleif/GraphSAGE/blob/master/graphsage/models.py#L299
│ │          hidden = [
│ │              self.get_raw_features(items, sparse_indices, dense_values, is_user=False)
│ │          ]
│ │          for n, s, d in zip(neighbors, neighbor_sparse_indices, neighbor_dense_values):
│ │              hidden.append(self.get_raw_features(n, s, d, is_user=False))
│ │          for layer in range(self.num_layers):
│ │              w_linear = self.w_linears[layer]
│ │   --- LibRecommender-1.0.1/libreco/algorithms/torch_modules/lightgcn_module.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/torch_modules/lightgcn_module.py
│ │┄ Files 1% similar despite different names
│ │ @@ -9,15 +9,15 @@
│ │          self,
│ │          n_users,
│ │          n_items,
│ │          embed_size,
│ │          n_layers,
│ │          dropout_rate,
│ │          user_consumed,
│ │ -        device=torch.device("cpu"),
│ │ +        device,
│ │      ):
│ │          super(LightGCNModel, self).__init__()
│ │          self.n_users = n_users
│ │          self.n_items = n_items
│ │          self.embed_size = embed_size
│ │          self.n_layers = n_layers
│ │          self.dropout_rate = dropout_rate
│ │   --- LibRecommender-1.0.1/libreco/algorithms/torch_modules/ngcf_module.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/torch_modules/ngcf_module.py
│ │┄ Files 1% similar despite different names
│ │ @@ -11,15 +11,15 @@
│ │          n_users,
│ │          n_items,
│ │          embed_size,
│ │          layers,
│ │          node_dropout,
│ │          message_dropout,
│ │          user_consumed,
│ │ -        device=torch.device("cpu"),
│ │ +        device,
│ │      ):
│ │          super(NGCFModel, self).__init__()
│ │          self.n_users = n_users
│ │          self.n_items = n_items
│ │          self.embed_size = embed_size
│ │          self.layers = layers
│ │          self.node_dropout = node_dropout
│ │ @@ -38,15 +38,15 @@
│ │                  "item_embed": nn.Parameter(
│ │                      nn.init.xavier_uniform_(torch.empty(self.n_items, self.embed_size))
│ │                  ),
│ │              }
│ │          )
│ │  
│ │          weight_dict = nn.ParameterDict()
│ │ -        layers = [self.embed_size] + self.layers
│ │ +        layers = [self.embed_size, *self.layers]
│ │          for k in range(len(self.layers)):
│ │              weight_dict[f"W_self_{k}"] = nn.Parameter(
│ │                  nn.init.xavier_uniform_(torch.empty(layers[k], layers[k + 1]))
│ │              )
│ │              weight_dict[f"b_self_{k}"] = nn.Parameter(
│ │                  nn.init.zeros_(torch.empty(1, layers[k + 1]))
│ │              )
│ │   --- LibRecommender-1.0.1/libreco/algorithms/torch_modules/pinsage_module.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/torch_modules/pinsage_module.py
│ │┄ Files 1% similar despite different names
│ │ @@ -78,17 +78,18 @@
│ │          self,
│ │          items,
│ │          sparse_indices,
│ │          dense_values,
│ │          neighbors,
│ │          neighbor_sparse_indices,
│ │          neighbor_dense_values,
│ │ -        weights,
│ │          offsets,
│ │ +        weights,
│ │      ):
│ │ +        # graphsage paper author's implementation: https://github.com/williamleif/GraphSAGE/blob/master/graphsage/models.py#L299
│ │          hidden = [
│ │              self.get_raw_features(items, sparse_indices, dense_values, is_user=False)
│ │          ]
│ │          for n, s, d in zip(neighbors, neighbor_sparse_indices, neighbor_dense_values):
│ │              hidden.append(self.get_raw_features(n, s, d, is_user=False))
│ │          for layer in range(self.num_layers):
│ │              q_linear, w_linear = self.q_linears[layer], self.w_linears[layer]
│ │   --- LibRecommender-1.0.1/libreco/algorithms/user_cf.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/user_cf.py
│ │┄ Files 1% similar despite different names
│ │ @@ -120,15 +120,15 @@
│ │  
│ │      def recommend_one(self, user_id, n_rec, filter_consumed, random_rec):
│ │          user_slice = slice(
│ │              self.sim_matrix.indptr[user_id], self.sim_matrix.indptr[user_id + 1]
│ │          )
│ │          sim_users = self.sim_matrix.indices[user_slice]
│ │          sim_values = self.sim_matrix.data[user_slice]
│ │ -        if sim_users.size == 0 or np.all(sim_values <= 0):
│ │ +        if sim_users.size == 0 or np.all(sim_values <= 0):  # pragma: no cover
│ │              self.print_count += 1
│ │              no_str = (
│ │                  f"no similar neighbor for user {user_id}, "
│ │                  f"return default recommendation"
│ │              )
│ │              if self.print_count < 7:
│ │                  print(f"{colorize(no_str, 'red')}")
│ │   --- LibRecommender-1.0.1/libreco/algorithms/wave_net.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/wave_net.py
│ │┄ Files 12% similar despite different names
│ │ @@ -1,26 +1,14 @@
│ │  """Implementation of WaveNet."""
│ │ -import numpy as np
│ │ -
│ │ -from ..bases import EmbedBase, ModelMeta
│ │ -from ..data.sequence import get_user_last_interacted
│ │ -from ..tfops import (
│ │ -    conv_nn,
│ │ -    dropout_config,
│ │ -    max_pool,
│ │ -    reg_config,
│ │ -    sess_config,
│ │ -    tf,
│ │ -    tf_dense,
│ │ -)
│ │ +from ..bases import ModelMeta, SeqEmbedBase
│ │ +from ..tfops import conv_nn, dropout_config, max_pool, reg_config, tf, tf_dense
│ │  from ..utils.misc import count_params
│ │ -from ..utils.validate import check_seq_mode
│ │  
│ │  
│ │ -class WaveNet(EmbedBase, metaclass=ModelMeta, backend="tensorflow"):
│ │ +class WaveNet(SeqEmbedBase, metaclass=ModelMeta, backend="tensorflow"):
│ │      """*WaveNet* algorithm.
│ │  
│ │      Parameters
│ │      ----------
│ │      task : {'rating', 'ranking'}
│ │          Recommendation task. See :ref:`Task`.
│ │      data_info : :class:`~libreco.data.DataInfo` object
│ │ @@ -41,14 +29,23 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ +    sampler : {'random', 'unconsumed', 'popular'}, default: 'random'
│ │ +        Negative sampling strategy.
│ │ +
│ │ +        - ``'random'`` means random sampling.
│ │ +        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ +        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ +
│ │ +        .. versionadded:: 1.1.0
│ │ +
│ │      num_neg : int, default: 1
│ │          Number of negative samples for each positive sample, only used in `ranking` task.
│ │      use_bn : bool, default: True
│ │          Whether to use batch normalization.
│ │      dropout_rate : float or None, default: None
│ │          Probability of an element to be zeroed. If it is None, dropout is not used.
│ │      n_filters : int, default: 16
│ │ @@ -87,46 +84,52 @@
│ │          embed_size=16,
│ │          n_epochs=20,
│ │          lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ +        sampler="random",
│ │          num_neg=1,
│ │          dropout_rate=None,
│ │          use_bn=False,
│ │          n_filters=16,
│ │          n_blocks=1,
│ │          n_layers_per_block=4,
│ │          recent_num=10,
│ │          random_num=None,
│ │          seed=42,
│ │          lower_upper_bound=None,
│ │          tf_sess_config=None,
│ │      ):
│ │ -        super().__init__(task, data_info, embed_size, lower_upper_bound)
│ │ -
│ │ +        super().__init__(
│ │ +            task,
│ │ +            data_info,
│ │ +            embed_size,
│ │ +            recent_num,
│ │ +            random_num,
│ │ +            lower_upper_bound,
│ │ +            tf_sess_config,
│ │ +        )
│ │          self.all_args = locals()
│ │ -        self.sess = sess_config(tf_sess_config)
│ │          self.loss_type = loss_type
│ │          self.n_epochs = n_epochs
│ │          self.lr = lr
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │          self.reg = reg_config(reg)
│ │          self.batch_size = batch_size
│ │ +        self.sampler = sampler
│ │          self.num_neg = num_neg
│ │          self.dropout_rate = dropout_config(dropout_rate)
│ │          self.use_bn = use_bn
│ │          self.n_filters = n_filters
│ │          self.n_blocks = n_blocks
│ │          self.n_layers_per_block = n_layers_per_block
│ │          self.seed = seed
│ │ -        self.seq_mode, self.max_seq_len = check_seq_mode(recent_num, random_num)
│ │ -        self.recent_seqs, self.recent_seq_lens = self._set_recent_seqs()
│ │  
│ │      def build_model(self):
│ │          tf.set_random_seed(self.seed)
│ │          self._build_placeholders()
│ │          self._build_variables()
│ │          self._build_user_embeddings()
│ │  
│ │ @@ -200,37 +203,12 @@
│ │              filters=self.n_filters,
│ │              kernel_size=1,
│ │              strides=1,
│ │              padding="valid",
│ │              activation=tf.nn.relu,
│ │          )(inputs=convs_out)
│ │  
│ │ -        # convs_out = tf.layers.flatten(convs_out)
│ │          p_size = convs_out.get_shape().as_list()[1]
│ │ -        convs_out = max_pool(pool_size=p_size, strides=1, padding="valid")(
│ │ -            inputs=convs_out
│ │ -        )
│ │ -
│ │ +        convs_out = max_pool(pool_size=p_size, strides=1, padding="valid")(convs_out)
│ │          convs_out = tf.squeeze(convs_out, axis=1)
│ │          convs_out = tf_dense(units=self.embed_size, activation=None)(convs_out)
│ │          self.user_vector = tf.concat([user_repr, convs_out], axis=1)
│ │ -
│ │ -    def _set_recent_seqs(self):
│ │ -        recent_seqs, recent_seq_lens = get_user_last_interacted(
│ │ -            self.n_users, self.user_consumed, self.n_items, self.max_seq_len
│ │ -        )
│ │ -        return recent_seqs, recent_seq_lens.astype(np.int64)
│ │ -
│ │ -    def set_embeddings(self):
│ │ -        feed_dict = {
│ │ -            self.user_indices: np.arange(self.n_users),
│ │ -            self.user_interacted_seq: self.recent_seqs,
│ │ -            self.user_interacted_len: self.recent_seq_lens,
│ │ -        }
│ │ -        user_vector = self.sess.run(self.user_vector, feed_dict)
│ │ -        item_weights = self.sess.run(self.item_weights)
│ │ -        item_biases = self.sess.run(self.item_biases)
│ │ -
│ │ -        user_bias = np.ones([len(user_vector), 1], dtype=user_vector.dtype)
│ │ -        item_bias = item_biases[:, None]
│ │ -        self.user_embed = np.hstack([user_vector, user_bias])
│ │ -        self.item_embed = np.hstack([item_weights, item_bias])
│ │   --- LibRecommender-1.0.1/libreco/algorithms/wide_deep.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/wide_deep.py
│ │┄ Files 2% similar despite different names
│ │ @@ -1,9 +1,10 @@
│ │  """Implementation of Wide & Deep."""
│ │  from ..bases import ModelMeta, TfBase
│ │ +from ..feature.multi_sparse import true_sparse_field_size
│ │  from ..tfops import (
│ │      dense_nn,
│ │      dropout_config,
│ │      multi_sparse_combine_embedding,
│ │      reg_config,
│ │      tf,
│ │      tf_dense,
│ │ @@ -13,15 +14,14 @@
│ │  from ..utils.validate import (
│ │      check_dense_values,
│ │      check_multi_sparse,
│ │      check_sparse_indices,
│ │      dense_field_size,
│ │      sparse_feat_size,
│ │      sparse_field_size,
│ │ -    true_sparse_field_size,
│ │  )
│ │  
│ │  
│ │  class WideDeep(TfBase, metaclass=ModelMeta):
│ │      """*Wide & Deep* algorithm.
│ │  
│ │      Parameters
│ │ @@ -47,14 +47,23 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ +    sampler : {'random', 'unconsumed', 'popular'}, default: 'random'
│ │ +        Negative sampling strategy.
│ │ +
│ │ +        - ``'random'`` means random sampling.
│ │ +        - ``'unconsumed'`` samples items that the target user did not consume before.
│ │ +        - ``'popular'`` has a higher probability to sample popular items as negative samples.
│ │ +
│ │ +        .. versionadded:: 1.1.0
│ │ +
│ │      num_neg : int, default: 1
│ │          Number of negative samples for each positive sample, only used in `ranking` task.
│ │      use_bn : bool, default: True
│ │          Whether to use batch normalization.
│ │      dropout_rate : float or None, default: None
│ │          Probability of an element to be zeroed. If it is None, dropout is not used.
│ │      hidden_units : int, list of int or tuple of (int,), default: (128, 64, 32)
│ │ @@ -99,14 +108,15 @@
│ │          embed_size=16,
│ │          n_epochs=20,
│ │          lr=None,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ +        sampler="random",
│ │          num_neg=1,
│ │          use_bn=True,
│ │          dropout_rate=None,
│ │          hidden_units=(128, 64, 32),
│ │          multi_sparse_combiner="sqrtn",
│ │          seed=42,
│ │          lower_upper_bound=None,
│ │ @@ -119,14 +129,15 @@
│ │          self.embed_size = embed_size
│ │          self.n_epochs = n_epochs
│ │          self.lr = self.check_lr(lr)
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │          self.reg = reg_config(reg)
│ │          self.batch_size = batch_size
│ │ +        self.sampler = sampler
│ │          self.num_neg = num_neg
│ │          self.use_bn = use_bn
│ │          self.dropout_rate = dropout_config(dropout_rate)
│ │          self.hidden_units = hidden_units_config(hidden_units)
│ │          self.seed = seed
│ │          self.sparse = check_sparse_indices(data_info)
│ │          self.dense = check_dense_values(data_info)
│ │   --- LibRecommender-1.0.1/libreco/algorithms/youtube_ranking.py
│ ├── +++ LibRecommender-1.1.0/libreco/algorithms/youtube_retrieval.py
│ │┄ Files 15% similar despite different names
│ │ @@ -1,51 +1,50 @@
│ │ -"""Implementation of YouTubeRanking."""
│ │ +"""Implementation of YouTubeRetrieval."""
│ │  import numpy as np
│ │  
│ │ -from ..bases import ModelMeta, TfBase
│ │ -from ..data.sequence import get_user_last_interacted
│ │ +from ..bases import EmbedBase, ModelMeta
│ │ +from ..feature.multi_sparse import true_sparse_field_size
│ │  from ..tfops import (
│ │      dense_nn,
│ │      dropout_config,
│ │      multi_sparse_combine_embedding,
│ │      reg_config,
│ │ +    sess_config,
│ │      tf,
│ │ -    tf_dense,
│ │  )
│ │  from ..torchops import hidden_units_config
│ │  from ..utils.misc import count_params
│ │  from ..utils.validate import (
│ │      check_dense_values,
│ │ -    check_seq_mode,
│ │      check_multi_sparse,
│ │ +    check_seq_mode,
│ │      check_sparse_indices,
│ │      dense_field_size,
│ │      sparse_feat_size,
│ │      sparse_field_size,
│ │ -    true_sparse_field_size,
│ │  )
│ │  
│ │  
│ │ -class YouTubeRanking(TfBase, metaclass=ModelMeta):
│ │ -    """*YouTubeRanking* algorithm.
│ │ +class YouTubeRetrieval(EmbedBase, metaclass=ModelMeta, backend="tensorflow"):
│ │ +    """*YouTubeRetrieval* algorithm.
│ │  
│ │      .. NOTE::
│ │ -        The algorithm implemented mainly corresponds to the ranking phase
│ │ -        based on the original paper.
│ │ +        The algorithm implemented mainly corresponds to the candidate generation
│ │ +        phase based on the original paper.
│ │  
│ │      .. WARNING::
│ │ -        YouTubeRanking can only be used in `ranking` task.
│ │ +        YouTubeRetrieval can only be used in `ranking` task.
│ │  
│ │      Parameters
│ │      ----------
│ │      task : {'ranking'}
│ │          Recommendation task. See :ref:`Task`.
│ │      data_info : :class:`~libreco.data.DataInfo` object
│ │          Object that contains useful information for training and inference.
│ │ -    loss_type : {'cross_entropy', 'focal'}, default: 'cross_entropy'
│ │ +    loss_type : {'sampled_softmax', 'nce'}, default: 'sampled_softmax'
│ │          Loss for model training.
│ │      embed_size: int, default: 16
│ │          Vector size of embeddings.
│ │      n_epochs: int, default: 10
│ │          Number of epochs for training.
│ │      lr : float, default 0.001
│ │          Learning rate for training.
│ │ @@ -57,26 +56,30 @@
│ │          According to the `official comment <https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L64>`_,
│ │          default value of `1e-8` for `epsilon` is generally not good, so here we choose `1e-5`.
│ │          Users can try tuning this hyperparameter if the training is unstable.
│ │      reg : float or None, default: None
│ │          Regularization parameter, must be non-negative or None.
│ │      batch_size : int, default: 256
│ │          Batch size for training.
│ │ -    num_neg : int, default: 1
│ │ -        Number of negative samples for each positive sample, only used in `ranking` task.
│ │      use_bn : bool, default: True
│ │          Whether to use batch normalization.
│ │      dropout_rate : float or None, default: None
│ │          Probability of an element to be zeroed. If it is None, dropout is not used.
│ │ -    hidden_units : int, list of int or tuple of (int,), default: (128, 64, 32)
│ │ +    hidden_units : int, list of int or tuple of (int,), default: (128, 64)
│ │          Number of layers and corresponding layer size in MLP.
│ │  
│ │          .. versionchanged:: 1.0.0
│ │             Accept type of ``int``, ``list`` or ``tuple``, instead of ``str``.
│ │  
│ │ +    num_sampled_per_batch : int or None, default: None
│ │ +        Number of negative samples in a batch. If None, it is set to `batch_size`.
│ │ +    sampler : str, default: 'uniform'
│ │ +        Negative Sampling strategy. 'uniform' will use uniform sampler, and setting to
│ │ +        other value will use `log_uniform_candidate_sampler` in TensorFlow.
│ │ +        In recommendation scenarios the uniform sampler is generally preferred.
│ │      recent_num : int or None, default: 10
│ │          Number of recent items to use in user behavior sequence.
│ │      random_num : int or None, default: None
│ │          Number of random sampled items to use in user behavior sequence.
│ │          If `recent_num` is not None, `random_num` is not considered.
│ │      multi_sparse_combiner : {'normal', 'mean', 'sum', 'sqrtn'}, default: 'sqrtn'
│ │          Options for combining `multi_sparse` features.
│ │ @@ -90,61 +93,64 @@
│ │  
│ │      References
│ │      ----------
│ │      *Paul Covington et al.* `Deep Neural Networks for YouTube Recommendations
│ │      <https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf>`_.
│ │      """
│ │  
│ │ -    user_variables = ["user_features"]
│ │ -    item_variables = ["item_features"]
│ │ +    item_variables = ["item_interaction_features", "nce_weights", "nce_biases"]
│ │      sparse_variables = ["sparse_features"]
│ │      dense_variables = ["dense_features"]
│ │  
│ │      def __init__(
│ │          self,
│ │          task="ranking",
│ │          data_info=None,
│ │ -        loss_type="cross_entropy",
│ │ +        loss_type="sampled_softmax",
│ │          embed_size=16,
│ │          n_epochs=20,
│ │          lr=0.001,
│ │          lr_decay=False,
│ │          epsilon=1e-5,
│ │          reg=None,
│ │          batch_size=256,
│ │ -        num_neg=1,
│ │          use_bn=True,
│ │          dropout_rate=None,
│ │ -        hidden_units=(128, 64, 32),
│ │ +        hidden_units=(128, 64),
│ │ +        num_sampled_per_batch=None,
│ │ +        sampler="uniform",
│ │          recent_num=10,
│ │          random_num=None,
│ │          multi_sparse_combiner="sqrtn",
│ │          seed=42,
│ │          lower_upper_bound=None,
│ │          tf_sess_config=None,
│ │      ):
│ │ -        super().__init__(task, data_info, lower_upper_bound, tf_sess_config)
│ │ +        super().__init__(task, data_info, embed_size, lower_upper_bound)
│ │  
│ │ -        assert task == "ranking", "YouTube models is only suitable for ranking"
│ │ +        assert task == "ranking", "YouTube-type models is only suitable for ranking"
│ │ +        if len(data_info.item_col) > 0:
│ │ +            raise ValueError("The `YouTuBeRetrieval` model assumes no item features.")
│ │          self.all_args = locals()
│ │          self.loss_type = loss_type
│ │ -        self.embed_size = embed_size
│ │          self.n_epochs = n_epochs
│ │          self.lr = lr
│ │          self.lr_decay = lr_decay
│ │          self.epsilon = epsilon
│ │ +        self.hidden_units = [*hidden_units_config(hidden_units), self.embed_size]
│ │          self.reg = reg_config(reg)
│ │          self.batch_size = batch_size
│ │ -        self.num_neg = num_neg
│ │          self.use_bn = use_bn
│ │          self.dropout_rate = dropout_config(dropout_rate)
│ │ -        self.hidden_units = hidden_units_config(hidden_units)
│ │ -        self.seq_mode, self.max_seq_len = check_seq_mode(recent_num, random_num)
│ │ -        self.recent_seqs, self.recent_seq_lens = self._set_recent_seqs()
│ │ +        self.num_sampled_per_batch = num_sampled_per_batch
│ │ +        self.sampler = sampler
│ │          self.seed = seed
│ │ +        self.sess = sess_config(tf_sess_config)
│ │ +        self.seq_mode, self.max_seq_len = check_seq_mode(recent_num, random_num)
│ │ +        self.recent_seq_indices, self.recent_seq_values = self._set_recent_seqs()
│ │          self.sparse = check_sparse_indices(data_info)
│ │          self.dense = check_dense_values(data_info)
│ │          if self.sparse:
│ │              self.sparse_feature_size = sparse_feat_size(data_info)
│ │              self.sparse_field_size = sparse_field_size(data_info)
│ │              self.multi_sparse_combiner = check_multi_sparse(
│ │                  data_info, multi_sparse_combiner
│ │ @@ -153,85 +159,76 @@
│ │                  data_info, self.sparse_field_size, self.multi_sparse_combiner
│ │              )
│ │          if self.dense:
│ │              self.dense_field_size = dense_field_size(data_info)
│ │  
│ │      def build_model(self):
│ │          tf.set_random_seed(self.seed)
│ │ -        self.user_indices = tf.placeholder(tf.int32, shape=[None])
│ │ -        self.item_indices = tf.placeholder(tf.int32, shape=[None])
│ │ -        self.user_interacted_seq = tf.placeholder(
│ │ -            tf.int32, shape=[None, self.max_seq_len]
│ │ -        )
│ │ -        self.user_interacted_len = tf.placeholder(tf.float32, shape=[None])
│ │ -        self.labels = tf.placeholder(tf.float32, shape=[None])
│ │ +        # item_indices actually serve as labels in `YouTubeRetrieval` model
│ │ +        self.item_indices = tf.placeholder(tf.int64, shape=[None])
│ │          self.is_training = tf.placeholder_with_default(False, shape=[])
│ │          self.concat_embed = []
│ │  
│ │ -        user_features = tf.get_variable(
│ │ -            name="user_features",
│ │ -            shape=[self.n_users + 1, self.embed_size],
│ │ -            initializer=tf.glorot_uniform_initializer(),
│ │ -            regularizer=self.reg,
│ │ -        )
│ │ -        item_features = tf.get_variable(
│ │ -            name="item_features",
│ │ -            shape=[self.n_items + 1, self.embed_size],
│ │ -            initializer=tf.glorot_uniform_initializer(),
│ │ -            regularizer=self.reg,
│ │ -        )
│ │ -        user_embed = tf.nn.embedding_lookup(user_features, self.user_indices)
│ │ -        item_embed = tf.nn.embedding_lookup(item_features, self.item_indices)
│ │ -
│ │ -        # unknown items are padded to 0-vector
│ │ -        zero_padding_op = tf.scatter_update(
│ │ -            item_features, self.n_items, tf.zeros([self.embed_size], dtype=tf.float32)
│ │ -        )
│ │ -        with tf.control_dependencies([zero_padding_op]):
│ │ -            # B * seq * K
│ │ -            multi_item_embed = tf.nn.embedding_lookup(
│ │ -                item_features, self.user_interacted_seq
│ │ -            )
│ │ -        pooled_embed = tf.div_no_nan(
│ │ -            tf.reduce_sum(multi_item_embed, axis=1),
│ │ -            tf.expand_dims(tf.sqrt(self.user_interacted_len), axis=1),
│ │ -        )
│ │ -        self.concat_embed.extend([user_embed, item_embed, pooled_embed])
│ │ -
│ │ +        self._build_item_interaction()
│ │ +        self._build_variables()
│ │          if self.sparse:
│ │              self._build_sparse()
│ │          if self.dense:
│ │              self._build_dense()
│ │  
│ │ -        concat_embed = tf.concat(self.concat_embed, axis=1)
│ │ -        mlp_layer = dense_nn(
│ │ -            concat_embed,
│ │ +        concat_features = tf.concat(self.concat_embed, axis=1)
│ │ +        self.user_vector_repr = dense_nn(
│ │ +            concat_features,
│ │              self.hidden_units,
│ │              use_bn=self.use_bn,
│ │              dropout_rate=self.dropout_rate,
│ │              is_training=self.is_training,
│ │          )
│ │ -        self.output = tf.reshape(tf_dense(units=1)(mlp_layer), [-1])
│ │          count_params()
│ │  
│ │ +    def _build_item_interaction(self):
│ │ +        self.item_interaction_indices = tf.placeholder(tf.int64, shape=[None, 2])
│ │ +        self.item_interaction_values = tf.placeholder(tf.int32, shape=[None])
│ │ +        # `batch_size` may change during training, especially first item in sequence
│ │ +        self.modified_batch_size = tf.placeholder(tf.int32, shape=[])
│ │ +
│ │ +        item_interaction_features = tf.get_variable(
│ │ +            name="item_interaction_features",
│ │ +            shape=[self.n_items, self.embed_size],
│ │ +            initializer=tf.glorot_uniform_initializer(),
│ │ +            regularizer=self.reg,
│ │ +        )
│ │ +        sparse_item_interaction = tf.SparseTensor(
│ │ +            self.item_interaction_indices,
│ │ +            self.item_interaction_values,
│ │ +            [self.modified_batch_size, self.n_items],
│ │ +        )
│ │ +        pooled_embed = tf.nn.safe_embedding_lookup_sparse(
│ │ +            item_interaction_features,
│ │ +            sparse_item_interaction,
│ │ +            sparse_weights=None,
│ │ +            combiner="sqrtn",
│ │ +            default_id=None,
│ │ +        )  # unknown user will return 0-vector
│ │ +        self.concat_embed.append(pooled_embed)
│ │ +
│ │      def _build_sparse(self):
│ │          self.sparse_indices = tf.placeholder(
│ │              tf.int32, shape=[None, self.sparse_field_size]
│ │          )
│ │          sparse_features = tf.get_variable(
│ │              name="sparse_features",
│ │              shape=[self.sparse_feature_size, self.embed_size],
│ │              initializer=tf.glorot_uniform_initializer(),
│ │              regularizer=self.reg,
│ │          )
│ │  
│ │ -        if self.data_info.multi_sparse_combine_info and self.multi_sparse_combiner in (
│ │ -            "sum",
│ │ -            "mean",
│ │ -            "sqrtn",
│ │ +        if (
│ │ +            self.data_info.multi_sparse_combine_info
│ │ +            and self.multi_sparse_combiner in ("sum", "mean", "sqrtn")
│ │          ):
│ │              sparse_embed = multi_sparse_combine_embedding(
│ │                  self.data_info,
│ │                  sparse_features,
│ │                  self.sparse_indices,
│ │                  self.multi_sparse_combiner,
│ │                  self.embed_size,
│ │ @@ -256,25 +253,74 @@
│ │          dense_features = tf.get_variable(
│ │              name="dense_features",
│ │              shape=[self.dense_field_size, self.embed_size],
│ │              initializer=tf.glorot_uniform_initializer(),
│ │              regularizer=self.reg,
│ │          )
│ │  
│ │ -        dense_embed = tf.tile(dense_features, [batch_size, 1])
│ │ -        dense_embed = tf.reshape(
│ │ -            dense_embed, [-1, self.dense_field_size, self.embed_size]
│ │ -        )
│ │ +        dense_embed = tf.expand_dims(dense_features, axis=0)
│ │ +        # B * F2 * K
│ │ +        dense_embed = tf.tile(dense_embed, [batch_size, 1, 1])
│ │          dense_embed = tf.multiply(dense_embed, dense_values_reshape)
│ │          dense_embed = tf.reshape(
│ │              dense_embed, [-1, self.dense_field_size * self.embed_size]
│ │          )
│ │          self.concat_embed.append(dense_embed)
│ │  
│ │ -    def _set_recent_seqs(self):
│ │ -        recent_seqs, recent_seq_lens = get_user_last_interacted(
│ │ -            self.n_users, self.user_consumed, self.n_items, self.max_seq_len
│ │ +    def _build_variables(self):
│ │ +        self.nce_weights = tf.get_variable(
│ │ +            name="nce_weights",
│ │ +            # n_classes, embed_size
│ │ +            shape=[self.n_items, self.embed_size],
│ │ +            initializer=tf.glorot_uniform_initializer(),
│ │ +            regularizer=self.reg,
│ │          )
│ │ -        oov = np.full(self.max_seq_len, self.n_items, dtype=np.int32)
│ │ -        recent_seqs = np.vstack([recent_seqs, oov])
│ │ -        recent_seq_lens = np.append(recent_seq_lens, [1])
│ │ -        return recent_seqs, recent_seq_lens
│ │ +        self.nce_biases = tf.get_variable(
│ │ +            name="nce_biases",
│ │ +            shape=[self.n_items],
│ │ +            initializer=tf.zeros_initializer(),
│ │ +            regularizer=self.reg,
│ │ +            trainable=True,
│ │ +        )
│ │ +
│ │ +    def _set_recent_seqs(self):
│ │ +        interacted_indices = []
│ │ +        interacted_items = []
│ │ +        for u in range(self.n_users):
│ │ +            u_consumed_items = self.user_consumed[u]
│ │ +            u_items_len = len(u_consumed_items)
│ │ +            if u_items_len < self.max_seq_len:
│ │ +                interacted_indices.extend([u] * u_items_len)
│ │ +                interacted_items.extend(u_consumed_items)
│ │ +            else:
│ │ +                interacted_indices.extend([u] * self.max_seq_len)
│ │ +                interacted_items.extend(u_consumed_items[-self.max_seq_len:])
│ │ +
│ │ +        interacted_indices = np.array(interacted_indices).reshape(-1, 1)
│ │ +        indices = np.concatenate(
│ │ +            [interacted_indices, np.zeros_like(interacted_indices)], axis=1
│ │ +        )
│ │ +        return indices, interacted_items
│ │ +
│ │ +    def set_embeddings(self):
│ │ +        feed_dict = {
│ │ +            self.item_interaction_indices: self.recent_seq_indices,
│ │ +            self.item_interaction_values: self.recent_seq_values,
│ │ +            self.modified_batch_size: self.n_users,
│ │ +            self.is_training: False,
│ │ +        }
│ │ +        if self.sparse:
│ │ +            # remove oov
│ │ +            user_sparse_indices = self.data_info.user_sparse_unique[:-1]
│ │ +            feed_dict.update({self.sparse_indices: user_sparse_indices})
│ │ +        if self.dense:
│ │ +            user_dense_values = self.data_info.user_dense_unique[:-1]
│ │ +            feed_dict.update({self.dense_values: user_dense_values})
│ │ +
│ │ +        user_vector = self.sess.run(self.user_vector_repr, feed_dict)
│ │ +        item_weights = self.sess.run(self.nce_weights)
│ │ +        item_biases = self.sess.run(self.nce_biases)
│ │ +
│ │ +        user_bias = np.ones([len(user_vector), 1], dtype=user_vector.dtype)
│ │ +        item_bias = item_biases[:, None]
│ │ +        self.user_embed = np.hstack([user_vector, user_bias])
│ │ +        self.item_embed = np.hstack([item_weights, item_bias])
│ │   --- LibRecommender-1.0.1/libreco/bases/base.py
│ ├── +++ LibRecommender-1.1.0/libreco/bases/base.py
│ │┄ Files 3% similar despite different names
│ │ @@ -41,21 +41,23 @@
│ │          elif task != "ranking":
│ │              raise ValueError("task must either be rating or ranking")
│ │  
│ │          self.default_pred = data_info.global_mean if task == "rating" else 0.0
│ │          self.default_recs = None
│ │  
│ │      @abc.abstractmethod
│ │ -    def fit(self, train_data, **kwargs):
│ │ +    def fit(self, train_data, neg_sampling, **kwargs):
│ │          """Fit model on the training data.
│ │  
│ │          Parameters
│ │          ----------
│ │          train_data : :class:`~libreco.data.TransformedSet` object
│ │              Data object used for training.
│ │ +        neg_sampling : bool
│ │ +            Whether to perform negative sampling for training or evaluating data.
│ │          """
│ │          raise NotImplementedError
│ │  
│ │      @abc.abstractmethod
│ │      def predict(self, user, item, **kwargs):
│ │          """Predict score for given user and item.
│ │   --- LibRecommender-1.0.1/libreco/bases/cf_base.py
│ ├── +++ LibRecommender-1.1.0/libreco/bases/cf_base.py
│ │┄ Files 2% similar despite different names
│ │ @@ -7,22 +7,23 @@
│ │  from operator import itemgetter
│ │  
│ │  import numpy as np
│ │  from scipy.sparse import issparse
│ │  from scipy.sparse import load_npz as load_sparse
│ │  from scipy.sparse import save_npz as save_sparse
│ │  
│ │ -from ..bases import Base
│ │ +from .base import Base
│ │  from ..evaluation import print_metrics
│ │ +from ..prediction.preprocess import convert_id
│ │  from ..recommendation import construct_rec, popular_recommendations
│ │  from ..recommendation.ranking import filter_items
│ │  from ..utils.misc import colorize, time_block
│ │  from ..utils.save_load import load_params, save_params
│ │  from ..utils.similarities import cosine_sim, jaccard_sim, pearson_sim
│ │ -from ..utils.validate import check_unknown, check_unknown_user, convert_id
│ │ +from ..utils.validate import check_fitting, check_unknown, check_unknown_user
│ │  
│ │  
│ │  class CfBase(Base):
│ │      """Base class for CF models.
│ │  
│ │      Parameters
│ │      ----------
│ │ @@ -47,14 +48,19 @@
│ │          Number of minimum common users to consider when computing similarities.
│ │      mode : {'forward', 'invert'}, default: 'invert'
│ │          Whether to use forward index or invert index.
│ │      seed : int, default: 42
│ │          Random seed.
│ │      lower_upper_bound : tuple or None, default: None
│ │          Lower and upper score bound for `rating` task.
│ │ +
│ │ +    See Also
│ │ +    --------
│ │ +    ~libreco.algorithms.UserCF
│ │ +    ~libreco.algorithms.ItemCF
│ │      """
│ │  
│ │      def __init__(
│ │          self,
│ │          task,
│ │          data_info,
│ │          cf_type,
│ │ @@ -98,27 +104,33 @@
│ │          if self.task == "rating" and self.sim_type == "jaccard":
│ │              caution_str = "Warning: jaccard is not suitable for explicit data"
│ │              print(f"{colorize(caution_str, 'red')}")
│ │  
│ │      def fit(
│ │          self,
│ │          train_data,
│ │ +        neg_sampling,
│ │          verbose=1,
│ │          eval_data=None,
│ │          metrics=None,
│ │          k=10,
│ │          eval_batch_size=8192,
│ │          eval_user_num=None,
│ │      ):
│ │          """Fit CF model on the training data.
│ │  
│ │          Parameters
│ │          ----------
│ │          train_data : :class:`~libreco.data.TransformedSet` object
│ │              Data object used for training.
│ │ +        neg_sampling : bool
│ │ +            Whether to perform negative sampling for evaluating data.
│ │ +
│ │ +            .. versionadded:: 1.1.0
│ │ +
│ │          verbose : int, default: 1
│ │              Print verbosity. If `eval_data` is provided, setting it to higher than 1
│ │              will print evaluation metrics during training.
│ │          eval_data : :class:`~libreco.data.TransformedSet` object, default: None
│ │              Data object used for evaluating.
│ │          metrics : list or None, default: None
│ │              List of metrics for evaluating.
│ │ @@ -126,14 +138,15 @@
│ │              Parameter of metrics, e.g. recall at k, ndcg at k
│ │          eval_batch_size : int, default: 8192
│ │              Batch size for evaluating.
│ │          eval_user_num : int or None, default: None
│ │              Number of users for evaluating. Setting it to a positive number will sample
│ │              users randomly from eval data.
│ │          """
│ │ +        check_fitting(self, train_data, eval_data, neg_sampling, k)
│ │          self.show_start_time()
│ │          self.user_interaction = train_data.sparse_interaction
│ │          self.item_interaction = self.user_interaction.T.tocsr()
│ │  
│ │          with time_block("sim_matrix", verbose=1):
│ │              if self.sim_type == "cosine":
│ │                  sim_func = cosine_sim
│ │ @@ -179,14 +192,15 @@
│ │              )
│ │          if self.store_top_k:
│ │              self.compute_top_k()
│ │  
│ │          if verbose > 1:
│ │              print_metrics(
│ │                  model=self,
│ │ +                neg_sampling=neg_sampling,
│ │                  eval_data=eval_data,
│ │                  metrics=metrics,
│ │                  eval_batch_size=eval_batch_size,
│ │                  k=k,
│ │                  sample_user_num=eval_user_num,
│ │                  seed=self.seed,
│ │              )
│ │ @@ -302,15 +316,15 @@
│ │          n_rec,
│ │          consumed,
│ │          filter_consumed,
│ │          random_rec,
│ │      ):
│ │          if filter_consumed:
│ │              ids, preds = filter_items(ids, preds, consumed)
│ │ -        if len(ids) == 0:
│ │ +        if len(ids) == 0:  # pragma: no cover
│ │              self.print_count += 1
│ │              no_str = (
│ │                  f"no suitable recommendation for user {user}, "
│ │                  f"return default recommendation"
│ │              )
│ │              if self.print_count < 11:
│ │                  print(f"{colorize(no_str, 'red')}")
│ │   --- LibRecommender-1.0.1/libreco/bases/embed_base.py
│ ├── +++ LibRecommender-1.1.0/libreco/bases/embed_base.py
│ │┄ Files 4% similar despite different names
│ │ @@ -1,34 +1,36 @@
│ │  """Embed model base class."""
│ │  import abc
│ │  import os
│ │  from operator import itemgetter
│ │  
│ │  import numpy as np
│ │  
│ │ +from .base import Base
│ │  from ..prediction import predict_from_embedding
│ │  from ..recommendation import cold_start_rec, construct_rec, recommend_from_embedding
│ │ -from ..training import get_trainer
│ │ +from ..training.dispatch import get_trainer
│ │ +from ..utils.constants import SEQUENCE_RECOMMEND_MODELS
│ │  from ..utils.misc import colorize
│ │  from ..utils.save_load import (
│ │      load_default_recs,
│ │      load_params,
│ │      save_default_recs,
│ │      save_params,
│ │      save_tf_variables,
│ │      save_torch_state_dict,
│ │  )
│ │ -from ..utils.validate import check_unknown_user
│ │ -from .base import Base
│ │ +from ..utils.validate import check_fitting, check_unknown_user
│ │  
│ │  
│ │  class EmbedBase(Base):
│ │      """Base class for embed models.
│ │  
│ │ -    Models that can generate user and item embeddings.
│ │ +    Models that can generate user and item embeddings for inference.
│ │ +    See `algorithm list <https://github.com/massquantity/LibRecommender#references>`_.
│ │  
│ │      Parameters
│ │      ----------
│ │      task : {'rating', 'ranking'}
│ │          Recommendation task. See :ref:`Task`.
│ │      data_info : :class:`~libreco.data.DataInfo` object
│ │          Object that contains useful information for training and inference.
│ │ @@ -55,40 +57,42 @@
│ │          self.trainer = None
│ │          self.loaded = False
│ │  
│ │      @abc.abstractmethod
│ │      def build_model(self):
│ │          raise NotImplementedError
│ │  
│ │ -    def check_attribute(self, eval_data, k):
│ │ -        if hasattr(self, "loaded") and self.loaded:
│ │ -            raise RuntimeError(
│ │ -                "Loaded model doesn't support retraining, use `rebuild_model` instead. "
│ │ -                "Or constructing a new model from scratch."
│ │ -            )
│ │ -        if eval_data is not None and k > self.n_items:
│ │ -            raise ValueError(f"eval `k` {k} exceeds num of items {self.n_items}")
│ │ -
│ │      def fit(
│ │          self,
│ │          train_data,
│ │ +        neg_sampling,
│ │          verbose=1,
│ │          shuffle=True,
│ │          eval_data=None,
│ │          metrics=None,
│ │          k=10,
│ │          eval_batch_size=8192,
│ │          eval_user_num=None,
│ │ +        num_workers=0,
│ │      ):
│ │          """Fit embed model on the training data.
│ │  
│ │          Parameters
│ │          ----------
│ │          train_data : :class:`~libreco.data.TransformedSet` object
│ │              Data object used for training.
│ │ +        neg_sampling : bool
│ │ +            Whether to perform negative sampling for training or evaluating data.
│ │ +
│ │ +            .. versionadded:: 1.1.0
│ │ +
│ │ +            .. NOTE::
│ │ +               Negative sampling is needed if your data is implicit(i.e., `task` is ranking)
│ │ +               and ONLY contains positive labels. Otherwise, it should be False.
│ │ +
│ │          verbose : int, default: 1
│ │              Print verbosity. If `eval_data` is provided, setting it to higher than 1
│ │              will print evaluation metrics during training.
│ │          shuffle : bool, default: True
│ │              Whether to shuffle the training data.
│ │          eval_data : :class:`~libreco.data.TransformedSet` object, default: None
│ │              Data object used for evaluating.
│ │ @@ -97,46 +101,60 @@
│ │          k : int, default: 10
│ │              Parameter of metrics, e.g. recall at k, ndcg at k
│ │          eval_batch_size : int, default: 8192
│ │              Batch size for evaluating.
│ │          eval_user_num : int or None, default: None
│ │              Number of users for evaluating. Setting it to a positive number will sample
│ │              users randomly from eval data.
│ │ +        num_workers : int, default: 0
│ │ +            How many subprocesses to use for training data loading.
│ │ +            0 means that the data will be loaded in the main process,
│ │ +            which is slower than multiprocessing.
│ │ +
│ │ +            .. versionadded:: 1.1.0
│ │ +
│ │ +            .. CAUTION::
│ │ +               Using multiprocessing(``num_workers`` > 0) may consume more memory than
│ │ +               single processing. See `Multi-process data loading <https://pytorch.org/docs/stable/data.html#multi-process-data-loading>`_.
│ │  
│ │          Raises
│ │          ------
│ │          RuntimeError
│ │              If :py:func:`fit` is called from a loaded model(:py:func:`load`).
│ │ +        AssertionError
│ │ +            If ``neg_sampling`` parameter is not bool type.
│ │          """
│ │ -        self.check_attribute(eval_data, k)
│ │ +        check_fitting(self, train_data, eval_data, neg_sampling, k)
│ │          self.show_start_time()
│ │          if not self.model_built:
│ │              self.build_model()
│ │              self.model_built = True
│ │          if self.trainer is None:
│ │              self.trainer = get_trainer(self)
│ │          self.trainer.run(
│ │              train_data,
│ │ +            neg_sampling,
│ │              verbose,
│ │              shuffle,
│ │              eval_data,
│ │              metrics,
│ │              k,
│ │              eval_batch_size,
│ │              eval_user_num,
│ │ +            num_workers,
│ │          )
│ │          self.set_embeddings()
│ │          self.assign_embedding_oov()
│ │          self.default_recs = recommend_from_embedding(
│ │ -            task=self.task,
│ │ +            model=self,
│ │              user_ids=[self.n_users],
│ │              n_rec=min(2000, self.n_items),
│ │ -            data_info=self.data_info,
│ │ -            user_embed=self.user_embed,
│ │ -            item_embed=self.item_embed,
│ │ +            user_embeddings=self.user_embed,
│ │ +            item_embeddings=self.item_embed,
│ │ +            seq=None,
│ │              filter_consumed=False,
│ │              random_rec=False,
│ │          ).flatten()
│ │  
│ │      def predict(self, user, item, cold_start="average", inner_id=False):
│ │          """Make prediction(s) on given user(s) and item(s).
│ │  
│ │ @@ -164,27 +182,35 @@
│ │          """
│ │          return predict_from_embedding(self, user, item, cold_start, inner_id)
│ │  
│ │      def recommend_user(
│ │          self,
│ │          user,
│ │          n_rec,
│ │ +        seq=None,
│ │          cold_start="average",
│ │          inner_id=False,
│ │          filter_consumed=True,
│ │          random_rec=False,
│ │      ):
│ │          """Recommend a list of items for given user(s).
│ │  
│ │          Parameters
│ │          ----------
│ │          user : int or str or array_like
│ │              User id or batch of user ids to recommend.
│ │          n_rec : int
│ │              Number of recommendations to return.
│ │ +        seq : list or numpy.ndarray
│ │ +            Extra item sequence for recommendation. If the sequence length is larger than
│ │ +            `recent_num` hyperparameter specified in the model, it will be truncated.
│ │ +            If it is smaller, it will be padded.
│ │ +
│ │ +            .. versionadded:: 1.1.0
│ │ +
│ │          cold_start : {'popular', 'average'}, default: 'average'
│ │              Cold start strategy.
│ │  
│ │              - 'popular' will sample from popular items.
│ │              - 'average' will use the average of all the user/item embeddings as the
│ │                representation of the cold-start user/item.
│ │  
│ │ @@ -195,46 +221,56 @@
│ │              Whether to filter out items that a user has previously consumed.
│ │          random_rec : bool, default: False
│ │              Whether to choose items for recommendation based on their prediction scores.
│ │  
│ │          Returns
│ │          -------
│ │          recommendation : dict of {Union[int, str, array_like] : numpy.ndarray}
│ │ -            Recommendation result with user ids as keys
│ │ -            and array_like recommended items as values.
│ │ +            Recommendation result with user ids as keys and array_like recommended items as values.
│ │          """
│ │ +        if seq is not None:
│ │ +            if self.model_name not in SEQUENCE_RECOMMEND_MODELS:
│ │ +                raise ValueError(
│ │ +                    f"`{self.model_name}` doesn't support arbitrary seq recommendation."
│ │ +                )
│ │ +            if not np.isscalar(user) and len(user) > 1:
│ │ +                raise ValueError(
│ │ +                    f"Batch recommend doesn't support arbitrary item sequence: {user}"
│ │ +                )
│ │ +
│ │          result_recs = dict()
│ │          user_ids, unknown_users = check_unknown_user(self.data_info, user, inner_id)
│ │          if unknown_users:
│ │              cold_recs = cold_start_rec(
│ │                  self.data_info,
│ │                  self.default_recs,
│ │                  cold_start,
│ │                  unknown_users,
│ │                  n_rec,
│ │                  inner_id,
│ │              )
│ │              result_recs.update(cold_recs)
│ │          if user_ids:
│ │              computed_recs = recommend_from_embedding(
│ │ -                self.task,
│ │ +                self,
│ │                  user_ids,
│ │                  n_rec,
│ │ -                self.data_info,
│ │                  self.user_embed,
│ │                  self.item_embed,
│ │ +                seq,
│ │                  filter_consumed,
│ │                  random_rec,
│ │ +                inner_id,
│ │              )
│ │              user_recs = construct_rec(self.data_info, user_ids, computed_recs, inner_id)
│ │              result_recs.update(user_recs)
│ │          return result_recs
│ │  
│ │      @abc.abstractmethod
│ │ -    def set_embeddings(self, *args, **kwargs):
│ │ +    def set_embeddings(self):
│ │          pass
│ │  
│ │      def assign_embedding_oov(self):
│ │          for v_name in ("user_embed", "item_embed"):
│ │              embed = getattr(self, v_name)
│ │              if embed.ndim == 1:
│ │                  new_embed = np.append(embed, np.mean(embed))
│ │ @@ -301,16 +337,16 @@
│ │          """
│ │          variable_path = os.path.join(path, f"{model_name}.npz")
│ │          variables = np.load(variable_path)
│ │          hparams = load_params(path, data_info, model_name)
│ │          model = cls(**hparams)
│ │          model.loaded = True
│ │          model.default_recs = load_default_recs(path, model_name)
│ │ -        setattr(model, "user_embed", variables["user_embed"])
│ │ -        setattr(model, "item_embed", variables["item_embed"])
│ │ +        model.user_embed = variables["user_embed"]
│ │ +        model.item_embed = variables["item_embed"]
│ │          return model
│ │  
│ │      def get_user_id(self, user):
│ │          if user not in self.data_info.user2id:
│ │              raise ValueError(f"unknown user: {user}")
│ │          return self.data_info.user2id[user]
│ │   --- LibRecommender-1.0.1/libreco/bases/gensim_base.py
│ ├── +++ LibRecommender-1.1.0/libreco/bases/gensim_base.py
│ │┄ Files 4% similar despite different names
│ │ @@ -1,21 +1,27 @@
│ │  import abc
│ │  import os
│ │  
│ │  import numpy as np
│ │  from gensim.models import Word2Vec
│ │  
│ │ +from .embed_base import EmbedBase
│ │  from ..evaluation import print_metrics
│ │  from ..recommendation import recommend_from_embedding
│ │  from ..utils.misc import time_block
│ │  from ..utils.save_load import save_default_recs, save_params
│ │ -from .embed_base import EmbedBase
│ │ +from ..utils.validate import check_fitting
│ │  
│ │  
│ │  class GensimBase(EmbedBase):
│ │ +    """Base class for models that use Gensim for training.
│ │ +
│ │ +    Including Item2Vec and Deepwalk.
│ │ +    """
│ │ +
│ │      def __init__(
│ │          self,
│ │          task,
│ │          data_info,
│ │          embed_size=16,
│ │          norm_embed=False,
│ │          window_size=5,
│ │ @@ -36,50 +42,53 @@
│ │      @abc.abstractmethod
│ │      def get_data(self):
│ │          raise NotImplementedError
│ │  
│ │      def fit(
│ │          self,
│ │          train_data,
│ │ +        neg_sampling,
│ │          verbose=1,
│ │          shuffle=True,
│ │          eval_data=None,
│ │          metrics=None,
│ │          k=10,
│ │          eval_batch_size=8192,
│ │          eval_user_num=None,
│ │ +        **kwargs,
│ │      ):
│ │ -        self.check_attribute(eval_data, k)
│ │ +        check_fitting(self, train_data, eval_data, neg_sampling, k)
│ │          self.show_start_time()
│ │          if self.data is None:
│ │              self.data = self.get_data()
│ │          if self.gensim_model is None:
│ │              self.gensim_model = self.build_model()
│ │          with time_block("gensim word2vec training", verbose):
│ │              self.gensim_model.train(
│ │                  self.data,
│ │                  total_examples=self.gensim_model.corpus_count,
│ │                  epochs=self.n_epochs,
│ │              )
│ │          self.set_embeddings()
│ │          self.assign_embedding_oov()
│ │          self.default_recs = recommend_from_embedding(
│ │ -            task=self.task,
│ │ +            model=self,
│ │              user_ids=[self.n_users],
│ │              n_rec=min(2000, self.n_items),
│ │ -            data_info=self.data_info,
│ │ -            user_embed=self.user_embed,
│ │ -            item_embed=self.item_embed,
│ │ +            user_embeddings=self.user_embed,
│ │ +            item_embeddings=self.item_embed,
│ │ +            seq=None,
│ │              filter_consumed=False,
│ │              random_rec=False,
│ │          ).flatten()
│ │  
│ │          if verbose > 1:
│ │              print_metrics(
│ │                  model=self,
│ │ +                neg_sampling=neg_sampling,
│ │                  eval_data=eval_data,
│ │                  metrics=metrics,
│ │                  eval_batch_size=eval_batch_size,
│ │                  k=k,
│ │                  sample_user_num=eval_user_num,
│ │                  seed=self.seed,
│ │              )
│ │   --- LibRecommender-1.0.1/libreco/bases/meta.py
│ ├── +++ LibRecommender-1.1.0/libreco/bases/meta.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/data/__init__.py
│ ├── +++ LibRecommender-1.1.0/libreco/data/__init__.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/data/data_generator.py
│ ├── +++ LibRecommender-1.1.0/libreco/graph/neighbor_walk.py
│ │┄ Files 26% similar despite different names
│ │ @@ -1,167 +1,173 @@
│ │ -import numpy as np
│ │ -import tqdm
│ │ +import torch
│ │  
│ │ -from .sequence import sparse_user_interacted, user_interacted_seq
│ │ +from .message import ItemMessage, ItemMessageDGL, UserMessage
│ │ +from ..sampling import bipartite_neighbors, bipartite_neighbors_with_weights
│ │  
│ │  
│ │ -class DataGenPure(object):
│ │ -    def __init__(self, data):
│ │ -        self.data_size = len(data)
│ │ -        self.user_indices = data.user_indices
│ │ -        self.item_indices = data.item_indices
│ │ -        self.labels = data.labels
│ │ -
│ │ -    def __iter__(self, batch_size):
│ │ -        for i in tqdm.trange(0, self.data_size, batch_size, desc="train"):
│ │ -            batch_slice = slice(i, i + batch_size)
│ │ -            yield (
│ │ -                self.user_indices[batch_slice],
│ │ -                self.item_indices[batch_slice],
│ │ -                self.labels[batch_slice],
│ │ -                None,
│ │ -                None,
│ │ -            )
│ │ -
│ │ -    def __call__(self, shuffle=True, batch_size=None):
│ │ -        if shuffle:
│ │ -            mask = np.random.permutation(range(self.data_size))
│ │ -            self.user_indices = self.user_indices[mask]
│ │ -            self.item_indices = self.item_indices[mask]
│ │ -            self.labels = self.labels[mask]
│ │ -        return self.__iter__(batch_size)
│ │ -
│ │ -
│ │ -class DataGenFeat(object):
│ │ -    def __init__(self, data, sparse, dense, class_name=None):
│ │ -        self.user_indices = data.user_indices
│ │ -        self.item_indices = data.item_indices
│ │ -        self.labels = data.labels
│ │ -        self.sparse_indices = data.sparse_indices
│ │ -        self.dense_values = data.dense_values
│ │ -        self.sparse = sparse
│ │ -        self.dense = dense
│ │ -        self.data_size = len(data)
│ │ -        self.class_name = class_name
│ │ -
│ │ -    def __iter__(self, batch_size):
│ │ -        for i in tqdm.trange(0, self.data_size, batch_size, desc="train"):
│ │ -            batch_slice = slice(i, i + batch_size)
│ │ -            pure_part = (
│ │ -                self.user_indices[batch_slice],
│ │ -                self.item_indices[batch_slice],
│ │ -                self.labels[batch_slice],
│ │ -            )
│ │ -            sparse_part = (
│ │ -                (self.sparse_indices[batch_slice],) if self.sparse else (None,)
│ │ -            )
│ │ -            dense_part = (self.dense_values[batch_slice],) if self.dense else (None,)
│ │ -            yield pure_part + sparse_part + dense_part
│ │ -
│ │ -    def __call__(self, shuffle=True, batch_size=None):
│ │ -        if shuffle:
│ │ -            mask = np.random.permutation(range(self.data_size))
│ │ -            if self.sparse:
│ │ -                self.sparse_indices = self.sparse_indices[mask]
│ │ -            if self.dense:
│ │ -                self.dense_values = self.dense_values[mask]
│ │ -            self.user_indices = self.user_indices[mask]
│ │ -            self.item_indices = self.item_indices[mask]
│ │ -            self.labels = self.labels[mask]
│ │ -        return self.__iter__(batch_size)
│ │ -
│ │ -
│ │ -class DataGenSequence(object):
│ │ -    def __init__(self, data, data_info, sparse, dense, mode, num, padding_idx):
│ │ +class NeighborWalker:
│ │ +    def __init__(self, model, data_info):
│ │ +        self.paradigm = model.paradigm
│ │ +        self.num_layers = model.num_layers
│ │ +        self.num_neighbors = model.num_neighbors
│ │ +        self.remove_edges = model.remove_edges
│ │          self.user_consumed = data_info.user_consumed
│ │ -        self.padding_idx = padding_idx
│ │ -        self.user_indices = data.user_indices
│ │ -        self.item_indices = data.item_indices
│ │ -        self.labels = data.labels
│ │ -        self.sparse_indices = data.sparse_indices
│ │ -        self.dense_values = data.dense_values
│ │ -        self.user_consumed_set = {
│ │ -            u: set(items) for u, items in self.user_consumed.items()
│ │ -        }
│ │ -        self.data_size = len(self.user_indices)
│ │ -        self.sparse = sparse
│ │ -        self.dense = dense
│ │ -        self.mode = mode
│ │ -        self.num = num
│ │ -        self.np_rng = data_info.np_rng
│ │ -
│ │ -    def __iter__(self, batch_size):
│ │ -        for i in tqdm.trange(0, self.data_size, batch_size, desc="train"):
│ │ -            batch_slice = slice(i, i + batch_size)
│ │ -            (batch_interacted, batch_interacted_len) = user_interacted_seq(
│ │ -                self.user_indices[batch_slice],
│ │ -                self.item_indices[batch_slice],
│ │ +        self.item_consumed = data_info.item_consumed
│ │ +        self.user_sparse_unique = data_info.user_sparse_unique
│ │ +        self.user_dense_unique = data_info.user_dense_unique
│ │ +        self.item_sparse_unique = data_info.item_sparse_unique
│ │ +        self.item_dense_unique = data_info.item_dense_unique
│ │ +        self.use_pinsage = "PinSage" in model.model_name
│ │ +        if self.use_pinsage:
│ │ +            self.num_walks = model.num_walks
│ │ +            self.walk_length = model.neighbor_walk_len
│ │ +            self.termination_prob = model.termination_prob
│ │ +
│ │ +    def __call__(self, items, items_pos=None):
│ │ +        nodes, sparse, dense = self.get_item_feats(items)
│ │ +        nbs, nbs_sparse, nbs_dense, offsets, weights = (
│ │ +            self.sample_pinsage(items, items_pos)
│ │ +            if self.use_pinsage
│ │ +            else self.sample_graphsage(items)
│ │ +        )
│ │ +        return ItemMessage(
│ │ +            nodes, sparse, dense, nbs, nbs_sparse, nbs_dense, offsets, weights
│ │ +        )
│ │ +
│ │ +    def sample_graphsage(self, items):
│ │ +        neighbors, neighbors_sparse, neighbors_dense, offsets = [], [], [], []
│ │ +        nodes = items
│ │ +        weights = None
│ │ +        for _ in range(self.num_layers):
│ │ +            nbs, offs = bipartite_neighbors(
│ │ +                nodes, self.user_consumed, self.item_consumed, self.num_neighbors
│ │ +            )
│ │ +            nbs, sparse, dense = self.get_item_feats(nbs)
│ │ +            neighbors.append(nbs)
│ │ +            neighbors_sparse.append(sparse)
│ │ +            neighbors_dense.append(dense)
│ │ +            offsets.append(offs)
│ │ +            nodes = nbs
│ │ +        return neighbors, neighbors_sparse, neighbors_dense, offsets, weights
│ │ +
│ │ +    def sample_pinsage(self, items, items_pos=None):
│ │ +        neighbors, neighbors_sparse, neighbors_dense = [], [], []
│ │ +        offsets, weights = [], []
│ │ +        nodes = items
│ │ +        if self.paradigm == "i2i" and self.remove_edges and items_pos is not None:
│ │ +            item_indices = list(range(len(items)))
│ │ +        else:
│ │ +            item_indices = None
│ │ +        for _ in range(self.num_layers):
│ │ +            nbs, ws, offs, item_indices_in_samples = bipartite_neighbors_with_weights(
│ │ +                nodes,
│ │                  self.user_consumed,
│ │ -                self.padding_idx,
│ │ -                self.mode,
│ │ -                self.num,
│ │ -                self.user_consumed_set,
│ │ -                self.np_rng,
│ │ -            )
│ │ -            pure_part = (
│ │ -                self.user_indices[batch_slice],
│ │ -                self.item_indices[batch_slice],
│ │ -                self.labels[batch_slice],
│ │ -            )
│ │ -            seq_part = (batch_interacted, batch_interacted_len)
│ │ -            sparse_part = (
│ │ -                (self.sparse_indices[batch_slice],) if self.sparse else (None,)
│ │ -            )
│ │ -            dense_part = (self.dense_values[batch_slice],) if self.dense else (None,)
│ │ -            yield pure_part + sparse_part + dense_part + seq_part
│ │ -
│ │ -    def __call__(self, shuffle=True, batch_size=None):
│ │ -        if shuffle:
│ │ -            mask = np.random.permutation(range(self.data_size))
│ │ -            if self.sparse:
│ │ -                self.sparse_indices = self.sparse_indices[mask]
│ │ -            if self.dense:
│ │ -                self.dense_values = self.dense_values[mask]
│ │ -            self.user_indices = self.user_indices[mask]
│ │ -            self.item_indices = self.item_indices[mask]
│ │ -            self.labels = self.labels[mask]
│ │ -        return self.__iter__(batch_size)
│ │ -
│ │ -
│ │ -class SparseTensorSequence(DataGenSequence):
│ │ -    def __init__(self, data, data_info, sparse, dense, mode, num, padding_idx):
│ │ -        super().__init__(data, data_info, sparse, dense, mode, num, padding_idx)
│ │ -        if data.has_sampled:
│ │ -            self.user_indices = data.user_indices_orig
│ │ -            self.item_indices = data.item_indices_orig
│ │ -            self.labels = data.labels_orig
│ │ -            self.sparse_indices = data.sparse_indices_orig
│ │ -            self.dense_values = data.dense_values_orig
│ │ -        self.data_size = len(self.user_indices)
│ │ -
│ │ -    def __iter__(self, batch_size):
│ │ -        for i in tqdm.trange(0, self.data_size, batch_size, desc="train"):
│ │ -            batch_slice = slice(i, i + batch_size)
│ │ -            (
│ │ -                interacted_indices,
│ │ -                interacted_values,
│ │ -                modified_batch_size,
│ │ -            ) = sparse_user_interacted(
│ │ -                self.user_indices[batch_slice],
│ │ -                self.item_indices[batch_slice],
│ │ -                self.user_consumed,
│ │ -                self.mode,
│ │ -                self.num,
│ │ -            )
│ │ -            pure_part = (
│ │ -                modified_batch_size,
│ │ -                interacted_indices,
│ │ -                interacted_values,
│ │ -                # self.user_indices[batch_slice],
│ │ -                self.item_indices[batch_slice],
│ │ -                # self.labels[batch_slice]
│ │ -            )
│ │ -            sparse_part = (
│ │ -                (self.sparse_indices[batch_slice],) if self.sparse else (None,)
│ │ -            )
│ │ -            dense_part = (self.dense_values[batch_slice],) if self.dense else (None,)
│ │ -            yield pure_part + sparse_part + dense_part
│ │ +                self.item_consumed,
│ │ +                self.num_neighbors,
│ │ +                self.num_walks,
│ │ +                self.walk_length,
│ │ +                items,
│ │ +                item_indices,
│ │ +                items_pos,
│ │ +                self.termination_prob,
│ │ +            )
│ │ +            nbs, sparse, dense = self.get_item_feats(nbs)
│ │ +            neighbors.append(nbs)
│ │ +            neighbors_sparse.append(sparse)
│ │ +            neighbors_dense.append(dense)
│ │ +            offsets.append(offs)
│ │ +            weights.append(ws)
│ │ +            nodes = nbs
│ │ +            item_indices = item_indices_in_samples
│ │ +        return neighbors, neighbors_sparse, neighbors_dense, offsets, weights
│ │ +
│ │ +    def get_item_feats(self, items):
│ │ +        if isinstance(items, torch.Tensor):
│ │ +            items = items.detach().cpu().numpy()
│ │ +        sparse, dense = None, None
│ │ +        if self.item_sparse_unique is not None:
│ │ +            sparse = self.item_sparse_unique[items]
│ │ +        if self.item_dense_unique is not None:
│ │ +            dense = self.item_dense_unique[items]
│ │ +        return items, sparse, dense
│ │ +
│ │ +    def get_user_feats(self, users):
│ │ +        if isinstance(users, torch.Tensor):
│ │ +            users = users.detach().cpu().numpy()
│ │ +        sparse, dense = None, None
│ │ +        if self.user_sparse_unique is not None:
│ │ +            sparse = self.user_sparse_unique[users]
│ │ +        if self.user_dense_unique is not None:
│ │ +            dense = self.user_dense_unique[users]
│ │ +        return UserMessage(users, sparse, dense)
│ │ +
│ │ +
│ │ +class NeighborWalkerDGL(NeighborWalker):
│ │ +    def __init__(self, model, data_info):
│ │ +        super().__init__(model, data_info)
│ │ +        self.graph = model.hetero_g if self.use_pinsage else model.homo_g
│ │ +
│ │ +    def __call__(self, items, target_nodes=None):
│ │ +        import dgl
│ │ +
│ │ +        # use torch tensor
│ │ +        blocks = self.transform_blocks(items, target_nodes)
│ │ +        start_nb_nodes = blocks[0].srcdata[dgl.NID]
│ │ +        start_nodes, nbs_sparse, nbs_dense = self.get_item_feats(start_nb_nodes)
│ │ +        return ItemMessageDGL(blocks, start_nodes, nbs_sparse, nbs_dense)
│ │ +
│ │ +    def sample_frontier(self, nodes):
│ │ +        import dgl
│ │ +
│ │ +        if self.use_pinsage:
│ │ +            sampler = dgl.sampling.PinSAGESampler(
│ │ +                self.graph,
│ │ +                ntype="item",
│ │ +                other_type="user",
│ │ +                num_traversals=self.walk_length,
│ │ +                termination_prob=self.termination_prob,
│ │ +                num_random_walks=self.num_walks,
│ │ +                num_neighbors=self.num_neighbors,
│ │ +            )
│ │ +            return sampler(nodes)
│ │ +        else:
│ │ +            return dgl.sampling.sample_neighbors(
│ │ +                g=self.graph,
│ │ +                nodes=nodes,
│ │ +                fanout=self.num_neighbors,
│ │ +                edge_dir="in",
│ │ +            )
│ │ +
│ │ +    def transform_blocks(self, nodes, target_nodes=None):
│ │ +        r"""Bipartite graph block: items(nodes) -> sampled neighbor nodes
│ │ +
│ │ +        -------------
│ │ +        |     / ... |
│ │ +        |    /  src |
│ │ +        |dst -  src |
│ │ +        |    \  src |
│ │ +        |     \ ... |
│ │ +        -------------
│ │ +        """
│ │ +        import dgl
│ │ +
│ │ +        blocks = []
│ │ +        for _ in range(self.num_layers):
│ │ +            frontier = self.sample_frontier(nodes)
│ │ +            if (
│ │ +                self.paradigm == "i2i"
│ │ +                and self.remove_edges
│ │ +                and target_nodes is not None
│ │ +            ):
│ │ +                heads_pos, heads_neg, tails_pos, tails_neg = target_nodes
│ │ +                eids = frontier.edge_ids(
│ │ +                    torch.cat([heads_pos, heads_neg]),
│ │ +                    torch.cat([tails_pos, tails_neg]),
│ │ +                    return_uv=True,
│ │ +                )[2]
│ │ +                if len(eids) > 0:
│ │ +                    frontier = dgl.remove_edges(frontier, eids)
│ │ +            block = dgl.to_block(frontier, dst_nodes=nodes)
│ │ +            nodes = block.srcdata[dgl.NID]
│ │ +            blocks.append(block)
│ │ +        blocks.reverse()
│ │ +        return blocks
│ │   --- LibRecommender-1.0.1/libreco/data/data_info.py
│ ├── +++ LibRecommender-1.1.0/libreco/data/data_info.py
│ │┄ Files 17% similar despite different names
│ │ @@ -1,61 +1,68 @@
│ │ -"""Class for Storing Various Data Information."""
│ │ +"""Classes for Storing Various Data Information."""
│ │  import inspect
│ │  import json
│ │  import os
│ │  from collections import namedtuple
│ │  from dataclasses import dataclass
│ │ -from typing import Iterable
│ │ +from typing import Any, Dict, Iterable, List
│ │  
│ │  import numpy as np
│ │  import pandas as pd
│ │ -from numpy.random import default_rng
│ │  
│ │ -from ..feature import check_oov, compute_sparse_feat_indices, interaction_consumed
│ │ +from .consumed import interaction_consumed
│ │ +from ..feature.update import (
│ │ +    get_row_id_masks,
│ │ +    update_new_dense_feats,
│ │ +    update_new_sparse_feats,
│ │ +)
│ │  
│ │  Feature = namedtuple("Feature", ["name", "index"])
│ │  
│ │  EmptyFeature = Feature(name=[], index=[])
│ │  
│ │  
│ │  # noinspection PyUnresolvedReferences
│ │  @dataclass
│ │  class MultiSparseInfo:
│ │ -    """:class:`dataclass` object for storing Multi-sparse features information.
│ │ +    """`dataclasses <https://docs.python.org/3/library/dataclasses.html>`_
│ │ +    for storing multi-sparse features information.
│ │  
│ │ -    A group of multi-sparse features are considered a "field".
│ │ -    e.g. ["genre1", "genre2", "genre3"] form a field "genre".
│ │ -    So this object contains fields' offset, field's length and fields' oov.
│ │ -    Since features belong to the same field share one oov.
│ │ +    A group of multi-sparse features are considered as a "field",
│ │ +    e.g., ("genre1", "genre2", "genre3") form a "genre" field,
│ │ +    and features belong to the same field share the same oov.
│ │  
│ │      Attributes
│ │      ----------
│ │      field_offset : list of int
│ │          All multi-sparse fields' offset in all expanded sparse features.
│ │ -    field_len: list of int
│ │ +    field_len : list of int
│ │          All multi-sparse fields' sizes.
│ │ -    feat_oov: numpy.ndarray
│ │ +    feat_oov : numpy.ndarray
│ │          All multi-sparse fields' oov.
│ │ +    pad_val : dict of {str : Any}
│ │ +        Padding value in multi-sparse columns.
│ │      """
│ │  
│ │ -    __slots__ = ("field_offset", "field_len", "feat_oov")
│ │ +    __slots__ = ("field_offset", "field_len", "feat_oov", "pad_val")
│ │  
│ │      field_offset: Iterable[int]
│ │      field_len: Iterable[int]
│ │      feat_oov: np.ndarray
│ │ +    pad_val: Dict[str, Any]
│ │  
│ │  
│ │  class DataInfo:
│ │ -    """Object for storing and updating indices and features information.
│ │ +    """Object for storing and updating information of indices and features.
│ │  
│ │      Parameters
│ │      ----------
│ │      col_name_mapping : dict of {dict : int} or None, default: None
│ │          Column name to index mapping, which has the format: ``{column_family_name: {column_name: index}}``.
│ │ -        If no such family, the default format would be: {column_family_name: {[]: []}
│ │ +        If no such family, the default format would be: {column_family_name: {[]: []}}
│ │      interaction_data : pandas.DataFrame or None, default: None
│ │          Data contains ``user``, ``item`` and ``label`` columns
│ │      user_sparse_unique : numpy.ndarray or None, default: None
│ │          Unique sparse features for all users in train data.
│ │      user_dense_unique : numpy.ndarray or None, default: None
│ │          Unique dense features for all users in train data.
│ │      item_sparse_unique : numpy.ndarray or None, default: None
│ │ @@ -85,16 +92,14 @@
│ │      ----------
│ │      col_name_mapping : dict of {dict : int} or None
│ │          See Parameters
│ │      user_consumed : dict of {int, list}
│ │          Every users' consumed items in train data.
│ │      item_consumed : dict of {int, list}
│ │          Every items' consumed users in train data.
│ │ -    popular_items : list
│ │ -        A number of popular items in train data. Often used in cold-start.
│ │  
│ │      See Also
│ │      --------
│ │      MultiSparseInfo
│ │      """
│ │  
│ │      def __init__(
│ │ @@ -123,53 +128,54 @@
│ │          self.item_dense_unique = item_dense_unique
│ │          self.user_consumed, self.item_consumed = interaction_consumed(
│ │              user_indices, item_indices
│ │          )
│ │          self.user_unique_vals = user_unique_vals
│ │          self.item_unique_vals = item_unique_vals
│ │          self.sparse_unique_vals = sparse_unique_vals
│ │ -        self.sparse_unique_idxs = DataInfo.map_unique_vals(sparse_unique_vals)
│ │          self.sparse_offset = sparse_offset
│ │          self.sparse_oov = sparse_oov
│ │          self.multi_sparse_unique_vals = multi_sparse_unique_vals
│ │ -        self.multi_sparse_unique_idxs = DataInfo.map_unique_vals(
│ │ -            multi_sparse_unique_vals
│ │ -        )
│ │          self.multi_sparse_combine_info = multi_sparse_combine_info
│ │ +        self.sparse_idx_mapping = DataInfo.map_sparse_vals(
│ │ +            sparse_unique_vals, multi_sparse_unique_vals
│ │ +        )
│ │          # Numpy doc states that it is recommended to use new random API
│ │          # https://numpy.org/doc/stable/reference/random/index.html
│ │ -        self.np_rng = default_rng()
│ │ +        self.np_rng = np.random.default_rng()
│ │          self._n_users = None
│ │          self._n_items = None
│ │          self._user2id = None
│ │          self._item2id = None
│ │          self._id2user = None
│ │          self._id2item = None
│ │          self._data_size = None
│ │ -        self.popular_items = None
│ │ +        self._popular_items = None
│ │          # store old info for rebuild models
│ │ -        self.old_n_users = None
│ │ -        self.old_n_items = None
│ │ -        self.old_sparse_len = None
│ │ -        self.old_sparse_oov = None
│ │ -        self.old_sparse_offset = None
│ │ +        self.old_info = None
│ │          self.all_args = locals()
│ │ -        self.add_oov()
│ │ -        if self.popular_items is None:
│ │ -            self.set_popular_items(100)
│ │ +        self.add_oovs()
│ │  
│ │      @staticmethod
│ │ -    def map_unique_vals(sparse_unique_vals):
│ │ -        if sparse_unique_vals is None:
│ │ -            return None
│ │ +    def map_sparse_vals(sparse_unique_vals, multi_sparse_unique_vals):
│ │ +        if sparse_unique_vals is None and multi_sparse_unique_vals is None:
│ │ +            return
│ │ +
│ │ +        def _map_vals(unique_vals):
│ │ +            mapping = dict()
│ │ +            if unique_vals is not None:
│ │ +                for col, vals in unique_vals.items():
│ │ +                    size = len(vals)
│ │ +                    mapping[col] = dict(zip(vals, range(size)))
│ │ +            return mapping
│ │ +
│ │          res = dict()
│ │ -        for col in sparse_unique_vals:
│ │ -            vals = sparse_unique_vals[col]
│ │ -            size = len(vals)
│ │ -            res[col] = dict(zip(vals, range(size)))
│ │ +        res.update(_map_vals(sparse_unique_vals))
│ │ +        res.update(_map_vals(multi_sparse_unique_vals))
│ │ +        assert len(res) > 0
│ │          return res
│ │  
│ │      @property
│ │      def global_mean(self):
│ │          """Mean value of all labels in `rating` task."""
│ │          return self.interaction_data.label.mean()
│ │  
│ │ @@ -177,94 +183,96 @@
│ │      def min_max_rating(self):
│ │          """Min and max value of all labels in `rating` task."""
│ │          return self.interaction_data.label.min(), self.interaction_data.label.max()
│ │  
│ │      @property
│ │      def sparse_col(self):
│ │          """Sparse column name to index mapping."""
│ │ -        if not self.col_name_mapping or not self.col_name_mapping["sparse_col"]:
│ │ +        if not self.col_name_mapping or "sparse_col" not in self.col_name_mapping:
│ │              return EmptyFeature
│ │          return Feature(
│ │              name=list(self.col_name_mapping["sparse_col"].keys()),
│ │              index=list(self.col_name_mapping["sparse_col"].values()),
│ │          )
│ │  
│ │      @property
│ │      def dense_col(self):
│ │          """Dense column name to index mapping."""
│ │ -        if not self.col_name_mapping or not self.col_name_mapping["dense_col"]:
│ │ +        if not self.col_name_mapping or "dense_col" not in self.col_name_mapping:
│ │              return EmptyFeature
│ │          return Feature(
│ │              name=list(self.col_name_mapping["dense_col"].keys()),
│ │              index=list(self.col_name_mapping["dense_col"].values()),
│ │          )
│ │  
│ │      @property
│ │      def user_sparse_col(self):
│ │          """User sparse column name to index mapping."""
│ │ -        if not self.col_name_mapping or not self.col_name_mapping["user_sparse_col"]:
│ │ +        if not self.col_name_mapping or "user_sparse_col" not in self.col_name_mapping:
│ │              return EmptyFeature
│ │          return Feature(
│ │              name=list(self.col_name_mapping["user_sparse_col"].keys()),
│ │              index=list(self.col_name_mapping["user_sparse_col"].values()),
│ │          )
│ │  
│ │      @property
│ │      def user_dense_col(self):
│ │          """User dense column name to index mapping."""
│ │ -        if not self.col_name_mapping or not self.col_name_mapping["user_dense_col"]:
│ │ +        if not self.col_name_mapping or "user_dense_col" not in self.col_name_mapping:
│ │              return EmptyFeature
│ │          return Feature(
│ │              name=list(self.col_name_mapping["user_dense_col"].keys()),
│ │              index=list(self.col_name_mapping["user_dense_col"].values()),
│ │          )
│ │  
│ │      @property
│ │      def item_sparse_col(self):
│ │          """Item sparse column name to index mapping."""
│ │ -        if not self.col_name_mapping or not self.col_name_mapping["item_sparse_col"]:
│ │ +        if not self.col_name_mapping or "item_sparse_col" not in self.col_name_mapping:
│ │              return EmptyFeature
│ │          return Feature(
│ │              name=list(self.col_name_mapping["item_sparse_col"].keys()),
│ │              index=list(self.col_name_mapping["item_sparse_col"].values()),
│ │          )
│ │  
│ │      @property
│ │      def item_dense_col(self):
│ │          """Item dense column name to index mapping."""
│ │ -        if not self.col_name_mapping or not self.col_name_mapping["item_dense_col"]:
│ │ +        if not self.col_name_mapping or "item_dense_col" not in self.col_name_mapping:
│ │              return EmptyFeature
│ │          return Feature(
│ │              name=list(self.col_name_mapping["item_dense_col"].keys()),
│ │              index=list(self.col_name_mapping["item_dense_col"].values()),
│ │          )
│ │  
│ │      @property
│ │      def user_col(self):
│ │          """All the user column names, including sparse and dense."""
│ │          if not self.col_name_mapping:
│ │              return []
│ │ -        # will be sorted by key
│ │ -        return (
│ │ -            self.col_name_mapping["user_sparse_col"]
│ │ -            .keys()
│ │ -            .__or__(self.col_name_mapping["user_dense_col"].keys())
│ │ -        )
│ │ +        user_sparse, user_dense = [], []
│ │ +        if "user_sparse_col" in self.col_name_mapping:
│ │ +            user_sparse = list(self.col_name_mapping["user_sparse_col"].keys())
│ │ +        if "user_dense_col" in self.col_name_mapping:
│ │ +            user_dense = list(self.col_name_mapping["user_dense_col"].keys())
│ │ +        # The result columns will be sorted by key
│ │ +        return user_sparse + user_dense
│ │  
│ │      @property
│ │      def item_col(self):
│ │          """All the item column names, including sparse and dense."""
│ │          if not self.col_name_mapping:
│ │              return []
│ │ -        # will be sorted by key
│ │ -        return (
│ │ -            self.col_name_mapping["item_sparse_col"]
│ │ -            .keys()
│ │ -            .__or__(self.col_name_mapping["item_dense_col"].keys())
│ │ -        )
│ │ +        item_sparse, item_dense = [], []
│ │ +        if "item_sparse_col" in self.col_name_mapping:
│ │ +            item_sparse = list(self.col_name_mapping["item_sparse_col"].keys())
│ │ +        if "item_dense_col" in self.col_name_mapping:
│ │ +            item_dense = list(self.col_name_mapping["item_dense_col"].keys())
│ │ +        # The result columns will be sorted by key
│ │ +        return item_sparse + item_dense
│ │  
│ │      @property
│ │      def n_users(self):
│ │          """Number of users in train data."""
│ │          if self._n_users is None:
│ │              self._n_users = len(self.user_unique_vals)
│ │          return self._n_users
│ │ @@ -318,301 +326,113 @@
│ │          n_labels = len(self.interaction_data)
│ │          return "n_users: %d, n_items: %d, data density: %.4f %%" % (
│ │              n_users,
│ │              n_items,
│ │              100 * n_labels / (n_users * n_items),
│ │          )
│ │  
│ │ -    def get_indexed_interaction(self):
│ │ -        data = self.interaction_data.copy()
│ │ -        data.user = data.user.map(self.user2id)
│ │ -        data.item = data.item.map(self.item2id)
│ │ -        if data.user.isnull().any():
│ │ -            data["user"].fillna(self.n_users, inplace=True)
│ │ -            data["user"] = data["user"].astype("int")
│ │ -        if data.item.isnull().any():
│ │ -            data["item"].fillna(self.n_items, inplace=True)
│ │ -            data["item"] = data["item"].astype("int")
│ │ -        return data
│ │ -
│ │ -    def update_consumed(self, user_indices, item_indices, merge):
│ │ -        if merge:
│ │ -            old_data = self.get_indexed_interaction()
│ │ -            user_indices = np.append(old_data.user.to_numpy(), user_indices)
│ │ -            item_indices = np.append(old_data.item.to_numpy(), item_indices)
│ │ -        self.user_consumed, self.item_consumed = interaction_consumed(
│ │ -            user_indices, item_indices
│ │ -        )
│ │ -        return user_indices, item_indices
│ │ -
│ │ -    def reset_property(self):
│ │ -        self._n_users = None
│ │ -        self._n_items = None
│ │ -        self._user2id = None
│ │ -        self._item2id = None
│ │ -        self._id2user = None
│ │ -        self._id2item = None
│ │ -        self._data_size = None
│ │ -
│ │ -    def store_old_info(self):
│ │ -        self.old_n_users = self.n_users
│ │ -        self.old_n_items = self.n_items
│ │ -        if (
│ │ -            self.sparse_unique_vals is not None
│ │ -            or self.multi_sparse_unique_vals is not None
│ │ -        ):
│ │ -            self.old_sparse_len = list()
│ │ -            self.old_sparse_oov = list()
│ │ -            self.old_sparse_offset = list()
│ │ -            for i, col in enumerate(self.sparse_col.name):
│ │ -                if (
│ │ -                    self.sparse_unique_vals is not None
│ │ -                    and col in self.sparse_unique_vals
│ │ -                ):
│ │ -                    self.old_sparse_len.append(len(self.sparse_unique_vals[col]))
│ │ -                    self.old_sparse_oov.append(self.sparse_oov[i])
│ │ -                    self.old_sparse_offset.append(self.sparse_offset[i])
│ │ -                elif (
│ │ -                    self.multi_sparse_unique_vals is not None
│ │ -                    and col in self.multi_sparse_unique_vals
│ │ -                ):
│ │ -                    self.old_sparse_len.append(len(self.multi_sparse_unique_vals[col]))
│ │ -                    self.old_sparse_oov.append(self.sparse_oov[i])
│ │ -                    self.old_sparse_offset.append(self.sparse_offset[i])
│ │ -                elif (
│ │ -                    self.multi_sparse_unique_vals is not None
│ │ -                    and col in self.col_name_mapping["multi_sparse"]
│ │ -                ):
│ │ -                    main_name = self.col_name_mapping["multi_sparse"][col]
│ │ -                    pos = self.sparse_col.name.index(main_name)
│ │ -                    # multi_sparse case, second to last is redundant.
│ │ -                    # Used in rebuild_tf_model(), rebuild_torch_model()
│ │ -                    self.old_sparse_len.append(-1)
│ │ -                    # self.old_sparse_oov.append(self.sparse_oov[pos])
│ │ -                    self.old_sparse_offset.append(self.sparse_offset[pos])
│ │ -
│ │ -    def expand_sparse_unique_vals_and_matrix(self, data):
│ │ -        self.store_old_info()
│ │ -        self.reset_property()
│ │ -
│ │ -        user_diff = np.setdiff1d(data.user.to_numpy(), self.user_unique_vals)
│ │ -        if len(user_diff) > 0:
│ │ -            self.user_unique_vals = np.append(self.user_unique_vals, user_diff)
│ │ -            self.extend_unique_matrix("user", len(user_diff))
│ │ -
│ │ -        item_diff = np.setdiff1d(data.item.to_numpy(), self.item_unique_vals)
│ │ -        if len(item_diff) > 0:
│ │ -            self.item_unique_vals = np.append(self.item_unique_vals, item_diff)
│ │ -            self.extend_unique_matrix("item", len(item_diff))
│ │ -
│ │ -        def update_sparse_unique(unique_dicts, unique_idxs):
│ │ -            for sparse_col in unique_dicts:
│ │ -                unique_vals = list(unique_dicts[sparse_col])
│ │ -                sparse_diff = np.setdiff1d(data[sparse_col].to_numpy(), unique_vals)
│ │ -                if len(sparse_diff) > 0:
│ │ -                    unique_vals = np.append(unique_vals, sparse_diff)
│ │ -                    unique_dicts[sparse_col] = unique_vals
│ │ -                    size = len(unique_vals)
│ │ -                    unique_idxs[sparse_col] = dict(zip(unique_vals, range(size)))
│ │ -
│ │ -        if self.sparse_unique_vals is not None:
│ │ -            update_sparse_unique(self.sparse_unique_vals, self.sparse_unique_idxs)
│ │ -        if self.multi_sparse_unique_vals is not None:
│ │ -            update_sparse_unique(
│ │ -                self.multi_sparse_unique_vals, self.multi_sparse_unique_idxs
│ │ -            )
│ │ -
│ │ -    def extend_unique_matrix(self, mode, diff_num):
│ │ -        if mode == "user":
│ │ -            if self.user_sparse_unique is not None:
│ │ -                new_users = np.zeros(
│ │ -                    [diff_num, self.user_sparse_unique.shape[1]],
│ │ -                    dtype=self.user_sparse_unique.dtype,
│ │ -                )
│ │ -                # exclude last oov unique values
│ │ -                self.user_sparse_unique = np.vstack(
│ │ -                    [self.user_sparse_unique[:-1], new_users]
│ │ -                )
│ │ -            if self.user_dense_unique is not None:
│ │ -                new_users = np.zeros(
│ │ -                    [diff_num, self.user_dense_unique.shape[1]],
│ │ -                    dtype=self.user_dense_unique.dtype,
│ │ -                )
│ │ -                self.user_dense_unique = np.vstack(
│ │ -                    [self.user_dense_unique[:-1], new_users]
│ │ -                )
│ │ -        elif mode == "item":
│ │ -            if self.item_sparse_unique is not None:
│ │ -                new_items = np.zeros(
│ │ -                    [diff_num, self.item_sparse_unique.shape[1]],
│ │ -                    dtype=self.item_sparse_unique.dtype,
│ │ -                )
│ │ -                self.item_sparse_unique = np.vstack(
│ │ -                    [self.item_sparse_unique[:-1], new_items]
│ │ -                )
│ │ -            if self.item_dense_unique is not None:
│ │ -                new_items = np.zeros(
│ │ -                    [diff_num, self.item_dense_unique.shape[1]],
│ │ -                    dtype=self.item_dense_unique.dtype,
│ │ -                )
│ │ -                self.item_dense_unique = np.vstack(
│ │ -                    [self.item_dense_unique[:-1], new_items]
│ │ -                )
│ │ -
│ │ -    # sparse_indices and offset will increase if sparse feature encounter new categories
│ │ -    def modify_sparse_indices(self):
│ │ -        # old_offset = [i + 1 for i in self.old_sparse_oov[:-1]]
│ │ -        # old_offset = np.insert(old_offset, 0, 0)
│ │ -        old_offset = np.array(self.old_sparse_offset)
│ │ -        if self.user_sparse_unique is not None:
│ │ -            user_idx = self.user_sparse_col.index
│ │ -            diff = self.sparse_offset[user_idx] - old_offset[user_idx]
│ │ -            self.user_sparse_unique += diff
│ │ -        if self.item_sparse_unique is not None:
│ │ -            item_idx = self.item_sparse_col.index
│ │ -            diff = self.sparse_offset[item_idx] - old_offset[item_idx]
│ │ -            self.item_sparse_unique += diff
│ │ -
│ │ -    # todo: ignore feature oov value
│ │ -    def assign_sparse_features(self, data, mode):
│ │ -        data = check_oov(self, data, mode)
│ │ -        if mode == "user":
│ │ -            row_idx = data["user"].to_numpy()
│ │ -            col_info = self.user_sparse_col
│ │ -            if self.user_sparse_unique is not None and col_info.name:
│ │ -                for feat_idx, col in enumerate(col_info.name):
│ │ -                    if col not in data.columns:
│ │ -                        continue
│ │ -                    self.user_sparse_unique[
│ │ -                        row_idx, feat_idx
│ │ -                    ] = compute_sparse_feat_indices(
│ │ -                        self, data, col_info.index[feat_idx], col
│ │ -                    )
│ │ -        elif mode == "item":
│ │ -            row_idx = data["item"].to_numpy()
│ │ -            col_info = self.item_sparse_col
│ │ -            if self.item_sparse_unique is not None and col_info.name:
│ │ -                for feat_idx, col in enumerate(col_info.name):
│ │ -                    if col not in data.columns:
│ │ -                        continue
│ │ -                    self.item_sparse_unique[
│ │ -                        row_idx, feat_idx
│ │ -                    ] = compute_sparse_feat_indices(
│ │ -                        self, data, col_info.index[feat_idx], col
│ │ -                    )
│ │ -        else:
│ │ -            raise ValueError("mode must be user or item.")
│ │ -
│ │ -    def assign_dense_features(self, data, mode):
│ │ -        data = check_oov(self, data, mode)
│ │ -        if mode == "user":
│ │ -            row_idx = data["user"].to_numpy()
│ │ -            col_info = self.user_dense_col
│ │ -            if self.user_dense_unique is not None and col_info.name:
│ │ -                for feat_idx, col in enumerate(col_info.name):
│ │ -                    if col not in data.columns:
│ │ -                        continue
│ │ -                    self.user_dense_unique[row_idx, feat_idx] = data[col].to_numpy()
│ │ -        elif mode == "item":
│ │ -            row_idx = data["item"].to_numpy()
│ │ -            col_info = self.item_dense_col
│ │ -            if self.item_dense_unique is not None and col_info.name:
│ │ -                for feat_idx, col in enumerate(col_info.name):
│ │ -                    if col not in data.columns:
│ │ -                        continue
│ │ -                    self.item_dense_unique[row_idx, feat_idx] = data[col].to_numpy()
│ │ -
│ │      def assign_user_features(self, user_data):
│ │          """Assign user features to this ``data_info`` object from ``user_data``.
│ │  
│ │          Parameters
│ │          ----------
│ │          user_data : pandas.DataFrame
│ │              Data contains new user features.
│ │          """
│ │ -        self.assign_sparse_features(user_data, "user")
│ │ -        self.assign_dense_features(user_data, "user")
│ │ +        assert "user" in user_data.columns, "Data must contain `user` column."
│ │ +        user_data = user_data.drop_duplicates(subset=["user"], keep="last")
│ │ +        user_row_idx, user_id_mask = get_row_id_masks(
│ │ +            user_data["user"], self.user_unique_vals
│ │ +        )
│ │ +        self.user_sparse_unique = update_new_sparse_feats(
│ │ +            user_data,
│ │ +            user_row_idx,
│ │ +            user_id_mask,
│ │ +            self.user_sparse_unique,
│ │ +            self.sparse_unique_vals,
│ │ +            self.multi_sparse_unique_vals,
│ │ +            self.user_sparse_col,
│ │ +            self.col_name_mapping,
│ │ +            self.sparse_offset,
│ │ +        )
│ │ +        self.user_dense_unique = update_new_dense_feats(
│ │ +            user_data,
│ │ +            user_row_idx,
│ │ +            user_id_mask,
│ │ +            self.user_dense_unique,
│ │ +            self.user_dense_col,
│ │ +        )
│ │  
│ │      def assign_item_features(self, item_data):
│ │          """Assign item features to this ``data_info`` object from ``item_data``.
│ │  
│ │          Parameters
│ │          ----------
│ │          item_data : pandas.DataFrame
│ │              Data contains new item features.
│ │          """
│ │ -        self.assign_sparse_features(item_data, "item")
│ │ -        self.assign_dense_features(item_data, "item")
│ │ +        assert "item" in item_data.columns, "Data must contain `item` column."
│ │ +        item_data = item_data.drop_duplicates(subset=["item"], keep="last")
│ │ +        item_row_idx, item_id_mask = get_row_id_masks(
│ │ +            item_data["item"], self.item_unique_vals
│ │ +        )
│ │ +        self.item_sparse_unique = update_new_sparse_feats(
│ │ +            item_data,
│ │ +            item_row_idx,
│ │ +            item_id_mask,
│ │ +            self.item_sparse_unique,
│ │ +            self.sparse_unique_vals,
│ │ +            self.multi_sparse_unique_vals,
│ │ +            self.item_sparse_col,
│ │ +            self.col_name_mapping,
│ │ +            self.sparse_offset,
│ │ +        )
│ │ +        self.item_dense_unique = update_new_dense_feats(
│ │ +            item_data,
│ │ +            item_row_idx,
│ │ +            item_id_mask,
│ │ +            self.item_dense_unique,
│ │ +            self.item_dense_col,
│ │ +        )
│ │ +
│ │ +    def add_oovs(self):
│ │ +        def _concat_oov(uniques, cols=None):
│ │ +            if uniques is None:
│ │ +                return
│ │ +            oov = self.sparse_oov[cols] if cols else np.mean(uniques, axis=0)
│ │ +            return np.vstack([uniques, oov])
│ │ +
│ │ +        self.user_sparse_unique = _concat_oov(
│ │ +            self.user_sparse_unique, self.user_sparse_col.index
│ │ +        )
│ │ +        self.item_sparse_unique = _concat_oov(
│ │ +            self.item_sparse_unique, self.item_sparse_col.index
│ │ +        )
│ │ +        self.user_dense_unique = _concat_oov(self.user_dense_unique)
│ │ +        self.item_dense_unique = _concat_oov(self.item_dense_unique)
│ │ +
│ │ +    @property
│ │ +    def popular_items(self):
│ │ +        """A number of popular items in train data which often used in cold-start."""
│ │ +        if self._popular_items is None:
│ │ +            self._popular_items = self._get_popular_items(100)
│ │ +        return self._popular_items
│ │  
│ │ -    def add_oov(self):
│ │ -        if (
│ │ -            self.user_sparse_unique is not None
│ │ -            and len(self.user_sparse_unique) == self.n_users
│ │ -        ):
│ │ -            user_sparse_oov = self.sparse_oov[self.user_sparse_col.index]
│ │ -            self.user_sparse_unique = np.vstack(
│ │ -                [self.user_sparse_unique, user_sparse_oov]
│ │ -            )
│ │ -        if (
│ │ -            self.item_sparse_unique is not None
│ │ -            and len(self.item_sparse_unique) == self.n_items
│ │ -        ):
│ │ -            item_sparse_oov = self.sparse_oov[self.item_sparse_col.index]
│ │ -            self.item_sparse_unique = np.vstack(
│ │ -                [self.item_sparse_unique, item_sparse_oov]
│ │ -            )
│ │ -        if (
│ │ -            self.user_dense_unique is not None
│ │ -            and len(self.user_dense_unique) == self.n_users
│ │ -        ):
│ │ -            user_dense_oov = np.mean(self.user_dense_unique, axis=0)
│ │ -            self.user_dense_unique = np.vstack([self.user_dense_unique, user_dense_oov])
│ │ -        if (
│ │ -            self.item_dense_unique is not None
│ │ -            and len(self.item_dense_unique) == self.n_items
│ │ -        ):
│ │ -            item_dense_oov = np.mean(self.item_dense_unique, axis=0)
│ │ -            self.item_dense_unique = np.vstack([self.item_dense_unique, item_dense_oov])
│ │ -
│ │ -    def set_popular_items(self, num):
│ │ +    def _get_popular_items(self, num):
│ │          count_items = (
│ │              self.interaction_data.drop_duplicates(subset=["user", "item"])
│ │              .groupby("item")["user"]
│ │              .count()
│ │          )
│ │          selected_items = count_items.sort_values(ascending=False).index.tolist()[:num]
│ │          # if not enough items, add old populars
│ │ -        if len(selected_items) < num and self.popular_items is not None:
│ │ +        if len(selected_items) < num and self.old_info is not None:
│ │              diff = num - len(selected_items)
│ │ -            selected_items.extend(self.popular_items[:diff])
│ │ -        self.popular_items = selected_items
│ │ -
│ │ -    def store_args(self, user_indices, item_indices):
│ │ -        self.all_args = dict()
│ │ -        inside_args = [
│ │ -            "col_name_mapping",
│ │ -            "interaction_data",
│ │ -            "user_sparse_unique",
│ │ -            "user_dense_unique",
│ │ -            "item_sparse_unique",
│ │ -            "item_dense_unique",
│ │ -            "user_unique_vals",
│ │ -            "item_unique_vals",
│ │ -            "sparse_unique_vals",
│ │ -            "sparse_offset",
│ │ -            "sparse_oov",
│ │ -            "multi_sparse_unique_vals",
│ │ -            "multi_sparse_combine_info",
│ │ -        ]
│ │ -        all_variables = vars(self)
│ │ -        for arg in inside_args:
│ │ -            if arg in all_variables and all_variables[arg] is not None:
│ │ -                self.all_args[arg] = all_variables[arg]
│ │ -        self.all_args["user_indices"] = user_indices
│ │ -        self.all_args["item_indices"] = item_indices
│ │ +            selected_items.extend(self.old_info.popular_items[:diff])
│ │ +        return selected_items
│ │  
│ │      def save(self, path, model_name):
│ │          """Save :class:`DataInfo` Object.
│ │  
│ │          Parameters
│ │          ----------
│ │          path : str
│ │ @@ -701,7 +521,45 @@
│ │                  if "multi_sparse_unique_vals" not in hparams:
│ │                      hparams["multi_sparse_unique_vals"] = dict()
│ │                  hparams["multi_sparse_unique_vals"][arg[8:]] = info[arg]
│ │              else:
│ │                  hparams[arg] = info[arg]
│ │  
│ │          return cls(**hparams)
│ │ +
│ │ +
│ │ +@dataclass
│ │ +class OldInfo:
│ │ +    n_users: int
│ │ +    n_items: int
│ │ +    sparse_len: List[int]
│ │ +    sparse_oov: List[int]
│ │ +    popular_items: List[Any]
│ │ +
│ │ +
│ │ +def store_old_info(data_info):
│ │ +    sparse_len = list()
│ │ +    sparse_oov = list()
│ │ +    sparse_unique = data_info.sparse_unique_vals
│ │ +    multi_sparse_unique = data_info.multi_sparse_unique_vals
│ │ +    for i, col in enumerate(data_info.sparse_col.name):
│ │ +        if sparse_unique is not None and col in sparse_unique:
│ │ +            sparse_len.append(len(sparse_unique[col]))
│ │ +            sparse_oov.append(data_info.sparse_oov[i])
│ │ +        elif multi_sparse_unique is not None and col in multi_sparse_unique:
│ │ +            sparse_len.append(len(multi_sparse_unique[col]))
│ │ +            sparse_oov.append(data_info.sparse_oov[i])
│ │ +        elif (
│ │ +            multi_sparse_unique is not None
│ │ +            and "multi_sparse" in data_info.col_name_mapping
│ │ +            and col in data_info.col_name_mapping["multi_sparse"]
│ │ +        ):
│ │ +            # multi_sparse case, second to last cols are redundant.
│ │ +            # Used in `rebuild_tf_model`, `rebuild_torch_model`
│ │ +            sparse_len.append(-1)
│ │ +    return OldInfo(
│ │ +        data_info.n_users,
│ │ +        data_info.n_items,
│ │ +        sparse_len,
│ │ +        sparse_oov,
│ │ +        data_info.popular_items,
│ │ +    )
│ │   --- LibRecommender-1.0.1/libreco/data/dataset.py
│ ├── +++ LibRecommender-1.1.0/libreco/data/dataset.py
│ │┄ Files 17% similar despite different names
│ │ @@ -1,26 +1,36 @@
│ │  """Classes for Transforming and Building Data."""
│ │ +import functools
│ │  import itertools
│ │  
│ │  import numpy as np
│ │  
│ │ -from ..feature import (
│ │ -    col_name2index,
│ │ -    construct_unique_feat,
│ │ +from .consumed import update_consumed
│ │ +from .data_info import DataInfo, store_old_info
│ │ +from .transformed import TransformedSet
│ │ +from ..feature.column_mapping import col_name2index
│ │ +from ..feature.multi_sparse import (
│ │ +    get_multi_sparse_info,
│ │ +    multi_sparse_col_map,
│ │ +    recover_sparse_cols,
│ │ +)
│ │ +from ..feature.sparse import (
│ │ +    get_id_indices,
│ │      get_oov_pos,
│ │ -    get_user_item_sparse_indices,
│ │      merge_offset,
│ │      merge_sparse_col,
│ │      merge_sparse_indices,
│ │ -    multi_sparse_col_map,
│ │ -    multi_sparse_combine_info,
│ │ -    recover_sparse_cols,
│ │  )
│ │ -from .data_info import DataInfo
│ │ -from .transformed import TransformedSet
│ │ +from ..feature.unique import construct_unique_feat
│ │ +from ..feature.update import (
│ │ +    update_id_unique,
│ │ +    update_multi_sparse_unique,
│ │ +    update_sparse_unique,
│ │ +    update_unique_feats,
│ │ +)
│ │  
│ │  
│ │  class _Dataset(object):
│ │      """Base class for loading dataset."""
│ │  
│ │      user_unique_vals = None
│ │      item_unique_vals = None
│ │ @@ -63,35 +73,28 @@
│ │              raise RuntimeError(
│ │                  "Must first build trainset before building evalset or testset"
│ │              )
│ │          cls._check_subclass()
│ │          cls._check_col_names(test_data, is_train=False)
│ │          if shuffle:
│ │              test_data = cls.shuffle_data(test_data, seed)
│ │ -        if data_info:
│ │ -            assert isinstance(data_info, DataInfo), "Invalid passed `data_info`."
│ │ -            user_unique_vals = data_info.user_unique_vals
│ │ -            item_unique_vals = data_info.item_unique_vals
│ │ -        else:
│ │ -            user_unique_vals = cls.user_unique_vals
│ │ -            item_unique_vals = cls.item_unique_vals
│ │  
│ │          if cls.__name__ == "DatasetPure":
│ │              return _build_transformed_set(
│ │                  test_data,
│ │ -                user_unique_vals,
│ │ -                item_unique_vals,
│ │ +                cls.user_unique_vals,
│ │ +                cls.item_unique_vals,
│ │                  is_train=False,
│ │                  is_ordered=False,
│ │              )
│ │          else:
│ │              return _build_transformed_set_feat(
│ │                  test_data,
│ │ -                user_unique_vals,
│ │ -                item_unique_vals,
│ │ +                cls.user_unique_vals,
│ │ +                cls.item_unique_vals,
│ │                  is_train=False,
│ │                  is_ordered=False,
│ │                  data_info=data_info,
│ │              )
│ │  
│ │      @classmethod
│ │      def build_evalset(cls, eval_data, shuffle=False, seed=42):
│ │ @@ -257,14 +260,18 @@
│ │      def merge_trainset(
│ │          cls, train_data, data_info, merge_behavior=True, shuffle=False, seed=42
│ │      ):
│ │          """Build transformed data by merging new train data with old data.
│ │  
│ │          .. versionadded:: 1.0.0
│ │  
│ │ +        .. versionchanged:: 1.1.0
│ │ +           Applying a more functional approach. A new ``data_info`` will be constructed
│ │ +           and returned, and the passed old ``data_info`` should be discarded.
│ │ +
│ │          Parameters
│ │          ----------
│ │          train_data : pandas.DataFrame
│ │              Data must contain at least three columns, i.e. ``user``, ``item``, ``label``.
│ │          data_info : DataInfo
│ │              Object that contains past data information.
│ │          merge_behavior : bool, default: True
│ │ @@ -272,38 +279,45 @@
│ │          shuffle : bool, default: False
│ │              Whether to fully shuffle data.
│ │          seed: int, default: 42
│ │              Random seed.
│ │  
│ │          Returns
│ │          -------
│ │ -        :class:`~libreco.data.TransformedSet`
│ │ -            Transformed Data object used for training.
│ │ +        new_trainset : :class:`~libreco.data.TransformedSet`
│ │ +            New transformed Data object used for training.
│ │ +        new_data_info : :class:`~libreco.data.DataInfo`
│ │ +            New ``data_info`` that contains some useful information.
│ │          """
│ │ -        cls._check_col_names(train_data, is_train=True)
│ │          assert isinstance(data_info, DataInfo), "Invalid passed `data_info`."
│ │ +        cls._check_col_names(train_data, is_train=True)
│ │ +        cls.user_unique_vals, cls.item_unique_vals = update_id_unique(
│ │ +            train_data, data_info
│ │ +        )
│ │          if shuffle:
│ │              train_data = cls.shuffle_data(train_data, seed)
│ │  
│ │ -        data_info.expand_sparse_unique_vals_and_matrix(train_data)
│ │          merge_transformed, user_indices, item_indices = _build_transformed_set(
│ │              train_data,
│ │ -            data_info.user_unique_vals,
│ │ -            data_info.item_unique_vals,
│ │ +            cls.user_unique_vals,
│ │ +            cls.item_unique_vals,
│ │              is_train=True,
│ │              is_ordered=False,
│ │          )
│ │ -        user_indices, item_indices = data_info.update_consumed(
│ │ -            user_indices, item_indices, merge=merge_behavior
│ │ +        new_data_info = DataInfo(
│ │ +            interaction_data=train_data[["user", "item", "label"]],
│ │ +            user_indices=user_indices,
│ │ +            item_indices=item_indices,
│ │ +            user_unique_vals=cls.user_unique_vals,
│ │ +            item_unique_vals=cls.item_unique_vals,
│ │          )
│ │ -        data_info.interaction_data = train_data[["user", "item", "label"]]
│ │ -        data_info.set_popular_items(100)
│ │ -        data_info.store_args(user_indices, item_indices)
│ │ +        new_data_info = update_consumed(new_data_info, data_info, merge_behavior)
│ │ +        new_data_info.old_info = store_old_info(data_info)
│ │          cls.train_called = True
│ │ -        return merge_transformed
│ │ +        return merge_transformed, new_data_info
│ │  
│ │  
│ │  class DatasetFeat(_Dataset):
│ │      """Dataset class used for building data contains features.
│ │  
│ │      Examples
│ │      --------
│ │ @@ -317,24 +331,51 @@
│ │      multi_sparse_unique_vals = None
│ │      sparse_col = None
│ │      multi_sparse_col = None
│ │      dense_col = None
│ │  
│ │      @classmethod
│ │      def _set_feature_col(cls, sparse_col, dense_col, multi_sparse_col):
│ │ -        cls.sparse_col = None if not sparse_col else sparse_col
│ │ -        cls.dense_col = None if not dense_col else dense_col
│ │ +        cls.sparse_col = sparse_col or None
│ │ +        cls.dense_col = dense_col or None
│ │          if multi_sparse_col:
│ │              if not all(isinstance(field, list) for field in multi_sparse_col):
│ │                  cls.multi_sparse_col = [multi_sparse_col]
│ │              else:
│ │                  cls.multi_sparse_col = multi_sparse_col
│ │          else:
│ │              cls.multi_sparse_col = None
│ │  
│ │ +    @classmethod
│ │ +    def _check_feature_cols(cls, user_col, item_col):
│ │ +        all_sparse_col = (
│ │ +            merge_sparse_col(cls.sparse_col, cls.multi_sparse_col)
│ │ +            if cls.multi_sparse_col is not None
│ │ +            else cls.sparse_col
│ │ +        )
│ │ +        sparse_cols = all_sparse_col or []
│ │ +        dense_cols = cls.dense_col or []
│ │ +        user_cols = user_col or []
│ │ +        item_cols = item_col or []
│ │ +        if len(sparse_cols) + len(dense_cols) != len(user_cols) + len(item_cols):
│ │ +            len_str = "len(sparse_cols) + len(dense_cols) == len(user_cols) + len(item_cols)"  # fmt: skip
│ │ +            raise ValueError(
│ │ +                f"Please make sure length of columns match, i.e. `{len_str}`, got "
│ │ +                f"sparse columns: {sparse_cols}, "
│ │ +                f"dense columns: {dense_cols}, "
│ │ +                f"user columns: {user_cols}, "
│ │ +                f"item columns: {item_cols}"
│ │ +            )
│ │ +        columns1, columns2 = sparse_cols + dense_cols, user_cols + item_cols
│ │ +        mis_match_cols = np.setxor1d(columns1, columns2)
│ │ +        if len(mis_match_cols) > 0:
│ │ +            raise ValueError(
│ │ +                f"Got inconsistent columns: {mis_match_cols}, please check the column names"
│ │ +            )
│ │ +
│ │      @classmethod  # TODO: pseudo pure
│ │      def build_trainset(
│ │          cls,
│ │          train_data,
│ │          user_col=None,
│ │          item_col=None,
│ │          sparse_col=None,
│ │ @@ -366,14 +407,20 @@
│ │              For example, ``[["a", "b", "c"], ["d", "e"]]``
│ │          dense_col : list of str or None, default: None
│ │              List of dense feature column names.
│ │          unique_feat : bool, default: False
│ │              Whether the features of users and items are unique in train data.
│ │          pad_val : int or str or list, default: "missing"
│ │              Padding value in multi_sparse columns to ensure same length of all samples.
│ │ +
│ │ +            .. Warning::
│ │ +                If the ``pad_val`` is a single value, it will be used in all ``multi_sparse`` columns.
│ │ +                So if you want to use different ``pad_val`` for different ``multi_sparse`` columns,
│ │ +                the ``pad_val`` should be a list.
│ │ +
│ │          shuffle : bool, default: False
│ │              Whether to fully shuffle data.
│ │  
│ │              .. Warning::
│ │                  If your data is order or time dependent, it is not recommended to shuffle data.
│ │  
│ │          seed: int, default: 42
│ │ @@ -381,22 +428,28 @@
│ │  
│ │          Returns
│ │          -------
│ │          trainset : :class:`~libreco.data.TransformedSet`
│ │              Transformed Data object used for training.
│ │          data_info : :class:`~libreco.data.DataInfo`
│ │              Object that contains some useful information.
│ │ +
│ │ +        Raises
│ │ +        ------
│ │ +        ValueError
│ │ +            If the feature columns specified by the user are inconsistent.
│ │          """
│ │          cls._check_subclass()
│ │          cls._check_col_names(train_data, is_train=True)
│ │          cls._set_feature_col(sparse_col, dense_col, multi_sparse_col)
│ │ +        cls._check_feature_cols(user_col, item_col)
│ │          cls.user_unique_vals = np.sort(train_data["user"].unique())
│ │          cls.item_unique_vals = np.sort(train_data["item"].unique())
│ │          cls.sparse_unique_vals = _get_sparse_unique_vals(cls.sparse_col, train_data)
│ │ -        cls.multi_sparse_unique_vals = _get_multi_sparse_unique_vals(
│ │ +        cls.multi_sparse_unique_vals, pad_val_dict = _get_multi_sparse_unique_vals(
│ │              cls.multi_sparse_col, train_data, pad_val
│ │          )
│ │          if shuffle:
│ │              train_data = cls.shuffle_data(train_data, seed)
│ │  
│ │          (
│ │              train_transformed,
│ │ @@ -413,59 +466,53 @@
│ │          )
│ │  
│ │          all_sparse_col = (
│ │              merge_sparse_col(cls.sparse_col, cls.multi_sparse_col)
│ │              if cls.multi_sparse_col
│ │              else sparse_col
│ │          )
│ │ -
│ │          col_name_mapping = col_name2index(
│ │              user_col, item_col, all_sparse_col, cls.dense_col
│ │          )
│ │ -        user_sparse_col_indices = list(col_name_mapping["user_sparse_col"].values())
│ │ -        user_dense_col_indices = list(col_name_mapping["user_dense_col"].values())
│ │ -        item_sparse_col_indices = list(col_name_mapping["item_sparse_col"].values())
│ │ -        item_dense_col_indices = list(col_name_mapping["item_dense_col"].values())
│ │ -
│ │          (
│ │              user_sparse_unique,
│ │              user_dense_unique,
│ │              item_sparse_unique,
│ │              item_dense_unique,
│ │          ) = construct_unique_feat(
│ │              user_indices,
│ │              item_indices,
│ │              train_sparse_indices,
│ │              train_dense_values,
│ │ -            user_sparse_col_indices,
│ │ -            user_dense_col_indices,
│ │ -            item_sparse_col_indices,
│ │ -            item_dense_col_indices,
│ │ +            col_name_mapping,
│ │              unique_feat,
│ │          )
│ │  
│ │ -        sparse_offset = (
│ │ -            merge_offset(cls, cls.sparse_col, cls.multi_sparse_col)
│ │ -            if cls.sparse_col or cls.multi_sparse_col
│ │ -            else None
│ │ -        )
│ │ -        sparse_oov = (
│ │ -            get_oov_pos(cls, sparse_col, multi_sparse_col)
│ │ -            if cls.sparse_col or cls.multi_sparse_col
│ │ -            else None
│ │ +        sparse_offset = merge_offset(
│ │ +            cls.sparse_col,
│ │ +            cls.multi_sparse_col,
│ │ +            cls.sparse_unique_vals,
│ │ +            cls.multi_sparse_unique_vals,
│ │          )
│ │ -
│ │ -        multi_sparse_info = (
│ │ -            multi_sparse_combine_info(cls, all_sparse_col, sparse_col, multi_sparse_col)
│ │ -            if cls.multi_sparse_col
│ │ -            else None
│ │ +        sparse_oov = get_oov_pos(
│ │ +            cls.sparse_col,
│ │ +            cls.multi_sparse_col,
│ │ +            cls.sparse_unique_vals,
│ │ +            cls.multi_sparse_unique_vals,
│ │ +        )
│ │ +        multi_sparse_info = get_multi_sparse_info(
│ │ +            all_sparse_col,
│ │ +            cls.sparse_col,
│ │ +            cls.multi_sparse_col,
│ │ +            cls.sparse_unique_vals,
│ │ +            cls.multi_sparse_unique_vals,
│ │ +            pad_val_dict,
│ │          )
│ │          if cls.multi_sparse_col:
│ │ -            multi_sparse_map = multi_sparse_col_map(multi_sparse_col)
│ │ -            col_name_mapping.update({"multi_sparse": multi_sparse_map})
│ │ +            col_name_mapping["multi_sparse"] = multi_sparse_col_map(multi_sparse_col)
│ │  
│ │          interaction_data = train_data[["user", "item", "label"]]
│ │          data_info = DataInfo(
│ │              col_name_mapping,
│ │              interaction_data,
│ │              user_sparse_unique,
│ │              user_dense_unique,
│ │ @@ -488,14 +535,18 @@
│ │      def merge_trainset(
│ │          cls, train_data, data_info, merge_behavior=True, shuffle=False, seed=42
│ │      ):
│ │          """Build transformed data by merging new train data with old data.
│ │  
│ │          .. versionadded:: 1.0.0
│ │  
│ │ +        .. versionchanged:: 1.1.0
│ │ +           Applying a more functional approach. A new ``data_info`` will be constructed
│ │ +           and returned, and the passed old ``data_info`` should be discarded.
│ │ +
│ │          Parameters
│ │          ----------
│ │          train_data : pandas.DataFrame
│ │              Data must contain at least three columns, i.e. ``user``, ``item``, ``label``.
│ │          data_info : DataInfo
│ │              Object that contains past data information.
│ │          merge_behavior : bool, default: True
│ │ @@ -503,182 +554,218 @@
│ │          shuffle : bool, default: False
│ │              Whether to fully shuffle data.
│ │          seed: int, default: 42
│ │              Random seed.
│ │  
│ │          Returns
│ │          -------
│ │ -        :class:`~libreco.data.TransformedSet`
│ │ -            Transformed Data object used for training.
│ │ +        new_trainset : :class:`~libreco.data.TransformedSet`
│ │ +            New transformed Data object used for training.
│ │ +        new_data_info : :class:`~libreco.data.DataInfo`
│ │ +            New ``data_info`` that contains some useful information.
│ │          """
│ │ -        cls._check_col_names(train_data, is_train=True)
│ │          assert isinstance(data_info, DataInfo), "Invalid passed `data_info`."
│ │ +        cls._check_col_names(train_data, is_train=True)
│ │ +        cls.user_unique_vals, cls.item_unique_vals = update_id_unique(
│ │ +            train_data, data_info
│ │ +        )
│ │ +        cls.sparse_unique_vals = update_sparse_unique(train_data, data_info)
│ │ +        cls.multi_sparse_unique_vals = update_multi_sparse_unique(train_data, data_info)
│ │          if shuffle:
│ │              train_data = cls.shuffle_data(train_data, seed)
│ │  
│ │ -        data_info.expand_sparse_unique_vals_and_matrix(train_data)
│ │          (
│ │              merge_transformed,
│ │              user_indices,
│ │              item_indices,
│ │              sparse_cols,
│ │              multi_sparse_cols,
│ │          ) = _build_transformed_set_feat(
│ │              train_data,
│ │ -            data_info.user_unique_vals,
│ │ -            data_info.item_unique_vals,
│ │ +            cls.user_unique_vals,
│ │ +            cls.item_unique_vals,
│ │              is_train=True,
│ │              is_ordered=False,
│ │              data_info=data_info,
│ │          )
│ │ +        sparse_offset = merge_offset(
│ │ +            sparse_cols,
│ │ +            multi_sparse_cols,
│ │ +            cls.sparse_unique_vals,
│ │ +            cls.multi_sparse_unique_vals,
│ │ +        )
│ │ +        sparse_oov = get_oov_pos(
│ │ +            sparse_cols,
│ │ +            multi_sparse_cols,
│ │ +            cls.sparse_unique_vals,
│ │ +            cls.multi_sparse_unique_vals,
│ │ +        )
│ │  
│ │ -        data_info.sparse_offset = (
│ │ -            merge_offset(data_info, sparse_cols, multi_sparse_cols)
│ │ -            if sparse_cols or multi_sparse_cols
│ │ -            else None
│ │ -        )
│ │ -        data_info.sparse_oov = (
│ │ -            get_oov_pos(data_info, sparse_cols, multi_sparse_cols)
│ │ -            if sparse_cols or multi_sparse_cols
│ │ -            else None
│ │ -        )
│ │ -        data_info.multi_sparse_combine_info = (
│ │ -            multi_sparse_combine_info(
│ │ -                data_info, data_info.sparse_col.name, sparse_cols, multi_sparse_cols
│ │ -            )
│ │ -            if multi_sparse_cols
│ │ -            else None
│ │ +        all_sparse_col = data_info.sparse_col.name
│ │ +        pad_val = (
│ │ +            data_info.multi_sparse_combine_info.pad_val
│ │ +            if cls.multi_sparse_unique_vals
│ │ +            else dict()
│ │ +        )
│ │ +        multi_sparse_info = get_multi_sparse_info(
│ │ +            all_sparse_col,
│ │ +            cls.sparse_col,
│ │ +            cls.multi_sparse_col,
│ │ +            cls.sparse_unique_vals,
│ │ +            cls.multi_sparse_unique_vals,
│ │ +            pad_val,
│ │          )
│ │ -        data_info.modify_sparse_indices()
│ │  
│ │ -        # if a user or item has duplicate features, will only update the last one.
│ │ -        user_data = train_data.drop_duplicates(subset=["user"], keep="last")
│ │ -        item_data = train_data.drop_duplicates(subset=["item"], keep="last")
│ │ -        data_info.assign_user_features(user_data)
│ │ -        data_info.assign_item_features(item_data)
│ │ -        data_info.add_oov()
│ │ -        user_indices, item_indices = data_info.update_consumed(
│ │ -            user_indices, item_indices, merge=merge_behavior
│ │ -        )
│ │ -        data_info.interaction_data = train_data[["user", "item", "label"]]
│ │ -        data_info.set_popular_items(100)
│ │ -        data_info.store_args(user_indices, item_indices)
│ │ +        _update_func = functools.partial(
│ │ +            update_unique_feats,
│ │ +            train_data,
│ │ +            data_info,
│ │ +            sparse_unique=cls.sparse_unique_vals,
│ │ +            multi_sparse_unique=cls.multi_sparse_unique_vals,
│ │ +            sparse_offset=sparse_offset,
│ │ +            sparse_oov=sparse_oov,
│ │ +        )
│ │ +        user_sparse_unique, user_dense_unique = _update_func(
│ │ +            unique_ids=cls.user_unique_vals, is_user=True
│ │ +        )
│ │ +        item_sparse_unique, item_dense_unique = _update_func(
│ │ +            unique_ids=cls.item_unique_vals, is_user=False
│ │ +        )
│ │ +
│ │ +        interaction_data = train_data[["user", "item", "label"]]
│ │ +        new_data_info = DataInfo(
│ │ +            data_info.col_name_mapping,
│ │ +            interaction_data,
│ │ +            user_sparse_unique,
│ │ +            user_dense_unique,
│ │ +            item_sparse_unique,
│ │ +            item_dense_unique,
│ │ +            user_indices,
│ │ +            item_indices,
│ │ +            cls.user_unique_vals,
│ │ +            cls.item_unique_vals,
│ │ +            cls.sparse_unique_vals,
│ │ +            sparse_offset,
│ │ +            sparse_oov,
│ │ +            cls.multi_sparse_unique_vals,
│ │ +            multi_sparse_info,
│ │ +        )
│ │ +        new_data_info = update_consumed(new_data_info, data_info, merge_behavior)
│ │ +        new_data_info.old_info = store_old_info(data_info)
│ │          cls.train_called = True
│ │ -        return merge_transformed
│ │ +        return merge_transformed, new_data_info
│ │  
│ │  
│ │  def _get_sparse_unique_vals(sparse_col, train_data):
│ │ -    if sparse_col:
│ │ -        sparse_unique_vals = dict()
│ │ -        for col in sparse_col:
│ │ -            sparse_unique_vals[col] = np.sort(train_data[col].unique())
│ │ -    else:
│ │ -        sparse_unique_vals = None
│ │ +    if not sparse_col:
│ │ +        return
│ │ +    sparse_unique_vals = dict()
│ │ +    for col in sparse_col:
│ │ +        sparse_unique_vals[col] = np.sort(train_data[col].unique())
│ │      return sparse_unique_vals
│ │  
│ │  
│ │  def _get_multi_sparse_unique_vals(multi_sparse_col, train_data, pad_val):
│ │ -    if multi_sparse_col:
│ │ -        multi_sparse_unique_vals = dict()
│ │ -        if not isinstance(pad_val, (list, tuple)):
│ │ -            pad_val = [pad_val] * len(multi_sparse_col)
│ │ -        assert len(multi_sparse_col) == len(
│ │ -            pad_val
│ │ -        ), "length of multi_sparse_col and pad_val doesn't match"
│ │ -        for i, field in enumerate(multi_sparse_col):
│ │ -            unique_vals = set(
│ │ -                itertools.chain.from_iterable(train_data[field].to_numpy().T)
│ │ -            )
│ │ -            if pad_val[i] in unique_vals:
│ │ -                unique_vals.remove(pad_val[i])
│ │ -            # use name of a field's first column as representative
│ │ -            multi_sparse_unique_vals[field[0]] = sorted(unique_vals)
│ │ -    else:
│ │ -        multi_sparse_unique_vals = None
│ │ -    return multi_sparse_unique_vals
│ │ +    if not multi_sparse_col:
│ │ +        return None, None
│ │ +    multi_sparse_unique_vals = dict()
│ │ +    if not isinstance(pad_val, (list, tuple)):
│ │ +        pad_val = [pad_val] * len(multi_sparse_col)
│ │ +    if len(multi_sparse_col) != len(pad_val):
│ │ +        raise ValueError("Length of `multi_sparse_col` and `pad_val` doesn't match")
│ │ +    pad_val_dict = dict()
│ │ +    for i, field in enumerate(multi_sparse_col):
│ │ +        unique_vals = set(itertools.chain.from_iterable(train_data[field].to_numpy().T))
│ │ +        if pad_val[i] in unique_vals:
│ │ +            unique_vals.remove(pad_val[i])
│ │ +        # use name of a field's first column as representative
│ │ +        multi_sparse_unique_vals[field[0]] = np.sort(list(unique_vals))
│ │ +        pad_val_dict[field[0]] = pad_val[i]
│ │ +    return multi_sparse_unique_vals, pad_val_dict
│ │  
│ │  
│ │  def _build_transformed_set(
│ │      data,
│ │      user_unique_vals,
│ │      item_unique_vals,
│ │      is_train,
│ │      is_ordered,
│ │      has_feats=False,
│ │  ):
│ │ -    mode = "train" if is_train else "test"
│ │ -    user_indices, item_indices = get_user_item_sparse_indices(
│ │ +    user_indices, item_indices = get_id_indices(
│ │          data,
│ │          user_unique_vals,
│ │          item_unique_vals,
│ │ -        mode=mode,
│ │ -        ordered=is_ordered,
│ │ +        is_train,
│ │ +        is_ordered,
│ │      )
│ │      if "label" in data.columns:
│ │          labels = data["label"].to_numpy(dtype=np.float32)
│ │      else:
│ │          # in case test_data has no label column, create dummy labels for consistency
│ │ -        labels = np.zeros(len(data))
│ │ +        labels = np.zeros(len(data), dtype=np.float32)
│ │  
│ │      transformed_data = TransformedSet(
│ │          user_indices, item_indices, labels, train=is_train
│ │      )
│ │      if has_feats:
│ │ -        return user_indices, item_indices, labels, mode
│ │ +        return user_indices, item_indices, labels
│ │ +
│ │      if is_train:
│ │          return transformed_data, user_indices, item_indices
│ │      else:
│ │          return transformed_data
│ │  
│ │  
│ │  def _build_transformed_set_feat(
│ │      data,
│ │      user_unique_vals,
│ │      item_unique_vals,
│ │      is_train,
│ │      is_ordered,
│ │      data_info=None,
│ │  ):
│ │ -    user_indices, item_indices, labels, mode = _build_transformed_set(
│ │ +    user_indices, item_indices, labels = _build_transformed_set(
│ │          data, user_unique_vals, item_unique_vals, is_train, is_ordered, has_feats=True
│ │      )
│ │  
│ │      sparse_indices, dense_values, sparse_cols, multi_sparse_cols = _build_features(
│ │ -        data, mode, is_ordered, data_info
│ │ +        data, is_train, is_ordered, data_info
│ │      )
│ │      transformed_data = TransformedSet(
│ │          user_indices, item_indices, labels, sparse_indices, dense_values, train=is_train
│ │      )
│ │      if not is_train:
│ │          return transformed_data
│ │  
│ │      pure_data = transformed_data, user_indices, item_indices
│ │      if not data_info:
│ │ -        return pure_data + (sparse_indices, dense_values)
│ │ +        return pure_data + (sparse_indices, dense_values)  # noqa: RUF005
│ │      else:
│ │ -        return pure_data + (sparse_cols, multi_sparse_cols)
│ │ +        return pure_data + (sparse_cols, multi_sparse_cols)  # noqa: RUF005
│ │  
│ │  
│ │ -def _build_features(data, mode, is_ordered, data_info):
│ │ +def _build_features(data, is_train, is_ordered, data_info):
│ │      sparse_indices, dense_values = None, None
│ │      if data_info:
│ │ -        data_class = data_info
│ │          sparse_cols, multi_sparse_cols = recover_sparse_cols(data_info)
│ │          dense_cols = data_info.dense_col.name
│ │      else:
│ │ -        data_class = DatasetFeat
│ │          sparse_cols = DatasetFeat.sparse_col
│ │          multi_sparse_cols = DatasetFeat.multi_sparse_col
│ │          dense_cols = DatasetFeat.dense_col
│ │  
│ │ +    sparse_unique = DatasetFeat.sparse_unique_vals
│ │ +    multi_sparse_unique = DatasetFeat.multi_sparse_unique_vals
│ │      if sparse_cols or multi_sparse_cols:
│ │          sparse_indices = merge_sparse_indices(
│ │ -            data_class,
│ │              data,
│ │              sparse_cols,
│ │              multi_sparse_cols,
│ │ -            mode=mode,
│ │ -            ordered=is_ordered,
│ │ +            sparse_unique,
│ │ +            multi_sparse_unique,
│ │ +            is_train,
│ │ +            is_ordered,
│ │          )
│ │      if dense_cols:
│ │ -        dense_values = data[dense_cols].to_numpy()
│ │ +        dense_values = data[dense_cols].to_numpy(dtype=np.float32)
│ │      return sparse_indices, dense_values, sparse_cols, multi_sparse_cols
│ │   --- LibRecommender-1.0.1/libreco/data/sequence.py
│ ├── +++ LibRecommender-1.1.0/libreco/batch/sequence.py
│ │┄ Files 16% similar despite different names
│ │ @@ -1,13 +1,13 @@
│ │  import random
│ │  
│ │  import numpy as np
│ │  
│ │  
│ │ -def sparse_user_interacted(
│ │ +def get_sparse_interacted(
│ │      user_indices, item_indices, user_consumed, mode=None, num=None
│ │  ):
│ │      interacted_indices = []
│ │      interacted_items = []
│ │      for j, (u, i) in enumerate(zip(user_indices, item_indices)):
│ │          consumed_items = user_consumed[u]
│ │          position = consumed_items.index(i)
│ │ @@ -21,39 +21,22 @@
│ │              interacted_indices.extend([j] * num)
│ │              interacted_items.extend(consumed_items[start_index:position])
│ │          elif position >= num and mode == "random":
│ │              interacted_indices.extend([j] * num)
│ │              chosen_items = np.random.choice(consumed_items, num, replace=False)
│ │              interacted_items.extend(chosen_items.tolist())
│ │  
│ │ -    assert len(interacted_indices) == len(
│ │ -        interacted_items
│ │ -    ), "length of indices and values doesn't match"
│ │      interacted_indices = np.asarray(interacted_indices).reshape(-1, 1)
│ │      indices = np.concatenate(
│ │          [interacted_indices, np.zeros_like(interacted_indices)], axis=1
│ │      )
│ │ -    return indices, interacted_items, len(user_indices)
│ │ +    return indices, np.array(interacted_items), len(user_indices)
│ │  
│ │  
│ │ -# def sample_item_with_tolerance(num, consumed_items, consumed_len, tolerance=5):
│ │ -#    assert num > tolerance
│ │ -#    sampled = []
│ │ -#    first_len = num - tolerance
│ │ -#    while len(sampled) < first_len:
│ │ -#        i = floor(random() * consumed_len)
│ │ -#        if consumed_items[i] not in sampled:
│ │ -#            sampled.append(consumed_items[i])
│ │ -#    for _ in range(tolerance):
│ │ -#        i = floor(random() * consumed_len)
│ │ -#        sampled.append(consumed_items[i])
│ │ -#    return sampled
│ │ -
│ │ -
│ │ -def user_interacted_seq(
│ │ +def get_interacted_seq(
│ │      user_indices,
│ │      item_indices,
│ │      user_consumed,
│ │      pad_index,
│ │      mode,
│ │      num,
│ │      user_consumed_set,
│ │ @@ -83,15 +66,15 @@
│ │                  start_index = position - num
│ │                  batch_interacted[j] = consumed_items[start_index:position]
│ │              elif mode == "random":
│ │                  chosen_items = np_rng.choice(consumed_items, num, replace=False)
│ │                  batch_interacted[j] = chosen_items
│ │              batch_interacted_len.append(float(num))
│ │  
│ │ -    return batch_interacted, batch_interacted_len
│ │ +    return batch_interacted, np.array(batch_interacted_len)
│ │  
│ │  
│ │  # most recent num items a user has interacted, assume already sorted by time.
│ │  def get_user_last_interacted(n_users, user_consumed, pad_index, recent_num=10):
│ │      u_last_interacted = np.full((n_users, recent_num), pad_index, dtype=np.int32)
│ │      interacted_len = []
│ │      for u in range(n_users):
│ │ @@ -100,8 +83,8 @@
│ │          if u_items_len < recent_num:
│ │              u_last_interacted[u, -u_items_len:] = u_consumed_items
│ │              interacted_len.append(float(u_items_len))
│ │          else:
│ │              u_last_interacted[u] = u_consumed_items[-recent_num:]
│ │              interacted_len.append(float(recent_num))
│ │  
│ │ -    return u_last_interacted, np.asarray(interacted_len)
│ │ +    return u_last_interacted, np.array(interacted_len)
│ │   --- LibRecommender-1.0.1/libreco/data/split.py
│ ├── +++ LibRecommender-1.1.0/libreco/data/split.py
│ │┄ Files 4% similar despite different names
│ │ @@ -336,17 +336,16 @@
│ │      --------
│ │      split_by_ratio
│ │      """
│ │      assert all(
│ │          ["user" in data.columns, "time" in data.columns]
│ │      ), "data must contains user and time column"
│ │  
│ │ -    data = data.sort_values(by=["time"])
│ │ -    data.reset_index(drop=True, inplace=True)
│ │ -    return split_by_ratio(**locals())
│ │ +    data = data.sort_values(by=["time"]).reset_index(drop=True)
│ │ +    return split_by_ratio(data, order, shuffle, test_size, multi_ratios, seed=seed)
│ │  
│ │  
│ │  def split_by_num_chrono(data, order=True, shuffle=False, test_size=1, seed=42):
│ │      """Assign a certain number of items to test data for each user, where items are sorted by time first.
│ │  
│ │      .. IMPORTANT::
│ │          This function implies the data should contain a **time** column.
│ │ @@ -378,17 +377,16 @@
│ │      --------
│ │      split_by_num
│ │      """
│ │      assert all(
│ │          ["user" in data.columns, "time" in data.columns]
│ │      ), "data must contains user and time column"
│ │  
│ │ -    data = data.sort_values(by=["time"])
│ │ -    data.reset_index(drop=True, inplace=True)
│ │ -    return split_by_num(**locals())
│ │ +    data = data.sort_values(by=["time"]).reset_index(drop=True)
│ │ +    return split_by_num(data, order, shuffle, test_size, seed=seed)
│ │  
│ │  
│ │  def _groupby_user(user_indices, order):
│ │      sort_kind = "mergesort" if order else "quicksort"
│ │      _, user_position, user_counts = np.unique(
│ │          user_indices, return_inverse=True, return_counts=True
│ │      )
│ │   --- LibRecommender-1.0.1/libreco/data/transformed.py
│ ├── +++ LibRecommender-1.1.0/libreco/data/transformed.py
│ │┄ Files 15% similar despite different names
│ │ @@ -1,28 +1,30 @@
│ │  """Transformed Dataset."""
│ │ +import warnings
│ │ +
│ │  import numpy as np
│ │  from scipy.sparse import csr_matrix
│ │  
│ │ -from ..feature import interaction_consumed
│ │ +from .consumed import interaction_consumed
│ │  from ..utils.sampling import NegativeSampling
│ │  
│ │  
│ │  class TransformedSet:
│ │      """Dataset after transforming.
│ │  
│ │      Often generated by calling functions in ``DatasetPure`` or ``DatasetFeat``,
│ │ -    then ``TransformedSet`` is used in formal training.
│ │ +    then ``TransformedSet`` will be used in formal training.
│ │  
│ │      Parameters
│ │      ----------
│ │ -    user_indices : numpy.ndarray or None, default: None
│ │ +    user_indices : numpy.ndarray
│ │          All user rows in data, represented in inner id.
│ │ -    item_indices : numpy.ndarray or None, default: None
│ │ +    item_indices : numpy.ndarray
│ │          All item rows in data, represented in inner id.
│ │ -    labels : numpy.ndarray or None, default: None
│ │ +    labels : numpy.ndarray
│ │          All labels in data.
│ │      sparse_indices : numpy.ndarray or None, default: None
│ │          All sparse rows in data, represented in inner id.
│ │      dense_values : numpy.ndarray or None, default: None
│ │          All dense rows in data.
│ │      train : bool, default: True
│ │          Whether it is train data.
│ │ @@ -62,25 +64,35 @@
│ │          self.dense_values_orig = None
│ │  
│ │      def build_negative_samples(
│ │          self, data_info, num_neg=1, item_gen_mode="random", seed=42
│ │      ):
│ │          """Perform negative sampling on all the data contained.
│ │  
│ │ +        .. deprecated:: 1.1.0
│ │ +           Use ``neg_sampling`` parameter instead of explicitly calling this method
│ │ +           for negative sampling. See :ref:`Negative Sampling`.
│ │ +
│ │          Parameters
│ │          ----------
│ │          data_info : DataInfo
│ │              Object contains data information.
│ │          num_neg : int, default: 1
│ │              Number of negative samples for each positive sample.
│ │          item_gen_mode : str, default: 'random'
│ │              Sampling strategy, currently only 'random' is supported.
│ │          seed : int, default: 42
│ │              Random seed.
│ │          """
│ │ +        warnings.warn(
│ │ +            "`build_negative_samples` is deprecated, and it will be removed in the future. "
│ │ +            "Use `neg_sampling` parameter instead",
│ │ +            DeprecationWarning,
│ │ +            stacklevel=2,
│ │ +        )
│ │          self.has_sampled = True
│ │          self.user_indices_orig = self._user_indices
│ │          self.item_indices_orig = self._item_indices
│ │          self.labels_orig = self._labels
│ │          self.sparse_indices_orig = self._sparse_indices
│ │          self.dense_values_orig = self._dense_values
│ │          self._sampling_impl(data_info, num_neg, item_gen_mode, seed)
│ │ @@ -101,28 +113,15 @@
│ │          ) = neg.generate_all(seed=seed, item_gen_mode=item_gen_mode)
│ │  
│ │      def __len__(self):
│ │          return len(self.labels)
│ │  
│ │      def __getitem__(self, index):
│ │          """Get a slice of data."""
│ │ -        pure_part = (
│ │ -            self.user_indices[index],
│ │ -            self.item_indices[index],
│ │ -            self.labels[index],
│ │ -        )
│ │ -        sparse_part = (
│ │ -            (self.sparse_indices[index],)
│ │ -            if self.sparse_indices is not None
│ │ -            else (None,)
│ │ -        )
│ │ -        dense_part = (
│ │ -            (self.dense_values[index],) if self.dense_values is not None else (None,)
│ │ -        )
│ │ -        return pure_part + sparse_part + dense_part
│ │ +        return self.user_indices[index], self.item_indices[index], self.labels[index]
│ │  
│ │      @property
│ │      def user_indices(self):
│ │          """All user rows in data"""
│ │          return self._user_indices
│ │  
│ │      @property
│ │   --- LibRecommender-1.0.1/libreco/evaluation/computation.py
│ ├── +++ LibRecommender-1.1.0/libreco/utils/validate.py
│ │┄ Files 27% similar despite different names
│ │ @@ -1,142 +1,162 @@
│ │  import numpy as np
│ │ -from scipy.special import expit
│ │ -from tqdm import tqdm
│ │  
│ │ -from ..data import TransformedSet
│ │ -from ..feature import features_from_batch_data
│ │ -from ..tfops import get_feed_dict
│ │ -from ..utils.constants import TF_FEAT_MODELS
│ │ -
│ │ -
│ │ -def build_eval_transformed_data(model, data, negative_sample, update_features, seed):
│ │ -    data_info = model.data_info
│ │ -    n_users = data_info.n_users
│ │ -    n_items = data_info.n_items
│ │ -    users = data.user.tolist()
│ │ -    items = data.item.tolist()
│ │ -    user_indices = np.array([data_info.user2id.get(u, n_users) for u in users])
│ │ -    item_indices = np.array([data_info.item2id.get(i, n_items) for i in items])
│ │ -    labels = data.label.to_numpy(dtype=np.float32)
│ │ -    sparse_indices, dense_values = None, None
│ │ -    if (
│ │ -        data_info.col_name_mapping is not None
│ │ -        and hasattr(model, "sparse")
│ │ -        and hasattr(model, "dense")
│ │ -    ):
│ │ -        sparse_indices, dense_values = features_from_batch_data(
│ │ -            data_info, model.sparse, model.dense, data
│ │ -        )
│ │ -    # todo: merge user_consumed
│ │ -    transformed_data = TransformedSet(
│ │ -        user_indices, item_indices, labels, sparse_indices, dense_values, train=False
│ │ -    )
│ │ -    if update_features:
│ │ -        # if a user or item has duplicate features, will only update the last one.
│ │ -        user_data = data.drop_duplicates(subset=["user"], keep="last")
│ │ -        item_data = data.drop_duplicates(subset=["item"], keep="last")
│ │ -        model.data_info.assign_user_features(user_data)
│ │ -        model.data_info.assign_item_features(item_data)
│ │ -    if negative_sample:
│ │ -        transformed_data.build_negative_samples(
│ │ -            data_info, item_gen_mode="random", seed=seed
│ │ -        )
│ │ -    return transformed_data
│ │ +from ..utils.misc import colorize
│ │  
│ │  
│ │ -def compute_preds(model, data, batch_size):
│ │ -    y_pred = list()
│ │ -    y_label = list()
│ │ -    predict_func = choose_pred_func(model)
│ │ -    for batch_data in tqdm(range(0, len(data), batch_size), desc="eval_pointwise"):
│ │ -        batch_slice = slice(batch_data, batch_data + batch_size)
│ │ -        labels = data.labels[batch_slice]
│ │ -        preds = predict_func(model, data, batch_slice)
│ │ -        y_pred.extend(preds)
│ │ -        y_label.extend(labels)
│ │ -    return y_pred, y_label
│ │ -
│ │ -
│ │ -def compute_probs(model, data, batch_size):
│ │ -    return compute_preds(model, data, batch_size)
│ │ -
│ │ -
│ │ -def compute_recommends(model, users, k, num_batch_users):
│ │ -    y_recommends = dict()
│ │ -    no_rec_num = 0
│ │ -    no_rec_users = []
│ │ -    for i in tqdm(range(0, len(users), num_batch_users), desc="eval_listwise"):
│ │ -        batch_users = users[i: i + num_batch_users]
│ │ -        batch_recs = model.recommend_user(
│ │ -            user=batch_users,
│ │ -            n_rec=k,
│ │ -            inner_id=True,
│ │ -            filter_consumed=True,
│ │ -            random_rec=False,
│ │ +def check_unknown(model, user, item):
│ │ +    unknown_user_indices = list(np.where(user == model.n_users)[0])
│ │ +    unknown_item_indices = list(np.where(item == model.n_items)[0])
│ │ +    unknown_index = list(set(unknown_user_indices) | set(unknown_item_indices))
│ │ +    unknown_num = len(unknown_index)
│ │ +    if unknown_num > 0:
│ │ +        unknown_str = (
│ │ +            f"Detect {unknown_num} unknown interaction(s), position: {unknown_index}"
│ │          )
│ │ -        for u in batch_users:
│ │ -            if len(batch_recs[u]) == 0:
│ │ -                # print("no recommend user: ", u)
│ │ -                no_rec_num += 1
│ │ -                no_rec_users.append(u)
│ │ -                continue
│ │ -            y_recommends[u] = batch_recs[u]
│ │ -    if no_rec_num > 0:
│ │ -        # print(f"{no_rec_num} users has no recommendation")
│ │ -        users = list(set(users).difference(no_rec_users))
│ │ -    return y_recommends, users
│ │ -
│ │ -
│ │ -def choose_pred_func(model):
│ │ -    if model.__class__.__name__ not in TF_FEAT_MODELS:
│ │ -        pred_func = predict_pure
│ │ +        print(f"{colorize(unknown_str, 'red')}")
│ │ +    return unknown_num, unknown_index, user, item
│ │ +
│ │ +
│ │ +def check_unknown_user(data_info, user, inner_id=False):
│ │ +    known_users_ids, unknown_users = [], []
│ │ +    users = [user] if np.isscalar(user) else user
│ │ +    for u in users:
│ │ +        user_id = data_info.user2id.get(u, -1) if not inner_id else u
│ │ +        if 0 <= user_id < data_info.n_users:
│ │ +            known_users_ids.append(user_id)
│ │ +        else:
│ │ +            if not inner_id:
│ │ +                unknown_str = f"Detect unknown user: {u}"
│ │ +                print(f"{colorize(unknown_str, 'red')}")
│ │ +            unknown_users.append(u)
│ │ +    return known_users_ids, unknown_users
│ │ +
│ │ +
│ │ +# def check_has_sampled(data, verbose):
│ │ +#    if not data.has_sampled and verbose > 1:
│ │ +#        exception_str = (
│ │ +#            "During training, "
│ │ +#            "one must do whole data sampling "
│ │ +#            "before evaluating on epochs."
│ │ +#        )
│ │ +#        raise NotSamplingError(f"{colorize(exception_str, 'red')}")
│ │ +
│ │ +
│ │ +def check_seq_mode(recent_num, random_num):
│ │ +    if recent_num is not None:
│ │ +        assert isinstance(recent_num, int), "recent_num must be integer"
│ │ +        mode = "recent"
│ │ +        num = recent_num
│ │ +    elif random_num is not None:
│ │ +        assert isinstance(random_num, int), "random_num must be integer"
│ │ +        mode = "random"
│ │ +        num = random_num
│ │      else:
│ │ -        pred_func = predict_tf_feat
│ │ -    return pred_func
│ │ +        mode = "recent"
│ │ +        num = 10  # by default choose 10 recent interactions
│ │ +    return mode, num
│ │  
│ │  
│ │ -def predict_pure(model, transformed_data, batch_slice):
│ │ -    user_indices, item_indices, _, _, _ = transformed_data[batch_slice]
│ │ -    preds = model.predict(user_indices, item_indices, inner_id=True)
│ │ -    if isinstance(preds, np.ndarray):
│ │ -        preds = preds.tolist()
│ │ -    elif not isinstance(preds, list):
│ │ -        preds = [preds]
│ │ -    return preds
│ │ -
│ │ -
│ │ -def predict_tf_feat(model, transformed_data, batch_slice):
│ │ -    (
│ │ -        user_indices,
│ │ -        item_indices,
│ │ -        _,
│ │ -        sparse_indices,
│ │ -        dense_values,
│ │ -    ) = transformed_data[batch_slice]
│ │ -
│ │ -    if model.model_category == "sequence":
│ │ -        feed_dict = get_feed_dict(
│ │ -            model=model,
│ │ -            user_indices=user_indices,
│ │ -            item_indices=item_indices,
│ │ -            sparse_indices=sparse_indices,
│ │ -            dense_values=dense_values,
│ │ -            user_interacted_seq=model.recent_seqs[user_indices],
│ │ -            user_interacted_len=model.recent_seq_lens[user_indices],
│ │ -            is_training=False,
│ │ +def check_sparse_indices(data_info):
│ │ +    return False if not data_info.sparse_col.name else True
│ │ +
│ │ +
│ │ +def check_dense_values(data_info):
│ │ +    return False if not data_info.dense_col.name else True
│ │ +
│ │ +
│ │ +def sparse_feat_size(data_info):
│ │ +    if (
│ │ +        data_info.user_sparse_unique is not None
│ │ +        and data_info.item_sparse_unique is not None
│ │ +    ):
│ │ +        return (
│ │ +            max(
│ │ +                np.max(data_info.user_sparse_unique),
│ │ +                np.max(data_info.item_sparse_unique),
│ │ +            )
│ │ +            + 1
│ │          )
│ │ +    elif data_info.user_sparse_unique is not None:
│ │ +        return np.max(data_info.user_sparse_unique) + 1
│ │ +    elif data_info.item_sparse_unique is not None:
│ │ +        return np.max(data_info.item_sparse_unique) + 1
│ │ +
│ │ +
│ │ +def sparse_field_size(data_info):
│ │ +    return len(data_info.sparse_col.name)
│ │ +
│ │ +
│ │ +def dense_field_size(data_info):
│ │ +    return len(data_info.dense_col.name)
│ │ +
│ │ +
│ │ +def check_multi_sparse(data_info, multi_sparse_combiner):
│ │ +    if data_info.multi_sparse_combine_info and multi_sparse_combiner is not None:
│ │ +        if multi_sparse_combiner not in ("normal", "sum", "mean", "sqrtn"):
│ │ +            raise ValueError(
│ │ +                f"unsupported multi_sparse_combiner type: {multi_sparse_combiner}"
│ │ +            )
│ │ +        else:
│ │ +            combiner = multi_sparse_combiner
│ │      else:
│ │ -        feed_dict = get_feed_dict(
│ │ -            model=model,
│ │ -            user_indices=user_indices,
│ │ -            item_indices=item_indices,
│ │ -            sparse_indices=sparse_indices,
│ │ -            dense_values=dense_values,
│ │ -            is_training=False,
│ │ +        combiner = "normal"
│ │ +    return combiner
│ │ +
│ │ +
│ │ +def check_fitting(model, train_data, eval_data, neg_sampling, k):
│ │ +    check_neg_sampling(model, neg_sampling)
│ │ +    check_labels(model, train_data.labels, neg_sampling)
│ │ +    check_retrain_loaded_model(model)
│ │ +    check_eval(eval_data, k, model.n_items)
│ │ +
│ │ +
│ │ +def check_neg_sampling(model, neg_sampling):
│ │ +    assert isinstance(neg_sampling, bool), (
│ │ +        f"`neg_sampling` in `fit()` must be bool, got `{neg_sampling}`. "
│ │ +        f"Set `model.fit(..., neg_sampling=True)` if your data is implicit(i.e., `task` is ranking) "
│ │ +        f"and ONLY contains positive labels. Otherwise, negative sampling is not needed."
│ │ +    )
│ │ +    if model.task == "rating" and neg_sampling:
│ │ +        raise ValueError("`rating` task should not use negative sampling")
│ │ +    if (
│ │ +        hasattr(model, "loss_type")
│ │ +        and model.loss_type in ("bpr", "max_margin")
│ │ +        and not neg_sampling
│ │ +    ):
│ │ +        raise ValueError(f"`{model.loss_type}` loss must use negative sampling.")
│ │ +
│ │ +
│ │ +def check_labels(model, labels, neg_sampling):
│ │ +    # still needs negative sampling when evaluating for these models
│ │ +    # if model.model_name in (
│ │ +    #    "YouTubeRetrieval",
│ │ +    #    "UserCF",
│ │ +    #    "ItemCF",
│ │ +    #    "Item2Vec",
│ │ +    #    "DeepWalk",
│ │ +    # ):
│ │ +    #    return
│ │ +    if model.task == "ranking" and not neg_sampling:
│ │ +        unique_labels = np.unique(labels)
│ │ +        if (
│ │ +            len(unique_labels) != 2
│ │ +            or min(unique_labels) != 0.0
│ │ +            or max(unique_labels) != 1.0
│ │ +        ):
│ │ +            raise ValueError(
│ │ +                f"For `ranking` task without negative sampling, labels in data must be 0 and 1, "
│ │ +                f"got unique labels: {unique_labels}"
│ │ +            )
│ │ +
│ │ +
│ │ +def check_retrain_loaded_model(model):
│ │ +    if hasattr(model, "loaded") and model.loaded:
│ │ +        raise RuntimeError(
│ │ +            "Loaded model doesn't support retraining, use `rebuild_model` instead. "
│ │ +            "Or constructing a new model from scratch."
│ │          )
│ │  
│ │ -    preds = model.sess.run(model.output, feed_dict)
│ │ -    if model.task == "rating":
│ │ -        preds = np.clip(preds, model.lower_bound, model.upper_bound)
│ │ -    elif model.task == "ranking":
│ │ -        preds = expit(preds)
│ │ -    return preds.tolist() if isinstance(preds, np.ndarray) else [preds]
│ │ +
│ │ +def check_eval(eval_data, k, n_items):
│ │ +    if eval_data is not None and k > n_items:
│ │ +        raise ValueError(f"eval `k` {k} exceeds num of items {n_items}")
│ │   --- LibRecommender-1.0.1/libreco/evaluation/evaluate.py
│ ├── +++ LibRecommender-1.1.0/libreco/evaluation/evaluate.py
│ │┄ Files 27% similar despite different names
│ │ @@ -1,266 +1,182 @@
│ │  """Utility Functions for Evaluating Data."""
│ │ +import functools
│ │  import math
│ │  import numbers
│ │  
│ │  import numpy as np
│ │  import pandas as pd
│ │ -from sklearn.metrics import (
│ │ -    auc,
│ │ -    balanced_accuracy_score,
│ │ -    log_loss,
│ │ -    mean_absolute_error,
│ │ -    mean_squared_error,
│ │ -    precision_recall_curve,
│ │ -    r2_score,
│ │ -    roc_auc_score,
│ │ -)
│ │ +from sklearn.metrics import log_loss, mean_absolute_error, r2_score, roc_auc_score
│ │  
│ │ -from ..data import TransformedSet
│ │  from .computation import (
│ │      build_eval_transformed_data,
│ │      compute_preds,
│ │      compute_probs,
│ │      compute_recommends,
│ │  )
│ │  from .metrics import (
│ │ -    ALLOWED_METRICS,
│ │      LISTWISE_METRICS,
│ │      POINTWISE_METRICS,
│ │ -    map_at_k,
│ │ +    RANKING_METRICS,
│ │ +    RATING_METRICS,
│ │ +    average_precision_at_k,
│ │ +    balanced_accuracy,
│ │ +    listwise_scores,
│ │      ndcg_at_k,
│ │ +    pr_auc_score,
│ │      precision_at_k,
│ │      recall_at_k,
│ │ +    rmse,
│ │  )
│ │ +from ..data import TransformedSet
│ │  
│ │  
│ │  def _check_metrics(task, metrics, k):
│ │      if not isinstance(metrics, (list, tuple)):
│ │          metrics = [metrics]
│ │      if task == "rating":
│ │          for m in metrics:
│ │ -            if m not in ALLOWED_METRICS["rating_metrics"]:
│ │ -                raise ValueError(f"metrics {m} is not suitable for rating task...")
│ │ +            if m not in RATING_METRICS:
│ │ +                raise ValueError(f"Metrics `{m}` is not suitable for rating task...")
│ │      elif task == "ranking":
│ │          for m in metrics:
│ │ -            if m not in ALLOWED_METRICS["ranking_metrics"]:
│ │ -                raise ValueError(f"metrics {m} is not suitable for ranking task...")
│ │ +            if m not in RANKING_METRICS:
│ │ +                raise ValueError(f"Metrics `{m}` is not suitable for ranking task...")
│ │  
│ │      if not isinstance(k, numbers.Integral):
│ │ -        raise TypeError("k must be integer")
│ │ -
│ │ +        raise TypeError("`k` must be integer")
│ │      return metrics
│ │  
│ │  
│ │ -def print_metrics(
│ │ -    model,
│ │ -    train_data=None,
│ │ -    eval_data=None,
│ │ -    metrics=None,
│ │ -    eval_batch_size=8192,
│ │ -    k=10,
│ │ -    sample_user_num=2048,
│ │ -    seed=42,
│ │ -):
│ │ -    if not metrics:
│ │ -        metrics = ["loss"]
│ │ -    metrics = _check_metrics(model.task, metrics, k)
│ │ -
│ │ -    if model.task == "rating":
│ │ -        if train_data:
│ │ -            y_pred, y_true = compute_preds(model, train_data, eval_batch_size)
│ │ -            print_metrics_rating(metrics, y_true, y_pred, train=True)
│ │ -        if eval_data:
│ │ -            y_pred, y_true = compute_preds(model, eval_data, eval_batch_size)
│ │ -            print_metrics_rating(metrics, y_true, y_pred, train=False)
│ │ -
│ │ -    elif model.task == "ranking":
│ │ -        if train_data and POINTWISE_METRICS.intersection(metrics):
│ │ -            y_prob, y_true = compute_probs(model, train_data, eval_batch_size)
│ │ -            log_loss_ = log_loss(y_true, y_prob, eps=1e-7)
│ │ -            print(f"\t train log_loss: {log_loss_:.4f}")
│ │ -        if eval_data:
│ │ -            if POINTWISE_METRICS.intersection(metrics):
│ │ -                y_prob, y_true = compute_probs(model, eval_data, eval_batch_size)
│ │ -                print_metrics_ranking_pointwise(metrics, y_prob, y_true)
│ │ -            if LISTWISE_METRICS.intersection(metrics):
│ │ -                chosen_users = sample_user(eval_data, seed, sample_user_num)
│ │ -                num_batch_users = max(1, math.floor(eval_batch_size / model.n_items))
│ │ -                y_true_list = eval_data.user_consumed
│ │ -                y_reco_list, users = compute_recommends(
│ │ -                    model, chosen_users, k, num_batch_users
│ │ -                )
│ │ -                print_metrics_ranking_listwise(
│ │ -                    metrics, y_reco_list, y_true_list, users, k
│ │ -                )
│ │ -
│ │ -
│ │ -# noinspection PyTypeChecker
│ │ -def sample_user(data, seed, num):
│ │ -    np.random.seed(seed)
│ │ -    unique_users = np.unique(data.user_indices)
│ │ +def sample_users(data, seed, num):
│ │ +    np_rng = np.random.default_rng(seed)
│ │ +    unique_users = list(data.user_consumed)
│ │      if isinstance(num, numbers.Integral) and 0 < num < len(unique_users):
│ │ -        users = np.random.choice(unique_users, num, replace=False)
│ │ +        users = np_rng.choice(unique_users, num, replace=False).tolist()
│ │      else:
│ │          users = unique_users
│ │ -    if isinstance(users, np.ndarray):
│ │ -        users = list(users)
│ │      return users
│ │  
│ │  
│ │ -def print_metrics_rating(metrics, y_true, y_pred, train=True, **kwargs):
│ │ -    if kwargs.get("lower_bound") and kwargs.get("upper_bound"):
│ │ -        lower_bound, upper_bound = (
│ │ -            kwargs.get("lower_bound"),
│ │ -            kwargs.get("upper_bound"),
│ │ -        )
│ │ -        y_pred = np.clip(y_pred, lower_bound, upper_bound)
│ │ -    if train:
│ │ -        for m in metrics:
│ │ -            if m in ["rmse", "loss"]:
│ │ -                rmse = np.sqrt(mean_squared_error(y_true, y_pred))
│ │ -                print(f"\t train rmse: {rmse:.4f}")
│ │ -    else:
│ │ -        for m in metrics:
│ │ -            if m in ["rmse", "loss"]:
│ │ -                rmse = np.sqrt(mean_squared_error(y_true, y_pred))
│ │ -                print(f"\t eval rmse: {rmse:.4f}")
│ │ -            elif m == "mae":
│ │ -                mae = mean_absolute_error(y_true, y_pred)
│ │ -                print(f"\t eval mae: {mae:.4f}")
│ │ -            elif m == "r2":
│ │ -                r_squared = r2_score(y_true, y_pred)
│ │ -                print(f"\t eval r2: {r_squared:.4f}")
│ │ -
│ │ -
│ │ -def print_metrics_ranking_pointwise(metrics, y_prob, y_true):
│ │ -    for m in metrics:
│ │ -        if m in ["log_loss", "loss"]:
│ │ -            log_loss_ = log_loss(y_true, y_prob, eps=1e-7)
│ │ -            print(f"\t eval log_loss: {log_loss_:.4f}")
│ │ -        elif m == "balanced_accuracy":
│ │ -            y_pred = np.round(y_prob)
│ │ -            accuracy = balanced_accuracy_score(y_true, y_pred)
│ │ -            print(f"\t eval balanced accuracy: {accuracy:.4f}")
│ │ -        elif m == "roc_auc":
│ │ -            roc_auc = roc_auc_score(y_true, y_prob)
│ │ -            print(f"\t eval roc_auc: {roc_auc:.4f}")
│ │ -        elif m == "pr_auc":
│ │ -            precision, recall, _ = precision_recall_curve(y_true, y_prob)
│ │ -            pr_auc = auc(recall, precision)
│ │ -            print(f"\t eval pr_auc: {pr_auc:.4f}")
│ │ -
│ │ -
│ │ -def print_metrics_ranking_listwise(metrics, y_reco_list, y_true_list, users, k):
│ │ -    for m in metrics:
│ │ -        if m == "precision":
│ │ -            precision_all = precision_at_k(y_true_list, y_reco_list, users, k)
│ │ -            print(f"\t eval precision@{k}: {precision_all:.4f}")
│ │ -        elif m == "recall":
│ │ -            recall_all = recall_at_k(y_true_list, y_reco_list, users, k)
│ │ -            print(f"\t eval recall@{k}: {recall_all:.4f}")
│ │ -        elif m == "map":
│ │ -            map_all = map_at_k(y_true_list, y_reco_list, users, k)
│ │ -            print(f"\t eval map@{k}: {map_all:.4f}")
│ │ -        elif m == "ndcg":
│ │ -            ndcg_all = ndcg_at_k(y_true_list, y_reco_list, users, k)
│ │ -            print(f"\t eval ndcg@{k}: {ndcg_all:.4f}")
│ │ -
│ │ -
│ │  def evaluate(
│ │      model,
│ │      data,
│ │ +    neg_sampling,
│ │      eval_batch_size=8192,
│ │      metrics=None,
│ │      k=10,
│ │ -    sample_user_num=2048,
│ │ -    neg_sample=False,
│ │ -    update_features=False,
│ │ +    sample_user_num=None,
│ │      seed=42,
│ │  ):
│ │      """Evaluate the model on specific data and metrics.
│ │  
│ │      Parameters
│ │      ----------
│ │      model : Base
│ │          Model for evaluation.
│ │ -    data : pandas.DataFrame or :class:`~libreco.data.TransformedSet`
│ │ +    data : :class:`pandas.DataFrame` or :class:`~libreco.data.TransformedSet`
│ │          Data to evaluate.
│ │ +    neg_sampling : bool
│ │ +        Whether to perform negative sampling for evaluating data.
│ │      eval_batch_size : int, default: 8192
│ │          Batch size used in evaluation.
│ │      metrics : list or None, default: None
│ │          List of metrics for evaluating.
│ │      k : int, default: 10
│ │          Parameter of metrics, e.g. recall at k, ndcg at k
│ │ -    sample_user_num : int, default: 2048
│ │ -        Number of users for evaluating. Setting it to a positive number will sample
│ │ -        users randomly from eval data.
│ │ -    neg_sample : bool, default: False
│ │ -        Whether to do negative sampling when evaluating.
│ │ -    update_features : bool, default: False
│ │ -        Whether to update model's ``data_info`` from features in data.
│ │ +    sample_user_num : int or None, default: None
│ │ +        Number of users used in evaluating. By default, it will use all the users in eval_data.
│ │ +        Setting it to a positive number will sample users randomly from eval data.
│ │      seed : int, default: 42
│ │          Random seed.
│ │  
│ │      Returns
│ │      -------
│ │ -    results : dict of {str : float}
│ │ +    eval_results : dict of {str : float}
│ │          Evaluation results for the model and data.
│ │  
│ │      Examples
│ │      --------
│ │ -    >>> eval_result = evaluate(model, data, metrics=["roc_auc", "precision", "recall"])
│ │ +    >>> eval_result = evaluate(model, data, neg_sampling=True, metrics=["roc_auc", "precision", "recall"])
│ │      """
│ │ -    if isinstance(data, pd.DataFrame):
│ │ -        data = build_eval_transformed_data(
│ │ -            model, data, neg_sample, update_features, seed
│ │ -        )
│ │ -    assert isinstance(
│ │ -        data, TransformedSet
│ │ -    ), "The data from evaluation must be TransformedSet object."
│ │ +    if not isinstance(data, (pd.DataFrame, TransformedSet)):
│ │ +        raise ValueError("`data` must be `pandas.DataFrame` or `TransformedSet`")
│ │ +    data = build_eval_transformed_data(model, data, neg_sampling, seed)
│ │      if not metrics:
│ │          metrics = ["loss"]
│ │      metrics = _check_metrics(model.task, metrics, k)
│ │      eval_result = dict()
│ │ -
│ │      if model.task == "rating":
│ │          y_pred, y_true = compute_preds(model, data, eval_batch_size)
│ │          for m in metrics:
│ │              if m in ["rmse", "loss"]:
│ │ -                eval_result[m] = np.sqrt(mean_squared_error(y_true, y_pred))
│ │ +                eval_result[m] = rmse(y_true, y_pred)
│ │              elif m == "mae":
│ │                  eval_result[m] = mean_absolute_error(y_true, y_pred)
│ │              elif m == "r2":
│ │                  eval_result[m] = r2_score(y_true, y_pred)
│ │ -
│ │ -    elif model.task == "ranking":
│ │ +    else:
│ │          if POINTWISE_METRICS.intersection(metrics):
│ │              y_prob, y_true = compute_probs(model, data, eval_batch_size)
│ │              for m in metrics:
│ │                  if m in ["log_loss", "loss"]:
│ │                      eval_result[m] = log_loss(y_true, y_prob, eps=1e-7)
│ │                  elif m == "balanced_accuracy":
│ │ -                    y_pred = np.round(y_prob)
│ │ -                    eval_result[m] = balanced_accuracy_score(y_true, y_pred)
│ │ +                    eval_result[m] = balanced_accuracy(y_true, y_prob)
│ │                  elif m == "roc_auc":
│ │                      eval_result[m] = roc_auc_score(y_true, y_prob)
│ │                  elif m == "pr_auc":
│ │ -                    precision, recall, _ = precision_recall_curve(y_true, y_prob)
│ │ -                    eval_result[m] = auc(recall, precision)
│ │ +                    eval_result[m] = pr_auc_score(y_true, y_prob)
│ │          if LISTWISE_METRICS.intersection(metrics):
│ │ -            chosen_users = sample_user(data, seed, sample_user_num)
│ │ +            users = sample_users(data, seed, sample_user_num)
│ │              num_batch_users = max(1, math.floor(eval_batch_size / model.n_items))
│ │ -            y_true_list = data.user_consumed
│ │ -            y_reco_list, users = compute_recommends(
│ │ -                model, chosen_users, k, num_batch_users
│ │ -            )
│ │ +            y_trues = data.user_consumed
│ │ +            y_recos = compute_recommends(model, users, k, num_batch_users)
│ │              for m in metrics:
│ │ +                if m not in LISTWISE_METRICS:
│ │ +                    continue
│ │                  if m == "precision":
│ │ -                    eval_result[m] = precision_at_k(y_true_list, y_reco_list, users, k)
│ │ +                    fn = precision_at_k
│ │                  elif m == "recall":
│ │ -                    eval_result[m] = recall_at_k(y_true_list, y_reco_list, users, k)
│ │ +                    fn = recall_at_k
│ │                  elif m == "map":
│ │ -                    eval_result[m] = map_at_k(y_true_list, y_reco_list, users, k)
│ │ +                    fn = average_precision_at_k
│ │                  elif m == "ndcg":
│ │ -                    eval_result[m] = ndcg_at_k(y_true_list, y_reco_list, users, k)
│ │ +                    fn = ndcg_at_k
│ │ +                # noinspection PyUnboundLocalVariable
│ │ +                eval_result[m] = listwise_scores(fn, y_trues, y_recos, users, k)
│ │  
│ │      return eval_result
│ │ +
│ │ +
│ │ +def print_metrics(
│ │ +    model,
│ │ +    neg_sampling,
│ │ +    train_data=None,
│ │ +    eval_data=None,
│ │ +    metrics=None,
│ │ +    eval_batch_size=8192,
│ │ +    k=10,
│ │ +    sample_user_num=2048,
│ │ +    seed=42,
│ │ +):
│ │ +    loss_name = "rmse" if model.task == "rating" else "log_loss"
│ │ +    metrics_fn = functools.partial(
│ │ +        evaluate,
│ │ +        model=model,
│ │ +        neg_sampling=neg_sampling,
│ │ +        eval_batch_size=eval_batch_size,
│ │ +        k=k,
│ │ +        sample_user_num=sample_user_num,
│ │ +        seed=seed,
│ │ +    )
│ │ +    if train_data:
│ │ +        train_metrics = metrics_fn(data=train_data, metrics=[loss_name])
│ │ +        print(f"\t train {loss_name}: {train_metrics[loss_name]:.4f}")
│ │ +    if eval_data:
│ │ +        eval_metrics = metrics_fn(data=eval_data, metrics=metrics)
│ │ +        for m, val in eval_metrics.items():
│ │ +            if m == "loss":
│ │ +                m = loss_name
│ │ +            if m in LISTWISE_METRICS:
│ │ +                m = f"{m}@{k}"
│ │ +            print(f"\t eval {m}: {val:.4f}")
│ │   --- LibRecommender-1.0.1/libreco/evaluation/metrics.py
│ ├── +++ LibRecommender-1.1.0/libreco/evaluation/metrics.py
│ │┄ Files 19% similar despite different names
│ │ @@ -1,87 +1,73 @@
│ │  import numpy as np
│ │ +from sklearn.metrics import (
│ │ +    auc,
│ │ +    balanced_accuracy_score,
│ │ +    mean_squared_error,
│ │ +    precision_recall_curve,
│ │ +)
│ │  
│ │ -# pairwise_metrics = bpr ??
│ │ +RATING_METRICS = {"loss", "rmse", "mae", "r2"}
│ │  POINTWISE_METRICS = {"loss", "log_loss", "balanced_accuracy", "roc_auc", "pr_auc"}
│ │  LISTWISE_METRICS = {"precision", "recall", "map", "ndcg"}
│ │ -ALLOWED_METRICS = {
│ │ -    "rating_metrics": ["loss", "rmse", "mae", "r2"],
│ │ -    "ranking_metrics": [
│ │ -        "loss",
│ │ -        "log_loss",
│ │ -        "balanced_accuracy",
│ │ -        "roc_auc",
│ │ -        "pr_auc",
│ │ -        "precision",
│ │ -        "recall",
│ │ -        "map",
│ │ -        "ndcg",
│ │ -    ],
│ │ -}
│ │ +RANKING_METRICS = POINTWISE_METRICS | LISTWISE_METRICS
│ │  
│ │  
│ │ -def precision_at_k(y_true_list, y_reco_list, users, k):
│ │ -    precision_all = list()
│ │ -    for u in users:
│ │ -        y_true = y_true_list[u]
│ │ -        y_reco = y_reco_list[u]
│ │ -        common_items = set(y_reco).intersection(y_true)
│ │ -        precision = len(common_items) / k
│ │ -        precision_all.append(precision)
│ │ -    return np.mean(precision_all)
│ │ +def rmse(y_true, y_pred):
│ │ +    return np.sqrt(mean_squared_error(y_true, y_pred))
│ │ +
│ │ +
│ │ +# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html
│ │ +def balanced_accuracy(y_true, y_prob):
│ │ +    y_pred = np.round(y_prob)
│ │ +    return balanced_accuracy_score(y_true, y_pred)
│ │ +
│ │ +
│ │ +def pr_auc_score(y_true, y_prob):
│ │ +    precision, recall, _ = precision_recall_curve(y_true, y_prob)
│ │ +    return auc(recall, precision)
│ │  
│ │  
│ │ -def recall_at_k(y_true_list, y_reco_list, users, k):
│ │ -    recall_all = list()
│ │ +def listwise_scores(fn, y_true_lists, y_reco_lists, users, k):
│ │ +    scores = list()
│ │      for u in users:
│ │ -        y_true = y_true_list[u]
│ │ -        y_reco = y_reco_list[u]
│ │ -        common_items = set(y_reco).intersection(y_true)
│ │ -        recall = len(common_items) / len(y_true)
│ │ -        recall_all.append(recall)
│ │ -    return np.mean(recall_all)
│ │ +        y_true = y_true_lists[u]
│ │ +        y_reco = y_reco_lists[u]
│ │ +        scores.append(fn(y_true, y_reco, k))
│ │ +    return np.mean(scores)
│ │ +
│ │ +
│ │ +def precision_at_k(y_true, y_reco, k):
│ │ +    common_items = set(y_reco).intersection(y_true)
│ │ +    return len(common_items) / k
│ │ +
│ │ +
│ │ +def recall_at_k(y_true, y_reco, _k):
│ │ +    common_items = set(y_reco).intersection(y_true)
│ │ +    return len(common_items) / len(y_true)
│ │  
│ │  
│ │  def average_precision_at_k(y_true, y_reco, k):
│ │      common_items, _, indices_in_reco = np.intersect1d(
│ │          y_true, y_reco, assume_unique=True, return_indices=True
│ │      )
│ │ -
│ │ -    rank_list = np.zeros(k)
│ │ -    if common_items.size > 0:
│ │ -        rank_list[indices_in_reco] = 1
│ │ -        ap = [np.mean(rank_list[: i + 1]) for i in range(k) if rank_list[i]]
│ │ -        assert len(ap) == common_items.size, "common size doesn't match..."
│ │ -        return np.mean(ap)
│ │ -    else:
│ │ +    if common_items.size == 0:
│ │          return 0
│ │ +    rank_list = np.zeros(k, np.float32)
│ │ +    rank_list[indices_in_reco] = 1
│ │ +    ap = [np.mean(rank_list[: i + 1]) for i in range(k) if rank_list[i]]
│ │ +    assert len(ap) == common_items.size, "common size doesn't match..."
│ │ +    return np.mean(ap)
│ │  
│ │  
│ │ -def map_at_k(y_true_list, y_reco_list, users, k):
│ │ -    map_all = list()
│ │ -    for u in users:
│ │ -        y_true = list(set(y_true_list[u]))
│ │ -        y_reco = y_reco_list[u]
│ │ -        map_all.append(average_precision_at_k(y_true, y_reco, k))
│ │ -    return np.mean(map_all)
│ │ -
│ │ -
│ │ -def ndcg_at_k(y_true_list, y_reco_list, users, k):
│ │ -    ndcg_all = list()
│ │ -    for u in users:
│ │ -        rank_list = np.zeros(k)
│ │ -        y_true = list(set(y_true_list[u]))
│ │ -        y_reco = y_reco_list[u]
│ │ -        common_items, _, indices_in_reco = np.intersect1d(
│ │ -            y_true, y_reco, assume_unique=True, return_indices=True
│ │ -        )
│ │ -
│ │ -        if common_items.size > 0:
│ │ -            rank_list[indices_in_reco] = 1
│ │ -            ideal_list = np.sort(rank_list)[::-1]
│ │ -            dcg = np.sum(rank_list / np.log2(np.arange(2, k + 2)))
│ │ -            idcg = np.sum(ideal_list / np.log2(np.arange(2, k + 2)))
│ │ -            ndcg = dcg / idcg
│ │ -        else:
│ │ -            ndcg = 0
│ │ -        ndcg_all.append(ndcg)
│ │ -    return np.mean(ndcg_all)
│ │ +def ndcg_at_k(y_true, y_reco, k):
│ │ +    common_items, _, indices_in_reco = np.intersect1d(
│ │ +        y_true, y_reco, assume_unique=True, return_indices=True
│ │ +    )
│ │ +    if common_items.size == 0:
│ │ +        return 0
│ │ +    rank_list = np.zeros(k, np.float32)
│ │ +    rank_list[indices_in_reco] = 1
│ │ +    ideal_list = np.sort(rank_list)[::-1]
│ │ +    dcg = np.sum(rank_list / np.log2(np.arange(2, k + 2)))
│ │ +    idcg = np.sum(ideal_list / np.log2(np.arange(2, k + 2)))
│ │ +    return dcg / idcg
│ │   --- LibRecommender-1.0.1/libreco/feature/column_mapping.py
│ ├── +++ LibRecommender-1.1.0/libreco/feature/column_mapping.py
│ │┄ Files 6% similar despite different names
│ │ @@ -1,15 +1,15 @@
│ │  from collections import OrderedDict, defaultdict
│ │  
│ │  import numpy as np
│ │  
│ │  
│ │ +# format: {column_family_name: {column_name: index}}
│ │ +# if no such family, default format would be: {column_family_name: {[]: []}
│ │  def col_name2index(user_col=None, item_col=None, sparse_col=None, dense_col=None):
│ │ -    # format: {column_family_name: {column_name: index}}
│ │ -    # if no such family, default format would be: {column_family_name: {[]: []}
│ │      name_mapping = defaultdict(OrderedDict)
│ │      if sparse_col:
│ │          sparse_col_dict = {col: i for i, col in enumerate(sparse_col)}
│ │          name_mapping["sparse_col"].update(sparse_col_dict)
│ │      if dense_col:
│ │          dense_col_dict = {col: i for i, col in enumerate(dense_col)}
│ │          name_mapping["dense_col"].update(dense_col_dict)
│ │ @@ -32,18 +32,17 @@
│ │                  {col: name_mapping["sparse_col"][col]}
│ │              )
│ │      if item_col and dense_col:
│ │          item_dense_col = _extract_common_col(dense_col, item_col)
│ │          for col in item_dense_col:
│ │              name_mapping["item_dense_col"].update({col: name_mapping["dense_col"][col]})
│ │  
│ │ -    return name_mapping
│ │ +    return dict(name_mapping)
│ │  
│ │  
│ │ +# `np.intersect1d` will return the sorted common column names,
│ │ +# but we also want to preserve the original order of common column in col1 and col2
│ │  def _extract_common_col(col1, col2):
│ │ -    # np.intersect1d will return the sorted common column names,
│ │ -    # but we also want to preserve the original order of common column in
│ │ -    # col1 and col2
│ │      common_col, indices_in_col1, _ = np.intersect1d(
│ │          col1, col2, assume_unique=True, return_indices=True
│ │      )
│ │      return common_col[np.lexsort((common_col, indices_in_col1))]
│ │   --- LibRecommender-1.0.1/libreco/feature/unique_features.py
│ ├── +++ LibRecommender-1.1.0/libreco/utils/sampling.py
│ │┄ Files 22% similar despite different names
│ │ @@ -1,349 +1,342 @@
│ │ -import numbers
│ │ +from math import floor
│ │ +from random import random
│ │ +from random import seed as set_random_seed
│ │  
│ │  import numpy as np
│ │ +from tqdm import tqdm
│ │  
│ │ +from ..utils.misc import time_block
│ │  
│ │ -def construct_unique_feat(
│ │ -    user_indices,
│ │ -    item_indices,
│ │ -    sparse_indices,
│ │ -    dense_values,
│ │ -    user_sparse_col,
│ │ -    user_dense_col,
│ │ -    item_sparse_col,
│ │ -    item_dense_col,
│ │ -    unique_feat,
│ │ -):
│ │ -    # use mergesort to preserve order
│ │ -    sort_kind = "quicksort" if unique_feat else "mergesort"
│ │ -    user_pos = np.argsort(user_indices, kind=sort_kind)
│ │ -    item_pos = np.argsort(item_indices, kind=sort_kind)
│ │ -
│ │ -    if user_sparse_col:
│ │ -        user_sparse_matrix = _compress_unique_values(
│ │ -            sparse_indices, user_sparse_col, user_indices, user_pos
│ │ -        )
│ │ -    else:
│ │ -        user_sparse_matrix = None
│ │  
│ │ -    if item_sparse_col:
│ │ -        item_sparse_matrix = _compress_unique_values(
│ │ -            sparse_indices, item_sparse_col, item_indices, item_pos
│ │ +class SamplingBase(object):
│ │ +    def __init__(self, dataset, data_info, num_neg=1):
│ │ +        self.dataset = dataset
│ │ +        self.data_info = data_info
│ │ +        self.num_neg = num_neg
│ │ +
│ │ +    def sample_items_random(self, seed=42):
│ │ +        set_random_seed(seed)
│ │ +        n_items = self.data_info.n_items
│ │ +        item_indices_sampled = list()
│ │ +        # set is much faster for search contains
│ │ +        user_consumed = {
│ │ +            u: set(items) for u, items in self.data_info.user_consumed.items()
│ │ +        }
│ │ +        # sample negative items for every user
│ │ +        with time_block("random neg item sampling"):
│ │ +            for u, i in zip(self.dataset.user_indices, self.dataset.item_indices):
│ │ +                item_indices_sampled.append(i)
│ │ +                for _ in range(self.num_neg):
│ │ +                    item_neg = floor(n_items * random())
│ │ +                    if u in user_consumed:
│ │ +                        while item_neg in user_consumed[u]:
│ │ +                            item_neg = floor(n_items * random())
│ │ +                    item_indices_sampled.append(item_neg)
│ │ +        return np.asarray(item_indices_sampled)
│ │ +
│ │ +    def sample_items_popular(self, seed=42):
│ │ +        data = self.data_info.get_indexed_interaction()
│ │ +        item_counts = data.item.value_counts().sort_index().to_numpy()
│ │ +        user_consumed = self.data_info.user_consumed
│ │ +        items = np.arange(self.data_info.n_items)
│ │ +
│ │ +        item_order = list()
│ │ +        item_indices_sampled = list()
│ │ +        with time_block("popularity-based neg item sampling"):
│ │ +            for user, u_data in data.groupby("user", sort=False):
│ │ +                item_indices = u_data.index.to_list()
│ │ +                item_indices = item_indices * (self.num_neg + 1)
│ │ +                item_order.extend(item_indices)
│ │ +
│ │ +                # add positive items
│ │ +                item_indices_sampled.extend(u_data.item.tolist())
│ │ +                u_consumed = user_consumed[user]
│ │ +                u_item_counts = item_counts.copy()
│ │ +                u_item_counts[u_consumed] = 0
│ │ +                item_prob = u_item_counts / np.sum(u_item_counts)
│ │ +                neg_size = len(u_consumed) * self.num_neg
│ │ +
│ │ +                neg_sampled = np.random.choice(
│ │ +                    items, size=neg_size, p=item_prob, replace=True
│ │ +                )
│ │ +                item_indices_sampled.extend(neg_sampled)
│ │ +
│ │ +        item_indices_sampled = np.asarray(item_indices_sampled)
│ │ +        # must be stable sort to keep relative order
│ │ +        item_order = np.argsort(item_order, kind="mergesort")
│ │ +        return item_indices_sampled[item_order]
│ │ +
│ │ +    def _label_negative_sampling(self, size):
│ │ +        factor = self.num_neg + 1
│ │ +        total_length = size * factor
│ │ +        labels = np.zeros(total_length, dtype=np.float32)
│ │ +        labels[::factor] = 1.0
│ │ +        return labels
│ │ +
│ │ +
│ │ +class NegativeSampling(SamplingBase):
│ │ +    def __init__(
│ │ +        self, dataset, data_info, num_neg, sparse=None, dense=None, batch_sampling=False
│ │ +    ):
│ │ +        super(NegativeSampling, self).__init__(dataset, data_info, num_neg)
│ │ +        if batch_sampling and dataset.has_sampled:
│ │ +            self.user_indices = dataset.user_indices_orig
│ │ +            self.item_indices = dataset.item_indices_orig
│ │ +            self.sparse_indices = dataset.sparse_indices_orig if sparse else None
│ │ +            self.dense_values = dataset.dense_values_orig if dense else None
│ │ +        else:
│ │ +            self.user_indices = dataset.user_indices
│ │ +            self.item_indices = dataset.item_indices
│ │ +            self.sparse_indices = dataset.sparse_indices if sparse else None
│ │ +            self.dense_values = dataset.dense_values if dense else None
│ │ +        self.data_size = len(self.user_indices)
│ │ +        self.sparse = sparse
│ │ +        self.dense = dense
│ │ +
│ │ +    def generate_all(self, seed=42, item_gen_mode="random"):
│ │ +        user_indices_sampled = np.repeat(self.user_indices, self.num_neg + 1, axis=0)
│ │ +        if item_gen_mode not in ["random", "popular"]:
│ │ +            raise ValueError(
│ │ +                "sampling item_gen_mode must either be 'random' or 'popular'"
│ │ +            )
│ │ +        elif item_gen_mode == "random":
│ │ +            item_indices_sampled = self.sample_items_random(seed=seed)
│ │ +        elif item_gen_mode == "popular":
│ │ +            item_indices_sampled = self.sample_items_popular(seed=seed)
│ │ +
│ │ +        sparse_indices_sampled = (
│ │ +            self._sparse_indices_sampling(self.sparse_indices, item_indices_sampled)
│ │ +            if self.sparse
│ │ +            else None
│ │          )
│ │ -    else:
│ │ -        item_sparse_matrix = None
│ │ -
│ │ -    if user_dense_col:
│ │ -        user_dense_matrix = _compress_unique_values(
│ │ -            dense_values, user_dense_col, user_indices, user_pos
│ │ +        dense_values_sampled = (
│ │ +            self._dense_values_sampling(self.dense_values, item_indices_sampled)
│ │ +            if self.dense
│ │ +            else None
│ │          )
│ │ -    else:
│ │ -        user_dense_matrix = None
│ │ +        label_sampled = self._label_negative_sampling(self.data_size)
│ │  
│ │ -    if item_dense_col:
│ │ -        item_dense_matrix = _compress_unique_values(
│ │ -            dense_values, item_dense_col, item_indices, item_pos
│ │ +        return (
│ │ +            user_indices_sampled,
│ │ +            item_indices_sampled,
│ │ +            label_sampled,
│ │ +            sparse_indices_sampled,
│ │ +            dense_values_sampled,
│ │          )
│ │ -    else:
│ │ -        item_dense_matrix = None
│ │  
│ │ -    return (
│ │ -        user_sparse_matrix,
│ │ -        user_dense_matrix,
│ │ -        item_sparse_matrix,
│ │ -        item_dense_matrix,
│ │ -    )
│ │ -
│ │ -
│ │ -def _compress_unique_values(orig_val, col, indices, pos):
│ │ -    values = np.take(orig_val, col, axis=1)
│ │ -    values = values.reshape(-1, 1) if orig_val.ndim == 1 else values
│ │ -    indices = indices[pos]
│ │ -    # https://stackoverflow.com/questions/46390376/drop-duplicates-from-structured-numpy-array-python3-x
│ │ -    mask = np.empty(len(indices), dtype=bool)
│ │ -    mask[:-1] = indices[:-1] != indices[1:]
│ │ -    mask[-1] = True
│ │ -    mask = pos[mask]
│ │ -    unique_values = values[mask]
│ │ -    assert len(np.unique(indices)) == len(unique_values)
│ │ -    return unique_values
│ │ -
│ │ -
│ │ -# def _compress_unique_values(orig_val, col, indices):
│ │ -# values = np.take(orig_val, col, axis=1)
│ │ -# values = values.reshape(-1, 1) if orig_val.ndim == 1 else values
│ │ -# indices = indices.reshape(-1, 1)
│ │ -# unique_indices = np.unique(indices)
│ │ -# indices_plus_values = np.concatenate([indices, values], axis=-1)
│ │ -#   np.unique(axis=0) will sort the data based on first column,
│ │ -#   so we can do direct mapping, then remove redundant unique_indices
│ │ -# unique_values = np.unique(indices_plus_values, axis=0)
│ │ -# diff = True if len(unique_indices) != len(unique_values) else False
│ │ -# if diff:
│ │ -#    print(colorize("some users or items contain different features, "
│ │ -#                   "will only keep the last one", "red"))
│ │ -#    mask = np.concatenate([unique_values[:-1, 0] != unique_values[1:, 0],
│ │ -#                           np.array([True])])
│ │ -#    unique_values = unique_values[mask]
│ │ -
│ │ -# assert len(unique_indices) == len(unique_values)
│ │ -# return unique_values[:, 1:]
│ │ -
│ │ -
│ │ -def get_predict_indices_and_values(data_info, user, item, n_items, sparse, dense):
│ │ -    if isinstance(user, numbers.Integral):
│ │ -        user = list([user])
│ │ -    if isinstance(item, numbers.Integral):
│ │ -        item = list([item])
│ │ -
│ │ -    sparse_indices = (
│ │ -        get_sparse_indices(data_info, user, item, mode="predict") if sparse else None
│ │ -    )
│ │ -    dense_values = (
│ │ -        get_dense_values(data_info, user, item, mode="predict") if dense else None
│ │ -    )
│ │ -    if sparse and dense:
│ │ -        assert len(sparse_indices) == len(
│ │ -            dense_values
│ │ -        ), "indices and values length must equal"
│ │ -    return user, item, sparse_indices, dense_values
│ │ -
│ │ -
│ │ -def get_recommend_indices_and_values(data_info, user, n_items, sparse, dense):
│ │ -    user_indices = np.repeat(user, n_items)
│ │ -    item_indices = np.arange(n_items)
│ │ -    sparse_indices = (
│ │ -        get_sparse_indices(data_info, user, n_items=n_items, mode="recommend")
│ │ -        if sparse
│ │ -        else None
│ │ -    )
│ │ -    dense_values = (
│ │ -        get_dense_values(data_info, user, n_items=n_items, mode="recommend")
│ │ -        if dense
│ │ -        else None
│ │ -    )
│ │ -    if sparse and dense:
│ │ -        assert len(sparse_indices) == len(
│ │ -            dense_values
│ │ -        ), "indices and values length must equal"
│ │ -    return user_indices, item_indices, sparse_indices, dense_values
│ │ -
│ │ -
│ │ -def get_sparse_indices(data_info, user, item=None, n_items=None, mode="predict"):
│ │ -    user_sparse_col = data_info.user_sparse_col.index
│ │ -    item_sparse_col = data_info.item_sparse_col.index
│ │ -    orig_cols = user_sparse_col + item_sparse_col
│ │ -    # keep column names in original order
│ │ -    col_reindex = np.arange(len(orig_cols))[np.argsort(orig_cols)]
│ │ +    def __call__(self, shuffle=True, batch_size=None):
│ │ +        if shuffle:
│ │ +            mask = np.random.permutation(range(self.data_size))
│ │ +            self.sparse_indices = self.sparse_indices[mask] if self.sparse else None
│ │ +            self.dense_values = self.dense_values[mask] if self.dense else None
│ │ +
│ │ +        user_consumed = {
│ │ +            u: set(items) for u, items in self.data_info.user_consumed.items()
│ │ +        }
│ │ +        n_items = self.data_info.n_items
│ │ +        return self.sample_batch(user_consumed, n_items, batch_size)
│ │ +
│ │ +    def sample_batch(self, user_consumed, n_items, batch_size):
│ │ +        for k in tqdm(
│ │ +            range(0, self.data_size, batch_size), desc="batch_sampling train"
│ │ +        ):
│ │ +            batch_slice = slice(k, k + batch_size)
│ │ +            batch_user_indices = self.user_indices[batch_slice]
│ │ +            batch_item_indices = self.item_indices[batch_slice]
│ │ +            batch_sparse_indices = (
│ │ +                self.sparse_indices[batch_slice] if self.sparse else None
│ │ +            )
│ │ +            batch_dense_values = self.dense_values[batch_slice] if self.dense else None
│ │  
│ │ -    if mode == "predict":
│ │ -        if user_sparse_col and item_sparse_col:
│ │ -            user_sparse_part = data_info.user_sparse_unique[user]
│ │ -            item_sparse_part = data_info.item_sparse_unique[item]
│ │ -            sparse_indices = np.concatenate(
│ │ -                [user_sparse_part, item_sparse_part], axis=-1
│ │ -            )[:, col_reindex]
│ │ -            return sparse_indices
│ │ -        elif user_sparse_col:
│ │ -            return data_info.user_sparse_unique[user]
│ │ -        elif item_sparse_col:
│ │ -            return data_info.item_sparse_unique[item]
│ │ +            user_indices_sampled = np.repeat(
│ │ +                batch_user_indices, self.num_neg + 1, axis=0
│ │ +            )
│ │ +            item_indices_sampled = list()
│ │ +            for u, i in zip(batch_user_indices, batch_item_indices):
│ │ +                item_indices_sampled.append(i)
│ │ +                for _ in range(self.num_neg):
│ │ +                    item_neg = floor(random() * n_items)
│ │ +                    while item_neg in user_consumed[u]:
│ │ +                        item_neg = floor(random() * n_items)
│ │ +                    item_indices_sampled.append(item_neg)
│ │ +            item_indices_sampled = np.array(item_indices_sampled)
│ │ +
│ │ +            sparse_indices_sampled = (
│ │ +                self._sparse_indices_sampling(
│ │ +                    batch_sparse_indices, item_indices_sampled
│ │ +                )
│ │ +                if self.sparse
│ │ +                else None
│ │ +            )
│ │ +            dense_values_sampled = (
│ │ +                self._dense_values_sampling(batch_dense_values, item_indices_sampled)
│ │ +                if self.dense
│ │ +                else None
│ │ +            )
│ │ +            label_sampled = self._label_negative_sampling(len(batch_user_indices))
│ │  
│ │ -    elif mode == "recommend":
│ │ -        if user_sparse_col and item_sparse_col:
│ │ -            user_sparse_part = np.tile(data_info.user_sparse_unique[user], (n_items, 1))
│ │ -            item_sparse_part = data_info.item_sparse_unique[:-1]  # remove oov
│ │ -            sparse_indices = np.concatenate(
│ │ -                [user_sparse_part, item_sparse_part], axis=-1
│ │ -            )[:, col_reindex]
│ │ -            return sparse_indices
│ │ -        elif user_sparse_col:
│ │ -            return np.tile(data_info.user_sparse_unique[user], (n_items, 1))
│ │ -        elif item_sparse_col:
│ │ -            return data_info.item_sparse_unique[:-1]
│ │ +            yield (
│ │ +                user_indices_sampled,
│ │ +                item_indices_sampled,
│ │ +                label_sampled,
│ │ +                sparse_indices_sampled,
│ │ +                dense_values_sampled,
│ │ +            )
│ │  
│ │ +    def _sparse_indices_sampling(self, sparse_indices, item_indices_sampled):
│ │ +        user_sparse_col = self.data_info.user_sparse_col.index
│ │ +        item_sparse_col = self.data_info.item_sparse_col.index
│ │  
│ │ -def get_dense_indices(data_info, user, n_items=None, mode="predict"):
│ │ -    user_dense_col = data_info.user_dense_col.index
│ │ -    item_dense_col = data_info.item_dense_col.index
│ │ -    total_dense_cols = len(user_dense_col) + len(item_dense_col)
│ │ -    if mode == "predict":
│ │ -        return np.tile(np.arange(total_dense_cols), (len(user), 1))
│ │ -    elif mode == "recommend":
│ │ -        return np.tile(np.arange(total_dense_cols), (n_items, 1))
│ │ -
│ │ -
│ │ -def get_dense_values(data_info, user, item=None, n_items=None, mode="predict"):
│ │ -    user_dense_col = data_info.user_dense_col.index
│ │ -    item_dense_col = data_info.item_dense_col.index
│ │ -    # keep column names in original order
│ │ -    orig_cols = user_dense_col + item_dense_col
│ │ -    col_reindex = np.arange(len(orig_cols))[np.argsort(orig_cols)]
│ │ +        if user_sparse_col and item_sparse_col:
│ │ +            user_sparse_indices = np.take(sparse_indices, user_sparse_col, axis=1)
│ │ +            user_sparse_sampled = np.repeat(
│ │ +                user_sparse_indices, self.num_neg + 1, axis=0
│ │ +            )
│ │ +            item_sparse_sampled = self.data_info.item_sparse_unique[
│ │ +                item_indices_sampled
│ │ +            ]
│ │  
│ │ -    if mode == "predict":
│ │ -        if user_dense_col and item_dense_col:
│ │ -            user_dense_part = data_info.user_dense_unique[user]
│ │ -            item_dense_part = data_info.item_dense_unique[item]
│ │ -            dense_values = np.concatenate([user_dense_part, item_dense_part], axis=-1)[
│ │ +            assert len(user_sparse_sampled) == len(
│ │ +                item_sparse_sampled
│ │ +            ), "num of user sampled must equal to num of item sampled"
│ │ +            # keep column names in original order
│ │ +            orig_cols = user_sparse_col + item_sparse_col
│ │ +            col_reindex = np.arange(len(orig_cols))[np.argsort(orig_cols)]
│ │ +            return np.concatenate([user_sparse_sampled, item_sparse_sampled], axis=-1)[
│ │                  :, col_reindex
│ │              ]
│ │ -            return dense_values
│ │ -        elif user_dense_col:
│ │ -            return data_info.user_dense_unique[user]
│ │ -        elif item_dense_col:
│ │ -            return data_info.item_dense_unique[item]
│ │  
│ │ -    elif mode == "recommend":
│ │ +        elif user_sparse_col:
│ │ +            user_sparse_indices = np.take(sparse_indices, user_sparse_col, axis=1)
│ │ +            user_sparse_sampled = np.repeat(
│ │ +                user_sparse_indices, self.num_neg + 1, axis=0
│ │ +            )
│ │ +            return user_sparse_sampled
│ │ +
│ │ +        elif item_sparse_col:
│ │ +            item_sparse_sampled = self.data_info.item_sparse_unique[
│ │ +                item_indices_sampled
│ │ +            ]
│ │ +            return item_sparse_sampled
│ │ +
│ │ +    # def _dense_indices_sampling(self, item_indices_sampled):
│ │ +    #    n_samples = len(item_indices_sampled)
│ │ +    #    user_dense_col = self.data_info.user_dense_col.index
│ │ +    #    item_dense_col = self.data_info.item_dense_col.index
│ │ +    #    total_dense_cols = len(user_dense_col) + len(item_dense_col)
│ │ +    #    return np.tile(np.arange(total_dense_cols), [n_samples, 1])
│ │ +
│ │ +    def _dense_values_sampling(self, dense_values, item_indices_sampled):
│ │ +        user_dense_col = self.data_info.user_dense_col.index
│ │ +        item_dense_col = self.data_info.item_dense_col.index
│ │ +
│ │          if user_dense_col and item_dense_col:
│ │ -            user_dense_part = np.tile(data_info.user_dense_unique[user], (n_items, 1))
│ │ -            item_dense_part = data_info.item_dense_unique[:-1]  # remove oov
│ │ -            dense_values = np.concatenate([user_dense_part, item_dense_part], axis=-1)[
│ │ +            user_dense_values = np.take(dense_values, user_dense_col, axis=1)
│ │ +            user_dense_sampled = np.repeat(user_dense_values, self.num_neg + 1, axis=0)
│ │ +            item_dense_sampled = self.data_info.item_dense_unique[item_indices_sampled]
│ │ +            assert len(user_dense_sampled) == len(
│ │ +                item_dense_sampled
│ │ +            ), "num of user sampled must equal to num of item sampled"
│ │ +            # keep column names in original order
│ │ +            orig_cols = user_dense_col + item_dense_col
│ │ +            col_reindex = np.arange(len(orig_cols))[np.argsort(orig_cols)]
│ │ +            return np.concatenate([user_dense_sampled, item_dense_sampled], axis=-1)[
│ │                  :, col_reindex
│ │              ]
│ │ -            return dense_values
│ │ +
│ │          elif user_dense_col:
│ │ -            return np.tile(data_info.user_dense_unique[user], (n_items, 1))
│ │ +            user_dense_values = np.take(dense_values, user_dense_col, axis=1)
│ │ +            user_dense_sampled = np.repeat(user_dense_values, self.num_neg + 1, axis=0)
│ │ +            return user_dense_sampled
│ │ +
│ │          elif item_dense_col:
│ │ -            return data_info.item_dense_unique[:-1]
│ │ +            item_dense_sampled = self.data_info.item_dense_unique[item_indices_sampled]
│ │ +            return item_dense_sampled
│ │ +
│ │  
│ │ +class PairwiseSampling(SamplingBase):
│ │ +    def __init__(self, dataset, data_info, num_neg=1):
│ │ +        super().__init__(dataset, data_info, num_neg)
│ │ +        if dataset.has_sampled:
│ │ +            self.user_indices = dataset.user_indices_orig
│ │ +            self.item_indices = dataset.item_indices_orig
│ │ +        else:
│ │ +            self.user_indices = dataset.user_indices
│ │ +            self.item_indices = dataset.item_indices
│ │ +        self.data_size = len(self.user_indices)
│ │ +
│ │ +    def __call__(self, shuffle=True, batch_size=None):
│ │ +        if shuffle:
│ │ +            mask = np.random.permutation(range(self.data_size))
│ │ +            self.user_indices = self.user_indices[mask]
│ │ +            self.item_indices = self.item_indices[mask]
│ │ +
│ │ +        user_consumed_set = {
│ │ +            u: set(items) for u, items in self.data_info.user_consumed.items()
│ │ +        }
│ │ +        n_items = self.data_info.n_items
│ │ +        return self.sample_batch(user_consumed_set, n_items, batch_size)
│ │ +
│ │ +    def sample_batch(self, user_consumed_set, n_items, batch_size):
│ │ +        for k in tqdm(range(0, self.data_size, batch_size), desc="pair_sampling train"):
│ │ +            batch_slice = slice(k, k + batch_size)
│ │ +            batch_user_indices = self.user_indices[batch_slice]
│ │ +            batch_item_indices_pos = self.item_indices[batch_slice]
│ │ +
│ │ +            batch_item_indices_neg = list()
│ │ +            for u in batch_user_indices:
│ │ +                item_neg = floor(n_items * random())
│ │ +                while item_neg in user_consumed_set[u]:
│ │ +                    item_neg = floor(n_items * random())
│ │ +                batch_item_indices_neg.append(item_neg)
│ │ +
│ │ +            batch_item_indices_neg = np.asarray(batch_item_indices_neg)
│ │ +            yield batch_user_indices, batch_item_indices_pos, batch_item_indices_neg
│ │ +
│ │ +
│ │ +class PairwiseSamplingSeq(PairwiseSampling):
│ │ +    def __init__(self, dataset, data_info, num_neg=1, mode=None, num=None):
│ │ +        super().__init__(dataset, data_info, num_neg)
│ │ +        self.seq_mode = mode
│ │ +        self.seq_num = num
│ │ +        self.n_items = data_info.n_items
│ │ +        self.user_consumed = data_info.user_consumed
│ │ +        self.np_rng = data_info.np_rng
│ │ +
│ │ +    def sample_batch(self, user_consumed_set, n_items, batch_size):
│ │ +        # avoid circular import
│ │ +        from ..batch.sequence import get_interacted_seq
│ │  
│ │ -# This function will try not to modify the original numpy arrays
│ │ -def features_from_dict(data_info, sparse_indices, dense_values, feats, mode):
│ │ -    if mode == "predict":
│ │ -        # data_info.col_name_mapping: {"sparse_col": {col: index}}
│ │ -        sparse_mapping = data_info.col_name_mapping["sparse_col"]
│ │ -        dense_mapping = data_info.col_name_mapping["dense_col"]
│ │ -    elif mode == "recommend":
│ │ -        # in recommend scenario will only change user features
│ │ -        sparse_mapping = data_info.col_name_mapping["user_sparse_col"]
│ │ -        dense_mapping = data_info.col_name_mapping["user_dense_col"]
│ │ -    else:
│ │ -        raise ValueError("mode must be predict or recommend")
│ │ -
│ │ -    sparse_indices_copy = None if sparse_indices is None else sparse_indices.copy()
│ │ -    dense_values_copy = None if dense_values is None else dense_values.copy()
│ │ -    for col, val in feats.items():
│ │ -        if (
│ │ -            sparse_indices is not None
│ │ -            and col in sparse_mapping
│ │ -            and col in data_info.sparse_unique_idxs
│ │ +        for k in tqdm(
│ │ +            range(0, self.data_size, batch_size), desc="pair_sampling sequence train"
│ │          ):
│ │ -            field_idx = sparse_mapping[col]
│ │ -            if val in data_info.sparse_unique_idxs[col]:
│ │ -                # data_info.sparse_unique_idxs: {col: {value: index}}
│ │ -                feat_idx = data_info.sparse_unique_idxs[col][val]
│ │ -                offset = data_info.sparse_offset[field_idx]
│ │ -                sparse_indices_copy[:, field_idx] = feat_idx + offset
│ │ -            # else:
│ │ -            # if val not exists, assign to oov position
│ │ -            #    sparse_indices_copy[:, field_idx] = (
│ │ -            #        data_info.sparse_oov[field_idx]
│ │ -            #    )
│ │ -        elif dense_values is not None and col in dense_mapping:
│ │ -            field_idx = dense_mapping[col]
│ │ -            dense_values_copy[:, field_idx] = val
│ │ -
│ │ -    return sparse_indices_copy, dense_values_copy
│ │ -
│ │ -
│ │ -def features_from_batch_data(data_info, sparse, dense, data):
│ │ -    if sparse:
│ │ -        sparse_col_num = len(data_info.col_name_mapping["sparse_col"])
│ │ -        sparse_indices = [_ for _ in range(sparse_col_num)]
│ │ -        for col, field_idx in data_info.col_name_mapping["sparse_col"].items():
│ │ -            if col not in data.columns:
│ │ -                continue
│ │ -            sparse_indices[field_idx] = compute_sparse_feat_indices(
│ │ -                data_info, data, field_idx, col
│ │ -            )
│ │ -        sparse_indices = np.array(sparse_indices).astype(np.int64).T
│ │ -    else:
│ │ -        sparse_indices = None
│ │ -
│ │ -    if dense:
│ │ -        dense_col_num = len(data_info.col_name_mapping["dense_col"])
│ │ -        dense_values = [_ for _ in range(dense_col_num)]
│ │ -        for col, field_idx in data_info.col_name_mapping["dense_col"].items():
│ │ -            if col not in data.columns:
│ │ -                continue
│ │ -            dense_values[field_idx] = data[col].to_numpy()
│ │ -        dense_values = np.array(dense_values).T
│ │ -    else:
│ │ -        dense_values = None
│ │ -
│ │ -    if sparse and dense:
│ │ -        assert len(sparse_indices) == len(
│ │ -            dense_values
│ │ -        ), "indices and values length must equal"
│ │ -    return sparse_indices, dense_values
│ │ -
│ │ -
│ │ -# This function will try not to modify the original numpy arrays
│ │ -def add_item_features(data_info, sparse_indices, dense_values, data):
│ │ -    data = check_oov(data_info, data, "item")
│ │ -    row_idx = data["item"].to_numpy()
│ │ -    sparse_indices_copy = None if sparse_indices is None else sparse_indices.copy()
│ │ -    col_info = data_info.item_sparse_col
│ │ -    if sparse_indices is not None and col_info.name:
│ │ -        sparse_indices_copy = sparse_indices.copy()
│ │ -        for feat_idx, col in enumerate(col_info.name):
│ │ -            if col not in data.columns:
│ │ -                continue
│ │ -            sparse_indices_copy[row_idx, feat_idx] = compute_sparse_feat_indices(
│ │ -                data_info, data, col_info.index[feat_idx], col
│ │ +            batch_slice = slice(k, k + batch_size)
│ │ +            batch_user_indices = self.user_indices[batch_slice]
│ │ +            batch_item_indices_pos = self.item_indices[batch_slice]
│ │ +
│ │ +            batch_interacted, batch_interacted_len = get_interacted_seq(
│ │ +                batch_user_indices,
│ │ +                batch_item_indices_pos,
│ │ +                self.user_consumed,
│ │ +                self.n_items,
│ │ +                self.seq_mode,
│ │ +                self.seq_num,
│ │ +                user_consumed_set,
│ │ +                self.np_rng,
│ │              )
│ │  
│ │ -    dense_values_copy = None if dense_values is None else dense_values.copy()
│ │ -    col_info = data_info.item_dense_col
│ │ -    if dense_values is not None and col_info.name:
│ │ -        dense_values_copy = dense_values.copy()
│ │ -        for feat_idx, col in enumerate(col_info.name):
│ │ -            if col not in data.columns:
│ │ -                continue
│ │ -            dense_values_copy[row_idx, feat_idx] = data[col].to_numpy()
│ │ -    return sparse_indices_copy, dense_values_copy
│ │ -
│ │ -
│ │ -def compute_sparse_feat_indices(data_info, data, field_idx, column):
│ │ -    offset = data_info.sparse_offset[field_idx]
│ │ -    oov_val = data_info.sparse_oov[field_idx]
│ │ -
│ │ -    if data_info.sparse_unique_idxs and column in data_info.sparse_unique_idxs:
│ │ -        map_vals = data_info.sparse_unique_idxs[column]
│ │ -    elif (
│ │ -        data_info.multi_sparse_unique_idxs
│ │ -        and column in data_info.multi_sparse_unique_idxs
│ │ -    ):
│ │ -        map_vals = data_info.multi_sparse_unique_idxs[column]
│ │ -    elif (
│ │ -        "multi_sparse" in data_info.col_name_mapping
│ │ -        and column in data_info.col_name_mapping["multi_sparse"]
│ │ -    ):
│ │ -        main_col = data_info.col_name_mapping["multi_sparse"][column]
│ │ -        map_vals = data_info.multi_sparse_unique_idxs[main_col]
│ │ -    else:
│ │ -        raise ValueError(f"Unknown sparse column: {column}")
│ │ -    # map_vals = data_info.sparse_unique_idxs[column]
│ │ -    values = data[column].tolist()
│ │ -    feat_indices = np.array(
│ │ -        [map_vals[v] + offset if v in map_vals else oov_val for v in values]
│ │ -    )
│ │ -    return feat_indices
│ │ -
│ │ -
│ │ -# This function will try not to modify the original data
│ │ -def check_oov(data_info, orig_data, mode):
│ │ -    data = orig_data.copy()
│ │ -    if mode == "user":
│ │ -        users = data.user.tolist()
│ │ -        user_mapping = data_info.user2id
│ │ -        user_ids = [user_mapping[u] if u in user_mapping else -1 for u in users]
│ │ -        data["user"] = user_ids
│ │ -        data = data[data.user != -1]
│ │ -    elif mode == "item":
│ │ -        items = data.item.tolist()
│ │ -        item_mapping = data_info.item2id
│ │ -        item_ids = [item_mapping[i] if i in item_mapping else -1 for i in items]
│ │ -        data["item"] = item_ids
│ │ -        data = data[data.item != -1]
│ │ -    return data
│ │ +            batch_item_indices_neg = list()
│ │ +            for u in batch_user_indices:
│ │ +                item_neg = floor(n_items * random())
│ │ +                while item_neg in user_consumed_set[u]:
│ │ +                    item_neg = floor(n_items * random())
│ │ +                batch_item_indices_neg.append(item_neg)
│ │ +
│ │ +            batch_item_indices_neg = np.asarray(batch_item_indices_neg)
│ │ +            yield (
│ │ +                batch_user_indices,
│ │ +                batch_item_indices_pos,
│ │ +                batch_item_indices_neg,
│ │ +                batch_interacted,
│ │ +                batch_interacted_len,
│ │ +            )
│ │   --- LibRecommender-1.0.1/libreco/graph/from_dgl.py
│ ├── +++ LibRecommender-1.1.0/libreco/graph/from_dgl.py
│ │┄ Files 27% similar despite different names
│ │ @@ -1,11 +1,13 @@
│ │ +import itertools
│ │  from importlib.util import find_spec
│ │  
│ │  import numpy as np
│ │  import torch
│ │ +import tqdm
│ │  
│ │  
│ │  # avoid exiting the program if dgl is not installed and user wants to use other algorithms
│ │  def check_dgl(cls: type) -> type:
│ │      if find_spec("dgl") is None:
│ │          dgl_model_name = cls.__name__
│ │          torch_model_name = dgl_model_name.replace("DGL", "")
│ │ @@ -14,14 +16,45 @@
│ │              f"`{dgl_model_name}` if you have trouble installing DGL library"
│ │          )
│ │      else:
│ │          cls.dgl_error = None
│ │      return cls
│ │  
│ │  
│ │ +def build_i2i_homo_graph(n_items, user_consumed, item_consumed):
│ │ +    import dgl
│ │ +
│ │ +    src_items, dst_items = [], []
│ │ +    for i in tqdm.trange(n_items, desc="building homo graph"):
│ │ +        neighbors = set()
│ │ +        for u in item_consumed[i]:
│ │ +            neighbors.update(user_consumed[u])
│ │ +        src_items.extend(neighbors)
│ │ +        dst_items.extend([i] * len(neighbors))
│ │ +    src = torch.tensor(src_items, dtype=torch.long)
│ │ +    dst = torch.tensor(dst_items, dtype=torch.long)
│ │ +    return dgl.graph((src, dst), num_nodes=n_items)
│ │ +
│ │ +
│ │ +def build_u2i_hetero_graph(n_users, n_items, user_consumed):
│ │ +    import dgl
│ │ +
│ │ +    items = [list(user_consumed[u]) for u in range(n_users)]
│ │ +    counts = [len(i) for i in items]
│ │ +    users = torch.arange(n_users).repeat_interleave(torch.tensor(counts))
│ │ +    items = list(itertools.chain.from_iterable(items))
│ │ +    items = torch.tensor(items, dtype=torch.long)
│ │ +    graph_data = {
│ │ +        ("user", "consumed", "item"): (users, items),
│ │ +        ("item", "consumed-by", "user"): (items, users),
│ │ +    }
│ │ +    num_nodes = {"user": n_users, "item": n_items}
│ │ +    return dgl.heterograph(graph_data, num_nodes)
│ │ +
│ │ +
│ │  def build_subgraphs(heads, item_pairs, paradigm, num_neg):
│ │      import dgl
│ │  
│ │      heads_pos = torch.as_tensor(heads, dtype=torch.long)
│ │      tails_pos = torch.as_tensor(item_pairs[0], dtype=torch.long)
│ │      tails_neg = torch.as_tensor(item_pairs[1], dtype=torch.long)
│ │      if num_neg > 1:
│ │   --- LibRecommender-1.0.1/libreco/prediction/predict.py
│ ├── +++ LibRecommender-1.1.0/libreco/prediction/predict.py
│ │┄ Files 22% similar despite different names
│ │ @@ -1,19 +1,21 @@
│ │  import numpy as np
│ │  import pandas as pd
│ │  from scipy.special import expit
│ │  from tqdm import tqdm
│ │  
│ │ -from ..feature import (
│ │ -    features_from_batch_data,
│ │ -    features_from_dict,
│ │ -    get_predict_indices_and_values,
│ │ +from .preprocess import (
│ │ +    convert_id,
│ │ +    features_from_batch,
│ │ +    get_cached_seqs,
│ │ +    get_original_feats,
│ │ +    set_temp_feats,
│ │  )
│ │  from ..tfops import get_feed_dict
│ │ -from ..utils.validate import check_unknown, convert_id
│ │ +from ..utils.validate import check_unknown
│ │  
│ │  
│ │  def normalize_prediction(preds, model, cold_start, unknown_num, unknown_index):
│ │      if model.task == "rating":
│ │          preds = np.clip(preds, model.lower_bound, model.upper_bound)
│ │      elif model.task == "ranking":
│ │          # preds = 1 / (1 + np.exp(-z))
│ │ @@ -36,88 +38,68 @@
│ │      preds = np.sum(np.multiply(model.user_embed[user], model.item_embed[item]), axis=1)
│ │      return normalize_prediction(preds, model, cold_start, unknown_num, unknown_index)
│ │  
│ │  
│ │  def predict_tf_feat(model, user, item, feats, cold_start, inner_id):
│ │      user, item = convert_id(model, user, item, inner_id)
│ │      unknown_num, unknown_index, user, item = check_unknown(model, user, item)
│ │ +    has_sparse = model.sparse if hasattr(model, "sparse") else None
│ │ +    has_dense = model.dense if hasattr(model, "dense") else None
│ │      (
│ │          user_indices,
│ │          item_indices,
│ │          sparse_indices,
│ │          dense_values,
│ │ -    ) = get_predict_indices_and_values(
│ │ -        model.data_info, user, item, model.n_items, model.sparse, model.dense
│ │ -    )
│ │ +    ) = get_original_feats(model.data_info, user, item, has_sparse, has_dense)
│ │  
│ │      if feats is not None:
│ │ -        assert isinstance(
│ │ -            feats, (dict, pd.Series)
│ │ -        ), "feats must be `dict` or `pandas.Series`."
│ │ -        assert len(user_indices) == 1, "only support single user for feats"
│ │ -        sparse_indices, dense_values = features_from_dict(
│ │ -            model.data_info, sparse_indices, dense_values, feats, "predict"
│ │ +        assert isinstance(feats, dict), "`feats` must be `dict`."
│ │ +        assert len(user_indices) == 1, "Predict with feats only supports single user."
│ │ +        sparse_indices, dense_values = set_temp_feats(
│ │ +            model.data_info, sparse_indices, dense_values, feats
│ │          )
│ │  
│ │ -    if model.model_category == "sequence":
│ │ -        feed_dict = get_feed_dict(
│ │ -            model=model,
│ │ -            user_indices=user_indices,
│ │ -            item_indices=item_indices,
│ │ -            sparse_indices=sparse_indices,
│ │ -            dense_values=dense_values,
│ │ -            user_interacted_seq=model.recent_seqs[user_indices],
│ │ -            user_interacted_len=model.recent_seq_lens[user_indices],
│ │ -            is_training=False,
│ │ -        )
│ │ -    else:
│ │ -        feed_dict = get_feed_dict(
│ │ -            model=model,
│ │ -            user_indices=user_indices,
│ │ -            item_indices=item_indices,
│ │ -            sparse_indices=sparse_indices,
│ │ -            dense_values=dense_values,
│ │ -            is_training=False,
│ │ -        )
│ │ +    seqs, seq_len = get_cached_seqs(model, user_indices, repeat=False)
│ │ +    feed_dict = get_feed_dict(
│ │ +        model=model,
│ │ +        user_indices=user_indices,
│ │ +        item_indices=item_indices,
│ │ +        sparse_indices=sparse_indices,
│ │ +        dense_values=dense_values,
│ │ +        user_interacted_seq=seqs,
│ │ +        user_interacted_len=seq_len,
│ │ +        is_training=False,
│ │ +    )
│ │      preds = model.sess.run(model.output, feed_dict)
│ │      return normalize_prediction(preds, model, cold_start, unknown_num, unknown_index)
│ │  
│ │  
│ │  def predict_data_with_feats(
│ │      model, data, batch_size=None, cold_start="average", inner_id=False
│ │  ):
│ │ -    assert isinstance(data, pd.DataFrame), "data must be pandas DataFrame"
│ │ +    assert isinstance(data, pd.DataFrame), "Data must be pandas DataFrame"
│ │      user, item = convert_id(model, data.user, data.item, inner_id)
│ │      unknown_num, unknown_index, user, item = check_unknown(model, user, item)
│ │      if not batch_size:
│ │          batch_size = len(data)
│ │      preds = np.zeros(len(data), dtype=np.float32)
│ │ -    for index in tqdm(range(0, len(data), batch_size), "pred_data"):
│ │ -        batch_slice = slice(index, index + batch_size)
│ │ +    for i in tqdm(range(0, len(data), batch_size), "pred_data"):
│ │ +        batch_slice = slice(i, i + batch_size)
│ │          batch_data = data.iloc[batch_slice]
│ │          user_indices = user[batch_slice]
│ │          item_indices = item[batch_slice]
│ │ -        sparse_indices, dense_values = features_from_batch_data(
│ │ +        sparse_indices, dense_values = features_from_batch(
│ │              model.data_info, model.sparse, model.dense, batch_data
│ │          )
│ │ -        if model.model_category == "sequence":
│ │ -            feed_dict = get_feed_dict(
│ │ -                model=model,
│ │ -                user_indices=user_indices,
│ │ -                item_indices=item_indices,
│ │ -                sparse_indices=sparse_indices,
│ │ -                dense_values=dense_values,
│ │ -                user_interacted_seq=model.recent_seqs[user_indices],
│ │ -                user_interacted_len=model.recent_seq_lens[user_indices],
│ │ -                is_training=False,
│ │ -            )
│ │ -        else:
│ │ -            feed_dict = get_feed_dict(
│ │ -                model=model,
│ │ -                user_indices=user_indices,
│ │ -                item_indices=item_indices,
│ │ -                sparse_indices=sparse_indices,
│ │ -                dense_values=dense_values,
│ │ -                is_training=False,
│ │ -            )
│ │ +        seqs, seq_len = get_cached_seqs(model, user_indices, repeat=False)
│ │ +        feed_dict = get_feed_dict(
│ │ +            model=model,
│ │ +            user_indices=user_indices,
│ │ +            item_indices=item_indices,
│ │ +            sparse_indices=sparse_indices,
│ │ +            dense_values=dense_values,
│ │ +            user_interacted_seq=seqs,
│ │ +            user_interacted_len=seq_len,
│ │ +            is_training=False,
│ │ +        )
│ │          preds[batch_slice] = model.sess.run(model.output, feed_dict)
│ │      return normalize_prediction(preds, model, cold_start, unknown_num, unknown_index)
│ │   --- LibRecommender-1.0.1/libreco/recommendation/cold_start.py
│ ├── +++ LibRecommender-1.1.0/libreco/recommendation/cold_start.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/recommendation/ranking.py
│ ├── +++ LibRecommender-1.1.0/libreco/recommendation/ranking.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/recommendation/recommend.py
│ ├── +++ LibRecommender-1.1.0/libreco/recommendation/preprocess.py
│ │┄ Files 21% similar despite different names
│ │ @@ -1,137 +1,113 @@
│ │  import numpy as np
│ │ -import pandas as pd
│ │  
│ │ -from ..feature import (
│ │ -    add_item_features,
│ │ -    features_from_dict,
│ │ -    get_recommend_indices_and_values,
│ │ -)
│ │ +from ..prediction.preprocess import get_cached_seqs, set_temp_feats
│ │  from ..tfops import get_feed_dict
│ │ -from .ranking import rank_recommendations
│ │  
│ │  
│ │ -def construct_rec(data_info, user_ids, computed_recs, inner_id):
│ │ -    result_recs = dict()
│ │ -    for i, u in enumerate(user_ids):
│ │ -        if inner_id:
│ │ -            result_recs[u] = computed_recs[i]
│ │ -        else:
│ │ -            u = data_info.id2user[u]
│ │ -            result_recs[u] = np.array(
│ │ -                [data_info.id2item[ri] for ri in computed_recs[i]]
│ │ -            )
│ │ -    return result_recs
│ │ -
│ │ -
│ │ -# def rank_recommendations(preds, model, user_id, n_rec, inner_id):
│ │ -#    if model.task == "ranking":
│ │ -#        preds = expit(preds)
│ │ -#    consumed = set(model.user_consumed[user_id])
│ │ -#    count = n_rec + len(consumed)
│ │ -#    ids = np.argpartition(preds, -count)[-count:]
│ │ -#    rank = sorted(zip(ids, preds[ids]), key=lambda x: -x[1])
│ │ -#    recs_and_scores = islice(
│ │ -#        (
│ │ -#            rec if inner_id else (model.data_info.id2item[rec[0]], rec[1])
│ │ -#            for rec in rank
│ │ -#            if rec[0] not in consumed and rec[0] in model.data_info.id2item
│ │ -#        ),
│ │ -#        n_rec,
│ │ -#    )
│ │ -#    return list(recs_and_scores)
│ │ -
│ │ -
│ │ -def recommend_from_embedding(
│ │ -    task,
│ │ -    user_ids,
│ │ -    n_rec,
│ │ -    data_info,
│ │ -    user_embed,
│ │ -    item_embed,
│ │ -    filter_consumed,
│ │ -    random_rec,
│ │ -):
│ │ -    preds = user_embed[user_ids] @ item_embed[:data_info.n_items].T  # exclude item oov
│ │ -    return rank_recommendations(
│ │ -        task,
│ │ -        user_ids,
│ │ -        preds,
│ │ -        n_rec,
│ │ -        data_info.n_items,
│ │ -        data_info.user_consumed,
│ │ -        filter_consumed,
│ │ -        random_rec,
│ │ -    )
│ │ +def embed_from_seq(model, user_ids, seq, inner_id):
│ │ +    seq, seq_len = build_rec_seq(seq, model, inner_id)
│ │ +    feed_dict = {
│ │ +        model.user_interacted_seq: seq,
│ │ +        model.user_interacted_len: seq_len,
│ │ +    }
│ │ +    if hasattr(model, "user_indices"):
│ │ +        feed_dict[model.user_indices] = np.array(user_ids, dtype=np.int32)
│ │ +    embed = model.sess.run(model.user_vector, feed_dict)
│ │ +    # embed = embed / np.linalg.norm(embed, axis=1, keepdims=True)
│ │ +    bias = np.ones([1, 1], dtype=embed.dtype)
│ │ +    return np.hstack([embed, bias])
│ │ +
│ │ +
│ │ +def build_rec_seq(seq, model, inner_id, repeat=False):
│ │ +    assert isinstance(seq, (list, np.ndarray)), "`seq` must be list or numpy.ndarray."
│ │ +    if not inner_id:
│ │ +        seq = [model.data_info.item2id.get(i, model.n_items) for i in seq]
│ │ +    recent_seq = np.full((1, model.max_seq_len), model.n_items, dtype=np.int32)
│ │ +    seq_len = min(model.max_seq_len, len(seq))
│ │ +    recent_seq[0, -seq_len:] = seq[-seq_len:]
│ │ +    seq_len = np.array([seq_len], dtype=np.float32)
│ │ +    if repeat:
│ │ +        recent_seq = np.repeat(recent_seq, model.n_items, axis=0)
│ │ +        seq_len = np.repeat(seq_len, model.n_items)
│ │ +    return recent_seq, seq_len
│ │  
│ │  
│ │ -def recommend_tf_feat(
│ │ -    model,
│ │ -    user_ids,
│ │ -    n_rec,
│ │ -    user_feats,
│ │ -    item_data,
│ │ -    filter_consumed,
│ │ -    random_rec,
│ │ -):
│ │ +def process_tf_feat(model, user_ids, user_feats, seq, inner_id):
│ │      user_indices, item_indices, sparse_indices, dense_values = [], [], [], []
│ │      has_sparse = model.sparse if hasattr(model, "sparse") else None
│ │      has_dense = model.dense if hasattr(model, "dense") else None
│ │ +    all_items = np.arange(model.n_items)
│ │      for u in user_ids:
│ │ -        u, i, s, d = get_recommend_indices_and_values(
│ │ +        user_indices.append(np.repeat(u, model.n_items))
│ │ +        item_indices.append(all_items)
│ │ +        sparse, dense = _get_original_feats(
│ │              model.data_info, u, model.n_items, has_sparse, has_dense
│ │          )
│ │ -        user_indices.append(u)
│ │ -        item_indices.append(i)
│ │ -        if s is not None:
│ │ -            sparse_indices.append(s)
│ │ -        if d is not None:
│ │ -            dense_values.append(d)
│ │ +        if sparse is not None:
│ │ +            sparse_indices.append(sparse)
│ │ +        if dense is not None:
│ │ +            dense_values.append(dense)
│ │      user_indices = np.concatenate(user_indices, axis=0)
│ │      item_indices = np.concatenate(item_indices, axis=0)
│ │      sparse_indices = np.concatenate(sparse_indices, axis=0) if sparse_indices else None
│ │      dense_values = np.concatenate(dense_values, axis=0) if dense_values else None
│ │  
│ │      if user_feats is not None:
│ │ -        assert isinstance(
│ │ -            user_feats, (dict, pd.Series)
│ │ -        ), "feats must be `dict` or `pandas.Series`."
│ │ -        sparse_indices, dense_values = features_from_dict(
│ │ -            model.data_info, sparse_indices, dense_values, user_feats, mode="recommend"
│ │ -        )
│ │ -    if item_data is not None:
│ │ -        assert isinstance(
│ │ -            item_data, pd.DataFrame
│ │ -        ), "item_data must be `pandas DataFrame`"
│ │ -        assert "item" in item_data.columns, "item_data must contain 'item' column"
│ │ -        sparse_indices, dense_values = add_item_features(
│ │ -            model.data_info, sparse_indices, dense_values, item_data
│ │ +        assert isinstance(user_feats, dict), "`user_feats` must be `dict`."
│ │ +        sparse_indices, dense_values = set_temp_feats(
│ │ +            model.data_info, sparse_indices, dense_values, user_feats
│ │          )
│ │ +    if seq is not None and len(seq) > 0:
│ │ +        seqs, seq_len = build_rec_seq(seq, model, inner_id, repeat=True)
│ │ +    else:
│ │ +        seqs, seq_len = get_cached_seqs(model, user_ids, repeat=True)
│ │ +    return get_feed_dict(
│ │ +        model=model,
│ │ +        user_indices=user_indices,
│ │ +        item_indices=item_indices,
│ │ +        sparse_indices=sparse_indices,
│ │ +        dense_values=dense_values,
│ │ +        user_interacted_seq=seqs,
│ │ +        user_interacted_len=seq_len,
│ │ +        is_training=False,
│ │ +    )
│ │  
│ │ -    params = {
│ │ -        "model": model,
│ │ -        "user_indices": user_indices,
│ │ -        "item_indices": item_indices,
│ │ -        "sparse_indices": sparse_indices,
│ │ -        "dense_values": dense_values,
│ │ -        "is_training": False,
│ │ -    }
│ │ -    if model.model_category == "sequence":
│ │ -        u_last_interacted = np.repeat(
│ │ -            model.recent_seqs[user_ids], model.n_items, axis=0
│ │ +
│ │ +def _get_original_feats(data_info, user, n_items, sparse, dense):
│ │ +    sparse_indices = (
│ │ +        _extract_feats(
│ │ +            user,
│ │ +            n_items,
│ │ +            data_info.user_sparse_col.index,
│ │ +            data_info.item_sparse_col.index,
│ │ +            data_info.user_sparse_unique,
│ │ +            data_info.item_sparse_unique,
│ │ +        )
│ │ +        if sparse
│ │ +        else None
│ │ +    )
│ │ +    dense_values = (
│ │ +        _extract_feats(
│ │ +            user,
│ │ +            n_items,
│ │ +            data_info.user_dense_col.index,
│ │ +            data_info.item_dense_col.index,
│ │ +            data_info.user_dense_unique,
│ │ +            data_info.item_dense_unique,
│ │          )
│ │ -        u_interacted_len = np.repeat(model.recent_seq_lens[user_ids], model.n_items)
│ │ -        params["user_interacted_seq"] = u_last_interacted
│ │ -        params["user_interacted_len"] = u_interacted_len
│ │ -
│ │ -    feed_dict = get_feed_dict(**params)
│ │ -    preds = model.sess.run(model.output, feed_dict)
│ │ -    return rank_recommendations(
│ │ -        model.task,
│ │ -        user_ids,
│ │ -        preds,
│ │ -        n_rec,
│ │ -        model.n_items,
│ │ -        model.user_consumed,
│ │ -        filter_consumed,
│ │ -        random_rec,
│ │ +        if dense
│ │ +        else None
│ │      )
│ │ +    return sparse_indices, dense_values
│ │ +
│ │ +
│ │ +def _extract_feats(user, n_items, user_col, item_col, user_unique, item_unique):
│ │ +    user_feats = np.tile(user_unique[user], (n_items, 1)) if user_col else None
│ │ +    item_feats = item_unique[:-1] if item_col else None  # remove oov
│ │ +    if user_col and item_col:
│ │ +        orig_cols = user_col + item_col
│ │ +        # keep column names in original order
│ │ +        col_reindex = np.arange(len(orig_cols))[np.argsort(orig_cols)]
│ │ +        features = np.concatenate([user_feats, item_feats], axis=1)
│ │ +        return features[:, col_reindex]
│ │ +    return user_feats if user_col else item_feats
│ │   --- LibRecommender-1.0.1/libreco/sampling/negatives.py
│ ├── +++ LibRecommender-1.1.0/libreco/sampling/negatives.py
│ │┄ Files 3% similar despite different names
│ │ @@ -18,21 +18,22 @@
│ │      np_rng, n_items, items_pos, num_neg, items=None, tolerance=10
│ │  ):
│ │      items_pos = np.repeat(items_pos, num_neg) if num_neg > 1 else items_pos
│ │      items = np.repeat(items, num_neg) if num_neg > 1 and items is not None else items
│ │      replace = False if len(items_pos) < n_items else True
│ │      negatives = np_rng.choice(n_items, size=len(items_pos), replace=replace)
│ │      invalid_indices = _check_invalid_negatives(negatives, items_pos, items)
│ │ -    for _ in range(tolerance):
│ │ -        negatives[invalid_indices] = np_rng.choice(
│ │ -            n_items, size=len(invalid_indices), replace=True
│ │ -        )
│ │ -        invalid_indices = _check_invalid_negatives(negatives, items_pos, items)
│ │ -        if not invalid_indices:
│ │ -            break
│ │ +    if invalid_indices:
│ │ +        for _ in range(tolerance):
│ │ +            negatives[invalid_indices] = np_rng.choice(
│ │ +                n_items, size=len(invalid_indices), replace=True
│ │ +            )
│ │ +            invalid_indices = _check_invalid_negatives(negatives, items_pos, items)
│ │ +            if not invalid_indices:
│ │ +                break
│ │      return negatives
│ │  
│ │  
│ │  def negatives_from_popular(np_rng, n_items, items_pos, num_neg, items=None, probs=None):
│ │      items_pos = np.repeat(items_pos, num_neg) if num_neg > 1 else items_pos
│ │      items = np.repeat(items, num_neg) if num_neg > 1 and items is not None else items
│ │      negatives = np_rng.choice(n_items, size=len(items_pos), replace=True, p=probs)
│ │ @@ -77,24 +78,25 @@
│ │                      n = sample_one()
│ │                      if n != i and n not in u_negs:
│ │                          success = True
│ │                          break
│ │              if not success:
│ │                  n = sample_one()
│ │                  print(f"possible not enough negatives for user {u} and item {i}.")
│ │ +            # noinspection PyUnboundLocalVariable
│ │              u_negs.append(n)
│ │          negatives.extend(u_negs)
│ │      return np.array(negatives)
│ │  
│ │  
│ │  def neg_probs_from_frequency(item_consumed, n_items, temperature):
│ │      freqs = []
│ │      for i in range(n_items):
│ │          freq = len(set(item_consumed[i]))
│ │ -        if temperature != 1:
│ │ +        if temperature != 1.0:
│ │              freq = pow(freq, temperature)
│ │          freqs.append(freq)
│ │      freqs = np.array(freqs)
│ │      return freqs / np.sum(freqs)
│ │  
│ │  
│ │  def pos_probs_from_frequency(item_consumed, n_users, n_items, alpha):
│ │   --- LibRecommender-1.0.1/libreco/sampling/random_walks.py
│ ├── +++ LibRecommender-1.1.0/libreco/sampling/random_walks.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/tfops/__init__.py
│ ├── +++ LibRecommender-1.1.0/libreco/tfops/__init__.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/tfops/configs.py
│ ├── +++ LibRecommender-1.1.0/libreco/tfops/configs.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/tfops/features.py
│ ├── +++ LibRecommender-1.1.0/libreco/tfops/features.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/tfops/layers.py
│ ├── +++ LibRecommender-1.1.0/libreco/tfops/layers.py
│ │┄ Files 1% similar despite different names
│ │ @@ -9,15 +9,15 @@
│ │  # https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout
│ │  # https://www.zhihu.com/question/283715823
│ │  # Also according to the discussions, it is generally NOT recommended to use
│ │  # `batch normalization` and `dropout` simultaneously.
│ │  def dense_nn(
│ │      net,
│ │      hidden_units,
│ │ -    activation=tf.nn.elu,
│ │ +    activation=tf.nn.relu,
│ │      use_bn=True,
│ │      bn_after_activation=True,
│ │      dropout_rate=None,
│ │      is_training=True,
│ │      name="mlp",
│ │  ):
│ │      if activation is None:
│ │ @@ -27,26 +27,27 @@
│ │  
│ │      with tf.variable_scope(name):
│ │          if use_bn:
│ │              net = tf.layers.batch_normalization(net, training=is_training)
│ │          for i, units in enumerate(hidden_units, start=1):
│ │              layer_name = name + "_layer" + str(i)
│ │              net = tf_dense(units, activation=None, name=layer_name)(net)
│ │ -            if use_bn:
│ │ -                if bn_after_activation:
│ │ -                    net = activation(net)
│ │ -                    net = tf.layers.batch_normalization(net, training=is_training)
│ │ +            if i != len(hidden_units):
│ │ +                if use_bn:
│ │ +                    if bn_after_activation:
│ │ +                        net = activation(net)
│ │ +                        net = tf.layers.batch_normalization(net, training=is_training)
│ │ +                    else:
│ │ +                        net = tf.layers.batch_normalization(net, training=is_training)
│ │ +                        net = activation(net)
│ │                  else:
│ │ -                    net = tf.layers.batch_normalization(net, training=is_training)
│ │                      net = activation(net)
│ │ -            else:
│ │ -                net = activation(net)
│ │  
│ │ -            if dropout_rate:
│ │ -                net = tf.layers.dropout(net, dropout_rate, training=is_training)
│ │ +                if dropout_rate:
│ │ +                    net = tf.layers.dropout(net, dropout_rate, training=is_training)
│ │  
│ │      return net
│ │  
│ │  
│ │  def tf_dense(
│ │      units,
│ │      activation=None,
│ │   --- LibRecommender-1.0.1/libreco/tfops/loss.py
│ ├── +++ LibRecommender-1.1.0/libreco/tfops/loss.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/tfops/rebuild.py
│ ├── +++ LibRecommender-1.1.0/libreco/tfops/rebuild.py
│ │┄ Files 6% similar despite different names
│ │ @@ -1,9 +1,10 @@
│ │  """Rebuild TensorFlow models."""
│ │  import os
│ │ +from dataclasses import astuple
│ │  
│ │  import numpy as np
│ │  
│ │  from .variables import modify_variable_names
│ │  from .version import tf
│ │  
│ │  
│ │ @@ -19,15 +20,15 @@
│ │      path : str
│ │          File folder path for the saved model variables.
│ │      model_name : str
│ │          Name of the saved model file.
│ │      full_assign : bool, default: True
│ │          Whether to also restore the variables of Adam optimizer.
│ │      """
│ │ -    from ..training import get_trainer
│ │ +    from ..training.dispatch import get_trainer
│ │  
│ │      self.model_built = True
│ │      self.build_model()
│ │      self.trainer = get_trainer(self)
│ │  
│ │      variable_path = os.path.join(path, f"{model_name}_tf_variables.npz")
│ │      variables = np.load(variable_path)
│ │ @@ -37,35 +38,38 @@
│ │          user_variables,
│ │          item_variables,
│ │          sparse_variables,
│ │          dense_variables,
│ │          manual_variables,
│ │      ) = modify_variable_names(self, trainable=True)
│ │  
│ │ +    sparse_offset = self.data_info.sparse_offset
│ │ +    old_n_users, old_n_items, old_sparse_len, old_sparse_oov, _ = astuple(
│ │ +        self.data_info.old_info
│ │ +    )
│ │ +
│ │      update_ops = []
│ │      for v in tf.trainable_variables():
│ │          if user_variables is not None and v.name in user_variables:
│ │              # remove oov values
│ │ -            old_var = variables[v.name][: self.data_info.old_n_users]
│ │ +            old_var = variables[v.name][: old_n_users]
│ │              user_op = tf.IndexedSlices(old_var, tf.range(len(old_var)))
│ │              update_ops.append(v.scatter_update(user_op))
│ │  
│ │          if item_variables is not None and v.name in item_variables:
│ │ -            old_var = variables[v.name][: self.data_info.old_n_items]
│ │ +            old_var = variables[v.name][: old_n_items]
│ │              item_op = tf.IndexedSlices(old_var, tf.range(len(old_var)))
│ │              update_ops.append(v.scatter_update(item_op))
│ │  
│ │          if sparse_variables is not None and v.name in sparse_variables:
│ │              old_var = variables[v.name]
│ │              # remove oov values
│ │ -            old_var = np.delete(old_var, self.data_info.old_sparse_oov, axis=0)
│ │ +            old_var = np.delete(old_var, old_sparse_oov, axis=0)
│ │              indices = []
│ │ -            for offset, size in zip(
│ │ -                self.data_info.sparse_offset, self.data_info.old_sparse_len
│ │ -            ):
│ │ +            for offset, size in zip(sparse_offset, old_sparse_len):
│ │                  if size != -1:
│ │                      indices.extend(range(offset, offset + size))
│ │              sparse_op = tf.IndexedSlices(old_var, indices)
│ │              update_ops.append(v.scatter_update(sparse_op))
│ │  
│ │          if dense_variables is not None and v.name in dense_variables:
│ │              # dense values are same, no need to scatter_update
│ │ @@ -85,37 +89,35 @@
│ │              v for v in tf.global_variables() if v.name not in manual_variables
│ │          ]
│ │          for v in other_variables:
│ │              if (
│ │                  optimizer_user_variables is not None
│ │                  and v.name in optimizer_user_variables
│ │              ):
│ │ -                old_var = variables[v.name][: self.data_info.old_n_users]
│ │ +                old_var = variables[v.name][: old_n_users]
│ │                  user_op = tf.IndexedSlices(old_var, tf.range(len(old_var)))
│ │                  update_ops.append(v.scatter_update(user_op))
│ │  
│ │              elif (
│ │                  optimizer_item_variables is not None
│ │                  and v.name in optimizer_item_variables
│ │              ):
│ │ -                old_var = variables[v.name][: self.data_info.old_n_items]
│ │ +                old_var = variables[v.name][: old_n_items]
│ │                  item_op = tf.IndexedSlices(old_var, tf.range(len(old_var)))
│ │                  update_ops.append(v.scatter_update(item_op))
│ │  
│ │              elif (
│ │                  optimizer_sparse_variables is not None
│ │                  and v.name in optimizer_sparse_variables
│ │              ):
│ │                  old_var = variables[v.name]
│ │                  # remove oov values
│ │ -                old_var = np.delete(old_var, self.data_info.old_sparse_oov, axis=0)
│ │ +                old_var = np.delete(old_var, old_sparse_oov, axis=0)
│ │                  indices = []
│ │ -                for offset, size in zip(
│ │ -                    self.data_info.sparse_offset, self.data_info.old_sparse_len
│ │ -                ):
│ │ +                for offset, size in zip(sparse_offset, old_sparse_len):
│ │                      if size != -1:
│ │                          indices.extend(range(offset, offset + size))
│ │                  sparse_op = tf.IndexedSlices(old_var, indices)
│ │                  update_ops.append(v.scatter_update(sparse_op))
│ │  
│ │              elif (
│ │                  optimizer_dense_variables is not None
│ │   --- LibRecommender-1.0.1/libreco/tfops/variables.py
│ ├── +++ LibRecommender-1.1.0/libreco/tfops/variables.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/torchops/__init__.py
│ ├── +++ LibRecommender-1.1.0/libreco/torchops/__init__.py
│ │┄ Files 18% similar despite different names
│ │ @@ -1,9 +1,8 @@
│ │  from .configs import device_config, hidden_units_config
│ │ -from .features import feat_to_tensor, item_unique_to_tensor, user_unique_to_tensor
│ │  from .loss import (
│ │      binary_cross_entropy_loss,
│ │      bpr_loss,
│ │      compute_pair_scores,
│ │      focal_loss,
│ │      max_margin_loss,
│ │      pairwise_bce_loss,
│ │ @@ -13,16 +12,13 @@
│ │  
│ │  __all__ = [
│ │      "binary_cross_entropy_loss",
│ │      "bpr_loss",
│ │      "compute_pair_scores",
│ │      "device_config",
│ │      "hidden_units_config",
│ │ -    "feat_to_tensor",
│ │      "focal_loss",
│ │ -    "item_unique_to_tensor",
│ │      "max_margin_loss",
│ │      "pairwise_bce_loss",
│ │      "pairwise_focal_loss",
│ │      "rebuild_torch_model",
│ │ -    "user_unique_to_tensor",
│ │  ]
│ │   --- LibRecommender-1.0.1/libreco/torchops/configs.py
│ ├── +++ LibRecommender-1.1.0/libreco/torchops/configs.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/torchops/loss.py
│ ├── +++ LibRecommender-1.1.0/libreco/torchops/loss.py
│ │┄ Files 10% similar despite different names
│ │ @@ -33,31 +33,41 @@
│ │  def pairwise_bce_loss(pos_scores, neg_scores, mean=True):
│ │      pos_bce = F.binary_cross_entropy_with_logits(
│ │          pos_scores, torch.ones_like(pos_scores), reduction="none"
│ │      )
│ │      neg_bce = F.binary_cross_entropy_with_logits(
│ │          neg_scores, torch.zeros_like(neg_scores), reduction="none"
│ │      )
│ │ +    pos_bce = _check_dim(pos_bce)
│ │ +    neg_bce = _check_dim(neg_bce)
│ │      loss = torch.cat([pos_bce, neg_bce])
│ │      if mean:
│ │          return torch.mean(loss)
│ │      else:
│ │          return torch.sum(loss)
│ │  
│ │  
│ │  def pairwise_focal_loss(pos_scores, neg_scores, mean=True):
│ │      pos_focal = focal_loss(pos_scores, torch.ones_like(pos_scores), mean=False)
│ │      neg_focal = focal_loss(neg_scores, torch.zeros_like(neg_scores), mean=False)
│ │ +    pos_focal = _check_dim(pos_focal)
│ │ +    neg_focal = _check_dim(neg_focal)
│ │      loss = torch.cat([pos_focal, neg_focal])
│ │      if mean:
│ │          return torch.mean(loss)
│ │      else:
│ │          return torch.sum(loss)
│ │  
│ │  
│ │ +def _check_dim(tensor):  # pragma: no cover
│ │ +    if tensor.dim() == 0:
│ │ +        tensor = tensor.unsqueeze(0)
│ │ +    return tensor
│ │ +
│ │ +
│ │  def compute_pair_scores(targets, items_pos, items_neg, repeat_positives=True):
│ │      if len(targets) == len(items_pos) == len(items_neg):
│ │          pos_scores = torch.sum(torch.mul(targets, items_pos), dim=1)
│ │          neg_scores = torch.sum(torch.mul(targets, items_neg), dim=1)
│ │          return pos_scores, neg_scores
│ │  
│ │      if len(targets) != len(items_pos):
│ │   --- LibRecommender-1.0.1/libreco/torchops/rebuild.py
│ ├── +++ LibRecommender-1.1.0/libreco/torchops/rebuild.py
│ │┄ Files 11% similar despite different names
│ │ @@ -1,8 +1,10 @@
│ │  """Rebuild PyTorch models."""
│ │ +from dataclasses import astuple
│ │ +
│ │  import torch
│ │  from torch import nn
│ │  
│ │  from ..utils.save_load import load_torch_state_dict
│ │  from ..utils.validate import sparse_feat_size
│ │  
│ │  
│ │ @@ -17,15 +19,15 @@
│ │      Parameters
│ │      ----------
│ │      path : str
│ │          File folder path for the saved model variables.
│ │      model_name : str
│ │          Name of the saved model file.
│ │      """
│ │ -    from ..training import get_trainer
│ │ +    from ..training.dispatch import get_trainer
│ │  
│ │      self.model_built = True
│ │      self.build_model()
│ │      self.trainer = get_trainer(self)
│ │  
│ │      model_state_dict, optimizer_state_dict = load_torch_state_dict(
│ │          path, model_name, self.device
│ │ @@ -44,63 +46,74 @@
│ │          elif "sparse" in name and "embed" in name:
│ │              sparse_param_indices.append(i)
│ │              sparse_params[name] = model_state_dict.pop(name)
│ │  
│ │      # `strict=False` ignores non-matching keys
│ │      self.torch_model.load_state_dict(model_state_dict, strict=False)
│ │  
│ │ +    sparse_offset = self.data_info.sparse_offset
│ │ +    old_n_users, old_n_items, old_sparse_len, old_sparse_oov, _ = astuple(
│ │ +        self.data_info.old_info
│ │ +    )
│ │ +
│ │      for name, param in self.torch_model.named_parameters():
│ │          if name in user_params:
│ │ -            index = torch.arange(self.data_info.old_n_users, device=self.device)
│ │ +            index = torch.arange(old_n_users, device=self.device)
│ │              param.index_copy_(0, index, user_params[name])
│ │          elif name in item_params:
│ │ -            index = torch.arange(self.data_info.old_n_items, device=self.device)
│ │ +            index = torch.arange(old_n_items, device=self.device)
│ │              param.index_copy_(0, index, item_params[name])
│ │          elif name in sparse_params:
│ │              old_indices, old_values = get_sparse_indices_values(
│ │ -                self.data_info, sparse_params[name], self.device
│ │ +                sparse_offset,
│ │ +                old_sparse_len,
│ │ +                old_sparse_oov,
│ │ +                sparse_params[name],
│ │ +                self.device,
│ │              )
│ │              param.index_copy_(0, old_indices, old_values)
│ │  
│ │      for i in user_param_indices:
│ │          optimizer_user_state = optimizer_state_dict["state"][i]
│ │          assign_adam_optimizer_states(
│ │              optimizer_user_state,
│ │ -            self.data_info.old_n_users,
│ │ +            old_n_users,
│ │              self.n_users,
│ │              self.embed_size,
│ │              self.device,
│ │          )
│ │      for i in item_param_indices:
│ │          optimizer_item_state = optimizer_state_dict["state"][i]
│ │          assign_adam_optimizer_states(
│ │              optimizer_item_state,
│ │ -            self.data_info.old_n_items,
│ │ +            old_n_items,
│ │              self.n_items,
│ │              self.embed_size,
│ │              self.device,
│ │          )
│ │      for i in sparse_param_indices:
│ │          optimizer_sparse_state = optimizer_state_dict["state"][i]
│ │          assign_adam_sparse_states(
│ │              optimizer_sparse_state, self.data_info, self.embed_size, self.device
│ │          )
│ │  
│ │      self.trainer.optimizer.load_state_dict(optimizer_state_dict)
│ │  
│ │  
│ │ -def get_sparse_indices_values(data_info, sparse_param_tensor, device):
│ │ +def get_sparse_indices_values(
│ │ +    sparse_offset, old_sparse_len, old_sparse_oov, sparse_param_tensor, device
│ │ +):
│ │      indices = list(range(len(sparse_param_tensor)))
│ │      # remove oov indices
│ │ -    for i in data_info.old_sparse_oov:
│ │ +    for i in old_sparse_oov:
│ │          indices.remove(i)
│ │      sparse_param_tensor = sparse_param_tensor[indices]
│ │  
│ │      indices = []
│ │ -    for offset, size in zip(data_info.sparse_offset, data_info.old_sparse_len):
│ │ +    for offset, size in zip(sparse_offset, old_sparse_len):
│ │          if size != -1:
│ │              indices.extend(range(offset, offset + size))
│ │      indices = torch.tensor(indices, dtype=torch.long, device=device)
│ │      return indices, sparse_param_tensor
│ │  
│ │  
│ │  def assign_adam_optimizer_states(state, old_num, new_num, embed_size, device):
│ │ @@ -110,24 +123,26 @@
│ │      state["exp_avg"] = new_first_moment
│ │      new_second_moment = nn.init.zeros_(torch.empty(new_num, embed_size, device=device))
│ │      new_second_moment.index_copy_(0, index, state["exp_avg_sq"])
│ │      state["exp_avg_sq"] = new_second_moment
│ │  
│ │  
│ │  def assign_adam_sparse_states(state, data_info, embed_size, device):
│ │ +    _, _, old_sparse_len, old_sparse_oov, _ = astuple(data_info.old_info)
│ │ +    sparse_offset = data_info.sparse_offset
│ │      sparse_size = sparse_feat_size(data_info)
│ │      new_first_moment = nn.init.zeros_(
│ │          torch.empty(sparse_size, embed_size, device=device)
│ │      )
│ │      old_indices, old_values = get_sparse_indices_values(
│ │ -        data_info, state["exp_avg"], device
│ │ +        sparse_offset, old_sparse_len, old_sparse_oov, state["exp_avg"], device
│ │      )
│ │      new_first_moment.index_copy_(0, old_indices, old_values)
│ │      state["exp_avg"] = new_first_moment
│ │      new_second_moment = nn.init.zeros_(
│ │          torch.empty(sparse_size, embed_size, device=device)
│ │      )
│ │      old_indices, old_values = get_sparse_indices_values(
│ │ -        data_info, state["exp_avg_sq"], device
│ │ +        sparse_offset, old_sparse_len, old_sparse_oov, state["exp_avg_sq"], device
│ │      )
│ │      new_second_moment.index_copy_(0, old_indices, old_values)
│ │      state["exp_avg_sq"] = new_second_moment
│ │   --- LibRecommender-1.0.1/libreco/training/torch_trainer.py
│ ├── +++ LibRecommender-1.1.0/libreco/batch/collators.py
│ │┄ Files 25% similar despite different names
│ │ @@ -1,487 +1,458 @@
│ │ -import math
│ │ -from statistics import mean
│ │ +import random
│ │  
│ │ +import numpy as np
│ │  import torch
│ │ -from torch.optim import Adam
│ │ -from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
│ │  
│ │ -from ..evaluation import print_metrics
│ │ -from ..graph import build_subgraphs, compute_i2i_edge_scores, compute_u2i_edge_scores
│ │ -from ..sampling import (
│ │ -    DataGenerator,
│ │ -    PairwiseDataGenerator,
│ │ -    PairwiseRandomWalkGenerator,
│ │ -    PointwiseDataGenerator,
│ │ +from .batch_unit import (
│ │ +    PairFeats,
│ │ +    PairwiseBatch,
│ │ +    PointwiseBatch,
│ │ +    PointwiseSepFeatBatch,
│ │ +    SeqFeats,
│ │ +    SparseBatch,
│ │ +    SparseSeqFeats,
│ │ +    TripleFeats,
│ │  )
│ │ -from ..torchops import (
│ │ -    binary_cross_entropy_loss,
│ │ -    bpr_loss,
│ │ -    compute_pair_scores,
│ │ -    focal_loss,
│ │ -    max_margin_loss,
│ │ -    pairwise_bce_loss,
│ │ -    pairwise_focal_loss,
│ │ +from .enums import FeatType
│ │ +from .sequence import get_interacted_seq, get_sparse_interacted
│ │ +from ..graph import build_subgraphs, pairs_from_dgl_graph
│ │ +from ..sampling import (
│ │ +    neg_probs_from_frequency,
│ │ +    negatives_from_out_batch,
│ │ +    negatives_from_popular,
│ │ +    negatives_from_random,
│ │ +    negatives_from_unconsumed,
│ │ +    pairs_from_random_walk,
│ │ +    pos_probs_from_frequency,
│ │  )
│ │ -from ..utils.misc import colorize, time_block
│ │ -from .trainer import BaseTrainer
│ │  
│ │  
│ │ -class TorchTrainer(BaseTrainer):
│ │ +class BaseCollator:
│ │      def __init__(
│ │          self,
│ │          model,
│ │ -        task,
│ │ -        loss_type,
│ │ -        n_epochs,
│ │ -        lr,
│ │ -        lr_decay,
│ │ -        epsilon,
│ │ -        amsgrad,
│ │ -        reg,
│ │ -        batch_size,
│ │ -        num_neg,
│ │ -        margin,
│ │ -        sampler,
│ │ -        device,
│ │ +        data_info,
│ │ +        backend,
│ │ +        separate_features=False,
│ │ +        temperature=0.75,
│ │      ):
│ │ -        super().__init__(
│ │ -            model,
│ │ -            task,
│ │ -            loss_type,
│ │ -            n_epochs,
│ │ -            lr,
│ │ -            lr_decay,
│ │ -            epsilon,
│ │ -            batch_size,
│ │ -            num_neg,
│ │ +        self.n_users = data_info.n_users
│ │ +        self.n_items = data_info.n_items
│ │ +        self.user_consumed = data_info.user_consumed
│ │ +        self.item_consumed = data_info.item_consumed
│ │ +        self.user_sparse_col_index = data_info.user_sparse_col.index
│ │ +        self.item_sparse_col_index = data_info.item_sparse_col.index
│ │ +        self.user_dense_col_index = data_info.user_dense_col.index
│ │ +        self.item_dense_col_index = data_info.item_dense_col.index
│ │ +        self.item_sparse_unique = data_info.item_sparse_unique
│ │ +        self.item_dense_unique = data_info.item_dense_unique
│ │ +        self.has_seq = True if model.model_category == "sequence" else False
│ │ +        self.seq_mode = model.seq_mode if self.has_seq else None
│ │ +        self.max_seq_len = model.max_seq_len if self.has_seq else None
│ │ +        self.separate_features = separate_features
│ │ +        self.backend = backend
│ │ +        self.seed = model.seed
│ │ +        self.temperature = temperature
│ │ +        self.user_consumed_set = None
│ │ +        self.neg_probs = None
│ │ +        self.np_rng = None
│ │ +
│ │ +    def __call__(self, batch):
│ │ +        sparse_batch = self.get_features(batch, FeatType.SPARSE)
│ │ +        dense_batch = self.get_features(batch, FeatType.DENSE)
│ │ +        seq_batch = self.get_seqs(batch["user"], batch["item"])
│ │ +        batch_cls = PointwiseSepFeatBatch if self.separate_features else PointwiseBatch
│ │ +        batch_data = batch_cls(
│ │ +            users=batch["user"],
│ │ +            items=batch["item"],
│ │ +            labels=batch["label"],
│ │ +            sparse_indices=sparse_batch,
│ │ +            dense_values=dense_batch,
│ │ +            seqs=seq_batch,
│ │ +            backend=self.backend,
│ │          )
│ │ -        self.amsgrad = amsgrad
│ │ -        self.reg = reg or 0
│ │ -        self.margin = margin
│ │ -        self.sampler = sampler
│ │ -        self.device = device
│ │ -        self.torch_model = model.torch_model
│ │ -        self.optimizer = Adam(
│ │ -            params=self.torch_model.parameters(),
│ │ -            lr=self.lr,
│ │ -            eps=self.epsilon,
│ │ -            weight_decay=self.reg,
│ │ -            amsgrad=self.amsgrad,
│ │ -        )
│ │ -        # lr_schedular based on paper SGDR: https://arxiv.org/abs/1608.03983
│ │ -        self.lr_scheduler = (
│ │ -            CosineAnnealingWarmRestarts(self.optimizer, T_0=1, T_mult=2)
│ │ -            if self.lr_decay
│ │ -            else None
│ │ +        return batch_data
│ │ +
│ │ +    def get_col_index(self, feat_type):
│ │ +        if feat_type is FeatType.SPARSE:
│ │ +            user_col_index = self.user_sparse_col_index
│ │ +            item_col_index = self.item_sparse_col_index
│ │ +        elif feat_type is FeatType.DENSE:
│ │ +            user_col_index = self.user_dense_col_index
│ │ +            item_col_index = self.item_dense_col_index
│ │ +        else:
│ │ +            raise ValueError("`feat_type` must be sparse or dense.")
│ │ +        return user_col_index, item_col_index
│ │ +
│ │ +    def get_features(self, batch, feat_type):
│ │ +        if feat_type.value not in batch:
│ │ +            return
│ │ +        features = batch[feat_type.value]
│ │ +        if self.separate_features:
│ │ +            user_col_index, item_col_index = self.get_col_index(feat_type)
│ │ +            user_features = features[:, user_col_index] if user_col_index else None
│ │ +            item_features = features[:, item_col_index] if item_col_index else None
│ │ +            features = PairFeats(user_features, item_features)
│ │ +        return features
│ │ +
│ │ +    def get_seqs(self, user_indices, item_indices):
│ │ +        if not self.has_seq:
│ │ +            return
│ │ +        self._set_random_seeds()
│ │ +        self._set_user_consumed()
│ │ +        batch_interacted, interacted_len = get_interacted_seq(
│ │ +            user_indices,
│ │ +            item_indices,
│ │ +            self.user_consumed,
│ │ +            self.n_items,
│ │ +            self.seq_mode,
│ │ +            self.max_seq_len,
│ │ +            self.user_consumed_set,
│ │ +            self.np_rng,
│ │          )
│ │ +        return SeqFeats(batch_interacted, interacted_len)
│ │  
│ │ -    def _check_params(self):
│ │ -        if self.loss_type in ("bpr", "max_margin") and not self.sampler:
│ │ -            raise ValueError(f"{self.loss_type} loss must use negative sampling")
│ │ -        if self.sampler:
│ │ -            n_items = self.model.data_info.n_items
│ │ -            assert 0 < self.num_neg < n_items, (
│ │ -                f"`num_neg` should be positive and smaller than total items, "
│ │ -                f"got {self.num_neg}, {n_items}"
│ │ -            )
│ │ -            if self.sampler not in ("random", "unconsumed", "popular"):
│ │ -                raise ValueError(
│ │ -                    f"`sampler` must be one of (`random`, `unconsumed`, `popular`), "
│ │ -                    f"got {self.sampler}"
│ │ -                )
│ │ -
│ │ -    def get_data_generator(self, train_data):
│ │ -        if not self.sampler:
│ │ -            return DataGenerator(
│ │ -                train_data,
│ │ -                self.model.data_info,
│ │ -                self.batch_size,
│ │ -                self.num_neg,
│ │ -                self.sampler,
│ │ -                self.model.seed,
│ │ -                separate_features=False,
│ │ -            )
│ │ -        if self.loss_type in ("cross_entropy", "focal"):
│ │ -            return PointwiseDataGenerator(
│ │ -                train_data,
│ │ -                self.model.data_info,
│ │ -                self.batch_size,
│ │ -                self.num_neg,
│ │ -                self.sampler,
│ │ -                self.model.seed,
│ │ -                separate_features=True,
│ │ -            )
│ │ -        elif self.loss_type in ("bpr", "max_margin"):
│ │ -            return PairwiseDataGenerator(
│ │ -                train_data,
│ │ -                self.model.data_info,
│ │ -                self.batch_size,
│ │ -                self.num_neg,
│ │ -                self.sampler,
│ │ -                self.model.seed,
│ │ -                repeat_positives=False,
│ │ +    def sample_neg_items(self, batch, sampler, num_neg):
│ │ +        if sampler == "unconsumed":
│ │ +            self._set_user_consumed()
│ │ +            items_neg = negatives_from_unconsumed(
│ │ +                self.user_consumed_set,
│ │ +                batch["user"],
│ │ +                batch["item"],
│ │ +                self.n_items,
│ │ +                num_neg,
│ │ +            )
│ │ +        elif sampler == "popular":
│ │ +            self._set_random_seeds()
│ │ +            self._set_neg_probs()
│ │ +            items_neg = negatives_from_popular(
│ │ +                self.np_rng,
│ │ +                self.n_items,
│ │ +                batch["item"],
│ │ +                num_neg,
│ │ +                probs=self.neg_probs,
│ │              )
│ │          else:
│ │ -            raise ValueError(f"unknown `loss_type`: {self.loss_type}")
│ │ +            self._set_random_seeds()
│ │ +            items_neg = negatives_from_random(
│ │ +                self.np_rng,
│ │ +                self.n_items,
│ │ +                batch["item"],
│ │ +                num_neg,
│ │ +            )
│ │ +        return items_neg
│ │ +
│ │ +    def _set_user_consumed(self):
│ │ +        if self.user_consumed_set is None:
│ │ +            self.user_consumed_set = [
│ │ +                set(self.user_consumed[u]) for u in range(self.n_users)
│ │ +            ]
│ │ +
│ │ +    def _set_neg_probs(self):
│ │ +        if self.neg_probs is None:
│ │ +            self.neg_probs = neg_probs_from_frequency(
│ │ +                self.item_consumed, self.n_items, self.temperature
│ │ +            )
│ │ +
│ │ +    def _set_random_seeds(self):
│ │ +        if self.np_rng is None:
│ │ +            worker_info = torch.utils.data.get_worker_info()
│ │ +            seed = self.seed if worker_info is None else worker_info.seed
│ │ +            seed = seed % 3407 * 11
│ │ +            random.seed(seed)
│ │ +            torch.manual_seed(seed)
│ │ +            self.np_rng = np.random.default_rng(seed)
│ │ +
│ │ +
│ │ +class SparseCollator(BaseCollator):
│ │ +    def __init__(self, model, data_info, backend):
│ │ +        super().__init__(model, data_info, backend)
│ │ +
│ │ +    def __call__(self, batch):
│ │ +        seq_batch = self.get_seqs(batch["user"], batch["item"])
│ │ +        sparse_batch = self.get_features(batch, FeatType.SPARSE)
│ │ +        dense_batch = self.get_features(batch, FeatType.DENSE)
│ │ +        return SparseBatch(
│ │ +            seqs=seq_batch,
│ │ +            items=batch["item"],
│ │ +            sparse_indices=sparse_batch,
│ │ +            dense_values=dense_batch,
│ │ +        )
│ │  
│ │ -    def run(
│ │ -        self,
│ │ -        train_data,
│ │ -        verbose,
│ │ -        shuffle,
│ │ -        eval_data,
│ │ -        metrics,
│ │ -        k,
│ │ -        eval_batch_size,
│ │ -        eval_user_num,
│ │ -    ):
│ │ -        self._check_params()
│ │ -        data_generator = self.get_data_generator(train_data)
│ │ -        n_batches = math.ceil(len(train_data) / self.batch_size)
│ │ -        for epoch in range(1, self.n_epochs + 1):
│ │ -            if self.lr_decay:
│ │ -                print(
│ │ -                    f"With lr_decay, epoch {epoch} learning rate: "
│ │ -                    f"{self.optimizer.param_groups[0]['lr']}"
│ │ -                )
│ │ -            with time_block(f"Epoch {epoch}", verbose):
│ │ -                self.torch_model.train()
│ │ -                train_total_loss = []
│ │ -                for i, data in enumerate(data_generator(shuffle)):
│ │ -                    loss = self.compute_loss(data)
│ │ -                    self.optimizer.zero_grad()
│ │ -                    loss.backward()
│ │ -                    self.optimizer.step()
│ │ -                    if self.lr_scheduler is not None:
│ │ -                        # noinspection PyTypeChecker
│ │ -                        self.lr_scheduler.step(epoch + i / n_batches)
│ │ -                    train_total_loss.append(loss.detach().cpu().item())
│ │ -
│ │ -            if verbose > 1:
│ │ -                train_loss_str = f"train_loss: {round(mean(train_total_loss), 4)}"
│ │ -                print(f"\t {colorize(train_loss_str, 'green')}")
│ │ -                # get embedding for evaluation
│ │ -                self.model.set_embeddings()
│ │ -                print_metrics(
│ │ -                    model=self.model,
│ │ -                    eval_data=eval_data,
│ │ -                    metrics=metrics,
│ │ -                    eval_batch_size=eval_batch_size,
│ │ -                    k=k,
│ │ -                    sample_user_num=eval_user_num,
│ │ -                    seed=self.model.seed,
│ │ -                )
│ │ -                print("=" * 30)
│ │ -
│ │ -    def compute_loss(self, data):
│ │ -        user_embeds, item_embeds = self.torch_model(use_dropout=True)
│ │ -        if self.loss_type in ("cross_entropy", "focal"):
│ │ -            users = torch.as_tensor(data.users, dtype=torch.long, device=self.device)
│ │ -            users = user_embeds[users]
│ │ -            items = torch.as_tensor(data.items, dtype=torch.long, device=self.device)
│ │ -            items = item_embeds[items]
│ │ -            logits = torch.sum(torch.mul(users, items), dim=1)
│ │ -            labels = torch.as_tensor(data.labels, dtype=torch.float, device=self.device)
│ │ -            if self.loss_type == "cross_entropy":
│ │ -                return binary_cross_entropy_loss(logits, labels)
│ │ -            else:
│ │ -                return focal_loss(logits, labels)
│ │ -        elif self.loss_type in ("bpr", "max_margin"):
│ │ -            users = torch.as_tensor(data.queries, dtype=torch.long, device=self.device)
│ │ -            users = user_embeds[users]
│ │ -            items_pos = torch.as_tensor(
│ │ -                data.item_pairs[0], dtype=torch.long, device=self.device
│ │ -            )
│ │ -            items_pos = item_embeds[items_pos]
│ │ -            items_neg = torch.as_tensor(
│ │ -                data.item_pairs[1], dtype=torch.long, device=self.device
│ │ -            )
│ │ -            items_neg = item_embeds[items_neg]
│ │ -            pos_scores, neg_scores = compute_pair_scores(users, items_pos, items_neg)
│ │ -            if self.loss_type == "bpr":
│ │ -                return bpr_loss(pos_scores, neg_scores)
│ │ -            else:
│ │ -                return max_margin_loss(pos_scores, neg_scores, self.margin)
│ │ -        else:
│ │ -            raise ValueError(f"unknown `loss_type`: {self.loss_type}")
│ │ +    def get_seqs(self, user_indices, item_indices):
│ │ +        batch_indices, batch_values, batch_size = get_sparse_interacted(
│ │ +            user_indices,
│ │ +            item_indices,
│ │ +            self.user_consumed,
│ │ +            self.seq_mode,
│ │ +            self.max_seq_len,
│ │ +        )
│ │ +        return SparseSeqFeats(batch_indices, batch_values, batch_size)
│ │  
│ │  
│ │ -class SageTrainer(TorchTrainer):
│ │ -    def __init__(
│ │ -        self,
│ │ -        model,
│ │ -        task,
│ │ -        loss_type,
│ │ -        n_epochs,
│ │ -        lr,
│ │ -        lr_decay,
│ │ -        epsilon,
│ │ -        amsgrad,
│ │ -        reg,
│ │ -        batch_size,
│ │ -        num_neg,
│ │ -        paradigm,
│ │ -        num_walks,
│ │ -        walk_len,
│ │ -        margin,
│ │ -        sampler,
│ │ -        start_node,
│ │ -        focus_start,
│ │ -        device,
│ │ -    ):
│ │ -        super().__init__(
│ │ -            model,
│ │ -            task,
│ │ -            loss_type,
│ │ -            n_epochs,
│ │ -            lr,
│ │ -            lr_decay,
│ │ -            epsilon,
│ │ -            amsgrad,
│ │ -            reg,
│ │ -            batch_size,
│ │ -            num_neg,
│ │ -            margin,
│ │ -            sampler,
│ │ -            device,
│ │ -        )
│ │ -        self.data_info = model.data_info
│ │ -        self.paradigm = paradigm
│ │ -        self.num_walks = num_walks
│ │ -        self.walk_len = walk_len
│ │ -        self.start_node = start_node
│ │ -        self.focus_start = focus_start
│ │ -
│ │ -    def _check_params(self):
│ │ -        if self.loss_type in ("bpr", "max_margin") and not self.sampler:
│ │ -            raise ValueError(f"{self.loss_type} loss must use negative sampling")
│ │ -        if self.sampler:
│ │ -            self._check_sampler()
│ │ -
│ │ -    def _check_sampler(self):
│ │ -        n_items = self.model.data_info.n_items
│ │ -        assert 0 < self.num_neg < n_items, (
│ │ -            f"`num_neg` should be positive and smaller than total items, "
│ │ -            f"got {self.num_neg}, {n_items}"
│ │ +class PointwiseCollator(BaseCollator):
│ │ +    def __init__(self, model, data_info, backend):
│ │ +        super().__init__(model, data_info, backend)
│ │ +        self.sampler = model.sampler
│ │ +        self.num_neg = model.num_neg
│ │ +
│ │ +    def __call__(self, batch):
│ │ +        user_batch = np.repeat(batch["user"], self.num_neg + 1)
│ │ +        item_batch = np.repeat(batch["item"], self.num_neg + 1)
│ │ +        label_batch = np.zeros_like(item_batch, dtype=np.float32)
│ │ +        label_batch[:: (self.num_neg + 1)] = 1.0
│ │ +        items_neg = self.sample_neg_items(batch, self.sampler, self.num_neg)
│ │ +        for i in range(self.num_neg):
│ │ +            item_batch[(i + 1) :: (self.num_neg + 1)] = items_neg[i :: self.num_neg]
│ │ +
│ │ +        sparse_batch = self.get_pointwise_feats(batch, FeatType.SPARSE, item_batch)
│ │ +        dense_batch = self.get_pointwise_feats(batch, FeatType.DENSE, item_batch)
│ │ +        seq_batch = self.get_seqs(user_batch, item_batch)
│ │ +        batch_data = PointwiseBatch(
│ │ +            users=user_batch,
│ │ +            items=item_batch,
│ │ +            labels=label_batch,
│ │ +            sparse_indices=sparse_batch,
│ │ +            dense_values=dense_batch,
│ │ +            seqs=seq_batch,
│ │ +            backend=self.backend,
│ │          )
│ │ -        if self.paradigm == "u2i" and self.sampler not in (
│ │ -            "random",
│ │ -            "unconsumed",
│ │ -            "popular",
│ │ -        ):
│ │ -            raise ValueError(
│ │ -                f"`sampler` must be one of (`random`, `unconsumed`, `popular`) "
│ │ -                f"for u2i, got {self.sampler}"
│ │ -            )
│ │ -        if self.paradigm == "i2i" and self.sampler not in (
│ │ -            "random",
│ │ -            "out-batch",
│ │ -            "popular",
│ │ -        ):
│ │ -            raise ValueError(
│ │ -                f"`sampler` must be one of (`random`, `out-batch`, `popular`) "
│ │ -                f"for i2i, got {self.sampler}, consider using u2i for no sampling"
│ │ -            )
│ │ +        return batch_data
│ │  
│ │ -    def get_data_generator(self, train_data):
│ │ -        if self.paradigm == "u2i":
│ │ -            if not self.sampler:
│ │ -                return DataGenerator(
│ │ -                    train_data,
│ │ -                    self.model.data_info,
│ │ -                    self.batch_size,
│ │ -                    self.num_neg,
│ │ -                    self.sampler,
│ │ -                    self.model.seed,
│ │ -                    separate_features=True,
│ │ -                )
│ │ -            else:
│ │ -                return PairwiseDataGenerator(
│ │ -                    train_data,
│ │ -                    self.model.data_info,
│ │ -                    self.batch_size,
│ │ -                    self.num_neg,
│ │ -                    self.sampler,
│ │ -                    self.model.seed,
│ │ -                    repeat_positives=False,
│ │ -                )
│ │ -        elif self.paradigm == "i2i":
│ │ -            return PairwiseRandomWalkGenerator(
│ │ -                train_data,
│ │ -                self.data_info,
│ │ -                self.batch_size,
│ │ -                self.num_neg,
│ │ -                self.num_walks,
│ │ -                self.walk_len,
│ │ -                self.sampler,
│ │ -                self.model.seed,
│ │ -                repeat_positives=False,
│ │ -                start_nodes=self.start_node,
│ │ -                focus_start=self.focus_start,
│ │ -            )
│ │ -
│ │ -    def compute_loss(self, data):
│ │ -        if (
│ │ -            self.paradigm == "u2i"
│ │ -            and not self.sampler
│ │ -            and self.loss_type in ("cross_entropy", "focal")
│ │ -        ):
│ │ -            user_feats = data.users, data.sparse_indices[0], data.dense_values[0]
│ │ -            user_reprs = self.model.get_user_repr(*user_feats)
│ │ -            item_feats = data.items, data.sparse_indices[1], data.dense_values[1]
│ │ -            item_reprs = self.model.get_item_repr(*item_feats)
│ │ -            logits = torch.sum(torch.mul(user_reprs, item_reprs), dim=1)
│ │ -            labels = torch.as_tensor(data.labels, dtype=torch.float, device=self.device)
│ │ -            if self.loss_type == "cross_entropy":
│ │ -                return binary_cross_entropy_loss(logits, labels)
│ │ -            else:
│ │ -                return focal_loss(logits, labels)
│ │ +    def get_pointwise_feats(self, batch, feat_type, items):
│ │ +        if feat_type.value not in batch:
│ │ +            return
│ │ +        batch_feats = batch[feat_type.value]
│ │ +        user_col_index, item_col_index = self.get_col_index(feat_type)
│ │ +        user_features = repeat_feats(batch_feats, user_col_index, self.num_neg)
│ │ +        item_features = get_sampled_item_feats(self, item_col_index, items, feat_type)
│ │ +        if self.separate_features:
│ │ +            return PairFeats(user_features, item_features)
│ │ +        if user_col_index and item_col_index:
│ │ +            return merge_columns(
│ │ +                user_features, item_features, user_col_index, item_col_index
│ │ +            )
│ │ +        return user_features if user_col_index else item_features
│ │ +
│ │ +
│ │ +class PairwiseCollator(BaseCollator):
│ │ +    def __init__(self, model, data_info, backend, repeat_positives):
│ │ +        super().__init__(model, data_info, backend, separate_features=True)
│ │ +        self.sampler = model.sampler
│ │ +        self.num_neg = model.num_neg
│ │ +        self.repeat_positives = repeat_positives
│ │ +
│ │ +    def __call__(self, batch):
│ │ +        if self.repeat_positives and self.num_neg > 1:
│ │ +            users = np.repeat(batch["user"], self.num_neg)
│ │ +            items_pos = np.repeat(batch["item"], self.num_neg)
│ │          else:
│ │ -            query_feats = data.queries, data.sparse_indices[0], data.dense_values[0]
│ │ -            if self.paradigm == "i2i":
│ │ -                query_reprs = self.model.get_item_repr(
│ │ -                    *query_feats, items_pos=data.item_pairs[0]
│ │ -                )
│ │ -            else:
│ │ -                query_reprs = self.model.get_user_repr(*query_feats)
│ │ -            item_pos_feats = (
│ │ -                data.item_pairs[0],
│ │ -                data.sparse_indices[1],
│ │ -                data.dense_values[1],
│ │ -            )
│ │ -            item_neg_feats = (
│ │ -                data.item_pairs[1],
│ │ -                data.sparse_indices[2],
│ │ -                data.dense_values[2],
│ │ -            )
│ │ -            item_pos_reprs = self.model.get_item_repr(*item_pos_feats)
│ │ -            item_neg_reprs = self.model.get_item_repr(*item_neg_feats)
│ │ -            repeat_positives = (
│ │ -                True if self.loss_type in ("bpr", "max_margin") else False
│ │ -            )
│ │ -            pos_scores, neg_scores = compute_pair_scores(
│ │ -                query_reprs, item_pos_reprs, item_neg_reprs, repeat_positives
│ │ -            )
│ │ -            return self._get_loss(pos_scores, neg_scores)
│ │ -
│ │ -    def _get_loss(self, pos_scores, neg_scores):
│ │ -        if self.loss_type in ("cross_entropy", "focal"):
│ │ -            loss_func = (
│ │ -                pairwise_bce_loss
│ │ -                if self.loss_type == "cross_entropy"
│ │ -                else pairwise_focal_loss
│ │ -            )
│ │ -            mean = True if self.paradigm == "u2i" else False
│ │ -            return loss_func(pos_scores, neg_scores, mean=mean)
│ │ -        if self.loss_type == "bpr":
│ │ -            return bpr_loss(pos_scores, neg_scores)
│ │ -        else:
│ │ -            return max_margin_loss(pos_scores, neg_scores, self.margin)
│ │ -
│ │ +            users = batch["user"]
│ │ +            items_pos = batch["item"]
│ │ +        items_neg = self.sample_neg_items(batch, self.sampler, self.num_neg)
│ │ +
│ │ +        sparse_batch = self.get_pairwise_feats(batch, FeatType.SPARSE, items_neg)
│ │ +        dense_batch = self.get_pairwise_feats(batch, FeatType.DENSE, items_neg)
│ │ +        seq_batch = self.get_seqs(users, items_pos)
│ │ +        if self.has_seq and not self.repeat_positives and self.num_neg > 1:
│ │ +            seq_batch = seq_batch.repeat(self.num_neg)
│ │ +        batch_data = PairwiseBatch(
│ │ +            queries=users,
│ │ +            item_pairs=(items_pos, items_neg),
│ │ +            sparse_indices=sparse_batch,
│ │ +            dense_values=dense_batch,
│ │ +            seqs=seq_batch,
│ │ +            backend=self.backend,
│ │ +        )
│ │ +        return batch_data
│ │  
│ │ -class SageDGLTrainer(SageTrainer):
│ │ -    def __init__(
│ │ -        self,
│ │ -        model,
│ │ -        task,
│ │ -        loss_type,
│ │ -        n_epochs,
│ │ -        lr,
│ │ -        lr_decay,
│ │ -        epsilon,
│ │ -        amsgrad,
│ │ -        reg,
│ │ -        batch_size,
│ │ -        num_neg,
│ │ -        paradigm,
│ │ -        num_walks,
│ │ -        walk_len,
│ │ -        margin,
│ │ -        sampler,
│ │ -        start_node,
│ │ -        focus_start,
│ │ -        device,
│ │ -    ):
│ │ -        super().__init__(
│ │ -            model,
│ │ -            task,
│ │ -            loss_type,
│ │ -            n_epochs,
│ │ -            lr,
│ │ -            lr_decay,
│ │ -            epsilon,
│ │ -            amsgrad,
│ │ -            reg,
│ │ -            batch_size,
│ │ -            num_neg,
│ │ -            paradigm,
│ │ -            num_walks,
│ │ -            walk_len,
│ │ -            margin,
│ │ -            sampler,
│ │ -            start_node,
│ │ -            focus_start,
│ │ -            device,
│ │ +    def get_pairwise_feats(self, batch, feat_type, items_neg):
│ │ +        if feat_type.value not in batch:
│ │ +            return
│ │ +        batch_feats = batch[feat_type.value]
│ │ +        user_col_index, item_col_index = self.get_col_index(feat_type)
│ │ +        if self.repeat_positives and self.num_neg > 1:
│ │ +            user_feats = repeat_feats(
│ │ +                batch_feats, user_col_index, self.num_neg, is_pairwise=True
│ │ +            )
│ │ +            item_pos_feats = repeat_feats(
│ │ +                batch_feats, item_col_index, self.num_neg, is_pairwise=True
│ │ +            )
│ │ +        else:
│ │ +            user_feats = batch_feats[:, user_col_index] if user_col_index else None
│ │ +            item_pos_feats = batch_feats[:, item_col_index] if item_col_index else None
│ │ +        item_neg_feats = get_sampled_item_feats(
│ │ +            self, item_col_index, items_neg, feat_type
│ │          )
│ │ +        return TripleFeats(user_feats, item_pos_feats, item_neg_feats)
│ │  
│ │ -    def _check_params(self):
│ │ -        if not self.sampler:
│ │ -            raise ValueError(f"{self.model.model_name} must use sampling")
│ │ -        self._check_sampler()
│ │  
│ │ -    def get_data_generator(self, train_data):
│ │ +class GraphCollator(BaseCollator):
│ │ +    def __init__(self, model, data_info, backend, alpha=1e-3):
│ │ +        super().__init__(model, data_info, backend)
│ │ +        self.neighbor_walker = model.neighbor_walker
│ │ +        self.paradigm = model.paradigm
│ │ +        self.sampler = model.sampler
│ │ +        self.num_neg = model.num_neg
│ │ +        self.num_walks = model.num_walks
│ │ +        self.walk_length = model.sample_walk_len
│ │ +        self.start_node = model.start_node
│ │ +        self.focus_start = model.focus_start
│ │ +        if self.start_node == "unpopular":
│ │ +            self.pos_probs = pos_probs_from_frequency(
│ │ +                self.item_consumed, self.n_users, self.n_items, alpha
│ │ +            )
│ │ +
│ │ +    def __call__(self, batch):
│ │ +        self._set_random_seeds()
│ │          if self.paradigm == "u2i":
│ │ -            return PairwiseDataGenerator(
│ │ -                train_data,
│ │ -                self.model.data_info,
│ │ -                self.batch_size,
│ │ +            users, items_pos = batch["user"], batch["item"]
│ │ +            items_neg = self.sample_neg_items(batch, self.sampler, self.num_neg)
│ │ +            user_data = self.neighbor_walker.get_user_feats(users)
│ │ +            item_pos_data = self.neighbor_walker(items_pos)
│ │ +            item_neg_data = self.neighbor_walker(items_neg)
│ │ +            return user_data, item_pos_data, item_neg_data
│ │ +        else:
│ │ +            start_nodes = self.get_start_nodes(batch)
│ │ +            items, items_pos = pairs_from_random_walk(
│ │ +                start_nodes,
│ │ +                self.user_consumed,
│ │ +                self.item_consumed,
│ │ +                self.num_walks,
│ │ +                self.walk_length,
│ │ +                self.focus_start,
│ │ +            )
│ │ +            items_neg = self.sample_i2i_negatives(items, items_pos)
│ │ +            item_data = self.neighbor_walker(items, items_pos)
│ │ +            item_pos_data = self.neighbor_walker(items_pos)
│ │ +            item_neg_data = self.neighbor_walker(items_neg)
│ │ +            return item_data, item_pos_data, item_neg_data
│ │ +
│ │ +    # exclude both items and items_pos
│ │ +    def sample_i2i_negatives(self, items, items_pos):
│ │ +        if self.sampler == "out-batch":
│ │ +            items_neg = negatives_from_out_batch(
│ │ +                self.np_rng, self.n_items, items_pos, items, self.num_neg
│ │ +            )
│ │ +        elif self.sampler == "popular":
│ │ +            items_neg = negatives_from_popular(
│ │ +                self.np_rng,
│ │ +                self.n_items,
│ │ +                items_pos,
│ │                  self.num_neg,
│ │ -                self.sampler,
│ │ -                self.model.seed,
│ │ -                use_features=False,
│ │ -                repeat_positives=False,
│ │ -            )
│ │ -        elif self.paradigm == "i2i":
│ │ -            return PairwiseRandomWalkGenerator(
│ │ -                train_data,
│ │ -                self.data_info,
│ │ -                self.batch_size,
│ │ +                items=items,
│ │ +                probs=self.neg_probs,
│ │ +            )
│ │ +        else:
│ │ +            items_neg = negatives_from_random(
│ │ +                self.np_rng,
│ │ +                self.n_items,
│ │ +                items_pos,
│ │                  self.num_neg,
│ │ -                self.num_walks,
│ │ -                self.walk_len,
│ │ -                self.sampler,
│ │ -                self.model.seed,
│ │ -                use_features=False,
│ │ -                repeat_positives=False,
│ │ -                start_nodes=self.start_node,
│ │ -                focus_start=self.focus_start,
│ │ -                graph=self.model.hetero_g,
│ │ -            )
│ │ -
│ │ -    def compute_loss(self, data):
│ │ -        import dgl
│ │ -
│ │ -        # nodes in pos_graph and neg_graph are same, difference is the connected edges
│ │ -        pos_graph, neg_graph, *target_nodes = build_subgraphs(
│ │ -            data.queries, data.item_pairs, self.paradigm, self.num_neg
│ │ -        )
│ │ +                items=items,
│ │ +            )
│ │ +        return items_neg
│ │ +
│ │ +    def get_start_nodes(self, batch):
│ │ +        size = len(batch["item"])
│ │ +        if self.start_node == "unpopular":
│ │ +            population = range(self.n_items)
│ │ +            start_nodes = random.choices(population, weights=self.pos_probs, k=size)
│ │ +        else:
│ │ +            start_nodes = self.np_rng.integers(0, self.n_items, size=size)
│ │ +            start_nodes = start_nodes.tolist()
│ │ +        return start_nodes
│ │ +
│ │ +
│ │ +class GraphDGLCollator(GraphCollator):
│ │ +    def __init__(self, model, data_info, backend, alpha=1e-3):
│ │ +        super().__init__(model, data_info, backend, alpha)
│ │ +        self.graph = model.hetero_g
│ │ +        self.dgl = model._dgl
│ │ +        self.dgl_seed = None
│ │ +        if self.start_node == "unpopular":
│ │ +            self.pos_probs = torch.tensor(self.pos_probs, dtype=torch.float)
│ │ +
│ │ +    def __call__(self, batch):
│ │ +        self._set_random_seeds()
│ │ +        self._set_dgl_seeds()
│ │          if self.paradigm == "u2i":
│ │ +            users, items_pos = batch["user"], batch["item"]
│ │ +            items_neg = self.sample_neg_items(batch, self.sampler, self.num_neg)
│ │ +            # nodes in pos_graph and neg_graph are same, difference is the connected edges
│ │ +            pos_graph, neg_graph, *_ = build_subgraphs(
│ │ +                users, (items_pos, items_neg), self.paradigm, self.num_neg
│ │ +            )
│ │              # user -> item heterogeneous graph, users on srcdata, items on dstdata
│ │ -            users, items = pos_graph.srcdata[dgl.NID], pos_graph.dstdata[dgl.NID]
│ │ -            user_reprs = self.model.get_user_repr(users)
│ │ -            item_reprs = self.model.get_item_repr(items)
│ │ -            pos_graph = pos_graph.to(self.device)
│ │ -            neg_graph = neg_graph.to(self.device)
│ │ -            pos_scores = compute_u2i_edge_scores(pos_graph, user_reprs, item_reprs)
│ │ -            neg_scores = compute_u2i_edge_scores(neg_graph, user_reprs, item_reprs)
│ │ +            all_users = pos_graph.srcdata[self.dgl.NID]
│ │ +            all_items = pos_graph.dstdata[self.dgl.NID]
│ │ +            user_data = self.neighbor_walker.get_user_feats(all_users)
│ │ +            item_data = self.neighbor_walker(all_items)
│ │ +            return user_data, item_data, pos_graph, neg_graph
│ │          else:
│ │ +            start_nodes = self.get_start_nodes(batch)
│ │ +            items, items_pos = pairs_from_dgl_graph(
│ │ +                self.graph,
│ │ +                start_nodes,
│ │ +                self.num_walks,
│ │ +                self.walk_length,
│ │ +                self.focus_start,
│ │ +            )
│ │ +            items_neg = self.sample_i2i_negatives(items, items_pos)
│ │ +            # nodes in pos_graph and neg_graph are same, difference is the connected edges
│ │ +            pos_graph, neg_graph, *target_nodes = build_subgraphs(
│ │ +                items, (items_pos, items_neg), self.paradigm, self.num_neg
│ │ +            )
│ │              # item -> item homogeneous graph, items on all nodes
│ │ -            items = pos_graph.ndata[dgl.NID]
│ │ -            item_reprs = self.model.get_item_repr(items, target_nodes)
│ │ -            pos_graph = pos_graph.to(self.device)
│ │ -            neg_graph = neg_graph.to(self.device)
│ │ -            pos_scores = compute_i2i_edge_scores(pos_graph, item_reprs)
│ │ -            neg_scores = compute_i2i_edge_scores(neg_graph, item_reprs)
│ │ -        if self.loss_type in ("bpr", "max_margin") and self.num_neg > 1:
│ │ -            pos_scores = pos_scores.repeat_interleave(self.num_neg)
│ │ -        return self._get_loss(pos_scores, neg_scores)
│ │ +            all_items = pos_graph.ndata[self.dgl.NID]
│ │ +            item_data = self.neighbor_walker(all_items, target_nodes)
│ │ +            return item_data, pos_graph, neg_graph
│ │ +
│ │ +    def get_start_nodes(self, batch):
│ │ +        size = len(batch["item"])
│ │ +        if self.start_node == "unpopular":
│ │ +            start_nodes = torch.multinomial(self.pos_probs, size, replacement=True)
│ │ +        else:
│ │ +            start_nodes = torch.randint(0, self.n_items, (size,))
│ │ +        return start_nodes
│ │ +
│ │ +    def _set_dgl_seeds(self):
│ │ +        if self.dgl_seed is None:
│ │ +            worker_info = torch.utils.data.get_worker_info()
│ │ +            seed = self.seed if worker_info is None else worker_info.seed
│ │ +            seed = seed % 3407 * 11
│ │ +            self.dgl.seed(seed)
│ │ +            self.dgl_seed = True
│ │ +
│ │ +
│ │ +def repeat_feats(batch_feats, col_index, num_neg, is_pairwise=False):
│ │ +    if not col_index:
│ │ +        return
│ │ +    column_features = batch_feats[:, col_index]
│ │ +    repeats = num_neg if is_pairwise else num_neg + 1
│ │ +    return np.repeat(column_features, repeats, axis=0)
│ │ +
│ │ +
│ │ +def get_sampled_item_feats(collator, item_col_index, items_sampled, feat_type):
│ │ +    if not item_col_index:
│ │ +        return
│ │ +    if feat_type is FeatType.SPARSE:
│ │ +        item_unique_features = collator.item_sparse_unique
│ │ +    elif feat_type is FeatType.DENSE:
│ │ +        item_unique_features = collator.item_dense_unique
│ │ +    else:
│ │ +        raise ValueError("`feat_type` must be sparse or dense.")
│ │ +    return item_unique_features[items_sampled]
│ │ +
│ │ +
│ │ +def merge_columns(user_features, item_features, user_col_index, item_col_index):
│ │ +    if len(user_features) != len(item_features):
│ │ +        raise ValueError(
│ │ +            f"length of user_features and length of item_features don't match, "
│ │ +            f"got {len(user_features)} and {len(item_features)}"
│ │ +        )
│ │ +    # keep column names in original order
│ │ +    orig_cols = user_col_index + item_col_index
│ │ +    col_reindex = np.arange(len(orig_cols))[np.argsort(orig_cols)]
│ │ +    concat_features = np.concatenate([user_features, item_features], axis=1)
│ │ +    return concat_features[:, col_reindex]
│ │   --- LibRecommender-1.0.1/libreco/utils/_similarities.cpp
│ ├── +++ LibRecommender-1.1.0/libreco/utils/_similarities.cpp
│ │┄ Files 1% similar despite different names
│ │ @@ -1,33 +1,33 @@
│ │ -/* Generated by Cython 0.29.33 */
│ │ +/* Generated by Cython 0.29.34 */
│ │  
│ │  /* BEGIN: Cython Metadata
│ │  {
│ │      "distutils": {
│ │          "depends": [
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include/numpy/arrayobject.h",
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include/numpy/arrayscalars.h",
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include/numpy/ndarrayobject.h",
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include/numpy/ndarraytypes.h",
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include/numpy/ufuncobject.h"
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include/numpy/arrayobject.h",
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include/numpy/arrayscalars.h",
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include/numpy/ndarrayobject.h",
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include/numpy/ndarraytypes.h",
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include/numpy/ufuncobject.h"
│ │          ],
│ │          "extra_compile_args": [
│ │              "-Wno-unused-function",
│ │              "-Wno-maybe-uninitialized",
│ │              "-O3",
│ │              "-ffast-math",
│ │              "-fopenmp",
│ │              "-std=c++11"
│ │          ],
│ │          "extra_link_args": [
│ │              "-fopenmp",
│ │              "-std=c++11"
│ │          ],
│ │          "include_dirs": [
│ │ -            "/tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/core/include"
│ │ +            "/tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/core/include"
│ │          ],
│ │          "language": "c++",
│ │          "name": "libreco.utils._similarities",
│ │          "sources": [
│ │              "libreco/utils/_similarities.pyx"
│ │          ]
│ │      },
│ │ @@ -40,16 +40,16 @@
│ │  #endif /* PY_SSIZE_T_CLEAN */
│ │  #include "Python.h"
│ │  #ifndef Py_PYTHON_H
│ │      #error Python headers needed to compile C extensions, please install development version of Python.
│ │  #elif PY_VERSION_HEX < 0x02060000 || (0x03000000 <= PY_VERSION_HEX && PY_VERSION_HEX < 0x03030000)
│ │      #error Cython requires Python 2.6+ or Python 3.3+.
│ │  #else
│ │ -#define CYTHON_ABI "0_29_33"
│ │ -#define CYTHON_HEX_VERSION 0x001D21F0
│ │ +#define CYTHON_ABI "0_29_34"
│ │ +#define CYTHON_HEX_VERSION 0x001D22F0
│ │  #define CYTHON_FUTURE_DIVISION 1
│ │  #include <stddef.h>
│ │  #ifndef offsetof
│ │    #define offsetof(type, member) ( (size_t) & ((type*)0) -> member )
│ │  #endif
│ │  #if !defined(WIN32) && !defined(MS_WINDOWS)
│ │    #ifndef __stdcall
│ │ @@ -234,15 +234,15 @@
│ │    #elif !defined(CYTHON_USE_ASYNC_SLOTS)
│ │      #define CYTHON_USE_ASYNC_SLOTS 1
│ │    #endif
│ │    #if PY_VERSION_HEX < 0x02070000
│ │      #undef CYTHON_USE_PYLONG_INTERNALS
│ │      #define CYTHON_USE_PYLONG_INTERNALS 0
│ │    #elif !defined(CYTHON_USE_PYLONG_INTERNALS)
│ │ -    #define CYTHON_USE_PYLONG_INTERNALS 1
│ │ +    #define CYTHON_USE_PYLONG_INTERNALS (PY_VERSION_HEX < 0x030C00A5)
│ │    #endif
│ │    #ifndef CYTHON_USE_PYLIST_INTERNALS
│ │      #define CYTHON_USE_PYLIST_INTERNALS 1
│ │    #endif
│ │    #ifndef CYTHON_USE_UNICODE_INTERNALS
│ │      #define CYTHON_USE_UNICODE_INTERNALS 1
│ │    #endif
│ │ @@ -273,15 +273,15 @@
│ │    #ifndef CYTHON_PEP489_MULTI_PHASE_INIT
│ │      #define CYTHON_PEP489_MULTI_PHASE_INIT (PY_VERSION_HEX >= 0x03050000)
│ │    #endif
│ │    #ifndef CYTHON_USE_TP_FINALIZE
│ │      #define CYTHON_USE_TP_FINALIZE (PY_VERSION_HEX >= 0x030400a1)
│ │    #endif
│ │    #ifndef CYTHON_USE_DICT_VERSIONS
│ │ -    #define CYTHON_USE_DICT_VERSIONS (PY_VERSION_HEX >= 0x030600B1)
│ │ +    #define CYTHON_USE_DICT_VERSIONS ((PY_VERSION_HEX >= 0x030600B1) && (PY_VERSION_HEX < 0x030C00A5))
│ │    #endif
│ │    #if PY_VERSION_HEX >= 0x030B00A4
│ │      #undef CYTHON_USE_EXC_INFO_STACK
│ │      #define CYTHON_USE_EXC_INFO_STACK 0
│ │    #elif !defined(CYTHON_USE_EXC_INFO_STACK)
│ │      #define CYTHON_USE_EXC_INFO_STACK (PY_VERSION_HEX >= 0x030700A3)
│ │    #endif
│ │ @@ -1146,195 +1146,195 @@
│ │    char enc_type;
│ │    char new_packmode;
│ │    char enc_packmode;
│ │    char is_valid_array;
│ │  } __Pyx_BufFmt_Context;
│ │  
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":689
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":689
│ │   * # in Cython to enable them only on the right systems.
│ │   * 
│ │   * ctypedef npy_int8       int8_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_int16      int16_t
│ │   * ctypedef npy_int32      int32_t
│ │   */
│ │  typedef npy_int8 __pyx_t_5numpy_int8_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":690
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":690
│ │   * 
│ │   * ctypedef npy_int8       int8_t
│ │   * ctypedef npy_int16      int16_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_int32      int32_t
│ │   * ctypedef npy_int64      int64_t
│ │   */
│ │  typedef npy_int16 __pyx_t_5numpy_int16_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":691
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":691
│ │   * ctypedef npy_int8       int8_t
│ │   * ctypedef npy_int16      int16_t
│ │   * ctypedef npy_int32      int32_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_int64      int64_t
│ │   * #ctypedef npy_int96      int96_t
│ │   */
│ │  typedef npy_int32 __pyx_t_5numpy_int32_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":692
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":692
│ │   * ctypedef npy_int16      int16_t
│ │   * ctypedef npy_int32      int32_t
│ │   * ctypedef npy_int64      int64_t             # <<<<<<<<<<<<<<
│ │   * #ctypedef npy_int96      int96_t
│ │   * #ctypedef npy_int128     int128_t
│ │   */
│ │  typedef npy_int64 __pyx_t_5numpy_int64_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":696
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":696
│ │   * #ctypedef npy_int128     int128_t
│ │   * 
│ │   * ctypedef npy_uint8      uint8_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_uint16     uint16_t
│ │   * ctypedef npy_uint32     uint32_t
│ │   */
│ │  typedef npy_uint8 __pyx_t_5numpy_uint8_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":697
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":697
│ │   * 
│ │   * ctypedef npy_uint8      uint8_t
│ │   * ctypedef npy_uint16     uint16_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_uint32     uint32_t
│ │   * ctypedef npy_uint64     uint64_t
│ │   */
│ │  typedef npy_uint16 __pyx_t_5numpy_uint16_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":698
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":698
│ │   * ctypedef npy_uint8      uint8_t
│ │   * ctypedef npy_uint16     uint16_t
│ │   * ctypedef npy_uint32     uint32_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_uint64     uint64_t
│ │   * #ctypedef npy_uint96     uint96_t
│ │   */
│ │  typedef npy_uint32 __pyx_t_5numpy_uint32_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":699
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":699
│ │   * ctypedef npy_uint16     uint16_t
│ │   * ctypedef npy_uint32     uint32_t
│ │   * ctypedef npy_uint64     uint64_t             # <<<<<<<<<<<<<<
│ │   * #ctypedef npy_uint96     uint96_t
│ │   * #ctypedef npy_uint128    uint128_t
│ │   */
│ │  typedef npy_uint64 __pyx_t_5numpy_uint64_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":703
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":703
│ │   * #ctypedef npy_uint128    uint128_t
│ │   * 
│ │   * ctypedef npy_float32    float32_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_float64    float64_t
│ │   * #ctypedef npy_float80    float80_t
│ │   */
│ │  typedef npy_float32 __pyx_t_5numpy_float32_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":704
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":704
│ │   * 
│ │   * ctypedef npy_float32    float32_t
│ │   * ctypedef npy_float64    float64_t             # <<<<<<<<<<<<<<
│ │   * #ctypedef npy_float80    float80_t
│ │   * #ctypedef npy_float128   float128_t
│ │   */
│ │  typedef npy_float64 __pyx_t_5numpy_float64_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":713
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":713
│ │   * # The int types are mapped a bit surprising --
│ │   * # numpy.int corresponds to 'l' and numpy.long to 'q'
│ │   * ctypedef npy_long       int_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_longlong   long_t
│ │   * ctypedef npy_longlong   longlong_t
│ │   */
│ │  typedef npy_long __pyx_t_5numpy_int_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":714
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":714
│ │   * # numpy.int corresponds to 'l' and numpy.long to 'q'
│ │   * ctypedef npy_long       int_t
│ │   * ctypedef npy_longlong   long_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_longlong   longlong_t
│ │   * 
│ │   */
│ │  typedef npy_longlong __pyx_t_5numpy_long_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":715
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":715
│ │   * ctypedef npy_long       int_t
│ │   * ctypedef npy_longlong   long_t
│ │   * ctypedef npy_longlong   longlong_t             # <<<<<<<<<<<<<<
│ │   * 
│ │   * ctypedef npy_ulong      uint_t
│ │   */
│ │  typedef npy_longlong __pyx_t_5numpy_longlong_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":717
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":717
│ │   * ctypedef npy_longlong   longlong_t
│ │   * 
│ │   * ctypedef npy_ulong      uint_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_ulonglong  ulong_t
│ │   * ctypedef npy_ulonglong  ulonglong_t
│ │   */
│ │  typedef npy_ulong __pyx_t_5numpy_uint_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":718
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":718
│ │   * 
│ │   * ctypedef npy_ulong      uint_t
│ │   * ctypedef npy_ulonglong  ulong_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_ulonglong  ulonglong_t
│ │   * 
│ │   */
│ │  typedef npy_ulonglong __pyx_t_5numpy_ulong_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":719
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":719
│ │   * ctypedef npy_ulong      uint_t
│ │   * ctypedef npy_ulonglong  ulong_t
│ │   * ctypedef npy_ulonglong  ulonglong_t             # <<<<<<<<<<<<<<
│ │   * 
│ │   * ctypedef npy_intp       intp_t
│ │   */
│ │  typedef npy_ulonglong __pyx_t_5numpy_ulonglong_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":721
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":721
│ │   * ctypedef npy_ulonglong  ulonglong_t
│ │   * 
│ │   * ctypedef npy_intp       intp_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_uintp      uintp_t
│ │   * 
│ │   */
│ │  typedef npy_intp __pyx_t_5numpy_intp_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":722
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":722
│ │   * 
│ │   * ctypedef npy_intp       intp_t
│ │   * ctypedef npy_uintp      uintp_t             # <<<<<<<<<<<<<<
│ │   * 
│ │   * ctypedef npy_double     float_t
│ │   */
│ │  typedef npy_uintp __pyx_t_5numpy_uintp_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":724
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":724
│ │   * ctypedef npy_uintp      uintp_t
│ │   * 
│ │   * ctypedef npy_double     float_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_double     double_t
│ │   * ctypedef npy_longdouble longdouble_t
│ │   */
│ │  typedef npy_double __pyx_t_5numpy_float_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":725
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":725
│ │   * 
│ │   * ctypedef npy_double     float_t
│ │   * ctypedef npy_double     double_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_longdouble longdouble_t
│ │   * 
│ │   */
│ │  typedef npy_double __pyx_t_5numpy_double_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":726
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":726
│ │   * ctypedef npy_double     float_t
│ │   * ctypedef npy_double     double_t
│ │   * ctypedef npy_longdouble longdouble_t             # <<<<<<<<<<<<<<
│ │   * 
│ │   * ctypedef npy_cfloat      cfloat_t
│ │   */
│ │  typedef npy_longdouble __pyx_t_5numpy_longdouble_t;
│ │ @@ -1374,42 +1374,42 @@
│ │  
│ │  /*--- Type declarations ---*/
│ │  struct __pyx_array_obj;
│ │  struct __pyx_MemviewEnum_obj;
│ │  struct __pyx_memoryview_obj;
│ │  struct __pyx_memoryviewslice_obj;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":728
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":728
│ │   * ctypedef npy_longdouble longdouble_t
│ │   * 
│ │   * ctypedef npy_cfloat      cfloat_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_cdouble     cdouble_t
│ │   * ctypedef npy_clongdouble clongdouble_t
│ │   */
│ │  typedef npy_cfloat __pyx_t_5numpy_cfloat_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":729
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":729
│ │   * 
│ │   * ctypedef npy_cfloat      cfloat_t
│ │   * ctypedef npy_cdouble     cdouble_t             # <<<<<<<<<<<<<<
│ │   * ctypedef npy_clongdouble clongdouble_t
│ │   * 
│ │   */
│ │  typedef npy_cdouble __pyx_t_5numpy_cdouble_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":730
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":730
│ │   * ctypedef npy_cfloat      cfloat_t
│ │   * ctypedef npy_cdouble     cdouble_t
│ │   * ctypedef npy_clongdouble clongdouble_t             # <<<<<<<<<<<<<<
│ │   * 
│ │   * ctypedef npy_cdouble     complex_t
│ │   */
│ │  typedef npy_clongdouble __pyx_t_5numpy_clongdouble_t;
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":732
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":732
│ │   * ctypedef npy_clongdouble clongdouble_t
│ │   * 
│ │   * ctypedef npy_cdouble     complex_t             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew1(a):
│ │   */
│ │  typedef npy_cdouble __pyx_t_5numpy_complex_t;
│ │ @@ -2082,20 +2082,28 @@
│ │  
│ │  /* SetupReduce.proto */
│ │  static int __Pyx_setup_reduce(PyObject* type_obj);
│ │  
│ │  /* TypeImport.proto */
│ │  #ifndef __PYX_HAVE_RT_ImportType_proto
│ │  #define __PYX_HAVE_RT_ImportType_proto
│ │ +#if __STDC_VERSION__ >= 201112L
│ │ +#include <stdalign.h>
│ │ +#endif
│ │ +#if __STDC_VERSION__ >= 201112L || __cplusplus >= 201103L
│ │ +#define __PYX_GET_STRUCT_ALIGNMENT(s) alignof(s)
│ │ +#else
│ │ +#define __PYX_GET_STRUCT_ALIGNMENT(s) sizeof(void*)
│ │ +#endif
│ │  enum __Pyx_ImportType_CheckSize {
│ │     __Pyx_ImportType_CheckSize_Error = 0,
│ │     __Pyx_ImportType_CheckSize_Warn = 1,
│ │     __Pyx_ImportType_CheckSize_Ignore = 2
│ │  };
│ │ -static PyTypeObject *__Pyx_ImportType(PyObject* module, const char *module_name, const char *class_name, size_t size, enum __Pyx_ImportType_CheckSize check_size);
│ │ +static PyTypeObject *__Pyx_ImportType(PyObject* module, const char *module_name, const char *class_name, size_t size, size_t alignment, enum __Pyx_ImportType_CheckSize check_size);
│ │  #endif
│ │  
│ │  /* CLineInTraceback.proto */
│ │  #ifdef CYTHON_CLINE_IN_TRACEBACK
│ │  #define __Pyx_CLineForTraceback(tstate, c_line)  (((CYTHON_CLINE_IN_TRACEBACK)) ? c_line : 0)
│ │  #else
│ │  static int __Pyx_CLineForTraceback(PyThreadState *tstate, int c_line);
│ │ @@ -8091,15 +8099,15 @@
│ │    __PYX_XDEC_MEMVIEW(&__pyx_v_data, 1);
│ │    __PYX_XDEC_MEMVIEW(&__pyx_v_x_count, 1);
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":734
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":734
│ │   * ctypedef npy_cdouble     complex_t
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew1(a):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(1, <void*>a)
│ │   * 
│ │   */
│ │  
│ │ @@ -8108,29 +8116,29 @@
│ │    __Pyx_RefNannyDeclarations
│ │    PyObject *__pyx_t_1 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("PyArray_MultiIterNew1", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":735
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":735
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew1(a):
│ │   *     return PyArray_MultiIterNew(1, <void*>a)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew2(a, b):
│ │   */
│ │    __Pyx_XDECREF(__pyx_r);
│ │    __pyx_t_1 = PyArray_MultiIterNew(1, ((void *)__pyx_v_a)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 735, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │    __pyx_r = __pyx_t_1;
│ │    __pyx_t_1 = 0;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":734
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":734
│ │   * ctypedef npy_cdouble     complex_t
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew1(a):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(1, <void*>a)
│ │   * 
│ │   */
│ │  
│ │ @@ -8141,15 +8149,15 @@
│ │    __pyx_r = 0;
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":737
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":737
│ │   *     return PyArray_MultiIterNew(1, <void*>a)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew2(a, b):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
│ │   * 
│ │   */
│ │  
│ │ @@ -8158,29 +8166,29 @@
│ │    __Pyx_RefNannyDeclarations
│ │    PyObject *__pyx_t_1 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("PyArray_MultiIterNew2", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":738
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":738
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew2(a, b):
│ │   *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew3(a, b, c):
│ │   */
│ │    __Pyx_XDECREF(__pyx_r);
│ │    __pyx_t_1 = PyArray_MultiIterNew(2, ((void *)__pyx_v_a), ((void *)__pyx_v_b)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 738, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │    __pyx_r = __pyx_t_1;
│ │    __pyx_t_1 = 0;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":737
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":737
│ │   *     return PyArray_MultiIterNew(1, <void*>a)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew2(a, b):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
│ │   * 
│ │   */
│ │  
│ │ @@ -8191,15 +8199,15 @@
│ │    __pyx_r = 0;
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":740
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":740
│ │   *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew3(a, b, c):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
│ │   * 
│ │   */
│ │  
│ │ @@ -8208,29 +8216,29 @@
│ │    __Pyx_RefNannyDeclarations
│ │    PyObject *__pyx_t_1 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("PyArray_MultiIterNew3", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":741
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":741
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew3(a, b, c):
│ │   *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew4(a, b, c, d):
│ │   */
│ │    __Pyx_XDECREF(__pyx_r);
│ │    __pyx_t_1 = PyArray_MultiIterNew(3, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 741, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │    __pyx_r = __pyx_t_1;
│ │    __pyx_t_1 = 0;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":740
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":740
│ │   *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew3(a, b, c):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
│ │   * 
│ │   */
│ │  
│ │ @@ -8241,15 +8249,15 @@
│ │    __pyx_r = 0;
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":743
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":743
│ │   *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew4(a, b, c, d):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
│ │   * 
│ │   */
│ │  
│ │ @@ -8258,29 +8266,29 @@
│ │    __Pyx_RefNannyDeclarations
│ │    PyObject *__pyx_t_1 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("PyArray_MultiIterNew4", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":744
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":744
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew4(a, b, c, d):
│ │   *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):
│ │   */
│ │    __Pyx_XDECREF(__pyx_r);
│ │    __pyx_t_1 = PyArray_MultiIterNew(4, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c), ((void *)__pyx_v_d)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 744, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │    __pyx_r = __pyx_t_1;
│ │    __pyx_t_1 = 0;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":743
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":743
│ │   *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew4(a, b, c, d):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
│ │   * 
│ │   */
│ │  
│ │ @@ -8291,15 +8299,15 @@
│ │    __pyx_r = 0;
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":746
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":746
│ │   *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
│ │   * 
│ │   */
│ │  
│ │ @@ -8308,29 +8316,29 @@
│ │    __Pyx_RefNannyDeclarations
│ │    PyObject *__pyx_t_1 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("PyArray_MultiIterNew5", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":747
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":747
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):
│ │   *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline tuple PyDataType_SHAPE(dtype d):
│ │   */
│ │    __Pyx_XDECREF(__pyx_r);
│ │    __pyx_t_1 = PyArray_MultiIterNew(5, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c), ((void *)__pyx_v_d), ((void *)__pyx_v_e)); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 747, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │    __pyx_r = __pyx_t_1;
│ │    __pyx_t_1 = 0;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":746
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":746
│ │   *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
│ │   * 
│ │   * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):             # <<<<<<<<<<<<<<
│ │   *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
│ │   * 
│ │   */
│ │  
│ │ @@ -8341,212 +8349,212 @@
│ │    __pyx_r = 0;
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":749
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":749
│ │   *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
│ │   * 
│ │   * cdef inline tuple PyDataType_SHAPE(dtype d):             # <<<<<<<<<<<<<<
│ │   *     if PyDataType_HASSUBARRAY(d):
│ │   *         return <tuple>d.subarray.shape
│ │   */
│ │  
│ │  static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyDataType_SHAPE(PyArray_Descr *__pyx_v_d) {
│ │    PyObject *__pyx_r = NULL;
│ │    __Pyx_RefNannyDeclarations
│ │    int __pyx_t_1;
│ │    __Pyx_RefNannySetupContext("PyDataType_SHAPE", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":750
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":750
│ │   * 
│ │   * cdef inline tuple PyDataType_SHAPE(dtype d):
│ │   *     if PyDataType_HASSUBARRAY(d):             # <<<<<<<<<<<<<<
│ │   *         return <tuple>d.subarray.shape
│ │   *     else:
│ │   */
│ │    __pyx_t_1 = (PyDataType_HASSUBARRAY(__pyx_v_d) != 0);
│ │    if (__pyx_t_1) {
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":751
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":751
│ │   * cdef inline tuple PyDataType_SHAPE(dtype d):
│ │   *     if PyDataType_HASSUBARRAY(d):
│ │   *         return <tuple>d.subarray.shape             # <<<<<<<<<<<<<<
│ │   *     else:
│ │   *         return ()
│ │   */
│ │      __Pyx_XDECREF(__pyx_r);
│ │      __Pyx_INCREF(((PyObject*)__pyx_v_d->subarray->shape));
│ │      __pyx_r = ((PyObject*)__pyx_v_d->subarray->shape);
│ │      goto __pyx_L0;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":750
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":750
│ │   * 
│ │   * cdef inline tuple PyDataType_SHAPE(dtype d):
│ │   *     if PyDataType_HASSUBARRAY(d):             # <<<<<<<<<<<<<<
│ │   *         return <tuple>d.subarray.shape
│ │   *     else:
│ │   */
│ │    }
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":753
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":753
│ │   *         return <tuple>d.subarray.shape
│ │   *     else:
│ │   *         return ()             # <<<<<<<<<<<<<<
│ │   * 
│ │   * 
│ │   */
│ │    /*else*/ {
│ │      __Pyx_XDECREF(__pyx_r);
│ │      __Pyx_INCREF(__pyx_empty_tuple);
│ │      __pyx_r = __pyx_empty_tuple;
│ │      goto __pyx_L0;
│ │    }
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":749
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":749
│ │   *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
│ │   * 
│ │   * cdef inline tuple PyDataType_SHAPE(dtype d):             # <<<<<<<<<<<<<<
│ │   *     if PyDataType_HASSUBARRAY(d):
│ │   *         return <tuple>d.subarray.shape
│ │   */
│ │  
│ │    /* function exit code */
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":928
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":928
│ │   *     int _import_umath() except -1
│ │   * 
│ │   * cdef inline void set_array_base(ndarray arr, object base):             # <<<<<<<<<<<<<<
│ │   *     Py_INCREF(base) # important to do this before stealing the reference below!
│ │   *     PyArray_SetBaseObject(arr, base)
│ │   */
│ │  
│ │  static CYTHON_INLINE void __pyx_f_5numpy_set_array_base(PyArrayObject *__pyx_v_arr, PyObject *__pyx_v_base) {
│ │    __Pyx_RefNannyDeclarations
│ │    __Pyx_RefNannySetupContext("set_array_base", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":929
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":929
│ │   * 
│ │   * cdef inline void set_array_base(ndarray arr, object base):
│ │   *     Py_INCREF(base) # important to do this before stealing the reference below!             # <<<<<<<<<<<<<<
│ │   *     PyArray_SetBaseObject(arr, base)
│ │   * 
│ │   */
│ │    Py_INCREF(__pyx_v_base);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":930
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":930
│ │   * cdef inline void set_array_base(ndarray arr, object base):
│ │   *     Py_INCREF(base) # important to do this before stealing the reference below!
│ │   *     PyArray_SetBaseObject(arr, base)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline object get_array_base(ndarray arr):
│ │   */
│ │    (void)(PyArray_SetBaseObject(__pyx_v_arr, __pyx_v_base));
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":928
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":928
│ │   *     int _import_umath() except -1
│ │   * 
│ │   * cdef inline void set_array_base(ndarray arr, object base):             # <<<<<<<<<<<<<<
│ │   *     Py_INCREF(base) # important to do this before stealing the reference below!
│ │   *     PyArray_SetBaseObject(arr, base)
│ │   */
│ │  
│ │    /* function exit code */
│ │    __Pyx_RefNannyFinishContext();
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":932
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":932
│ │   *     PyArray_SetBaseObject(arr, base)
│ │   * 
│ │   * cdef inline object get_array_base(ndarray arr):             # <<<<<<<<<<<<<<
│ │   *     base = PyArray_BASE(arr)
│ │   *     if base is NULL:
│ │   */
│ │  
│ │  static CYTHON_INLINE PyObject *__pyx_f_5numpy_get_array_base(PyArrayObject *__pyx_v_arr) {
│ │    PyObject *__pyx_v_base;
│ │    PyObject *__pyx_r = NULL;
│ │    __Pyx_RefNannyDeclarations
│ │    int __pyx_t_1;
│ │    __Pyx_RefNannySetupContext("get_array_base", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":933
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":933
│ │   * 
│ │   * cdef inline object get_array_base(ndarray arr):
│ │   *     base = PyArray_BASE(arr)             # <<<<<<<<<<<<<<
│ │   *     if base is NULL:
│ │   *         return None
│ │   */
│ │    __pyx_v_base = PyArray_BASE(__pyx_v_arr);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":934
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":934
│ │   * cdef inline object get_array_base(ndarray arr):
│ │   *     base = PyArray_BASE(arr)
│ │   *     if base is NULL:             # <<<<<<<<<<<<<<
│ │   *         return None
│ │   *     return <object>base
│ │   */
│ │    __pyx_t_1 = ((__pyx_v_base == NULL) != 0);
│ │    if (__pyx_t_1) {
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":935
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":935
│ │   *     base = PyArray_BASE(arr)
│ │   *     if base is NULL:
│ │   *         return None             # <<<<<<<<<<<<<<
│ │   *     return <object>base
│ │   * 
│ │   */
│ │      __Pyx_XDECREF(__pyx_r);
│ │      __pyx_r = Py_None; __Pyx_INCREF(Py_None);
│ │      goto __pyx_L0;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":934
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":934
│ │   * cdef inline object get_array_base(ndarray arr):
│ │   *     base = PyArray_BASE(arr)
│ │   *     if base is NULL:             # <<<<<<<<<<<<<<
│ │   *         return None
│ │   *     return <object>base
│ │   */
│ │    }
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":936
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":936
│ │   *     if base is NULL:
│ │   *         return None
│ │   *     return <object>base             # <<<<<<<<<<<<<<
│ │   * 
│ │   * # Versions of the import_* functions which are more suitable for
│ │   */
│ │    __Pyx_XDECREF(__pyx_r);
│ │    __Pyx_INCREF(((PyObject *)__pyx_v_base));
│ │    __pyx_r = ((PyObject *)__pyx_v_base);
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":932
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":932
│ │   *     PyArray_SetBaseObject(arr, base)
│ │   * 
│ │   * cdef inline object get_array_base(ndarray arr):             # <<<<<<<<<<<<<<
│ │   *     base = PyArray_BASE(arr)
│ │   *     if base is NULL:
│ │   */
│ │  
│ │    /* function exit code */
│ │    __pyx_L0:;
│ │    __Pyx_XGIVEREF(__pyx_r);
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":940
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":940
│ │   * # Versions of the import_* functions which are more suitable for
│ │   * # Cython code.
│ │   * cdef inline int import_array() except -1:             # <<<<<<<<<<<<<<
│ │   *     try:
│ │   *         __pyx_import_array()
│ │   */
│ │  
│ │ @@ -8562,15 +8570,15 @@
│ │    PyObject *__pyx_t_7 = NULL;
│ │    PyObject *__pyx_t_8 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("import_array", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":941
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":941
│ │   * # Cython code.
│ │   * cdef inline int import_array() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         __pyx_import_array()
│ │   *     except Exception:
│ │   */
│ │    {
│ │ @@ -8578,53 +8586,53 @@
│ │      __Pyx_PyThreadState_assign
│ │      __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
│ │      __Pyx_XGOTREF(__pyx_t_1);
│ │      __Pyx_XGOTREF(__pyx_t_2);
│ │      __Pyx_XGOTREF(__pyx_t_3);
│ │      /*try:*/ {
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":942
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":942
│ │   * cdef inline int import_array() except -1:
│ │   *     try:
│ │   *         __pyx_import_array()             # <<<<<<<<<<<<<<
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.multiarray failed to import")
│ │   */
│ │        __pyx_t_4 = _import_array(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(1, 942, __pyx_L3_error)
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":941
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":941
│ │   * # Cython code.
│ │   * cdef inline int import_array() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         __pyx_import_array()
│ │   *     except Exception:
│ │   */
│ │      }
│ │      __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
│ │      __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
│ │      __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
│ │      goto __pyx_L8_try_end;
│ │      __pyx_L3_error:;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":943
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":943
│ │   *     try:
│ │   *         __pyx_import_array()
│ │   *     except Exception:             # <<<<<<<<<<<<<<
│ │   *         raise ImportError("numpy.core.multiarray failed to import")
│ │   * 
│ │   */
│ │      __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
│ │      if (__pyx_t_4) {
│ │        __Pyx_AddTraceback("numpy.import_array", __pyx_clineno, __pyx_lineno, __pyx_filename);
│ │        if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(1, 943, __pyx_L5_except_error)
│ │        __Pyx_GOTREF(__pyx_t_5);
│ │        __Pyx_GOTREF(__pyx_t_6);
│ │        __Pyx_GOTREF(__pyx_t_7);
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":944
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":944
│ │   *         __pyx_import_array()
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.multiarray failed to import")             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline int import_umath() except -1:
│ │   */
│ │        __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple_, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(1, 944, __pyx_L5_except_error)
│ │ @@ -8632,30 +8640,30 @@
│ │        __Pyx_Raise(__pyx_t_8, 0, 0, 0);
│ │        __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
│ │        __PYX_ERR(1, 944, __pyx_L5_except_error)
│ │      }
│ │      goto __pyx_L5_except_error;
│ │      __pyx_L5_except_error:;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":941
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":941
│ │   * # Cython code.
│ │   * cdef inline int import_array() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         __pyx_import_array()
│ │   *     except Exception:
│ │   */
│ │      __Pyx_XGIVEREF(__pyx_t_1);
│ │      __Pyx_XGIVEREF(__pyx_t_2);
│ │      __Pyx_XGIVEREF(__pyx_t_3);
│ │      __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
│ │      goto __pyx_L1_error;
│ │      __pyx_L8_try_end:;
│ │    }
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":940
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":940
│ │   * # Versions of the import_* functions which are more suitable for
│ │   * # Cython code.
│ │   * cdef inline int import_array() except -1:             # <<<<<<<<<<<<<<
│ │   *     try:
│ │   *         __pyx_import_array()
│ │   */
│ │  
│ │ @@ -8670,15 +8678,15 @@
│ │    __Pyx_AddTraceback("numpy.import_array", __pyx_clineno, __pyx_lineno, __pyx_filename);
│ │    __pyx_r = -1;
│ │    __pyx_L0:;
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":946
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":946
│ │   *         raise ImportError("numpy.core.multiarray failed to import")
│ │   * 
│ │   * cdef inline int import_umath() except -1:             # <<<<<<<<<<<<<<
│ │   *     try:
│ │   *         _import_umath()
│ │   */
│ │  
│ │ @@ -8694,15 +8702,15 @@
│ │    PyObject *__pyx_t_7 = NULL;
│ │    PyObject *__pyx_t_8 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("import_umath", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":947
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":947
│ │   * 
│ │   * cdef inline int import_umath() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   */
│ │    {
│ │ @@ -8710,53 +8718,53 @@
│ │      __Pyx_PyThreadState_assign
│ │      __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
│ │      __Pyx_XGOTREF(__pyx_t_1);
│ │      __Pyx_XGOTREF(__pyx_t_2);
│ │      __Pyx_XGOTREF(__pyx_t_3);
│ │      /*try:*/ {
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":948
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":948
│ │   * cdef inline int import_umath() except -1:
│ │   *     try:
│ │   *         _import_umath()             # <<<<<<<<<<<<<<
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.umath failed to import")
│ │   */
│ │        __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(1, 948, __pyx_L3_error)
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":947
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":947
│ │   * 
│ │   * cdef inline int import_umath() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   */
│ │      }
│ │      __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
│ │      __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
│ │      __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
│ │      goto __pyx_L8_try_end;
│ │      __pyx_L3_error:;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":949
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":949
│ │   *     try:
│ │   *         _import_umath()
│ │   *     except Exception:             # <<<<<<<<<<<<<<
│ │   *         raise ImportError("numpy.core.umath failed to import")
│ │   * 
│ │   */
│ │      __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
│ │      if (__pyx_t_4) {
│ │        __Pyx_AddTraceback("numpy.import_umath", __pyx_clineno, __pyx_lineno, __pyx_filename);
│ │        if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(1, 949, __pyx_L5_except_error)
│ │        __Pyx_GOTREF(__pyx_t_5);
│ │        __Pyx_GOTREF(__pyx_t_6);
│ │        __Pyx_GOTREF(__pyx_t_7);
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":950
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":950
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:
│ │   */
│ │        __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__2, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(1, 950, __pyx_L5_except_error)
│ │ @@ -8764,30 +8772,30 @@
│ │        __Pyx_Raise(__pyx_t_8, 0, 0, 0);
│ │        __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
│ │        __PYX_ERR(1, 950, __pyx_L5_except_error)
│ │      }
│ │      goto __pyx_L5_except_error;
│ │      __pyx_L5_except_error:;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":947
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":947
│ │   * 
│ │   * cdef inline int import_umath() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   */
│ │      __Pyx_XGIVEREF(__pyx_t_1);
│ │      __Pyx_XGIVEREF(__pyx_t_2);
│ │      __Pyx_XGIVEREF(__pyx_t_3);
│ │      __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
│ │      goto __pyx_L1_error;
│ │      __pyx_L8_try_end:;
│ │    }
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":946
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":946
│ │   *         raise ImportError("numpy.core.multiarray failed to import")
│ │   * 
│ │   * cdef inline int import_umath() except -1:             # <<<<<<<<<<<<<<
│ │   *     try:
│ │   *         _import_umath()
│ │   */
│ │  
│ │ @@ -8802,15 +8810,15 @@
│ │    __Pyx_AddTraceback("numpy.import_umath", __pyx_clineno, __pyx_lineno, __pyx_filename);
│ │    __pyx_r = -1;
│ │    __pyx_L0:;
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":952
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":952
│ │   *         raise ImportError("numpy.core.umath failed to import")
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:             # <<<<<<<<<<<<<<
│ │   *     try:
│ │   *         _import_umath()
│ │   */
│ │  
│ │ @@ -8826,15 +8834,15 @@
│ │    PyObject *__pyx_t_7 = NULL;
│ │    PyObject *__pyx_t_8 = NULL;
│ │    int __pyx_lineno = 0;
│ │    const char *__pyx_filename = NULL;
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("import_ufunc", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":953
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":953
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   */
│ │    {
│ │ @@ -8842,53 +8850,53 @@
│ │      __Pyx_PyThreadState_assign
│ │      __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
│ │      __Pyx_XGOTREF(__pyx_t_1);
│ │      __Pyx_XGOTREF(__pyx_t_2);
│ │      __Pyx_XGOTREF(__pyx_t_3);
│ │      /*try:*/ {
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":954
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":954
│ │   * cdef inline int import_ufunc() except -1:
│ │   *     try:
│ │   *         _import_umath()             # <<<<<<<<<<<<<<
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.umath failed to import")
│ │   */
│ │        __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(1, 954, __pyx_L3_error)
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":953
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":953
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   */
│ │      }
│ │      __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
│ │      __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
│ │      __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
│ │      goto __pyx_L8_try_end;
│ │      __pyx_L3_error:;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":955
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":955
│ │   *     try:
│ │   *         _import_umath()
│ │   *     except Exception:             # <<<<<<<<<<<<<<
│ │   *         raise ImportError("numpy.core.umath failed to import")
│ │   * 
│ │   */
│ │      __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
│ │      if (__pyx_t_4) {
│ │        __Pyx_AddTraceback("numpy.import_ufunc", __pyx_clineno, __pyx_lineno, __pyx_filename);
│ │        if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(1, 955, __pyx_L5_except_error)
│ │        __Pyx_GOTREF(__pyx_t_5);
│ │        __Pyx_GOTREF(__pyx_t_6);
│ │        __Pyx_GOTREF(__pyx_t_7);
│ │  
│ │ -      /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":956
│ │ +      /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":956
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef extern from *:
│ │   */
│ │        __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__2, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(1, 956, __pyx_L5_except_error)
│ │ @@ -8896,30 +8904,30 @@
│ │        __Pyx_Raise(__pyx_t_8, 0, 0, 0);
│ │        __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
│ │        __PYX_ERR(1, 956, __pyx_L5_except_error)
│ │      }
│ │      goto __pyx_L5_except_error;
│ │      __pyx_L5_except_error:;
│ │  
│ │ -    /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":953
│ │ +    /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":953
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:
│ │   *     try:             # <<<<<<<<<<<<<<
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   */
│ │      __Pyx_XGIVEREF(__pyx_t_1);
│ │      __Pyx_XGIVEREF(__pyx_t_2);
│ │      __Pyx_XGIVEREF(__pyx_t_3);
│ │      __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
│ │      goto __pyx_L1_error;
│ │      __pyx_L8_try_end:;
│ │    }
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":952
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":952
│ │   *         raise ImportError("numpy.core.umath failed to import")
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:             # <<<<<<<<<<<<<<
│ │   *     try:
│ │   *         _import_umath()
│ │   */
│ │  
│ │ @@ -8934,176 +8942,176 @@
│ │    __Pyx_AddTraceback("numpy.import_ufunc", __pyx_clineno, __pyx_lineno, __pyx_filename);
│ │    __pyx_r = -1;
│ │    __pyx_L0:;
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":966
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":966
│ │   * 
│ │   * 
│ │   * cdef inline bint is_timedelta64_object(object obj):             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     Cython equivalent of `isinstance(obj, np.timedelta64)`
│ │   */
│ │  
│ │  static CYTHON_INLINE int __pyx_f_5numpy_is_timedelta64_object(PyObject *__pyx_v_obj) {
│ │    int __pyx_r;
│ │    __Pyx_RefNannyDeclarations
│ │    __Pyx_RefNannySetupContext("is_timedelta64_object", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":978
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":978
│ │   *     bool
│ │   *     """
│ │   *     return PyObject_TypeCheck(obj, &PyTimedeltaArrType_Type)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * 
│ │   */
│ │    __pyx_r = PyObject_TypeCheck(__pyx_v_obj, (&PyTimedeltaArrType_Type));
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":966
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":966
│ │   * 
│ │   * 
│ │   * cdef inline bint is_timedelta64_object(object obj):             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     Cython equivalent of `isinstance(obj, np.timedelta64)`
│ │   */
│ │  
│ │    /* function exit code */
│ │    __pyx_L0:;
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":981
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":981
│ │   * 
│ │   * 
│ │   * cdef inline bint is_datetime64_object(object obj):             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     Cython equivalent of `isinstance(obj, np.datetime64)`
│ │   */
│ │  
│ │  static CYTHON_INLINE int __pyx_f_5numpy_is_datetime64_object(PyObject *__pyx_v_obj) {
│ │    int __pyx_r;
│ │    __Pyx_RefNannyDeclarations
│ │    __Pyx_RefNannySetupContext("is_datetime64_object", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":993
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":993
│ │   *     bool
│ │   *     """
│ │   *     return PyObject_TypeCheck(obj, &PyDatetimeArrType_Type)             # <<<<<<<<<<<<<<
│ │   * 
│ │   * 
│ │   */
│ │    __pyx_r = PyObject_TypeCheck(__pyx_v_obj, (&PyDatetimeArrType_Type));
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":981
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":981
│ │   * 
│ │   * 
│ │   * cdef inline bint is_datetime64_object(object obj):             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     Cython equivalent of `isinstance(obj, np.datetime64)`
│ │   */
│ │  
│ │    /* function exit code */
│ │    __pyx_L0:;
│ │    __Pyx_RefNannyFinishContext();
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":996
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":996
│ │   * 
│ │   * 
│ │   * cdef inline npy_datetime get_datetime64_value(object obj) nogil:             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     returns the int64 value underlying scalar numpy datetime64 object
│ │   */
│ │  
│ │  static CYTHON_INLINE npy_datetime __pyx_f_5numpy_get_datetime64_value(PyObject *__pyx_v_obj) {
│ │    npy_datetime __pyx_r;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1003
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1003
│ │   *     also needed.  That can be found using `get_datetime64_unit`.
│ │   *     """
│ │   *     return (<PyDatetimeScalarObject*>obj).obval             # <<<<<<<<<<<<<<
│ │   * 
│ │   * 
│ │   */
│ │    __pyx_r = ((PyDatetimeScalarObject *)__pyx_v_obj)->obval;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":996
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":996
│ │   * 
│ │   * 
│ │   * cdef inline npy_datetime get_datetime64_value(object obj) nogil:             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     returns the int64 value underlying scalar numpy datetime64 object
│ │   */
│ │  
│ │    /* function exit code */
│ │    __pyx_L0:;
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1006
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1006
│ │   * 
│ │   * 
│ │   * cdef inline npy_timedelta get_timedelta64_value(object obj) nogil:             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     returns the int64 value underlying scalar numpy timedelta64 object
│ │   */
│ │  
│ │  static CYTHON_INLINE npy_timedelta __pyx_f_5numpy_get_timedelta64_value(PyObject *__pyx_v_obj) {
│ │    npy_timedelta __pyx_r;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1010
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1010
│ │   *     returns the int64 value underlying scalar numpy timedelta64 object
│ │   *     """
│ │   *     return (<PyTimedeltaScalarObject*>obj).obval             # <<<<<<<<<<<<<<
│ │   * 
│ │   * 
│ │   */
│ │    __pyx_r = ((PyTimedeltaScalarObject *)__pyx_v_obj)->obval;
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1006
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1006
│ │   * 
│ │   * 
│ │   * cdef inline npy_timedelta get_timedelta64_value(object obj) nogil:             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     returns the int64 value underlying scalar numpy timedelta64 object
│ │   */
│ │  
│ │    /* function exit code */
│ │    __pyx_L0:;
│ │    return __pyx_r;
│ │  }
│ │  
│ │ -/* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1013
│ │ +/* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1013
│ │   * 
│ │   * 
│ │   * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     returns the unit part of the dtype for a numpy datetime64 object.
│ │   */
│ │  
│ │  static CYTHON_INLINE NPY_DATETIMEUNIT __pyx_f_5numpy_get_datetime64_unit(PyObject *__pyx_v_obj) {
│ │    NPY_DATETIMEUNIT __pyx_r;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1017
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1017
│ │   *     returns the unit part of the dtype for a numpy datetime64 object.
│ │   *     """
│ │   *     return <NPY_DATETIMEUNIT>(<PyDatetimeScalarObject*>obj).obmeta.base             # <<<<<<<<<<<<<<
│ │   */
│ │    __pyx_r = ((NPY_DATETIMEUNIT)((PyDatetimeScalarObject *)__pyx_v_obj)->obmeta.base);
│ │    goto __pyx_L0;
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":1013
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":1013
│ │   * 
│ │   * 
│ │   * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
│ │   *     """
│ │   *     returns the unit part of the dtype for a numpy datetime64 object.
│ │   */
│ │  
│ │ @@ -23180,26 +23188,26 @@
│ │    return -1;
│ │  }
│ │  
│ │  static CYTHON_SMALL_CODE int __Pyx_InitCachedConstants(void) {
│ │    __Pyx_RefNannyDeclarations
│ │    __Pyx_RefNannySetupContext("__Pyx_InitCachedConstants", 0);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":944
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":944
│ │   *         __pyx_import_array()
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.multiarray failed to import")             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline int import_umath() except -1:
│ │   */
│ │    __pyx_tuple_ = PyTuple_Pack(1, __pyx_kp_u_numpy_core_multiarray_failed_to); if (unlikely(!__pyx_tuple_)) __PYX_ERR(1, 944, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_tuple_);
│ │    __Pyx_GIVEREF(__pyx_tuple_);
│ │  
│ │ -  /* "../../../../../tmp/build-env-glkoam7d/lib/python3.8/site-packages/numpy/__init__.pxd":950
│ │ +  /* "../../../../../tmp/build-env-y58uqb76/lib/python3.8/site-packages/numpy/__init__.pxd":950
│ │   *         _import_umath()
│ │   *     except Exception:
│ │   *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
│ │   * 
│ │   * cdef inline int import_ufunc() except -1:
│ │   */
│ │    __pyx_tuple__2 = PyTuple_Pack(1, __pyx_kp_u_numpy_core_umath_failed_to_impor); if (unlikely(!__pyx_tuple__2)) __PYX_ERR(1, 950, __pyx_L1_error)
│ │ @@ -23602,52 +23610,67 @@
│ │    int __pyx_clineno = 0;
│ │    __Pyx_RefNannySetupContext("__Pyx_modinit_type_import_code", 0);
│ │    /*--- Type import code ---*/
│ │    __pyx_t_1 = PyImport_ImportModule(__Pyx_BUILTIN_MODULE_NAME); if (unlikely(!__pyx_t_1)) __PYX_ERR(3, 9, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │    __pyx_ptype_7cpython_4type_type = __Pyx_ImportType(__pyx_t_1, __Pyx_BUILTIN_MODULE_NAME, "type", 
│ │    #if defined(PYPY_VERSION_NUM) && PYPY_VERSION_NUM < 0x050B0000
│ │ -  sizeof(PyTypeObject),
│ │ +  sizeof(PyTypeObject), __PYX_GET_STRUCT_ALIGNMENT(PyTypeObject),
│ │    #else
│ │ -  sizeof(PyHeapTypeObject),
│ │ +  sizeof(PyHeapTypeObject), __PYX_GET_STRUCT_ALIGNMENT(PyHeapTypeObject),
│ │    #endif
│ │    __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_7cpython_4type_type) __PYX_ERR(3, 9, __pyx_L1_error)
│ │    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
│ │    __pyx_t_1 = PyImport_ImportModule("numpy"); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 199, __pyx_L1_error)
│ │    __Pyx_GOTREF(__pyx_t_1);
│ │ -  __pyx_ptype_5numpy_dtype = __Pyx_ImportType(__pyx_t_1, "numpy", "dtype", sizeof(PyArray_Descr), __Pyx_ImportType_CheckSize_Ignore);
│ │ +  __pyx_ptype_5numpy_dtype = __Pyx_ImportType(__pyx_t_1, "numpy", "dtype", sizeof(PyArray_Descr), __PYX_GET_STRUCT_ALIGNMENT(PyArray_Descr),
│ │ +  __Pyx_ImportType_CheckSize_Ignore);
│ │     if (!__pyx_ptype_5numpy_dtype) __PYX_ERR(1, 199, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_flatiter = __Pyx_ImportType(__pyx_t_1, "numpy", "flatiter", sizeof(PyArrayIterObject), __Pyx_ImportType_CheckSize_Ignore);
│ │ +  __pyx_ptype_5numpy_flatiter = __Pyx_ImportType(__pyx_t_1, "numpy", "flatiter", sizeof(PyArrayIterObject), __PYX_GET_STRUCT_ALIGNMENT(PyArrayIterObject),
│ │ +  __Pyx_ImportType_CheckSize_Ignore);
│ │     if (!__pyx_ptype_5numpy_flatiter) __PYX_ERR(1, 222, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_broadcast = __Pyx_ImportType(__pyx_t_1, "numpy", "broadcast", sizeof(PyArrayMultiIterObject), __Pyx_ImportType_CheckSize_Ignore);
│ │ +  __pyx_ptype_5numpy_broadcast = __Pyx_ImportType(__pyx_t_1, "numpy", "broadcast", sizeof(PyArrayMultiIterObject), __PYX_GET_STRUCT_ALIGNMENT(PyArrayMultiIterObject),
│ │ +  __Pyx_ImportType_CheckSize_Ignore);
│ │     if (!__pyx_ptype_5numpy_broadcast) __PYX_ERR(1, 226, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_ndarray = __Pyx_ImportType(__pyx_t_1, "numpy", "ndarray", sizeof(PyArrayObject), __Pyx_ImportType_CheckSize_Ignore);
│ │ +  __pyx_ptype_5numpy_ndarray = __Pyx_ImportType(__pyx_t_1, "numpy", "ndarray", sizeof(PyArrayObject), __PYX_GET_STRUCT_ALIGNMENT(PyArrayObject),
│ │ +  __Pyx_ImportType_CheckSize_Ignore);
│ │     if (!__pyx_ptype_5numpy_ndarray) __PYX_ERR(1, 238, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_generic = __Pyx_ImportType(__pyx_t_1, "numpy", "generic", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_generic = __Pyx_ImportType(__pyx_t_1, "numpy", "generic", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_generic) __PYX_ERR(1, 770, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_number = __Pyx_ImportType(__pyx_t_1, "numpy", "number", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_number = __Pyx_ImportType(__pyx_t_1, "numpy", "number", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_number) __PYX_ERR(1, 772, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_integer = __Pyx_ImportType(__pyx_t_1, "numpy", "integer", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_integer = __Pyx_ImportType(__pyx_t_1, "numpy", "integer", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_integer) __PYX_ERR(1, 774, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_signedinteger = __Pyx_ImportType(__pyx_t_1, "numpy", "signedinteger", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_signedinteger = __Pyx_ImportType(__pyx_t_1, "numpy", "signedinteger", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_signedinteger) __PYX_ERR(1, 776, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_unsignedinteger = __Pyx_ImportType(__pyx_t_1, "numpy", "unsignedinteger", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_unsignedinteger = __Pyx_ImportType(__pyx_t_1, "numpy", "unsignedinteger", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_unsignedinteger) __PYX_ERR(1, 778, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_inexact = __Pyx_ImportType(__pyx_t_1, "numpy", "inexact", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_inexact = __Pyx_ImportType(__pyx_t_1, "numpy", "inexact", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_inexact) __PYX_ERR(1, 780, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_floating = __Pyx_ImportType(__pyx_t_1, "numpy", "floating", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_floating = __Pyx_ImportType(__pyx_t_1, "numpy", "floating", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_floating) __PYX_ERR(1, 782, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_complexfloating = __Pyx_ImportType(__pyx_t_1, "numpy", "complexfloating", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_complexfloating = __Pyx_ImportType(__pyx_t_1, "numpy", "complexfloating", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_complexfloating) __PYX_ERR(1, 784, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_flexible = __Pyx_ImportType(__pyx_t_1, "numpy", "flexible", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_flexible = __Pyx_ImportType(__pyx_t_1, "numpy", "flexible", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_flexible) __PYX_ERR(1, 786, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_character = __Pyx_ImportType(__pyx_t_1, "numpy", "character", sizeof(PyObject), __Pyx_ImportType_CheckSize_Warn);
│ │ +  __pyx_ptype_5numpy_character = __Pyx_ImportType(__pyx_t_1, "numpy", "character", sizeof(PyObject), __PYX_GET_STRUCT_ALIGNMENT(PyObject),
│ │ +  __Pyx_ImportType_CheckSize_Warn);
│ │     if (!__pyx_ptype_5numpy_character) __PYX_ERR(1, 788, __pyx_L1_error)
│ │ -  __pyx_ptype_5numpy_ufunc = __Pyx_ImportType(__pyx_t_1, "numpy", "ufunc", sizeof(PyUFuncObject), __Pyx_ImportType_CheckSize_Ignore);
│ │ +  __pyx_ptype_5numpy_ufunc = __Pyx_ImportType(__pyx_t_1, "numpy", "ufunc", sizeof(PyUFuncObject), __PYX_GET_STRUCT_ALIGNMENT(PyUFuncObject),
│ │ +  __Pyx_ImportType_CheckSize_Ignore);
│ │     if (!__pyx_ptype_5numpy_ufunc) __PYX_ERR(1, 826, __pyx_L1_error)
│ │    __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
│ │    __Pyx_RefNannyFinishContext();
│ │    return 0;
│ │    __pyx_L1_error:;
│ │    __Pyx_XDECREF(__pyx_t_1);
│ │    __Pyx_RefNannyFinishContext();
│ │ @@ -25019,28 +25042,28 @@
│ │                              "BaseException");
│ │              goto bad;
│ │          }
│ │          PyException_SetCause(value, fixed_cause);
│ │      }
│ │      PyErr_SetObject(type, value);
│ │      if (tb) {
│ │ -#if CYTHON_COMPILING_IN_PYPY
│ │ -        PyObject *tmp_type, *tmp_value, *tmp_tb;
│ │ -        PyErr_Fetch(&tmp_type, &tmp_value, &tmp_tb);
│ │ -        Py_INCREF(tb);
│ │ -        PyErr_Restore(tmp_type, tmp_value, tb);
│ │ -        Py_XDECREF(tmp_tb);
│ │ -#else
│ │ +#if CYTHON_FAST_THREAD_STATE
│ │          PyThreadState *tstate = __Pyx_PyThreadState_Current;
│ │          PyObject* tmp_tb = tstate->curexc_traceback;
│ │          if (tb != tmp_tb) {
│ │              Py_INCREF(tb);
│ │              tstate->curexc_traceback = tb;
│ │              Py_XDECREF(tmp_tb);
│ │          }
│ │ +#else
│ │ +        PyObject *tmp_type, *tmp_value, *tmp_tb;
│ │ +        PyErr_Fetch(&tmp_type, &tmp_value, &tmp_tb);
│ │ +        Py_INCREF(tb);
│ │ +        PyErr_Restore(tmp_type, tmp_value, tb);
│ │ +        Py_XDECREF(tmp_tb);
│ │  #endif
│ │      }
│ │  bad:
│ │      Py_XDECREF(owned_instance);
│ │      return;
│ │  }
│ │  #endif
│ │ @@ -25989,44 +26012,62 @@
│ │      return ret;
│ │  }
│ │  
│ │  /* TypeImport */
│ │  #ifndef __PYX_HAVE_RT_ImportType
│ │  #define __PYX_HAVE_RT_ImportType
│ │  static PyTypeObject *__Pyx_ImportType(PyObject *module, const char *module_name, const char *class_name,
│ │ -    size_t size, enum __Pyx_ImportType_CheckSize check_size)
│ │ +    size_t size, size_t alignment, enum __Pyx_ImportType_CheckSize check_size)
│ │  {
│ │      PyObject *result = 0;
│ │      char warning[200];
│ │      Py_ssize_t basicsize;
│ │ +    Py_ssize_t itemsize;
│ │  #ifdef Py_LIMITED_API
│ │      PyObject *py_basicsize;
│ │ +    PyObject *py_itemsize;
│ │  #endif
│ │      result = PyObject_GetAttrString(module, class_name);
│ │      if (!result)
│ │          goto bad;
│ │      if (!PyType_Check(result)) {
│ │          PyErr_Format(PyExc_TypeError,
│ │              "%.200s.%.200s is not a type object",
│ │              module_name, class_name);
│ │          goto bad;
│ │      }
│ │  #ifndef Py_LIMITED_API
│ │      basicsize = ((PyTypeObject *)result)->tp_basicsize;
│ │ +    itemsize = ((PyTypeObject *)result)->tp_itemsize;
│ │  #else
│ │      py_basicsize = PyObject_GetAttrString(result, "__basicsize__");
│ │      if (!py_basicsize)
│ │          goto bad;
│ │      basicsize = PyLong_AsSsize_t(py_basicsize);
│ │      Py_DECREF(py_basicsize);
│ │      py_basicsize = 0;
│ │      if (basicsize == (Py_ssize_t)-1 && PyErr_Occurred())
│ │          goto bad;
│ │ +    py_itemsize = PyObject_GetAttrString(result, "__itemsize__");
│ │ +    if (!py_itemsize)
│ │ +        goto bad;
│ │ +    itemsize = PyLong_AsSsize_t(py_itemsize);
│ │ +    Py_DECREF(py_itemsize);
│ │ +    py_itemsize = 0;
│ │ +    if (itemsize == (Py_ssize_t)-1 && PyErr_Occurred())
│ │ +        goto bad;
│ │  #endif
│ │ -    if ((size_t)basicsize < size) {
│ │ +    if (itemsize) {
│ │ +        if (size % alignment) {
│ │ +            alignment = size % alignment;
│ │ +        }
│ │ +        if (itemsize < (Py_ssize_t)alignment)
│ │ +            itemsize = (Py_ssize_t)alignment;
│ │ +    }
│ │ +    if ((size_t)(basicsize + itemsize) < size) {
│ │          PyErr_Format(PyExc_ValueError,
│ │              "%.200s.%.200s size changed, may indicate binary incompatibility. "
│ │              "Expected %zd from C header, got %zd from PyObject",
│ │              module_name, class_name, size, basicsize);
│ │          goto bad;
│ │      }
│ │      if (check_size == __Pyx_ImportType_CheckSize_Error && (size_t)basicsize != size) {
│ │   --- LibRecommender-1.0.1/libreco/utils/_similarities.pyx
│ ├── +++ LibRecommender-1.1.0/libreco/utils/_similarities.pyx
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/utils/constants.py
│ ├── +++ LibRecommender-1.1.0/libreco/utils/constants.py
│ │┄ Files 13% similar despite different names
│ │ @@ -12,18 +12,18 @@
│ │  )
│ │  
│ │  FEAT_MODELS = (
│ │      "WideDeep",
│ │      "FM",
│ │      "DeepFM",
│ │      "AutoInt",
│ │ -    "PinSage",
│ │ -    "PinSageDGL",
│ │      "GraphSage",
│ │      "GraphSageDGL",
│ │ +    "PinSage",
│ │ +    "PinSageDGL",
│ │  )
│ │  
│ │  SEQUENCE_MODELS = (
│ │      "YouTubeRetrieval",
│ │      "YouTubeRanking",
│ │      "DIN",
│ │      "Item2Vec",
│ │ @@ -32,18 +32,18 @@
│ │      "WaveNet",
│ │  )
│ │  
│ │  GRAPH_MODELS = (
│ │      "DeepWalk",
│ │      "NGCF",
│ │      "LightGCN",
│ │ -    "PinSage",
│ │ -    "PinSageDGL",
│ │      "GraphSage",
│ │      "GraphSageDGL",
│ │ +    "PinSage",
│ │ +    "PinSageDGL",
│ │  )
│ │  
│ │  TF_FEAT_MODELS = (
│ │      "WideDeep",
│ │      "FM",
│ │      "DeepFM",
│ │      "YouTubeRanking",
│ │ @@ -64,25 +64,47 @@
│ │      "AutoInt",
│ │      "DIN",
│ │      "RNN4Rec",
│ │      "Caser",
│ │      "WaveNet",
│ │  )
│ │  
│ │ +FEAT_TRAIN_MODELS = (
│ │ +    "WideDeep",
│ │ +    "FM",
│ │ +    "DeepFM",
│ │ +    "AutoInt",
│ │ +    "YouTubeRetrieval",
│ │ +    "YouTubeRanking",
│ │ +    "DIN",
│ │ +    "GraphSage",
│ │ +    "GraphSageDGL",
│ │ +    "PinSage",
│ │ +    "PinSageDGL",
│ │ +)
│ │ +
│ │  EMBEDDING_MODELS = (
│ │      "SVD",
│ │      "SVDpp",
│ │      "ALS",
│ │      "BPR",
│ │      "YouTubeRetrieval",
│ │      "Item2Vec",
│ │      "RNN4Rec",
│ │      "Caser",
│ │      "WaveNet",
│ │      "DeepWalk",
│ │      "NGCF",
│ │      "LightGCN",
│ │ -    "PinSage",
│ │ -    "PinSageDGL",
│ │      "GraphSage",
│ │      "GraphSageDGL",
│ │ +    "PinSage",
│ │ +    "PinSageDGL",
│ │ +)
│ │ +
│ │ +SEQUENCE_RECOMMEND_MODELS = (
│ │ +    "YouTubeRanking",
│ │ +    "DIN",
│ │ +    "RNN4Rec",
│ │ +    "Caser",
│ │ +    "WaveNet",
│ │  )
│ │   --- LibRecommender-1.0.1/libreco/utils/initializers.py
│ ├── +++ LibRecommender-1.1.0/libreco/utils/initializers.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/utils/misc.py
│ ├── +++ LibRecommender-1.1.0/libreco/utils/misc.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/utils/save_load.py
│ ├── +++ LibRecommender-1.1.0/libreco/utils/save_load.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libreco/utils/similarities.py
│ ├── +++ LibRecommender-1.1.0/libreco/utils/similarities.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libserving/benchmark.py
│ ├── +++ LibRecommender-1.1.0/libserving/sanic_serving/benchmark.py
│ │┄ Files 2% similar despite different names
│ │ @@ -1,16 +1,17 @@
│ │  import argparse
│ │  import asyncio
│ │  import time
│ │  from concurrent.futures import ThreadPoolExecutor, as_completed
│ │  
│ │  import aiohttp
│ │  import requests
│ │ +import ujson
│ │  
│ │ -REQUEST_LIMIT = 20
│ │ +REQUEST_LIMIT = 64
│ │  
│ │  
│ │  def parse_args():
│ │      parser = argparse.ArgumentParser()
│ │  
│ │      parser.add_argument("--host", default="127.0.0.1")
│ │      parser.add_argument("--port", default=8000)
│ │ @@ -25,15 +26,15 @@
│ │  async def get_reco_async(
│ │      session: aiohttp.ClientSession, url: str, data: dict, semaphore: asyncio.Semaphore
│ │  ):
│ │      async with semaphore, session.post(url, json=data) as resp:
│ │          # if semaphore.locked():
│ │          #     await asyncio.sleep(1.0)
│ │          resp.raise_for_status()
│ │ -        reco = await resp.json()
│ │ +        reco = await resp.json(loads=ujson.loads)
│ │      return reco
│ │  
│ │  
│ │  async def main_async(args):
│ │      url = f"http://{args.host}:{args.port}/{args.algo}/recommend"
│ │      data = {"user": args.user, "n_rec": args.n_rec}
│ │      semaphore = asyncio.Semaphore(REQUEST_LIMIT)
│ │   --- LibRecommender-1.0.1/libserving/request.py
│ ├── +++ LibRecommender-1.1.0/libserving/request.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libserving/sanic_serving/common.py
│ ├── +++ LibRecommender-1.1.0/libserving/sanic_serving/common.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libserving/sanic_serving/embed_deploy.py
│ ├── +++ LibRecommender-1.1.0/libserving/sanic_serving/embed_deploy.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libserving/sanic_serving/knn_deploy.py
│ ├── +++ LibRecommender-1.1.0/libserving/sanic_serving/knn_deploy.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libserving/sanic_serving/tf_deploy.py
│ ├── +++ LibRecommender-1.1.0/libserving/sanic_serving/tf_deploy.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libserving/serialization/common.py
│ ├── +++ LibRecommender-1.1.0/libserving/serialization/common.py
│ │┄ Files identical despite different names
│ │   --- LibRecommender-1.0.1/libserving/serialization/embed.py
│ ├── +++ LibRecommender-1.1.0/libserving/serialization/embed.py
│ │┄ Files 24% similar despite different names
│ │ @@ -10,14 +10,23 @@
│ │      save_model_name,
│ │      save_to_json,
│ │      save_user_consumed,
│ │  )
│ │  
│ │  
│ │  def save_embed(path: str, model: EmbedBase):
│ │ +    """Save Embed model to disk.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    path : str
│ │ +        Model saving path.
│ │ +    model : EmbedBase
│ │ +        Model to save.
│ │ +    """
│ │      check_path_exists(path)
│ │      save_model_name(path, model)
│ │      save_id_mapping(path, model.data_info)
│ │      save_user_consumed(path, model.data_info)
│ │      save_vectors(path, model.user_embed, model.n_users, "user_embed.json")
│ │      save_vectors(path, model.item_embed, model.n_items, "item_embed.json")
│ │   --- LibRecommender-1.0.1/libserving/serialization/knn.py
│ ├── +++ LibRecommender-1.1.0/libserving/serialization/knn.py
│ │┄ Files 21% similar despite different names
│ │ @@ -10,14 +10,25 @@
│ │      save_model_name,
│ │      save_to_json,
│ │      save_user_consumed,
│ │  )
│ │  
│ │  
│ │  def save_knn(path: str, model: CfBase, k: int):
│ │ +    """Save KNN model to disk.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    path : str
│ │ +        Model saving path.
│ │ +    model : CfBase
│ │ +        Model to save.
│ │ +    k : int
│ │ +        Number of similar users/items to save.
│ │ +    """
│ │      check_path_exists(path)
│ │      save_model_name(path, model)
│ │      save_id_mapping(path, model.data_info)
│ │      save_user_consumed(path, model.data_info)
│ │      save_sim_matrix(path, model.sim_matrix, k)
│ │   --- LibRecommender-1.0.1/libserving/serialization/redis.py
│ ├── +++ LibRecommender-1.1.0/libserving/serialization/redis.py
│ │┄ Files 16% similar despite different names
│ │ @@ -15,30 +15,69 @@
│ │          raise
│ │      finally:
│ │          if r:
│ │              r.close()
│ │  
│ │  
│ │  def knn2redis(path: str, host: str = "localhost", port: int = 6379, db: int = 0):
│ │ +    """Save KNN model to redis.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    path : str
│ │ +        Model saving path.
│ │ +    host : str, default: "localhost"
│ │ +        Redis host.
│ │ +    port : int, default: 6379
│ │ +        Redis port
│ │ +    db : int, default: 0
│ │ +        Redis db number
│ │ +    """
│ │      with redis_connection(host, port, db) as r:
│ │          model_name2redis(path, r)
│ │          id_mapping2redis(path, r)
│ │          user_consumed2redis(path, r)
│ │          sim2redis(path, r)
│ │  
│ │  
│ │  def embed2redis(path: str, host: str = "localhost", port: int = 6379, db: int = 0):
│ │ +    """Save Embed model to redis.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    path : str
│ │ +        Model saving path.
│ │ +    host : str, default: "localhost"
│ │ +        Redis host.
│ │ +    port : int, default: 6379
│ │ +        Redis port
│ │ +    db : int, default: 0
│ │ +        Redis db number
│ │ +    """
│ │      with redis_connection(host, port, db) as r:
│ │          model_name2redis(path, r)
│ │          id_mapping2redis(path, r)
│ │          user_consumed2redis(path, r)
│ │          user_embed2redis(path, r)
│ │  
│ │  
│ │  def tf2redis(path: str, host: str = "localhost", port: int = 6379, db: int = 0):
│ │ +    """Save TF model to redis.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    path : str
│ │ +        Model saving path.
│ │ +    host : str, default: "localhost"
│ │ +        Redis host.
│ │ +    port : int, default: 6379
│ │ +        Redis port
│ │ +    db : int, default: 0
│ │ +        Redis db number
│ │ +    """
│ │      with redis_connection(host, port, db) as r:
│ │          model_name2redis(path, r)
│ │          id_mapping2redis(path, r)
│ │          user_consumed2redis(path, r)
│ │          features2redis(path, r)
│ │   --- LibRecommender-1.0.1/libserving/serialization/tfmodel.py
│ ├── +++ LibRecommender-1.1.0/libserving/serialization/tfmodel.py
│ │┄ Files 2% similar despite different names
│ │ @@ -14,14 +14,25 @@
│ │      save_model_name,
│ │      save_to_json,
│ │      save_user_consumed,
│ │  )
│ │  
│ │  
│ │  def save_tf(path: str, model: TfBase, version: int = 1):
│ │ +    """Save TF model to disk.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    path : str
│ │ +        Model saving path.
│ │ +    model : TfBase
│ │ +        Model to save.
│ │ +    version : int, default: 1
│ │ +        Version number used in ``tf.saved_model``.
│ │ +    """
│ │      check_path_exists(path)
│ │      save_model_name(path, model)
│ │      save_id_mapping(path, model.data_info)
│ │      save_user_consumed(path, model.data_info)
│ │      save_features(path, model.data_info, model)
│ │      save_tf_serving_model(path, model, version)
│ │  
│ │ @@ -55,15 +66,15 @@
│ │  # include oov
│ │  def _check_num_match(v, num):
│ │      assert len(v) == num + 1, f"feature sizes don't match, got {len(v)} and {num + 1}"
│ │  
│ │  
│ │  def save_tf_serving_model(path: str, model: TfBase, version: int):
│ │      model_name = model.model_name.lower()
│ │ -    if not path:
│ │ +    if not path:  # pragma: no cover
│ │          model_base_path = os.path.realpath("..")
│ │          export_path = os.path.join(
│ │              model_base_path, "serving", "models", f"{model_name}", f"{version}"
│ │          )
│ │      else:
│ │          export_path = os.path.join(path, f"{model_name}", f"{version}")
│ │   --- LibRecommender-1.0.1/pyproject.toml
│ ├── +++ LibRecommender-1.1.0/pyproject.toml
│ │┄ Files 20% similar despite different names
│ │ @@ -7,43 +7,44 @@
│ │      "scipy>=1.2.1",
│ │      "tomli",
│ │  ]
│ │  build-backend = "setuptools.build_meta"
│ │  
│ │  [project]
│ │  name = "LibRecommender"
│ │ -version = "1.0.1"
│ │ +version = "1.1.0"
│ │  description = "Versatile end-to-end recommender system."
│ │  authors = [
│ │      { name = "massquantity", email = "jinxin_madie@163.com" },
│ │  ]
│ │  readme = "README.md"
│ │  license = { text = "MIT" }
│ │  requires-python = ">=3.6"
│ │  keywords = ["Collaborative Filtering", "Recommender System"]
│ │ -classifiers= [
│ │ +classifiers = [
│ │      "Development Status :: 5 - Production/Stable",
│ │      "Intended Audience :: Developers",
│ │      "Intended Audience :: Education",
│ │      "Intended Audience :: Science/Research",
│ │      "License :: OSI Approved :: MIT License",
│ │      "Programming Language :: Python :: 3.6",
│ │      "Programming Language :: Python :: 3.7",
│ │      "Programming Language :: Python :: 3.8",
│ │      "Programming Language :: Python :: 3.9",
│ │      "Programming Language :: Python :: 3.10",
│ │      "Programming Language :: Cython",
│ │ +    "Programming Language :: Rust",
│ │  ]
│ │  dependencies = [
│ │      "gensim >= 4.0.0",
│ │      "tqdm",
│ │  ]
│ │  
│ │  [project.urls]
│ │ -documentation = "https://librecommender.readthedocs.io/en/stable/"
│ │ +documentation = "https://librecommender.readthedocs.io/en/latest/"
│ │  repository = "https://github.com/massquantity/LibRecommender"
│ │  
│ │  [tool.setuptools]
│ │  include-package-data = true
│ │  
│ │  [tool.setuptools.packages.find]
│ │  where = ["."]
│ │ @@ -53,21 +54,23 @@
│ │  [tool.black]
│ │  line-length = 88
│ │  target-version = ["py36", "py38", "py310"]
│ │  
│ │  [tool.isort]
│ │  atomic = true
│ │  default_section = "THIRDPARTY"
│ │ +extend_skip_glob = ["*.pyx"]
│ │  force_grid_wrap = 0
│ │  include_trailing_comma = true
│ │  known_first_party = ["libreco", "libserving"]
│ │  known_third_party = "pytest"
│ │  line_length = 88
│ │  multi_line_output = 3
│ │  profile = "black"
│ │ +reverse_relative = true
│ │  
│ │  [tool.pydocstyle]
│ │  add-ignore = ["D102"]
│ │  add-select = ["D212", "D402", "D415", "D416", "D417"]
│ │  convention = "numpy"
│ │  
│ │  [tool.pytest.ini_options]
│ │ @@ -75,27 +78,32 @@
│ │  addopts = "-v --durations=20 --color=yes"
│ │  filterwarnings = [
│ │      "ignore:the imp module is deprecated in favour of importlib:DeprecationWarning",
│ │      "ignore:distutils Version classes are deprecated. Use packaging.version instead:DeprecationWarning",
│ │      "ignore:The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package:UserWarning",
│ │      "ignore:.(tf.layers|layer|tf.nn).* is deprecated:UserWarning",
│ │      "ignore:User provided device_type of 'cuda', but CUDA is not available:UserWarning",
│ │ +    "ignore:`build_negative_samples` is deprecated",
│ │  ]
│ │  
│ │  [tool.ruff]
│ │  line-length = 88
│ │  target-version = "py38"
│ │  show-source = true
│ │ -ignore = ["E501", "F401", "B008", "B010", "S101", "S104"]
│ │ +ignore = ["E501", "F401", "B008", "B010", "NPY002"]
│ │  select = [
│ │      # pyflakes
│ │      "F",
│ │      # pycodestyle
│ │      "E",
│ │      "W",
│ │      # flake8-2020
│ │      "YTT",
│ │      # flake8-bugbear
│ │      "B",
│ │ -    # flake8-bandit
│ │ -    "S",
│ │ +    # pandas-vet
│ │ +    "PD",
│ │ +    # NumPy-specific rules
│ │ +    "NPY",
│ │ +    # Ruff-specific rules
│ │ +    "RUF",
│ │  ]
│ │   --- LibRecommender-1.0.1/setup.py
│ ├── +++ LibRecommender-1.1.0/setup.py
│ │┄ Files identical despite different names
