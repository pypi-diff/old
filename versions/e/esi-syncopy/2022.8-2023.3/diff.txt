--- tmp/esi-syncopy-2022.8.tar.gz
+++ tmp/esi_syncopy-2023.3.tar.gz
├── filetype from file(1)
│ @@ -1 +1 @@
│ -gzip compressed data, was "esi-syncopy-2022.8.tar", max compression
│ +gzip compressed data, was "esi_syncopy-2023.3.tar", max compression
│   --- esi-syncopy-2022.8.tar
├── +++ esi_syncopy-2023.3.tar
│ ├── file list
│ │ @@ -1,108 +1,125 @@
│ │ --rw-r--r--   0        0        0     1597 2022-01-28 08:45:14.763558 esi-syncopy-2022.8/LICENSE
│ │ --rw-r--r--   0        0        0     3312 2022-03-09 14:45:17.004286 esi-syncopy-2022.8/README.rst
│ │ --rw-r--r--   0        0        0     1343 2022-08-11 13:15:12.151142 esi-syncopy-2022.8/pyproject.toml
│ │ --rw-r--r--   0        0        0     4261 2022-08-03 15:50:53.524690 esi-syncopy-2022.8/syncopy/__init__.py
│ │ --rw-r--r--   0        0        0      934 2022-08-03 15:50:53.524690 esi-syncopy-2022.8/syncopy/datatype/__init__.py
│ │ --rw-r--r--   0        0        0    78237 2022-08-03 15:50:53.524690 esi-syncopy-2022.8/syncopy/datatype/base_data.py
│ │ --rw-r--r--   0        0        0    31201 2022-08-03 15:50:53.524690 esi-syncopy-2022.8/syncopy/datatype/continuous_data.py
│ │ --rw-r--r--   0        0        0    23274 2022-08-03 15:50:53.524690 esi-syncopy-2022.8/syncopy/datatype/discrete_data.py
│ │ --rw-r--r--   0        0        0    22341 2022-08-03 15:50:53.524690 esi-syncopy-2022.8/syncopy/datatype/methods/arithmetic.py
│ │ --rw-r--r--   0        0        0     2086 2022-08-03 15:50:53.524690 esi-syncopy-2022.8/syncopy/datatype/methods/copy.py
│ │ --rw-r--r--   0        0        0    14644 2022-05-13 08:30:12.557793 esi-syncopy-2022.8/syncopy/datatype/methods/definetrial.py
│ │ --rw-r--r--   0        0        0    31243 2022-08-03 15:50:53.524690 esi-syncopy-2022.8/syncopy/datatype/methods/padding.py
│ │ --rw-r--r--   0        0        0    17789 2022-08-03 15:50:53.524690 esi-syncopy-2022.8/syncopy/datatype/methods/selectdata.py
│ │ --rw-r--r--   0        0        0     7227 2022-08-03 15:50:53.524690 esi-syncopy-2022.8/syncopy/datatype/methods/show.py
│ │ --rw-r--r--   0        0        0     5521 2022-08-03 15:50:53.524690 esi-syncopy-2022.8/syncopy/datatype/statistical_data.py
│ │ --rw-r--r--   0        0        0      622 2022-08-05 14:30:13.229506 esi-syncopy-2022.8/syncopy/io/__init__.py
│ │ --rw-r--r--   0        0        0    10591 2022-08-09 07:45:11.013982 esi-syncopy-2022.8/syncopy/io/_load_nwb.py
│ │ --rw-r--r--   0        0        0    17293 2022-08-05 14:30:13.229506 esi-syncopy-2022.8/syncopy/io/load_ft.py
│ │ --rw-r--r--   0        0        0    12262 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/io/load_spy_container.py
│ │ --rw-r--r--   0        0        0    38859 2022-08-05 14:30:13.229506 esi-syncopy-2022.8/syncopy/io/load_tdt.py
│ │ --rw-r--r--   0        0        0    11389 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/io/save_spy_container.py
│ │ --rw-r--r--   0        0        0    11243 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/io/utils.py
│ │ --rw-r--r--   0        0        0    18496 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/nwanalysis/AV_compRoutines.py
│ │ --rw-r--r--   0        0        0    14380 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/nwanalysis/ST_compRoutines.py
│ │ --rw-r--r--   0        0        0      252 2022-01-28 08:45:14.779558 esi-syncopy-2022.8/syncopy/nwanalysis/__init__.py
│ │ --rw-r--r--   0        0        0    16358 2022-08-08 13:15:11.089683 esi-syncopy-2022.8/syncopy/nwanalysis/connectivity_analysis.py
│ │ --rw-r--r--   0        0        0     6694 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/nwanalysis/csd.py
│ │ --rw-r--r--   0        0        0     2536 2022-01-28 08:45:14.779558 esi-syncopy-2022.8/syncopy/nwanalysis/granger.py
│ │ --rw-r--r--   0        0        0     6982 2022-04-12 15:00:17.117628 esi-syncopy-2022.8/syncopy/nwanalysis/wilson_sf.py
│ │ --rw-r--r--   0        0        0      258 2022-03-22 14:00:12.079000 esi-syncopy-2022.8/syncopy/plotting/__init__.py
│ │ --rw-r--r--   0        0        0     4689 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/plotting/_helpers.py
│ │ --rw-r--r--   0        0        0     5202 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/plotting/_plotting.py
│ │ --rw-r--r--   0        0        0     1838 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/plotting/config.py
│ │ --rw-r--r--   0        0        0     8123 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/plotting/mp_plotting.py
│ │ --rw-r--r--   0        0        0     7356 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/plotting/sp_plotting.py
│ │ --rw-r--r--   0        0        0     2244 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/plotting/spy_plotting.py
│ │ --rw-r--r--   0        0        0      248 2022-04-13 18:15:15.779232 esi-syncopy-2022.8/syncopy/preproc/__init__.py
│ │ --rw-r--r--   0        0        0    27711 2022-08-08 11:45:13.854430 esi-syncopy-2022.8/syncopy/preproc/compRoutines.py
│ │ --rw-r--r--   0        0        0     6953 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/preproc/firws.py
│ │ --rw-r--r--   0        0        0    14478 2022-08-08 11:45:13.854430 esi-syncopy-2022.8/syncopy/preproc/preprocessing.py
│ │ --rw-r--r--   0        0        0     8623 2022-08-10 14:30:13.326874 esi-syncopy-2022.8/syncopy/preproc/resampledata.py
│ │ --rw-r--r--   0        0        0     3944 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/preproc/resampling.py
│ │ --rw-r--r--   0        0        0      647 2022-01-28 08:45:14.779558 esi-syncopy-2022.8/syncopy/shared/__init__.py
│ │ --rw-r--r--   0        0        0    45241 2022-08-08 11:45:13.858430 esi-syncopy-2022.8/syncopy/shared/computational_routine.py
│ │ --rw-r--r--   0        0        0     1390 2022-08-11 09:34:57.095041 esi-syncopy-2022.8/syncopy/shared/const_def.py
│ │ --rw-r--r--   0        0        0    12779 2022-01-28 08:45:14.779558 esi-syncopy-2022.8/syncopy/shared/errors.py
│ │ --rw-r--r--   0        0        0      739 2022-01-28 08:45:14.779558 esi-syncopy-2022.8/syncopy/shared/filetypes.py
│ │ --rw-r--r--   0        0        0    13717 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/shared/input_processors.py
│ │ --rw-r--r--   0        0        0    35009 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/shared/kwarg_decorators.py
│ │ --rw-r--r--   0        0        0    30169 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/shared/parsers.py
│ │ --rw-r--r--   0        0        0     1742 2022-01-28 08:45:14.779558 esi-syncopy-2022.8/syncopy/shared/queries.py
│ │ --rw-r--r--   0        0        0     9038 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/shared/tools.py
│ │ --rw-r--r--   0        0        0      837 2022-01-28 08:45:14.779558 esi-syncopy-2022.8/syncopy/specest/README.md
│ │ --rw-r--r--   0        0        0      254 2022-01-28 08:45:14.779558 esi-syncopy-2022.8/syncopy/specest/__init__.py
│ │ --rw-r--r--   0        0        0     1078 2022-05-11 13:30:12.737107 esi-syncopy-2022.8/syncopy/specest/_norm_spec.py
│ │ --rw-r--r--   0        0        0    38755 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/specest/compRoutines.py
│ │ --rw-r--r--   0        0        0     8386 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/specest/fooofspy.py
│ │ --rw-r--r--   0        0        0    41806 2022-08-08 13:15:11.089683 esi-syncopy-2022.8/syncopy/specest/freqanalysis.py
│ │ --rw-r--r--   0        0        0     4415 2022-03-18 14:30:11.978118 esi-syncopy-2022.8/syncopy/specest/mtmconvol.py
│ │ --rw-r--r--   0        0        0     3753 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/specest/mtmfft.py
│ │ --rw-r--r--   0        0        0     5275 2022-05-11 13:30:12.741107 esi-syncopy-2022.8/syncopy/specest/stft.py
│ │ --rw-r--r--   0        0        0    12898 2022-01-28 08:45:14.783558 esi-syncopy-2022.8/syncopy/specest/superlet.py
│ │ --rw-r--r--   0        0        0     3450 2022-01-28 08:45:14.783558 esi-syncopy-2022.8/syncopy/specest/wavelet.py
│ │ --rw-r--r--   0        0        0      129 2022-01-28 08:45:14.783558 esi-syncopy-2022.8/syncopy/specest/wavelets/__init__.py
│ │ --rw-r--r--   0        0        0    19699 2022-01-28 08:45:14.783558 esi-syncopy-2022.8/syncopy/specest/wavelets/transform.py
│ │ --rw-r--r--   0        0        0    10511 2022-01-28 08:45:14.783558 esi-syncopy-2022.8/syncopy/specest/wavelets/wavelets.py
│ │ --rw-r--r--   0        0        0      246 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/spikes/__init__.py
│ │ --rw-r--r--   0        0        0     4692 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/spikes/compRoutines.py
│ │ --rw-r--r--   0        0        0     4056 2022-08-03 15:50:53.528690 esi-syncopy-2022.8/syncopy/spikes/psth.py
│ │ --rw-r--r--   0        0        0     6252 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/spikes/spike_psth.py
│ │ --rw-r--r--   0        0        0      284 2022-01-28 08:45:14.783558 esi-syncopy-2022.8/syncopy/statistics/__init__.py
│ │ --rw-r--r--   0        0        0     3931 2022-01-28 08:45:14.783558 esi-syncopy-2022.8/syncopy/statistics/timelockanalysis.py
│ │ --rw-r--r--   0        0        0      479 2022-02-21 09:45:12.921882 esi-syncopy-2022.8/syncopy/tests/README.md
│ │ --rw-r--r--   0        0        0        0 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/__init__.py
│ │ --rw-r--r--   0        0        0        0 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/backend/__init__.py
│ │ --rwxr-xr-x   0        0        0     1796 2022-01-28 08:45:14.783558 esi-syncopy-2022.8/syncopy/tests/backend/run_tests.sh
│ │ --rw-r--r--   0        0        0     9493 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/backend/test_conn.py
│ │ --rw-r--r--   0        0        0     9946 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/backend/test_fooofspy.py
│ │ --rw-r--r--   0        0        0     4377 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/backend/test_resampling.py
│ │ --rw-r--r--   0        0        0    14593 2022-05-11 13:30:12.741107 esi-syncopy-2022.8/syncopy/tests/backend/test_timefreq.py
│ │ --rw-r--r--   0        0        0     2506 2022-04-26 19:00:24.221705 esi-syncopy-2022.8/syncopy/tests/conftest.py
│ │ --rw-r--r--   0        0        0     5959 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/helpers.py
│ │ --rw-r--r--   0        0        0     1562 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/local_spy.py
│ │ --rw-r--r--   0        0        0    11472 2022-01-28 08:45:14.783558 esi-syncopy-2022.8/syncopy/tests/misc.py
│ │ --rwxr-xr-x   0        0        0      195 2022-08-10 13:30:12.524977 esi-syncopy-2022.8/syncopy/tests/no_slurm.sh
│ │ --rw-r--r--   0        0        0      646 2022-01-28 08:45:14.783558 esi-syncopy-2022.8/syncopy/tests/run_tests.cmd
│ │ --rwxr-xr-x   0        0        0     2530 2022-03-11 21:00:14.867739 esi-syncopy-2022.8/syncopy/tests/run_tests.sh
│ │ --rw-r--r--   0        0        0     9625 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/synth_data.py
│ │ --rw-r--r--   0        0        0    16446 2022-08-10 14:45:10.993350 esi-syncopy-2022.8/syncopy/tests/test_basedata.py
│ │ --rw-r--r--   0        0        0     5813 2022-08-09 14:28:30.365771 esi-syncopy-2022.8/syncopy/tests/test_cfg.py
│ │ --rw-r--r--   0        0        0    24059 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/test_computationalroutine.py
│ │ --rw-r--r--   0        0        0    15533 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/test_connectivity.py
│ │ --rw-r--r--   0        0        0    56617 2022-08-10 10:15:12.135252 esi-syncopy-2022.8/syncopy/tests/test_continuousdata.py
│ │ --rw-r--r--   0        0        0     8622 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/test_decorators.py
│ │ --rw-r--r--   0        0        0    27294 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/test_discretedata.py
│ │ --rw-r--r--   0        0        0     2959 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/test_info.py
│ │ --rw-r--r--   0        0        0     3227 2022-03-11 22:30:19.934891 esi-syncopy-2022.8/syncopy/tests/test_packagesetup.py
│ │ --rwxr-xr-x   0        0        0    13207 2022-02-22 10:15:16.110951 esi-syncopy-2022.8/syncopy/tests/test_parsers.py
│ │ --rw-r--r--   0        0        0    12842 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/test_plotting.py
│ │ --rw-r--r--   0        0        0    20309 2022-08-10 13:30:12.524977 esi-syncopy-2022.8/syncopy/tests/test_preproc.py
│ │ --rw-r--r--   0        0        0     8827 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/test_resampledata.py
│ │ --rw-r--r--   0        0        0    44129 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/test_selectdata.py
│ │ --rw-r--r--   0        0        0    65453 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/test_specest.py
│ │ --rw-r--r--   0        0        0    11080 2022-08-03 15:50:53.532690 esi-syncopy-2022.8/syncopy/tests/test_specest_fooof.py
│ │ --rw-r--r--   0        0        0    24083 2022-08-05 14:30:13.229506 esi-syncopy-2022.8/syncopy/tests/test_spyio.py
│ │ --rw-r--r--   0        0        0     3737 2022-01-28 08:45:14.783558 esi-syncopy-2022.8/syncopy/tests/test_spytools.py
│ │ --rw-r--r--   0        0        0     4629 2022-08-11 13:24:33.607501 esi-syncopy-2022.8/setup.py
│ │ --rw-r--r--   0        0        0     4475 2022-08-11 13:24:33.607901 esi-syncopy-2022.8/PKG-INFO
│ │ +-rw-r--r--   0        0        0     1597 2023-04-05 11:26:16.656767 esi_syncopy-2023.3/LICENSE
│ │ +-rw-r--r--   0        0        0     3566 2023-04-05 11:26:16.656767 esi_syncopy-2023.3/README.rst
│ │ +-rw-r--r--   0        0        0     1365 2023-04-06 08:45:16.506910 esi_syncopy-2023.3/pyproject.toml
│ │ +-rw-r--r--   0        0        0     6850 2023-04-05 23:03:37.371806 esi_syncopy-2023.3/syncopy/__init__.py
│ │ +-rw-r--r--   0        0        0    17907 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/connectivity/AV_compRoutines.py
│ │ +-rw-r--r--   0        0        0    22230 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/connectivity/ST_compRoutines.py
│ │ +-rw-r--r--   0        0        0      252 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/connectivity/__init__.py
│ │ +-rw-r--r--   0        0        0    28388 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/connectivity/connectivity_analysis.py
│ │ +-rw-r--r--   0        0        0     6277 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/connectivity/csd.py
│ │ +-rw-r--r--   0        0        0     2537 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/connectivity/granger.py
│ │ +-rw-r--r--   0        0        0     7250 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/connectivity/wilson_sf.py
│ │ +-rw-r--r--   0        0        0      859 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/datatype/__init__.py
│ │ +-rw-r--r--   0        0        0    59351 2023-04-05 23:03:37.371806 esi_syncopy-2023.3/syncopy/datatype/base_data.py
│ │ +-rw-r--r--   0        0        0    30728 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/datatype/continuous_data.py
│ │ +-rw-r--r--   0        0        0    28178 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/datatype/discrete_data.py
│ │ +-rw-r--r--   0        0        0    21790 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/datatype/methods/arithmetic.py
│ │ +-rw-r--r--   0        0        0     2331 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/datatype/methods/copy.py
│ │ +-rw-r--r--   0        0        0    14482 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/datatype/methods/definetrial.py
│ │ +-rw-r--r--   0        0        0     8708 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/datatype/methods/redefinetrial.py
│ │ +-rw-r--r--   0        0        0    20139 2023-04-05 23:03:37.371806 esi_syncopy-2023.3/syncopy/datatype/methods/selectdata.py
│ │ +-rw-r--r--   0        0        0     7450 2023-04-05 11:26:16.676768 esi_syncopy-2023.3/syncopy/datatype/methods/show.py
│ │ +-rw-r--r--   0        0        0    41610 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/datatype/selector.py
│ │ +-rw-r--r--   0        0        0     3342 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/datatype/util.py
│ │ +-rw-r--r--   0        0        0      591 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/io/__init__.py
│ │ +-rw-r--r--   0        0        0    18316 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/io/load_ft.py
│ │ +-rw-r--r--   0        0        0    11592 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/io/load_nwb.py
│ │ +-rw-r--r--   0        0        0    13581 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/io/load_spy_container.py
│ │ +-rw-r--r--   0        0        0    39102 2023-04-05 23:03:37.371806 esi_syncopy-2023.3/syncopy/io/load_tdt.py
│ │ +-rw-r--r--   0        0        0    12147 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/io/save_spy_container.py
│ │ +-rw-r--r--   0        0        0     8532 2023-04-06 08:45:16.506910 esi_syncopy-2023.3/syncopy/io/utils.py
│ │ +-rw-r--r--   0        0        0      258 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/plotting/__init__.py
│ │ +-rw-r--r--   0        0        0     5589 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/plotting/_helpers.py
│ │ +-rw-r--r--   0        0        0     5239 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/plotting/_plotting.py
│ │ +-rw-r--r--   0        0        0     2039 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/plotting/config.py
│ │ +-rw-r--r--   0        0        0     5347 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/plotting/helpers.py
│ │ +-rw-r--r--   0        0        0     9335 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/plotting/mp_plotting.py
│ │ +-rw-r--r--   0        0        0    11023 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/plotting/sp_plotting.py
│ │ +-rw-r--r--   0        0        0     2244 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/plotting/spy_plotting.py
│ │ +-rw-r--r--   0        0        0      248 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/preproc/__init__.py
│ │ +-rw-r--r--   0        0        0    29554 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/preproc/compRoutines.py
│ │ +-rw-r--r--   0        0        0     6953 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/preproc/firws.py
│ │ +-rw-r--r--   0        0        0    14478 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/preproc/preprocessing.py
│ │ +-rw-r--r--   0        0        0     8623 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/preproc/resampledata.py
│ │ +-rw-r--r--   0        0        0     3944 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/preproc/resampling.py
│ │ +-rw-r--r--   0        0        0      647 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/shared/__init__.py
│ │ +-rw-r--r--   0        0        0    51088 2023-04-05 23:03:37.371806 esi_syncopy-2023.3/syncopy/shared/computational_routine.py
│ │ +-rw-r--r--   0        0        0     1826 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/shared/const_def.py
│ │ +-rw-r--r--   0        0        0     1674 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/shared/dask_helpers.py
│ │ +-rw-r--r--   0        0        0    16260 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/shared/errors.py
│ │ +-rw-r--r--   0        0        0      739 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/shared/filetypes.py
│ │ +-rw-r--r--   0        0        0    14519 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/shared/input_processors.py
│ │ +-rw-r--r--   0        0        0    38857 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/shared/kwarg_decorators.py
│ │ +-rw-r--r--   0        0        0     5905 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/shared/latency.py
│ │ +-rw-r--r--   0        0        0     8819 2023-04-06 08:45:16.506910 esi_syncopy-2023.3/syncopy/shared/log.py
│ │ +-rw-r--r--   0        0        0    13913 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/shared/metadata.py
│ │ +-rw-r--r--   0        0        0    30214 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/shared/parsers.py
│ │ +-rw-r--r--   0        0        0     1742 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/shared/queries.py
│ │ +-rw-r--r--   0        0        0    12045 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/shared/tools.py
│ │ +-rw-r--r--   0        0        0      829 2023-04-05 13:30:10.091089 esi_syncopy-2023.3/syncopy/specest/README.md
│ │ +-rw-r--r--   0        0        0      254 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/specest/__init__.py
│ │ +-rw-r--r--   0        0        0     1078 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/specest/_norm_spec.py
│ │ +-rw-r--r--   0        0        0    44187 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/specest/compRoutines.py
│ │ +-rw-r--r--   0        0        0     9165 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/specest/fooofspy.py
│ │ +-rw-r--r--   0        0        0    45570 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/specest/freqanalysis.py
│ │ +-rw-r--r--   0        0        0     4694 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/specest/mtmconvol.py
│ │ +-rw-r--r--   0        0        0     4446 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/specest/mtmfft.py
│ │ +-rw-r--r--   0        0        0     5550 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/specest/stft.py
│ │ +-rw-r--r--   0        0        0    13340 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/specest/superlet.py
│ │ +-rw-r--r--   0        0        0     3655 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/specest/wavelet.py
│ │ +-rw-r--r--   0        0        0      129 2023-04-05 11:26:16.680768 esi_syncopy-2023.3/syncopy/specest/wavelets/__init__.py
│ │ +-rw-r--r--   0        0        0    19699 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/specest/wavelets/transform.py
│ │ +-rw-r--r--   0        0        0    10511 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/specest/wavelets/wavelets.py
│ │ +-rw-r--r--   0        0        0      443 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/statistics/__init__.py
│ │ +-rw-r--r--   0        0        0    12921 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/statistics/compRoutines.py
│ │ +-rw-r--r--   0        0        0     6788 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/statistics/jackknifing.py
│ │ +-rw-r--r--   0        0        0     7166 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/statistics/psth.py
│ │ +-rw-r--r--   0        0        0     8734 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/statistics/spike_psth.py
│ │ +-rw-r--r--   0        0        0    17812 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/statistics/summary_stats.py
│ │ +-rw-r--r--   0        0        0     9407 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/statistics/timelockanalysis.py
│ │ +-rw-r--r--   0        0        0      958 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/README.md
│ │ +-rw-r--r--   0        0        0        0 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/__init__.py
│ │ +-rw-r--r--   0        0        0        0 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/backend/__init__.py
│ │ +-rwxr-xr-x   0        0        0     1796 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/backend/run_tests.sh
│ │ +-rw-r--r--   0        0        0     9750 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/backend/test_conn.py
│ │ +-rw-r--r--   0        0        0     9950 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/backend/test_fooofspy.py
│ │ +-rw-r--r--   0        0        0     4377 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/backend/test_resampling.py
│ │ +-rw-r--r--   0        0        0    14593 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/backend/test_timefreq.py
│ │ +-rw-r--r--   0        0        0     2708 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/conftest.py
│ │ +-rw-r--r--   0        0        0     3975 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/helpers.py
│ │ +-rw-r--r--   0        0        0     2037 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/local_spy.py
│ │ +-rw-r--r--   0        0        0    11500 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/misc.py
│ │ +-rwxr-xr-x   0        0        0      195 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/no_slurm.sh
│ │ +-rw-r--r--   0        0        0      646 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/run_tests.cmd
│ │ +-rwxr-xr-x   0        0        0     2530 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/run_tests.sh
│ │ +-rw-r--r--   0        0        0    14267 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/synth_data.py
│ │ +-rw-r--r--   0        0        0    13196 2023-04-05 23:03:37.371806 esi_syncopy-2023.3/syncopy/tests/test_attach_dataset.py
│ │ +-rw-r--r--   0        0        0    17727 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/test_basedata.py
│ │ +-rw-r--r--   0        0        0     5637 2023-04-05 23:03:37.371806 esi_syncopy-2023.3/syncopy/tests/test_cfg.py
│ │ +-rw-r--r--   0        0        0    23769 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/test_computationalroutine.py
│ │ +-rw-r--r--   0        0        0    35539 2023-04-05 23:03:37.371806 esi_syncopy-2023.3/syncopy/tests/test_connectivity.py
│ │ +-rw-r--r--   0        0        0    34902 2023-04-05 23:03:37.375806 esi_syncopy-2023.3/syncopy/tests/test_continuousdata.py
│ │ +-rw-r--r--   0        0        0      958 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/test_datatype_util.py
│ │ +-rw-r--r--   0        0        0     9276 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/test_decorators.py
│ │ +-rw-r--r--   0        0        0    25596 2023-04-05 23:03:37.375806 esi_syncopy-2023.3/syncopy/tests/test_discretedata.py
│ │ +-rw-r--r--   0        0        0     2959 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/test_info.py
│ │ +-rw-r--r--   0        0        0     5131 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/test_logging.py
│ │ +-rw-r--r--   0        0        0    21863 2023-04-05 23:03:37.375806 esi_syncopy-2023.3/syncopy/tests/test_metadata.py
│ │ +-rw-r--r--   0        0        0     3611 2023-04-06 08:45:16.506910 esi_syncopy-2023.3/syncopy/tests/test_packagesetup.py
│ │ +-rwxr-xr-x   0        0        0    13207 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/test_parsers.py
│ │ +-rw-r--r--   0        0        0    14282 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/test_plotting.py
│ │ +-rw-r--r--   0        0        0    20034 2023-04-05 23:03:37.375806 esi_syncopy-2023.3/syncopy/tests/test_preproc.py
│ │ +-rw-r--r--   0        0        0    10961 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/test_redefinetrial.py
│ │ +-rw-r--r--   0        0        0     8983 2023-04-05 23:03:37.375806 esi_syncopy-2023.3/syncopy/tests/test_resampledata.py
│ │ +-rw-r--r--   0        0        0    21742 2023-04-05 23:03:37.375806 esi_syncopy-2023.3/syncopy/tests/test_selectdata.py
│ │ +-rw-r--r--   0        0        0    67192 2023-04-05 11:26:16.684768 esi_syncopy-2023.3/syncopy/tests/test_specest.py
│ │ +-rw-r--r--   0        0        0     9735 2023-04-05 23:03:37.375806 esi_syncopy-2023.3/syncopy/tests/test_specest_fooof.py
│ │ +-rw-r--r--   0        0        0    13851 2023-04-05 23:03:37.375806 esi_syncopy-2023.3/syncopy/tests/test_spike_psth.py
│ │ +-rw-r--r--   0        0        0    25769 2023-04-05 11:26:16.688768 esi_syncopy-2023.3/syncopy/tests/test_spyio.py
│ │ +-rw-r--r--   0        0        0     3737 2023-04-05 11:26:16.688768 esi_syncopy-2023.3/syncopy/tests/test_spytools.py
│ │ +-rw-r--r--   0        0        0    23373 2023-04-05 23:03:37.375806 esi_syncopy-2023.3/syncopy/tests/test_statistics.py
│ │ +-rw-r--r--   0        0        0     4692 2023-04-05 11:26:16.688768 esi_syncopy-2023.3/syncopy/tests/test_synth_data.py
│ │ +-rw-r--r--   0        0        0     9213 2023-04-05 23:03:37.375806 esi_syncopy-2023.3/syncopy/tests/test_timelockanalysis.py
│ │ +-rw-r--r--   0        0        0     5112 2023-04-05 11:26:16.688768 esi_syncopy-2023.3/syncopy/tests/test_tools.py
│ │ +-rw-r--r--   0        0        0    16832 2023-04-05 23:03:37.375806 esi_syncopy-2023.3/syncopy/tests/test_welch.py
│ │ +-rw-r--r--   0        0        0     4805 1970-01-01 00:00:00.000000 esi_syncopy-2023.3/PKG-INFO
│ │   --- esi-syncopy-2022.8/LICENSE
│ ├── +++ esi_syncopy-2023.3/LICENSE
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/pyproject.toml
│ ├── +++ esi_syncopy-2023.3/pyproject.toml
│ │┄ Files 14% similar despite different names
│ │ @@ -1,13 +1,13 @@
│ │  [tool.poetry]
│ │  name = "esi-syncopy"
│ │  packages = [
│ │      {include = "syncopy"}
│ │  ]
│ │ -version = "2022.8"
│ │ +version = "2023.03"
│ │  license = "BSD-3-Clause"
│ │  readme="README.rst"
│ │  homepage="https://syncopy.org"
│ │  repository="https://github.com/esi-neuroscience/syncopy"
│ │  include = [
│ │      "LICENSE",
│ │  ]
│ │ @@ -17,34 +17,34 @@
│ │  	    "Framework :: Jupyter",
│ │  	    "Operating System :: OS Independent"
│ │  ]
│ │  description = "A toolkit for user-friendly large-scale electrophysiology data analysis. Syncopy is compatible with the Matlab toolbox FieldTrip."
│ │  authors = ["Stefan Fürtinger <sfuerti@esi-frankfurt.de>", "Tim Schäfer <tim.schaefer@esi-frankfurt.de>", "Joscha Schmiedt <schmiedt@uni-bremen.de>", "Gregor Mönke <gregor.moenke@esi-frankfurt.de>"]
│ │  
│ │  [tool.poetry.dependencies]
│ │ -# acme needs hard python version pinning
│ │ -python = "^3.8, <3.9"
│ │ +python = "^3.8"
│ │  h5py = ">=2.9"
│ │ +dask = {version=">=2022.6", extras=["distributed"]}
│ │ +dask-jobqueue = ">=0.8"
│ │  numpy = ">=1.10"
│ │  scipy = ">=1.5"
│ │  matplotlib = ">=3.5"
│ │  tqdm = ">=4.31"
│ │  natsort = "^8.1.0"
│ │ -psutil = ""
│ │ +psutil = ">=5.9"
│ │  fooof = ">=1.0"
│ │ -esi-acme = "2022.7"
│ │ -ipdb = "^0.13.9"
│ │ -memory-profiler = "^0.60.0"
│ │ -numpydoc = "^1.4.0"
│ │  
│ │ -[tool.poetry.dev-dependencies]
│ │ +[tool.poetry.group.dev.dependencies]
│ │  black = "^22.6.0"
│ │  pytest = "^7.0"
│ │  ipython = "^8.0"
│ │  pytest-cov = "^3.0.0"
│ │ -sphinx-bootstrap-theme = ""
│ │ +sphinx-book-theme = "^0.3.3"
│ │  sphinx-automodapi = "^0.14.1"
│ │ +numpydoc = "^1.4.0"
│ │ +ipdb = "^0.13.9"
│ │ +memory-profiler = "^0.60.0"
│ │  flake8 = "^3.9"
│ │  
│ │  [build-system]
│ │  requires = ["poetry-core>=1.0.0"]
│ │  build-backend = "poetry.core.masonry.api"
│ │   --- esi-syncopy-2022.8/syncopy/datatype/__init__.py
│ ├── +++ esi_syncopy-2023.3/syncopy/datatype/__init__.py
│ │┄ Files 13% similar despite different names
│ │ @@ -1,29 +1,28 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │  # Populate namespace with datatype routines and classes
│ │  #
│ │  
│ │  # Import __all__ routines from local modules
│ │ -from . import base_data, continuous_data, discrete_data, methods, statistical_data
│ │ +from . import base_data, continuous_data, discrete_data, methods
│ │  from .base_data import *
│ │  from .continuous_data import *
│ │  from .discrete_data import *
│ │ -from .statistical_data import *
│ │  from .methods.definetrial import *
│ │ -from .methods.padding import *
│ │  from .methods.selectdata import *
│ │  from .methods.show import *
│ │  from .methods.copy import *
│ │ +from .methods.redefinetrial import *
│ │ +from .util import *
│ │  
│ │  # Populate local __all__ namespace
│ │  __all__ = []
│ │  __all__.extend(base_data.__all__)
│ │  __all__.extend(continuous_data.__all__)
│ │  __all__.extend(discrete_data.__all__)
│ │ -__all__.extend(statistical_data.__all__)
│ │ +__all__.extend(util.__all__)
│ │  __all__.extend(methods.definetrial.__all__)
│ │ -# this is broken / has no current use case
│ │ -# __all__.extend(methods.padding.__all__)
│ │  __all__.extend(methods.selectdata.__all__)
│ │  __all__.extend(methods.show.__all__)
│ │  __all__.extend(methods.copy.__all__)
│ │ +__all__.extend(methods.redefinetrial.__all__)
│ │   --- esi-syncopy-2022.8/syncopy/datatype/continuous_data.py
│ ├── +++ esi_syncopy-2023.3/syncopy/datatype/continuous_data.py
│ │┄ Files 8% similar despite different names
│ │ @@ -19,33 +19,36 @@
│ │  from .methods.definetrial import definetrial
│ │  from syncopy.shared.parsers import scalar_parser, array_parser
│ │  from syncopy.shared.errors import SPYValueError, SPYWarning
│ │  from syncopy.shared.tools import best_match
│ │  from syncopy.plotting import sp_plotting, mp_plotting
│ │  
│ │  
│ │ -__all__ = ["AnalogData", "SpectralData", "CrossSpectralData"]
│ │ +__all__ = ["AnalogData", "SpectralData", "CrossSpectralData", "TimeLockData"]
│ │  
│ │  
│ │  class ContinuousData(BaseData, ABC):
│ │      """Abstract class for uniformly sampled data
│ │  
│ │      Notes
│ │      -----
│ │      This class cannot be instantiated. Use one of the children instead.
│ │  
│ │      """
│ │  
│ │      _infoFileProperties = BaseData._infoFileProperties + ("samplerate", "channel",)
│ │ -    _hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties + ("samplerate", "channel",)
│ │      _hdfFileDatasetProperties = BaseData._hdfFileDatasetProperties + ("data",)
│ │ +    # all continuous data types have a time axis
│ │ +    _selectionKeyWords = BaseData._selectionKeyWords + ('latency',)
│ │  
│ │      @property
│ │      def data(self):
│ │ -        """array-like object representing data without trials
│ │ +        """
│ │ +        HDF5 dataset property representing contiguous
│ │ +        data without trialdefinition.
│ │  
│ │          Trials are concatenated along the time axis.
│ │          """
│ │  
│ │          if getattr(self._data, "id", None) is not None:
│ │              if self._data.id.valid == 0:
│ │                  lgl = "open HDF5 file"
│ │ @@ -177,36 +180,14 @@
│ │      @property
│ │      def time(self):
│ │          """list(float): trigger-relative time axes of each trial """
│ │          if self.samplerate is not None and self.sampleinfo is not None:
│ │              return [(np.arange(0, stop - start) + self._t0[tk]) / self.samplerate \
│ │                      for tk, (start, stop) in enumerate(self.sampleinfo)]
│ │  
│ │ -    # # Helper function that reads a single trial into memory
│ │ -    # @staticmethod
│ │ -    # def _copy_trial(trialno, filename, dimord, sampleinfo):
│ │ -    #     """
│ │ -    #     # FIXME: currently unused - check back to see if we need this functionality
│ │ -    #     """
│ │ -    #     idx = [slice(None)] * len(dimord)
│ │ -    #     idx[dimord.index("time")] = slice(int(sampleinfo[trialno, 0]), int(sampleinfo[trialno, 1]))
│ │ -    #     idx = tuple(idx)
│ │ -    #     try:
│ │ -    #         with h5py.File(filename, mode="r") as h5f:
│ │ -    #             h5keys = list(h5f.keys())
│ │ -    #             cnt = [h5keys.count(dclass) for dclass in spy.datatype.__all__
│ │ -    #                    if not inspect.isfunction(getattr(spy.datatype, dclass))]
│ │ -    #             if len(h5keys) == 1:
│ │ -    #                 arr = h5f[h5keys[0]][idx]
│ │ -    #             else:
│ │ -    #                 arr = h5f[spy.datatype.__all__[cnt.index(1)]][idx]
│ │ -    #     except:
│ │ -    #         raise SPYIOError(filename)
│ │ -    #     return arr
│ │ -
│ │      # Helper function that grabs a single trial
│ │      def _get_trial(self, trialno):
│ │          idx = [slice(None)] * len(self.dimord)
│ │          idx[self._stackingDim] = slice(int(self.sampleinfo[trialno, 0]), int(self.sampleinfo[trialno, 1]))
│ │          return self._data[tuple(idx)]
│ │  
│ │      def _is_empty(self):
│ │ @@ -248,15 +229,15 @@
│ │          shp[self._stackingDim] = stop - start
│ │          idx[self._stackingDim] = slice(start, stop)
│ │  
│ │          # process existing data selections
│ │          if self.selection is not None:
│ │  
│ │              # time-selection is most delicate due to trial-offset
│ │ -            tsel = self.selection.time[self.selection.trials.index(trialno)]
│ │ +            tsel = self.selection.time[self.selection.trial_ids.index(trialno)]
│ │              if isinstance(tsel, slice):
│ │                  if tsel.start is not None:
│ │                      tstart = tsel.start
│ │                  else:
│ │                      tstart = 0
│ │                  if tsel.stop is not None:
│ │                      tstop = tsel.stop
│ │ @@ -302,14 +283,15 @@
│ │  
│ │          return FauxTrial(shp, tuple(idx), self.data.dtype, self.dimord)
│ │  
│ │      # Helper function that extracts timing-related indices
│ │      def _get_time(self, trials, toi=None, toilim=None):
│ │          """
│ │          Get relative by-trial indices of time-selections
│ │ +        `toi` is legacy.. `toilim ` is used by selections via `latency`
│ │  
│ │          Parameters
│ │          ----------
│ │          trials : list
│ │              List of trial-indices to perform selection on
│ │          toi : None or list
│ │              Time-points to be selected (in seconds) on a by-trial scale.
│ │ @@ -322,22 +304,22 @@
│ │              List of by-trial sample-indices corresponding to provided
│ │              time-selection. If both `toi` and `toilim` are `None`, `timing`
│ │              is a list of universal (i.e., ``slice(None)``) selectors.
│ │  
│ │          Notes
│ │          -----
│ │          This class method is intended to be solely used by
│ │ -        :class:`syncopy.datatype.base_data.Selector` objects and thus has purely
│ │ +        :class:`syncopy.datatype.selector.Selector` objects and thus has purely
│ │          auxiliary character. Therefore, all input sanitization and error checking
│ │ -        is left to :class:`syncopy.datatype.base_data.Selector` and not
│ │ +        is left to :class:`syncopy.datatype.selector.Selector` and not
│ │          performed here.
│ │  
│ │          See also
│ │          --------
│ │ -        syncopy.datatype.base_data.Selector : Syncopy data selectors
│ │ +        syncopy.datatype.selector.Selector : Syncopy data selectors
│ │          """
│ │          timing = []
│ │          if toilim is not None:
│ │              for trlno in trials:
│ │                  _, selTime = best_match(self.time[trlno], toilim, span=True)
│ │                  selTime = selTime.tolist()
│ │                  if len(selTime) > 1:
│ │ @@ -368,15 +350,17 @@
│ │          self._data = None
│ │  
│ │          self.samplerate = samplerate     # use setter for error-checking
│ │  
│ │          # Call initializer
│ │          super().__init__(data=data, **kwargs)
│ │  
│ │ -        self.channel = channel
│ │ +        # might be set from concatenation
│ │ +        if self.channel is None:
│ │ +            self.channel = channel
│ │  
│ │          if self.data is not None:
│ │  
│ │              # In case of manual data allocation (reading routine would leave a
│ │              # mark in `cfg`), fill in missing info
│ │              if self.sampleinfo is None:
│ │  
│ │ @@ -403,14 +387,15 @@
│ │  
│ │      Data is only read from disk on demand, similar to HDF5 files.
│ │      """
│ │  
│ │      _infoFileProperties = ContinuousData._infoFileProperties
│ │      _defaultDimord = ["time", "channel"]
│ │      _stackingDimLabel = "time"
│ │ +    _selectionKeyWords = ContinuousData._selectionKeyWords + ('channel',)
│ │  
│ │      # "Constructor"
│ │      def __init__(self,
│ │                   data=None,
│ │                   filename=None,
│ │                   trialdefinition=None,
│ │                   samplerate=None,
│ │ @@ -439,25 +424,28 @@
│ │          --------
│ │          :func:`syncopy.definetrial`
│ │  
│ │          """
│ │  
│ │          # FIXME: I think escalating `dimord` to `BaseData` should be sufficient so that
│ │          # the `if any(key...) loop in `BaseData.__init__()` takes care of assigning a default dimord
│ │ -        if data is not None and dimord is None:
│ │ +        if dimord is None:
│ │              dimord = self._defaultDimord
│ │  
│ │          # Call parent initializer
│ │          super().__init__(data=data,
│ │                           filename=filename,
│ │                           trialdefinition=trialdefinition,
│ │                           samplerate=samplerate,
│ │                           channel=channel,
│ │                           dimord=dimord)
│ │  
│ │ +        # set as instance attribute to allow modification
│ │ +        self._hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties + ("samplerate", "channel",)
│ │ +
│ │      # implement plotting
│ │      def singlepanelplot(self, shifted=True, **show_kwargs):
│ │  
│ │          figax = sp_plotting.plot_AnalogData(self, shifted, **show_kwargs)
│ │          return figax
│ │  
│ │      def multipanelplot(self, **show_kwargs):
│ │ @@ -471,16 +459,19 @@
│ │      Multi-channel, real or complex spectral data
│ │  
│ │      This class can be used for representing any data with a frequency, channel,
│ │      and optionally a time axis. The datatype can be complex or float.
│ │      """
│ │  
│ │      _infoFileProperties = ContinuousData._infoFileProperties + ("taper", "freq",)
│ │ +    _hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties +\
│ │ +        ("samplerate", "channel", "freq",)
│ │      _defaultDimord = ["time", "taper", "freq", "channel"]
│ │      _stackingDimLabel = "time"
│ │ +    _selectionKeyWords = ContinuousData._selectionKeyWords + ('channel', 'frequency', 'taper',)
│ │  
│ │      @property
│ │      def taper(self):
│ │          """ :class:`numpy.ndarray` : list of window functions used """
│ │          if self._taper is None and self._data is not None:
│ │              nTaper = self.data.shape[self.dimord.index("taper")]
│ │              return np.array(["taper" + str(i + 1).zfill(len(str(nTaper)))
│ │ @@ -521,27 +512,23 @@
│ │              self._freq = None
│ │              return
│ │  
│ │          if self.data is None:
│ │              print("Syncopy core - freq: Cannot assign `freq` without data. "+\
│ │                    "Please assing data first")
│ │              return
│ │ -        try:
│ │ -
│ │ -            array_parser(freq, varname="freq", hasnan=False, hasinf=False,
│ │ -                         dims=(self.data.shape[self.dimord.index("freq")],))
│ │ -        except Exception as exc:
│ │ -            raise exc
│ │  
│ │ +        array_parser(freq, varname="freq", hasnan=False, hasinf=False,
│ │ +                     dims=(self.data.shape[self.dimord.index("freq")],))
│ │          self._freq = np.array(freq)
│ │  
│ │      # Helper function that extracts frequency-related indices
│ │      def _get_freq(self, foi=None, foilim=None):
│ │          """
│ │ -        Coming soon...
│ │ +        `foi` is legacy, we use `foilim` for frequency selection
│ │          Error checking is performed by `Selector` class
│ │          """
│ │          if foilim is not None:
│ │              _, selFreq = best_match(self.freq, foilim, span=True)
│ │              selFreq = selFreq.tolist()
│ │              if len(selFreq) > 1:
│ │                  selFreq = slice(selFreq[0], selFreq[-1] + 1, 1)
│ │ @@ -570,48 +557,46 @@
│ │                   freq=None,
│ │                   dimord=None):
│ │  
│ │          self._taper = None
│ │          self._freq = None
│ │  
│ │          # FIXME: See similar comment above in `AnalogData.__init__()`
│ │ -        if data is not None and dimord is None:
│ │ +        if dimord is None:
│ │              dimord = self._defaultDimord
│ │  
│ │          # Call parent initializer
│ │          super().__init__(data=data,
│ │                           filename=filename,
│ │                           trialdefinition=trialdefinition,
│ │                           samplerate=samplerate,
│ │                           channel=channel,
│ │ -                         taper=taper,
│ │ -                         freq=freq,
│ │                           dimord=dimord)
│ │  
│ │          # If __init__ attached data, be careful
│ │          if self.data is not None:
│ │ -
│ │              # In case of manual data allocation (reading routine would leave a
│ │              # mark in `cfg`), fill in missing info
│ │              if len(self.cfg) == 0:
│ │ -                self.freq = freq
│ │ -                self.taper = taper
│ │ +                # concat operations will set this!
│ │ +                if self.freq is None:
│ │ +                    self.freq = freq
│ │ +                if self.taper is None:
│ │ +                    self.taper = taper
│ │  
│ │          # Dummy assignment: if we have no data but freq/taper labels,
│ │          # assign bogus to trigger setter warnings
│ │          else:
│ │ -            if freq is not None:
│ │ -                self.freq = [1]
│ │ -            if taper is not None:
│ │ -                self.taper = ['taper']
│ │ +            self.freq = freq
│ │ +            self.taper = taper
│ │  
│ │      # implement plotting
│ │ -    def singlepanelplot(self, **show_kwargs):
│ │ +    def singlepanelplot(self, logscale=True, **show_kwargs):
│ │  
│ │ -        figax = sp_plotting.plot_SpectralData(self, **show_kwargs)
│ │ +        figax = sp_plotting.plot_SpectralData(self, logscale, **show_kwargs)
│ │          return figax
│ │  
│ │      def multipanelplot(self, **show_kwargs):
│ │  
│ │          figax = mp_plotting.plot_SpectralData(self, **show_kwargs)
│ │          return figax
│ │  
│ │ @@ -627,14 +612,15 @@
│ │      # Adapt `infoFileProperties` and `hdfFileAttributeProperties` from `ContinuousData`
│ │      _infoFileProperties = BaseData._infoFileProperties +\
│ │          ("samplerate", "channel_i", "channel_j", "freq", )
│ │      _hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties +\
│ │          ("samplerate", "channel_i", "channel_j", "freq", )
│ │      _defaultDimord = ["time", "freq", "channel_i", "channel_j"]
│ │      _stackingDimLabel = "time"
│ │ +    _selectionKeyWords = ContinuousData._selectionKeyWords + ('channel_i', 'channel_j', 'frequency',)
│ │      _channel_i = None
│ │      _channel_j = None
│ │      _samplerate = None
│ │      _data = None
│ │  
│ │      # Steal frequency-related stuff from `SpectralData`
│ │      _get_freq = SpectralData._get_freq
│ │ @@ -718,149 +704,147 @@
│ │                   filename=None,
│ │                   channel_i=None,
│ │                   channel_j=None,
│ │                   samplerate=None,
│ │                   freq=None,
│ │                   dimord=None):
│ │  
│ │ +        self._freq = None
│ │          # Set dimensional labels
│ │          self.dimord = dimord
│ │ -        # set frequencies
│ │ -        self.freq = freq
│ │  
│ │          # Call parent initializer
│ │          super().__init__(data=data,
│ │                           filename=filename,
│ │                           samplerate=samplerate,
│ │ -                         freq=freq,
│ │                           dimord=dimord)
│ │  
│ │ +        if freq is not None:
│ │ +            # set frequencies
│ │ +            self.freq = freq
│ │ +
│ │ +
│ │      def singlepanelplot(self, **show_kwargs):
│ │  
│ │          sp_plotting.plot_CrossSpectralData(self, **show_kwargs)
│ │  
│ │  
│ │  class TimeLockData(ContinuousData):
│ │  
│ │      """
│ │      Multi-channel, uniformly-sampled, time-locked data.
│ │      """
│ │  
│ │ +    _infoFileProperties = ContinuousData._infoFileProperties
│ │      _defaultDimord = ["time", "channel"]
│ │ +    _selectionKeyWords = ContinuousData._selectionKeyWords + ('channel',)
│ │      _stackingDimLabel = "time"
│ │  
│ │      # "Constructor"
│ │      def __init__(self,
│ │                   data=None,
│ │                   filename=None,
│ │                   trialdefinition=None,
│ │                   samplerate=None,
│ │                   channel=None,
│ │                   dimord=None):
│ │  
│ │ -        """Initialize an :class:`TimeLockData` object.
│ │ +        """
│ │ +        Initialize an :class:`TimeLockData` object.
│ │  
│ │ -            Parameters
│ │ -            ----------
│ │ -                data : 2D :class:numpy.ndarray or HDF5 dataset
│ │ -                    multi-channel time series data with uniform sampling
│ │ -                filename : str
│ │ -                    path to target filename that should be used for writing
│ │ -                samplerate : float
│ │ -                    sampling rate in Hz
│ │ -                channel : str or list/array(str)
│ │ -                dimord : list(str)
│ │ -                    ordered list of dimension labels
│ │ -
│ │ -            1. `filename` + `data` : create hdf dataset incl. sampleinfo @filename
│ │ -            2. just `data` : try to attach data (error checking done by :meth:`AnalogData.data.setter`)
│ │ -
│ │ -            See also
│ │ -            --------
│ │ -            :func:`syncopy.definetrial`
│ │ +        Parameters
│ │ +        ----------
│ │ +        data : 2D :class:numpy.ndarray or HDF5 dataset
│ │ +            multi-channel time series data with uniform sampling
│ │ +        filename : str
│ │ +            path to target filename that should be used for writing
│ │ +        samplerate : float
│ │ +            sampling rate in Hz
│ │ +        channel : str or list/array(str)
│ │ +        dimord : list(str)
│ │ +            ordered list of dimension labels
│ │  
│ │ -            """
│ │ +        See also
│ │ +        --------
│ │ +        :func:`syncopy.definetrial`
│ │ +        """
│ │  
│ │ -        if data is not None and dimord is None:
│ │ +        if dimord is None:
│ │              dimord = self._defaultDimord
│ │  
│ │ -        self._avg = None
│ │ -        self._var = None
│ │ -
│ │ -        # for stacking, we would have to
│ │ -        # re-define the trialdefinition here?!
│ │ -
│ │          # Call parent initializer
│ │          # trialdefinition has to come from a CR!
│ │          super().__init__(data=data,
│ │                           filename=filename,
│ │                           trialdefinition=trialdefinition,
│ │                           samplerate=samplerate,
│ │                           channel=channel,
│ │                           dimord=dimord)
│ │  
│ │ -    @property
│ │ -    def avg(self):
│ │ -        """
│ │ -        The 'single trial' sized trial average stacked at the second last
│ │ -        position (which could be the first if no single trials are stored)"""
│ │ +        # A `h5py.Dataset` holding the average of `data`, or `None` if not computed yet.
│ │ +        self._avg = None
│ │  
│ │ -        # stacking stub
│ │ -        # if self._avg is None and self._data is not None:
│ │ -        #     # all channels
│ │ -        #     nStacked = len(self.trials)
│ │ -        #     self._avg = self._get_trial(nStacked - 2)
│ │ +        # A `h5py.Dataset` holding variance of `data`, or `None` if not computed yet.
│ │ +        self._var = None
│ │  
│ │ -        return self._avg
│ │ +        # A `h5py.Dataset` holding covariance of `data`, or `None` if not computed yet.
│ │ +        self._cov = None
│ │  
│ │ -    @avg.setter
│ │ -    def avg(self, trl_av):
│ │ +        # set as instance attribute to allow modification
│ │ +        self._hdfFileDatasetProperties = ContinuousData._hdfFileDatasetProperties + ("avg", "var", "cov",)
│ │  
│ │ -        """
│ │ -        :class:`numpy.ndarray`: time x nChannel/nUnits
│ │ -        Set single-trial sized average """
│ │ -        pass
│ │ +    @property
│ │ +    def avg(self):
│ │ +        return self._avg
│ │  
│ │      @property
│ │      def var(self):
│ │ -        """ :class:`numpy.ndarray`: time x nChannel / nUnits
│ │ -        The 'single trial' sized variance over trials stacked at the last
│ │ -        position (which could be the 2nd if no single trials are stored)"""
│ │ -
│ │ -        # stacking stub
│ │ -        # if self._var is None and self._data is not None:
│ │ -        #     # all channels
│ │ -        #     nStacked = len(self.trials)
│ │ -        #     self._var = self._get_trial(nStacked - 1)
│ │ -
│ │          return self._var
│ │  
│ │ +    @property
│ │ +    def cov(self):
│ │ +        return self._cov
│ │ +
│ │      @ContinuousData.trialdefinition.setter
│ │ -    def trialdefinition(self, trl):
│ │ +    def trialdefinition(self, trldef):
│ │          """
│ │          Override trialdefinition setter, which is special for time-locked data:
│ │          all trials have to have the same length and relative timings.
│ │  
│ │ -        So the trialdefinition has 0 offsets everywhere, and it has the general
│ │ +        So the trialdefinition has the same offsets everywhere, and it has the general
│ │          simple structure:
│ │ -                              [[0, nTime, 0],
│ │ -                              [nTime, 2 * nTime, 0],
│ │ -                              [2 * nTime, 3 * nTime, 0],
│ │ +                              [[0, nSamples, offset],
│ │ +                              [nSamples, 2 * nSamples, offset],
│ │ +                              [2 * nSamples, 3 * nSamples, offset],
│ │                                ...]
│ │          """
│ │  
│ │          # first harness all parsers here
│ │ -        _definetrial(self, trialdefinition=trl)
│ │ +        _definetrial(self, trialdefinition=trldef)
│ │  
│ │          # now check for additional conditions
│ │ -        if not np.all(trl[:, 2] == 0):
│ │ -            self.trialdefinition = None
│ │ -            lgl = "no offsets for timelocked data"
│ │ -            act = "non-zero offsets"
│ │ -            raise SPYValueError(lgl, varname="trialdefinition", actual=act)
│ │ +
│ │ +        # FIXME: not clear, is timelocked data to be expected
│ │ +        # to have same offsets?!
│ │ +        # if not np.unique(trldef[:, 2]).size == 1:
│ │ +        #     lgl = "equal offsets for timelocked data"
│ │ +        #     act = "different offsets"
│ │ +        #     raise SPYValueError(lgl, varname="trialdefinition", actual=act)
│ │  
│ │          # diff-diff should give 0 -> same number of samples for each trial
│ │ -        if not np.all(np.diff(trl, axis=0, n=2) == 0):
│ │ -            self.trialdefinition = None
│ │ -            lgl = "all trials/entities of same length for timelocked data"
│ │ -            act = "non-equally sized trials defined"
│ │ +        if not np.all(np.diff(trldef, axis=0, n=2) == 0):
│ │ +            lgl = "all trials of same length for timelocked data"
│ │ +            act = "unequal sized trials defined"
│ │              raise SPYValueError(lgl, varname="trialdefinition", actual=act)
│ │ +
│ │ +    # TODO - overload `time` property, as there is only one by definition!
│ │ +
│ │ +    # implement plotting
│ │ +    def singlepanelplot(self, shifted=True, **show_kwargs):
│ │ +
│ │ +        figax = sp_plotting.plot_AnalogData(self, shifted, **show_kwargs)
│ │ +        return figax
│ │ +
│ │ +    def multipanelplot(self, **show_kwargs):
│ │ +
│ │ +        figax = mp_plotting.plot_AnalogData(self, **show_kwargs)
│ │ +        return figax
│ │   --- esi-syncopy-2022.8/syncopy/datatype/discrete_data.py
│ ├── +++ esi_syncopy-2023.3/syncopy/datatype/discrete_data.py
│ │┄ Files 23% similar despite different names
│ │ @@ -7,57 +7,61 @@
│ │  import numpy as np
│ │  from abc import ABC
│ │  from collections.abc import Iterator
│ │  import inspect
│ │  
│ │  
│ │  # Local imports
│ │ -from .base_data import BaseData, Indexer, FauxTrial
│ │ +from .base_data import BaseData, FauxTrial
│ │  from .methods.definetrial import definetrial
│ │  from syncopy.shared.parsers import scalar_parser, array_parser
│ │ -from syncopy.shared.errors import SPYValueError
│ │ +from syncopy.shared.errors import SPYValueError, SPYError, SPYTypeError
│ │  from syncopy.shared.tools import best_match
│ │  
│ │  __all__ = ["SpikeData", "EventData"]
│ │  
│ │  
│ │  class DiscreteData(BaseData, ABC):
│ │ -    """Abstract class for non-uniformly sampled data where only time-stamps are recorded
│ │ +    """
│ │ +    Abstract class for non-uniformly sampled data where only time-stamps are recorded.
│ │  
│ │      Notes
│ │      -----
│ │      This class cannot be instantiated. Use one of the children instead.
│ │      """
│ │  
│ │      _infoFileProperties = BaseData._infoFileProperties + ("samplerate", )
│ │      _hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties + ("samplerate",)
│ │ -    _hdfFileDatasetProperties = BaseData._hdfFileDatasetProperties + ("data",)
│ │ +    _selectionKeyWords = BaseData._selectionKeyWords + ('latency',)
│ │  
│ │      @property
│ │      def data(self):
│ │ -        """array-like object representing data without trials
│ │ +        """
│ │ +        Array-like object representing data without trials.
│ │  
│ │          Trials are concatenated along the time axis.
│ │          """
│ │  
│ │          if getattr(self._data, "id", None) is not None:
│ │              if self._data.id.valid == 0:
│ │                  lgl = "open HDF5 file"
│ │                  act = "backing HDF5 file {} has been closed"
│ │                  raise SPYValueError(legal=lgl, actual=act.format(self.filename),
│ │                                      varname="data")
│ │          return self._data
│ │  
│ │      @data.setter
│ │      def data(self, inData):
│ │ -
│ │ +        """ Also checks for integer type of data """
│ │ +        # this comes from BaseData
│ │          self._set_dataset_property(inData, "data")
│ │  
│ │ -        if inData is None:
│ │ -            return
│ │ +        if inData is not None:
│ │ +            if not np.issubdtype(self.data.dtype, np.integer):
│ │ +                raise SPYTypeError(self.data.dtype, 'data', "integer like")
│ │  
│ │      def __str__(self):
│ │          # Get list of print-worthy attributes
│ │          ppattrs = [attr for attr in self.__dir__()
│ │                     if not (attr.startswith("_") or attr in ["log", "trialdefinition"])]
│ │          ppattrs = [attr for attr in ppattrs
│ │                     if not (inspect.ismethod(getattr(self, attr))
│ │ @@ -113,19 +117,19 @@
│ │          return ppstr
│ │  
│ │      @property
│ │      def sample(self):
│ │          """Indices of all recorded samples"""
│ │          if self.data is None:
│ │              return None
│ │ -        return np.unique(self.data[:, self.dimord.index("sample")])
│ │ +        return self.data[:, self.dimord.index("sample")]
│ │  
│ │      @property
│ │      def samplerate(self):
│ │ -        """float: underlying sampling rate of non-uniformly data acquisition"""
│ │ +        """float: underlying sampling rate of non-uniform data acquisition"""
│ │          return self._samplerate
│ │  
│ │      @samplerate.setter
│ │      def samplerate(self, sr):
│ │          if sr is None:
│ │              self._samplerate = None
│ │              return
│ │ @@ -151,46 +155,39 @@
│ │      @trialid.setter
│ │      def trialid(self, trlid):
│ │          if trlid is None:
│ │              self._trialid = None
│ │              return
│ │  
│ │          if self.data is None:
│ │ -            print("SyNCoPy core - trialid: Cannot assign `trialid` without data. " +
│ │ -                  "Please assing data first")
│ │ +            SPYError("SyNCoPy core - trialid: Cannot assign `trialid` without data. " +
│ │ +                     "Please assign data first")
│ │ +            return
│ │ +        if (self.data.shape[0] == 0) and (trlid.shape[0] == 0):
│ │ +            self._trialid = np.array(trlid, dtype=int)
│ │              return
│ │          scount = np.nanmax(self.data[:, self.dimord.index("sample")])
│ │          try:
│ │              array_parser(trlid, varname="trialid", dims=(self.data.shape[0],),
│ │                           hasnan=False, hasinf=False, ntype="int_like", lims=[-1, scount])
│ │          except Exception as exc:
│ │              raise exc
│ │          self._trialid = np.array(trlid, dtype=int)
│ │  
│ │      @property
│ │ -    def trials(self):
│ │ -        """list-like([sample x (>=2)] :class:`numpy.ndarray`) : trial slices of :attr:`data` property"""
│ │ -        if self.trialid is not None:
│ │ -            valid_trls = np.unique(self.trialid[self.trialid >= 0])
│ │ -            return Indexer(map(self._get_trial, valid_trls),
│ │ -                           valid_trls.size)
│ │ -        else:
│ │ -            return None
│ │ -
│ │ -    @property
│ │      def trialtime(self):
│ │          """list(:class:`numpy.ndarray`): trigger-relative sample times in s"""
│ │          if self.samplerate is not None and self.sampleinfo is not None:
│ │ -            sample0 = self.sampleinfo[:,0] - self._t0[:]
│ │ +            sample0 = self.sampleinfo[:, 0] - self._t0[:]
│ │              sample0 = np.append(sample0, np.nan)[self.trialid]
│ │ -            return (self.data[:,self.dimord.index("sample")] - sample0)/self.samplerate
│ │ +            return (self.data[:, self.dimord.index("sample")] - sample0) / self.samplerate
│ │  
│ │      # Helper function that grabs a single trial
│ │      def _get_trial(self, trialno):
│ │ -        return self._data[self.trialid == trialno, :]
│ │ +        return self._data[self._trialslice[trialno], :]
│ │  
│ │      # Helper function that spawns a `FauxTrial` object given actual trial information
│ │      def _preview_trial(self, trialno):
│ │          """
│ │          Generate a `FauxTrial` instance of a trial
│ │  
│ │          Parameters
│ │ @@ -213,77 +210,80 @@
│ │          reports to only contain 2 channels)
│ │  
│ │          See also
│ │          --------
│ │          syncopy.datatype.base_data.FauxTrial : class definition and further details
│ │          syncopy.shared.computational_routine.ComputationalRoutine : Syncopy compute engine
│ │          """
│ │ -
│ │ -        trialIdx = np.where(self.trialid == trialno)[0]
│ │ +        trlSlice = self._trialslice[trialno]
│ │ +        trialIdx = np.arange(trlSlice.start, trlSlice.stop) #np.where(self.trialid == trialno)[0]
│ │          nCol = len(self.dimord)
│ │ -        idx = [trialIdx.tolist(), slice(0, nCol)]
│ │ +        idx = [[], slice(0, nCol)]
│ │          if self.selection is not None: # selections are harmonized, just take `.time`
│ │ -            idx[0] = trialIdx[self.selection.time[self.selection.trials.index(trialno)]].tolist()
│ │ +            idx[0] = trialIdx[self.selection.time[self.selection.trial_ids.index(trialno)]].tolist()
│ │ +        else:
│ │ +            idx[0] = trialIdx.tolist()
│ │ +
│ │          shp = [len(idx[0]), nCol]
│ │  
│ │          return FauxTrial(shp, tuple(idx), self.data.dtype, self.dimord)
│ │  
│ │      # Helper function that extracts by-trial timing-related indices
│ │      def _get_time(self, trials, toi=None, toilim=None):
│ │          """
│ │ -        Get relative by-trial indices of time-selections
│ │ +        Get relative by-trial indices of time-selections.
│ │  
│ │          Parameters
│ │          ----------
│ │          trials : list
│ │ -            List of trial-indices to perform selection on
│ │ +            List of trial-indices to perform selection on.
│ │          toi : None or list
│ │              Time-points to be selected (in seconds) on a by-trial scale.
│ │          toilim : None or list
│ │ -            Time-window to be selected (in seconds) on a by-trial scale
│ │ +            Time-window to be selected (in seconds) on a by-trial scale.
│ │  
│ │          Returns
│ │          -------
│ │          timing : list of lists
│ │              List of by-trial sample-indices corresponding to provided
│ │              time-selection. If both `toi` and `toilim` are `None`, `timing`
│ │              is a list of universal (i.e., ``slice(None)``) selectors.
│ │  
│ │          Notes
│ │          -----
│ │          This class method is intended to be solely used by
│ │ -        :class:`syncopy.datatype.base_data.Selector` objects and thus has purely
│ │ +        :class:`syncopy.datatype.selector.Selector` objects and thus has purely
│ │          auxiliary character. Therefore, all input sanitization and error checking
│ │ -        is left to :class:`syncopy.datatype.base_data.Selector` and not
│ │ +        is left to :class:`syncopy.datatype.selector.Selector` and not
│ │          performed here.
│ │  
│ │          See also
│ │          --------
│ │ -        syncopy.datatype.base_data.Selector : Syncopy data selectors
│ │ +        syncopy.datatype.selector.Selector : Syncopy data selectors
│ │          """
│ │          timing = []
│ │          if toilim is not None:
│ │              allTrials = self.trialtime
│ │              for trlno in trials:
│ │ -                trlTime = allTrials[self.trialid == trlno]
│ │ +                trlTime = allTrials[self._trialslice[trlno]]
│ │                  _, selTime = best_match(trlTime, toilim, span=True)
│ │                  selTime = selTime.tolist()
│ │                  if len(selTime) > 1 and np.diff(trlTime).min() > 0:
│ │                      timing.append(slice(selTime[0], selTime[-1] + 1, 1))
│ │                  else:
│ │                      timing.append(selTime)
│ │  
│ │          elif toi is not None:
│ │              allTrials = self.trialtime
│ │              for trlno in trials:
│ │ -                trlTime = allTrials[self.trialid == trlno]
│ │ +                trlTime = allTrials[self._trialslice[trlno]]
│ │                  _, arrayIdx = best_match(trlTime, toi)
│ │                  # squash duplicate values then readd
│ │                  _, xdi = np.unique(trlTime[arrayIdx], return_index=True)
│ │ -                arrayIdx = arrayIdx[np.sort(xdi)]
│ │ +                arrayIdx = arrayIdx[xdi] # we assume sorted data
│ │                  selTime = []
│ │                  for t in arrayIdx:
│ │                      selTime += np.where(trlTime[t] == trlTime)[0].tolist()
│ │                  # convert to slice if possible
│ │                  if len(selTime) > 1:
│ │                      timeSteps = np.diff(selTime)
│ │                      if timeSteps.min() == timeSteps.max() == 1:
│ │ @@ -293,31 +293,37 @@
│ │          else:
│ │              timing = [slice(None)] * len(trials)
│ │  
│ │          return timing
│ │  
│ │      def __init__(self, data=None, samplerate=None, trialid=None, **kwargs):
│ │  
│ │ +        # set as instance attribute to allow (un-)registering of additional datasets
│ │ +        self._hdfFileDatasetProperties = BaseData._hdfFileDatasetProperties + ("data",)
│ │ +
│ │          # Assign (default) values
│ │          self._trialid = None
│ │          self._samplerate = None
│ │          self._data = None
│ │  
│ │          self.samplerate = samplerate
│ │          self.trialid = trialid
│ │  
│ │          # Call initializer
│ │          super().__init__(data=data, **kwargs)
│ │  
│ │ -        if self.data is not None and self.data.size != 0:
│ │ +        if self.data is not None:
│ │ +
│ │ +            if self.data.size == 0:
│ │ +                # initialization with empty data not allowed
│ │ +                raise SPYValueError("non empty data set", 'data')
│ │  
│ │              # In case of manual data allocation (reading routine would leave a
│ │              # mark in `cfg`), fill in missing info
│ │              if self.sampleinfo is None:
│ │ -
│ │                  # Fill in dimensional info
│ │                  definetrial(self, kwargs.get("trialdefinition"))
│ │  
│ │  
│ │  class SpikeData(DiscreteData):
│ │      """Spike times of multi- and/or single units
│ │  
│ │ @@ -325,80 +331,137 @@
│ │      stored as a two-dimensional [nSpikes x 3] array on disk with the columns
│ │      being ``["sample", "channel", "unit"]``.
│ │  
│ │      Data is only read from disk on demand, similar to HDF5 files.
│ │      """
│ │  
│ │      _infoFileProperties = DiscreteData._infoFileProperties + ("channel", "unit",)
│ │ -    _hdfFileAttributeProperties = DiscreteData._hdfFileAttributeProperties + ("channel",)
│ │      _defaultDimord = ["sample", "channel", "unit"]
│ │      _stackingDimLabel = "sample"
│ │ +    _selectionKeyWords = DiscreteData._selectionKeyWords + ('channel', 'unit',)
│ │ +
│ │ +    def _compute_unique_idx(self):
│ │ +        """
│ │ +        Use `np.unique` on whole(!) dataset to compute globally
│ │ +        available channel and unit indices only once
│ │ +
│ │ +        This function gets triggered by the constructor
│ │ +        `if data is not None` or latest when channel/unit
│ │ +        labels are assigned with the respective setters.
│ │ +        """
│ │ +
│ │ +        # after data was added via selection or loading from file
│ │ +        # this function gets re-triggered by the channel/unit setters!
│ │ +        if self.data is None:
│ │ +            return
│ │ +
│ │ +        # this is costly and loads the entire hdf5 dataset into memory!
│ │ +        self.channel_idx = np.unique(self.data[:, self.dimord.index("channel")])
│ │ +        self.unit_idx = np.unique(self.data[:, self.dimord.index("unit")])
│ │  
│ │      @property
│ │      def channel(self):
│ │          """ :class:`numpy.ndarray` : list of original channel names for each unit"""
│ │ -        # if data exists but no user-defined channel labels, create them on the fly
│ │ -        if self._channel is None and self._data is not None:
│ │ -            channelNumbers = np.unique(self.data[:, self.dimord.index("channel")])
│ │ -            return np.array(["channel" + str(int(i + 1)).zfill(len(str(channelNumbers.max() + 1)))
│ │ -                             for i in channelNumbers])
│ │  
│ │          return self._channel
│ │  
│ │      @channel.setter
│ │      def channel(self, chan):
│ │ -        if chan is None:
│ │ -            self._channel = None
│ │ -            return
│ │          if self.data is None:
│ │ -            raise SPYValueError("Syncopy: Cannot assign `channels` without data. " +
│ │ -                  "Please assign data first")
│ │ -        try:
│ │ -            array_parser(chan, varname="channel", ntype="str")
│ │ -        except Exception as exc:
│ │ -            raise exc
│ │ +            if chan is not None:
│ │ +                raise SPYValueError(f"non-empty SpikeData", "cannot assign `channel` without data. " +
│ │ +                                    "Please assign data first")
│ │ +            # No labels for no data is fine
│ │ +            self._channel = chan
│ │ +            return
│ │ +
│ │ +        # there is data
│ │ +        elif chan is None:
│ │ +            raise SPYValueError("channel labels, cannot set `channel` to `None` with existing data.")
│ │ +
│ │ +        # if we landed here, we have data and new labels
│ │ +
│ │ +        # in case of selections and/or loading from file
│ │ +        # the constructor was called with data=None, hence
│ │ +        # we have to compute the unique indices here
│ │ +        if self.channel_idx is None:
│ │ +            self._compute_unique_idx()
│ │  
│ │ -        # Remove duplicate entries from channel array but preserve original order
│ │ -        # (e.g., `[2, 0, 0, 1]` -> `[2, 0, 1`); allows for complex subset-selections
│ │ -        _, idx = np.unique(chan, return_index=True)
│ │ -        chan = np.array(chan)[np.sort(idx)]
│ │ -        nchan = np.unique(self.data[:, self.dimord.index("channel")]).size
│ │ -        if chan.size != nchan:
│ │ -            lgl = "channel label array of length {0:d}".format(nchan)
│ │ -            act = "array of length {0:d}".format(chan.size)
│ │ -            raise SPYValueError(legal=lgl, varname="channel", actual=act)
│ │ +        # we need as many labels as there are distinct channels
│ │ +        nChan = self.channel_idx.size
│ │  
│ │ -        self._channel = chan
│ │ +        if nChan != len(chan):
│ │ +            raise SPYValueError(f"exactly {nChan} channel label(s)")
│ │ +        array_parser(chan, varname="channel", ntype="str", dims=(nChan, ))
│ │ +        self._channel = np.array(chan)
│ │ +
│ │ +    def _default_channel_labels(self):
│ │ +
│ │ +        """
│ │ +        Creates the default channel labels
│ │ +        """
│ │ +
│ │ +        # channel entries in self.data are 0-based
│ │ +        chan_max = self.channel_idx.max()
│ │ +        channel_labels = np.array(["channel" + str(int(i + 1)).zfill(len(str(chan_max)) + 1)
│ │ +                                   for i in self.channel_idx])
│ │ +        return channel_labels
│ │  
│ │      @property
│ │      def unit(self):
│ │          """ :class:`numpy.ndarray(str)` : unit names"""
│ │ -        if self.data is not None and self._unit is None:
│ │ -            unitIndices = np.unique(self.data[:, self.dimord.index("unit")])
│ │ -            return np.array(["unit" + str(int(i)).zfill(len(str(unitIndices.max())))
│ │ -                             for i in unitIndices])
│ │ +
│ │          return self._unit
│ │  
│ │      @unit.setter
│ │      def unit(self, unit):
│ │ -        if unit is None:
│ │ -            self._unit = None
│ │ +        if self.data is None:
│ │ +            if unit is not None:
│ │ +                raise SPYValueError(f"non-empty SpikeData", "cannot assign `unit` without data. " +
│ │ +                                    "Please assign data first")
│ │ +            # empy labels for empty data is fine
│ │ +            self._unit = unit
│ │              return
│ │  
│ │ -        if self.data is None:
│ │ +        # there is data
│ │ +        elif unit is None:
│ │ +            raise SPYValueError("unit labels, cannot set `unit` to `None` with existing data.")
│ │ +
│ │ +        # in case of selections and/or loading from file
│ │ +        # the constructor was called with data=None, hence
│ │ +        # we have to compute this here
│ │ +        if self.unit_idx is None:
│ │ +            self._compute_unique_idx()
│ │ +
│ │ +        if unit is None and self.data is not None:
│ │ +            raise SPYValueError("Cannot set `unit` to `None` with existing data.")
│ │ +        elif self.data is None and unit is not None:
│ │              raise SPYValueError("Syncopy - SpikeData - unit: Cannot assign `unit` without data. " +
│ │                    "Please assign data first")
│ │ +        elif unit is None:
│ │ +            self._unit = None
│ │ +            return
│ │ +
│ │ +        nunit = self.unit_idx.size
│ │ +        if nunit != len(unit):
│ │ +            raise SPYValueError(f"exactly {nunit} unit label(s)")
│ │ +        array_parser(unit, varname="unit", ntype="str", dims=(nunit,))
│ │  
│ │ -        nunit = np.unique(self.data[:, self.dimord.index("unit")]).size
│ │ -        try:
│ │ -            array_parser(unit, varname="unit", ntype="str", dims=(nunit,))
│ │ -        except Exception as exc:
│ │ -            raise exc
│ │          self._unit = np.array(unit)
│ │  
│ │ +    def _default_unit_labels(self):
│ │ +
│ │ +        """
│ │ +        Creates the default unit labels
│ │ +        """
│ │ +
│ │ +        unit_max = self.unit_idx.max()
│ │ +        return np.array(["unit" + str(int(i + 1)).zfill(len(str(unit_max)) + 1)
│ │ +                         for i in self.unit_idx])
│ │ +
│ │      # Helper function that extracts by-trial unit-indices
│ │      def _get_unit(self, trials, units=None):
│ │          """
│ │          Get relative by-trial indices of unit selections
│ │  
│ │          Parameters
│ │          ----------
│ │ @@ -413,41 +476,72 @@
│ │              List of by-trial sample-indices corresponding to provided
│ │              unit-selection. If `units` is `None`, `indices` is a list of universal
│ │              (i.e., ``slice(None)``) selectors.
│ │  
│ │          Notes
│ │          -----
│ │          This class method is intended to be solely used by
│ │ -        :class:`syncopy.datatype.base_data.Selector` objects and thus has purely
│ │ +        :class:`syncopy.datatype.selector.Selector` objects and thus has purely
│ │          auxiliary character. Therefore, all input sanitization and error checking
│ │ -        is left to :class:`syncopy.datatype.base_data.Selector` and not
│ │ +        is left to :class:`syncopy.datatype.selector.Selector` and not
│ │          performed here.
│ │  
│ │          See also
│ │          --------
│ │ -        syncopy.datatype.base_data.Selector : Syncopy data selectors
│ │ +        syncopy.datatype.selector.Selector : Syncopy data selectors
│ │          """
│ │          if units is not None:
│ │              indices = []
│ │ -            allUnits = self.data[:, self.dimord.index("unit")]
│ │              for trlno in trials:
│ │ -                thisTrial = allUnits[self.trialid == trlno]
│ │ +                thisTrial = self.data[self._trialslice[trlno], self.dimord.index("unit")]
│ │                  trialUnits = []
│ │                  for unit in units:
│ │                      trialUnits += list(np.where(thisTrial == unit)[0])
│ │                  if len(trialUnits) > 1:
│ │                      steps = np.diff(trialUnits)
│ │                      if steps.min() == steps.max() == 1:
│ │                          trialUnits = slice(trialUnits[0], trialUnits[-1] + 1, 1)
│ │                  indices.append(trialUnits)
│ │          else:
│ │              indices = [slice(None)] * len(trials)
│ │  
│ │          return indices
│ │  
│ │ +    @property
│ │ +    def waveform(self):
│ │ +        """The waveform of the spikes in the data.
│ │ +
│ │ +        This is a tiny part of the raw data around the time point at which a spike was detected
│ │ +        (by the recording hardware/software: keep in mind that the spike data Syncopy is working
│ │ +        with is already pre-processed). From this sequence of samples, one can derive
│ │ +        the waveform type of the spike (via classification), which may allow to
│ │ +        derive the type of neuron that produced the spike.
│ │ +        """
│ │ +        return self._waveform
│ │ +
│ │ +    @waveform.setter
│ │ +    def waveform(self, waveform):
│ │ +        """Set a waveform dataset from a numpy array, `None`, or an `h5py.Dataset` instance."""
│ │ +        if self.data is None:
│ │ +            if waveform is not None:
│ │ +                raise SPYValueError(legal="non-empty SpikeData", varname="waveform",
│ │ +                actual="empty SpikeData main dataset (None). Cannot assign `waveform` without data. Please assign data first.")
│ │ +        if waveform is None:
│ │ +            self._set_dataset_property(waveform, 'waveform') # None
│ │ +            self._unregister_dataset("waveform", del_attr=False)
│ │ +            return
│ │ +
│ │ +        if waveform.ndim < 2:
│ │ +            raise SPYValueError(legal="waveform data with at least 2 dimensions", varname="waveform", actual=f"data with {waveform.ndim} dimensions")
│ │ +
│ │ +        if waveform.shape[0] != self.data.shape[0]:
│ │ +            raise SPYValueError(f"waveform shape[0]={waveform.shape[0]} must equal nSpikes={self.data.shape[0]}. " +
│ │ +                                "Please create one waveform per spike in data.", varname="waveform", actual=f"wrong size waveform with shape {waveform.shape}")
│ │ +        self._set_dataset_property(waveform, 'waveform')
│ │ +
│ │      # "Constructor"
│ │      def __init__(self,
│ │                   data=None,
│ │                   filename=None,
│ │                   trialdefinition=None,
│ │                   samplerate=None,
│ │                   channel=None,
│ │ @@ -479,40 +573,67 @@
│ │  
│ │          See also
│ │          --------
│ │          :func:`syncopy.definetrial`
│ │  
│ │          """
│ │  
│ │ +        # instance attribute to allow modification
│ │ +        self._hdfFileAttributeProperties = DiscreteData._hdfFileAttributeProperties + ("channel", "unit")
│ │ +
│ │          self._unit = None
│ │ +        self.unit_idx = None
│ │          self._channel = None
│ │ +        self.channel_idx = None
│ │  
│ │          # Call parent initializer
│ │          super().__init__(data=data,
│ │                           filename=filename,
│ │                           trialdefinition=trialdefinition,
│ │                           samplerate=samplerate,
│ │                           dimord=dimord)
│ │  
│ │ -        self.channel = channel
│ │ -        self.unit = unit
│ │ +        self._hdfFileDatasetProperties +=  ("waveform",)
│ │ +
│ │ +        self._waveform = None
│ │ +
│ │ +        # for fast lookup and labels
│ │ +        self._compute_unique_idx()
│ │ +
│ │ +        # constructor gets `data=None` for
│ │ +        # empty inits, selections and loading from file
│ │ +        # can't set any labels in that case
│ │ +        if channel is not None:
│ │ +            # setter raises exception if data=None
│ │ +            self.channel = channel
│ │ +        elif data is not None:
│ │ +            # data but no given labels
│ │ +            self.channel = self._default_channel_labels()
│ │ +
│ │ +        # same for unit
│ │ +        if unit is not None:
│ │ +            # setter raises exception if data=None
│ │ +            self.unit = unit
│ │ +        elif data is not None:
│ │ +            self.unit = self._default_unit_labels()
│ │  
│ │  
│ │  class EventData(DiscreteData):
│ │      """Timestamps and integer codes of experimental events
│ │  
│ │      This class can be used for representing events during an experiment, e.g.
│ │      stimulus was turned on, etc. These usually occur at non-regular time points
│ │      and have associated event codes.
│ │  
│ │      Data is only read from disk on demand, similar to HDF5 files.
│ │      """
│ │  
│ │      _defaultDimord = ["sample", "eventid"]
│ │      _stackingDimLabel = "sample"
│ │ +    _selectionKeyWords = DiscreteData._selectionKeyWords + ('eventid',)
│ │  
│ │      @property
│ │      def eventid(self):
│ │          """numpy.ndarray(int): integer event code assocated with each event"""
│ │          if self.data is None:
│ │              return None
│ │          return np.unique(self.data[:, self.dimord.index("eventid")])
│ │ @@ -535,28 +656,27 @@
│ │              List of by-trial sample-indices corresponding to provided
│ │              event-id-selection. If `eventids` is `None`, `indices` is a list of
│ │              universal (i.e., ``slice(None)``) selectors.
│ │  
│ │          Notes
│ │          -----
│ │          This class method is intended to be solely used by
│ │ -        :class:`syncopy.datatype.base_data.Selector` objects and thus has purely
│ │ +        :class:`syncopy.datatype.selector.Selector` objects and thus has purely
│ │          auxiliary character. Therefore, all input sanitization and error checking
│ │ -        is left to :class:`syncopy.datatype.base_data.Selector` and not
│ │ +        is left to :class:`syncopy.datatype.selector.Selector` and not
│ │          performed here.
│ │  
│ │          See also
│ │          --------
│ │ -        syncopy.datatype.base_data.Selector : Syncopy data selectors
│ │ +        syncopy.datatype.selector.Selector : Syncopy data selectors
│ │          """
│ │          if eventids is not None:
│ │              indices = []
│ │ -            allEvents = self.data[:, self.dimord.index("eventid")]
│ │              for trlno in trials:
│ │ -                thisTrial = allEvents[self.trialid == trlno]
│ │ +                thisTrial = self.data[self._trialslice[trlno], self.dimord.index("eventid")]
│ │                  trialEvents = []
│ │                  for event in eventids:
│ │                      trialEvents += list(np.where(thisTrial == event)[0])
│ │                  if len(trialEvents) > 1:
│ │                      steps = np.diff(trialEvents)
│ │                      if steps.min() == steps.max() == 1:
│ │                          trialEvents = slice(trialEvents[0], trialEvents[-1] + 1, 1)
│ │ @@ -573,24 +693,24 @@
│ │                   trialdefinition=None,
│ │                   samplerate=None,
│ │                   dimord=None):
│ │          """Initialize a :class:`EventData` object.
│ │  
│ │          Parameters
│ │          ----------
│ │ -            data : [nEvents x 2] :class:`numpy.ndarray`
│ │ +        data : [nEvents x 2] :class:`numpy.ndarray`
│ │  
│ │ -            filename : str
│ │ -                path to filename or folder (spy container)
│ │ -            trialdefinition : :class:`EventData` object or nTrials x 3 array
│ │ -                [start, stop, trigger_offset] sample indices for `M` trials
│ │ -            samplerate : float
│ │ -                sampling rate in Hz
│ │ -            dimord : list(str)
│ │ -                ordered list of dimension labels
│ │ +        filename : str
│ │ +            path to filename or folder (spy container)
│ │ +        trialdefinition : :class:`EventData` object or nTrials x 3 array
│ │ +            [start, stop, trigger_offset] sample indices for `M` trials
│ │ +        samplerate : float
│ │ +            sampling rate in Hz
│ │ +        dimord : list(str)
│ │ +            ordered list of dimension labels
│ │  
│ │          1. `filename` + `data` : create hdf dataset incl. sampleinfo @filename
│ │          2. `filename` no `data` : read from file(spy, hdf5)
│ │          3. just `data` : try to attach data (error checking done by
│ │             :meth:`EventData.data.setter`)
│ │  
│ │          See also
│ │ @@ -610,7 +730,9 @@
│ │  
│ │          # Call parent initializer
│ │          super().__init__(data=data,
│ │                           filename=filename,
│ │                           trialdefinition=trialdefinition,
│ │                           samplerate=samplerate,
│ │                           dimord=dimord)
│ │ +
│ │ +        self._hdfFileAttributeProperties = BaseData._hdfFileAttributeProperties + ("samplerate",)
│ │   --- esi-syncopy-2022.8/syncopy/datatype/methods/arithmetic.py
│ ├── +++ esi_syncopy-2023.3/syncopy/datatype/methods/arithmetic.py
│ │┄ Files 2% similar despite different names
│ │ @@ -9,16 +9,16 @@
│ │  
│ │  # Local imports
│ │  from syncopy import __acme__
│ │  from .selectdata import _get_selection_size
│ │  from syncopy.shared.parsers import data_parser
│ │  from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYWarning, SPYInfo
│ │  from syncopy.shared.computational_routine import ComputationalRoutine
│ │ -from syncopy.shared.kwarg_decorators import process_io
│ │ -from syncopy.shared.computational_routine import ComputationalRoutine
│ │ +from syncopy.shared.kwarg_decorators import process_io, detect_parallel_client
│ │ +
│ │  if __acme__:
│ │      import dask.distributed as dd
│ │  
│ │  __all__ = []
│ │  
│ │  
│ │  # Main entry point for overloaded operators
│ │ @@ -136,15 +136,15 @@
│ │          raise exc
│ │  
│ │      # If no active selection is present, create a "fake" all-to-all selection
│ │      # to harmonize processing down the road (and attach `_cleanup` attribute for later removal)
│ │      if baseObj.selection is None:
│ │          baseObj.selectdata(inplace=True)
│ │          baseObj.selection._cleanup = True
│ │ -    baseTrialList = baseObj.selection.trials
│ │ +    baseTrialList = baseObj.selection.trial_ids
│ │  
│ │      # Use the `_preview_trial` functionality of Syncopy objects to get each trial's
│ │      # shape and dtype (existing selections are taken care of automatically)
│ │      baseTrials = [baseObj._preview_trial(trlno) for trlno in baseTrialList]
│ │  
│ │      # Depending on the what is thrown at `baseObj` perform more or less extensive parsing
│ │      # First up: operand is a scalar
│ │ @@ -214,15 +214,15 @@
│ │  
│ │          # If only a subset of `operand` is selected, adjust for this (and warn
│ │          # that arbitrarily ugly things might happen with mis-matched selections)
│ │          if operand.selection is not None:
│ │              wrng = "Found existing in-place selection in operand. " +\
│ │                  "Shapes and trial counts of base and operand objects have to match up!"
│ │              SPYWarning(wrng, caller=operator)
│ │ -            opndTrialList = operand.selection.trials
│ │ +            opndTrialList = operand.selection.trial_ids
│ │          else:
│ │              opndTrialList = list(range(len(operand.trials)))
│ │  
│ │          # Ensure the same number of trials is about to be processed
│ │          opndTrials = [operand._preview_trial(trlno) for trlno in opndTrialList]
│ │          if len(opndTrials) != len(baseTrials):
│ │              lgl = "Syncopy object with same number of trials (selected)"
│ │ @@ -366,27 +366,14 @@
│ │      elif operator == "/":
│ │          operation = lambda x, y : x / y
│ │      elif operator == "**":
│ │          operation = lambda x, y : x ** y
│ │      else:
│ │          raise SPYValueError("supported arithmetic operator", actual=operator)
│ │  
│ │ -    # Inform about the amount of data is about to be moved around
│ │ -    operandSize = _get_selection_size(baseObj)
│ │ -    sUnit = "MB"
│ │ -    if operandSize > 1000:
│ │ -        operandSize /= 1024
│ │ -        sUnit = "GB"
│ │ -    msg = "Allocating {dsize:3.2f} {dunit:s} {objkind:s} object on disk for " +\
│ │ -        "result of {op:s} operation"
│ │ -    SPYInfo(msg.format(dsize=operandSize,
│ │ -                       dunit=sUnit,
│ │ -                       objkind=out.__class__.__name__,
│ │ -                       op=operator), caller=operator)
│ │ -
│ │      # If ACME is available, try to attach (already running) parallel computing client
│ │      parallel = False
│ │      if __acme__:
│ │          try:
│ │              dd.get_client()
│ │              parallel = True
│ │          except ValueError:
│ │   --- esi-syncopy-2022.8/syncopy/datatype/methods/copy.py
│ ├── +++ esi_syncopy-2023.3/syncopy/datatype/methods/copy.py
│ │┄ Files 14% similar despite different names
│ │ @@ -13,21 +13,21 @@
│ │  from syncopy.shared.parsers import data_parser
│ │  from syncopy.shared.errors import SPYInfo
│ │  
│ │  __all__ = ["copy"]
│ │  
│ │  
│ │  # Return a deep copy of the current class instance
│ │ -def copy(data):
│ │ +def copy(spydata):
│ │      """
│ │      Create a copy of the entire Syncopy object `data` on disk
│ │  
│ │      Parameters
│ │      ----------
│ │ -    data : Syncopy data object
│ │ +    spydata : Syncopy data object
│ │          Object to be copied on disk
│ │  
│ │      Returns
│ │      -------
│ │      cpy : Syncopy data object
│ │          Reference to the copied data object
│ │          on disk
│ │ @@ -43,34 +43,43 @@
│ │      See also
│ │      --------
│ │      :func:`syncopy.save` : save to specific file path
│ │      :func:`syncopy.selectdata` : creates copy of a selection with `inplace=False`
│ │      """
│ │  
│ │      # Make sure `data` is a valid Syncopy data object
│ │ -    data_parser(data, varname="data", writable=None, empty=False)
│ │ +    data_parser(spydata, varname="data", writable=None, empty=False)
│ │  
│ │ -    dsize = np.prod(data.data.shape) * data.data.dtype.itemsize / 1024**2
│ │ -    msg = (f"Copying {dsize:.2f} MB of data "
│ │ -           f"to create new {data.__class__.__name__} object on disk")
│ │ +    dsize = np.prod(spydata.data.shape) * spydata.data.dtype.itemsize / 1024 ** 2
│ │ +    msg = (
│ │ +        f"Copying {dsize:.2f} MB of data "
│ │ +        f"to create new {spydata.__class__.__name__} object on disk"
│ │ +    )
│ │      SPYInfo(msg)
│ │  
│ │ -    # shallow copy, captures also non-default/temporary attributes
│ │ -    cpy = py_copy(data)
│ │ -    data.clear()
│ │ -    filename = data._gen_filename()
│ │ -
│ │ -    # copy data on disk
│ │ -    shutil.copyfile(data.filename, filename)
│ │ -
│ │ -    # reattach properties
│ │ -    for propertyName in data._hdfFileDatasetProperties:
│ │ -        prop = getattr(data, propertyName)
│ │ +    # Shallow copy, captures also non-default/temporary attributes.
│ │ +    copy_spydata = py_copy(spydata)
│ │ +    copy_filename = spydata._gen_filename()
│ │ +    copy_spydata.filename = copy_filename
│ │ +    spydata.clear()
│ │ +
│ │ +    spydata._close()
│ │ +
│ │ +    # Copy data on disk.
│ │ +    shutil.copyfile(spydata.filename, copy_filename, follow_symlinks=False)
│ │ +
│ │ +    spydata._reopen()
│ │ +
│ │ +    # Reattach properties
│ │ +    for propertyName in spydata._hdfFileDatasetProperties:
│ │ +        prop = getattr(spydata, "_" + propertyName)
│ │          if isinstance(prop, h5py.Dataset):
│ │ -            sourceName = getattr(data, propertyName).name
│ │ -            setattr(cpy, propertyName,
│ │ -                    h5py.File(filename, mode=cpy.mode)[sourceName])
│ │ -        else:
│ │ -            setattr(cpy, propertyName, prop)
│ │ +            sourceName = prop.name
│ │ +            setattr(
│ │ +                copy_spydata,
│ │ +                "_" + propertyName,
│ │ +                h5py.File(copy_filename, mode=copy_spydata.mode)[sourceName],
│ │ +            )
│ │ +        else:  # np.ndarray
│ │ +            setattr(copy_spydata, "_" + propertyName, prop)
│ │  
│ │ -    cpy.filename = filename
│ │ -    return cpy
│ │ +    return copy_spydata
│ │   --- esi-syncopy-2022.8/syncopy/datatype/methods/definetrial.py
│ ├── +++ esi_syncopy-2023.3/syncopy/datatype/methods/definetrial.py
│ │┄ Files 3% similar despite different names
│ │ @@ -11,15 +11,15 @@
│ │  from syncopy.shared.parsers import data_parser, array_parser, scalar_parser
│ │  from syncopy.shared.errors import SPYTypeError, SPYValueError
│ │  
│ │  __all__ = ["definetrial"]
│ │  
│ │  
│ │  def definetrial(obj, trialdefinition=None, pre=None, post=None, start=None,
│ │ -                  trigger=None, stop=None, clip_edges=False):
│ │ +                trigger=None, stop=None, clip_edges=False):
│ │      """(Re-)define trials of a Syncopy data object
│ │  
│ │      Data can be structured into trials based on timestamps of a start, trigger
│ │      and end events::
│ │  
│ │                      start    trigger    stop
│ │          |---- pre ----|--------|---------|--- post----|
│ │ @@ -89,18 +89,15 @@
│ │              try:
│ │                  data_parser(trialdefinition, varname="trialdefinition",
│ │                              writable=None, empty=False)
│ │              except Exception as exc:
│ │                  raise exc
│ │              evt = True
│ │          else:
│ │ -            try:
│ │ -                array_parser(trialdefinition, varname="trialdefinition", dims=2)
│ │ -            except Exception as exc:
│ │ -                raise exc
│ │ +            array_parser(trialdefinition, varname="trialdefinition", dims=2)
│ │  
│ │              if any(["ContinuousData" in str(base) for base in obj.__class__.__mro__]):
│ │                  scount = obj.data.shape[obj.dimord.index("time")]
│ │              else:
│ │                  scount = np.inf
│ │              try:
│ │                  array_parser(trialdefinition[:, :2], varname="sampleinfo", dims=(None, 2), hasnan=False,
│ │ @@ -331,23 +328,21 @@
│ │      tgt._trialdefinition = trl
│ │  
│ │      # In the discrete case, we have some additinal work to do
│ │      if any(["DiscreteData" in str(base) for base in tgt.__class__.__mro__]):
│ │  
│ │          # Compute trial-IDs by matching data samples with provided trial-bounds
│ │          samples = tgt.data[:, tgt.dimord.index("sample")]
│ │ -        starts = tgt.sampleinfo[:, 0]
│ │ -        ends = tgt.sampleinfo[:, 1]
│ │ -        startids = np.searchsorted(starts, samples, side="right")
│ │ -        endids = np.searchsorted(ends, samples, side="left")
│ │ -        mask = startids == endids
│ │ -        startids -= 1
│ │ -        # Samples not belonging into any trial get a trial-ID of -1
│ │ -        startids[mask] = int(startids.min() <= 0) * (-1)
│ │ -        tgt.trialid = startids
│ │ +        idx = np.searchsorted(samples, tgt.sampleinfo.ravel())
│ │ +        idx = idx.reshape(tgt.sampleinfo.shape)
│ │ +
│ │ +        tgt._trialslice = [slice(st,end) for st,end in idx]
│ │ +        tgt.trialid = np.full((samples.shape), -1, dtype=int)
│ │ +        for itrl, itrl_slice in enumerate(tgt._trialslice):
│ │ +            tgt.trialid[itrl_slice] = itrl
│ │  
│ │      # Write log entry
│ │      if ref == tgt:
│ │          ref.log = "updated trial-definition with [" \
│ │                    + " x ".join([str(numel) for numel in trl.shape]) \
│ │                    + "] element array"
│ │      else:
│ │   --- esi-syncopy-2022.8/syncopy/datatype/methods/selectdata.py
│ ├── +++ esi_syncopy-2023.3/syncopy/datatype/methods/selectdata.py
│ │┄ Files 9% similar despite different names
│ │ @@ -1,36 +1,37 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │  # Syncopy data selection methods
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │ +import h5py
│ │  
│ │  # Local imports
│ │ +import syncopy as spy
│ │  from syncopy.shared.tools import get_frontend_cfg, get_defaults
│ │  from syncopy.shared.parsers import data_parser
│ │ -from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYInfo
│ │ +from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYInfo, SPYWarning
│ │  from syncopy.shared.kwarg_decorators import unwrap_cfg, process_io, detect_parallel_client
│ │  from syncopy.shared.computational_routine import ComputationalRoutine
│ │ +from syncopy.shared.latency import get_analysis_window, create_trial_selection
│ │  
│ │  __all__ = ["selectdata"]
│ │  
│ │  
│ │  @unwrap_cfg
│ │  @detect_parallel_client
│ │  def selectdata(data,
│ │                 trials=None,
│ │                 channel=None,
│ │                 channel_i=None,
│ │                 channel_j=None,
│ │ -               toi=None,
│ │ -               toilim=None,
│ │ -               foi=None,
│ │ -               foilim=None,
│ │ +               latency=None,
│ │ +               frequency=None,
│ │                 taper=None,
│ │                 unit=None,
│ │                 eventid=None,
│ │                 inplace=False,
│ │                 clear=False,
│ │                 **kwargs):
│ │      """
│ │ @@ -123,38 +124,22 @@
│ │          are counted starting at zero, and range and slice selections are half-open
│ │          intervals of the form `[low, high)`, i.e., low is included , high is
│ │          excluded. Thus, ``channel = [0, 1, 2]`` or ``channel = slice(0, 3)``
│ │          selects the first up to (and including) the third channel. Selections can
│ │          be unsorted and may include repetitions but must match exactly, be finite
│ │          and not NaN. If `channel` is `None`, or ``channel = "all"`` all channels
│ │          are selected.
│ │ -    toi : list (floats), float, None or "all"
│ │ -        Time-points to be selected (in seconds) in each trial. Timing is expected
│ │ -        to be on a by-trial basis (e.g., relative to trigger onsets). Selections
│ │ -        can be approximate, unsorted and may include repetitions but must be
│ │ -        finite and not NaN. Fuzzy matching is performed for approximate selections
│ │ -        (i.e., selected time-points are close but not identical to timing information
│ │ -        found in `data`) using a nearest-neighbor search for elements of `toi`.
│ │ -        If `toi` is `None` or ``toi = "all"``, the entire time-span in each trial
│ │ -        is selected.
│ │ -    toilim : list (floats [tmin, tmax]) or None or "all"
│ │ -        Time-window ``[tmin, tmax]`` (in seconds) to be extracted from each trial.
│ │ -        Window specifications must be sorted (e.g., ``[2.2, 1.1]`` is invalid)
│ │ -        and not NaN but may be unbounded (e.g., ``[1.1, np.inf]`` is valid). Edges
│ │ -        `tmin` and `tmax` are included in the selection.
│ │ -        If `toilim` is `None` or ``toilim = "all"``, the entire time-span in each
│ │ -        trial is selected.
│ │ -    foi : list (floats), float, None or "all"
│ │ -        Frequencies to be selected (in Hz). Selections can be approximate, unsorted
│ │ -        and may include repetitions but must be finite and not NaN. Fuzzy matching
│ │ -        is performed for approximate selections (i.e., selected frequencies are
│ │ -        close but not identical to frequencies found in `data`) using a nearest-
│ │ -        neighbor search for elements of `foi` in `data.freq`. If `foi` is `None`
│ │ -        or ``foi = "all"``, all frequencies are selected.
│ │ -    foilim : list (floats [fmin, fmax]) or None or "all"
│ │ +    latency : [begin, end], {'maxperiod', 'minperiod', 'prestim', 'poststim', 'all'} or None
│ │ +        Either set desired time window (`[begin, end]`) in
│ │ +        seconds, 'maxperiod' (default) for the maximum period
│ │ +        available or `'minperiod' for minimal time-window all trials share,
│ │ +        or `'prestim'` (all t < 0) or `'poststim'` (all t > 0)
│ │ +        If set this will apply a selection which is timelocked,
│ │ +        meaning non-fitting (effectively too short) trials will be excluded
│ │ +    frequency : list (floats [fmin, fmax]) or None or "all"
│ │          Frequency-window ``[fmin, fmax]`` (in Hz) to be extracted. Window
│ │          specifications must be sorted (e.g., ``[90, 70]`` is invalid) and not NaN
│ │          but may be unbounded (e.g., ``[-np.inf, 60.5]`` is valid). Edges `fmin`
│ │          and `fmax` are included in the selection. If `foilim` is `None` or
│ │          ``foilim = "all"``, all frequencies are selected.
│ │      taper : list (integers or strings), slice, range, str, int, None or "all"
│ │          Taper-selection; can be a list of taper names (``['dpss-win-1', 'dpss-win-3']``),
│ │ @@ -256,99 +241,163 @@
│ │  
│ │      See also
│ │      --------
│ │      :func:`syncopy.show` : Show (subsets) of Syncopy objects
│ │      """
│ │  
│ │      # Ensure our one mandatory input is usable
│ │ -    try:
│ │ -        data_parser(data, varname="data", empty=False)
│ │ -    except Exception as exc:
│ │ -        raise exc
│ │ +    data_parser(data, varname="data", empty=False)
│ │  
│ │      # Vet the only inputs not checked by `Selector`
│ │      if not isinstance(inplace, bool):
│ │          raise SPYTypeError(inplace, varname="inplace", expected="Boolean")
│ │      if not isinstance(clear, bool):
│ │          raise SPYTypeError(clear, varname="clear", expected="Boolean")
│ │  
│ │ +    # there is no `@unwrap_select` decorator in place here,
│ │ +    # a `select` dictionary must therefore be directly passed via ** unpacking:
│ │ +    # select = {'channel': [0]}; spy.selectdata(data, **select)
│ │ +    if 'select' in kwargs:
│ │ +        lgl = "unpacked selection keywords directly, try `**select`"
│ │ +        act = "`select` as explicit parameter"
│ │ +        raise SPYValueError(legal=lgl, varname="selection kwargs", actual=act)
│ │ +
│ │      # get input arguments into cfg dict
│ │      new_cfg = get_frontend_cfg(get_defaults(selectdata), locals(), kwargs)
│ │  
│ │ -    # If provided, make sure output object is appropriate
│ │      if not inplace:
│ │          out = data.__class__(dimord=data.dimord)
│ │  
│ │ -    # Collect provided selection keywords in dict
│ │ +    # First collect all available keyword values into a dict
│ │      selectDict = {"trials": trials,
│ │                    "channel": channel,
│ │                    "channel_i": channel_i,
│ │                    "channel_j": channel_j,
│ │ -                  "toi": toi,
│ │ -                  "toilim": toilim,
│ │ -                  "foi": foi,
│ │ -                  "foilim": foilim,
│ │ +                  "latency": latency,
│ │ +                  "frequency": frequency,
│ │                    "taper": taper,
│ │                    "unit": unit,
│ │                    "eventid": eventid}
│ │  
│ │ -    # The only valid anonymous kw is "parallel" (i.e., filter out typos like 'trails' etc.)
│ │ +    # relevant selection keywords for the type of `data`
│ │ +    expected = list(data._selectionKeyWords)
│ │ +
│ │ +    # filter out typos like 'trails'
│ │      if len(kwargs) > 0:
│ │ -        if list(kwargs.keys()) != ["parallel"]:
│ │ -            kwargs.pop("parallel", None)
│ │ -            expected = list(selectDict.keys()) + ["out", "inplace", "clear", "parallel"]
│ │ -            lgl = "dict with one or all of the following keys: '" +\
│ │ -                  "'".join(opt + "', " for opt in expected)[:-2]
│ │ +        kwargs.pop("parallel", None)
│ │ +        if any([key not in expected for key in kwargs]):
│ │ +            lgl = f"the following keywords for {data.__class__.__name__}: '" +\
│ │ +                "'".join(opt + "', " for opt in expected)[:-2]
│ │ +            lgl += " and 'inplace', 'clear', 'parallel'"
│ │              act = "dict with keys '" +\
│ │                    "'".join(key + "', " for key in kwargs.keys())[:-2]
│ │ -            raise SPYValueError(legal=lgl, varname="kwargs", actual=act)
│ │ +            raise SPYValueError(legal=lgl, varname="selection kwargs", actual=act)
│ │ +
│ │ +    # get out if unsuitable selection keywords given, e.g. 'frequency' for AnalogData
│ │ +    for key, value in selectDict.items():
│ │ +        if key not in expected and value is not None:
│ │ +            lgl = f"one of {data.__class__._selectionKeyWords}"
│ │ +            act = f"no `{key}` selection available for {data.__class__.__name__}"
│ │ +            raise SPYValueError(lgl, 'selection arguments', act)
│ │ +
│ │ +    # now just keep going with the selection keys relevant for that particular data type
│ │ +    selectDict = {key: selectDict[key] for key in data._selectionKeyWords}
│ │  
│ │      # First simplest case: determine whether we just need to clear an existing selection
│ │      if clear:
│ │          if any(value is not None for value in selectDict.values()):
│ │              lgl = "no data selectors if `clear = True`"
│ │              raise SPYValueError(lgl, varname="select", actual=selectDict)
│ │          if data.selection is None:
│ │              SPYInfo("No in-place selection found. ")
│ │          else:
│ │              data.selection = None
│ │              SPYInfo("In-place selection cleared")
│ │          return
│ │  
│ │ +    # first do a selection without latency as a possible subselection
│ │ +    # of trials needs to be applied before the latency digesting functions
│ │ +    # can be called (if the user by himself throws out non-fitting trials)
│ │ +    selectDict.pop('latency')
│ │ +
│ │      # Pass provided selections on to `Selector` class which performs error checking
│ │ +    # this is an in-place selection!
│ │      data.selection = selectDict
│ │  
│ │ -    # If an in-place selection was requested we're done
│ │ +    # -- sort out trials if latency is set --
│ │ +
│ │ +    if latency is not None:
│ │ +        if not isinstance(latency, str) or latency != 'all':
│ │ +            # sanity check done here, converts str arguments
│ │ +            # ('maxperiod' and so on) into time window [start, end] of analysis
│ │ +            window = get_analysis_window(data, latency)
│ │ +
│ │ +            # this respects active inplace selections and
│ │ +            # might update the trial selection to exclude non-fitting trials
│ │ +            selectDict, numDiscard = create_trial_selection(data, window)
│ │ +
│ │ +            if numDiscard > 0:
│ │ +                msg = f"Discarded {numDiscard} trial(s) which did not fit into latency window"
│ │ +                SPYInfo(msg)
│ │ +
│ │ +            # update inplace selection
│ │ +            selectDict['latency'] = window
│ │ +            data.selection = selectDict
│ │ +
│ │ +    # If an in-place selection was requested we're done.
│ │      if inplace:
│ │          # attach frontend parameters for replay
│ │          data.cfg.update({'selectdata': new_cfg})
│ │          return
│ │  
│ │      # Inform the user what's about to happen
│ │      selectionSize = _get_selection_size(data)
│ │ -    sUnit = "MB"
│ │      if selectionSize > 1000:
│ │          selectionSize /= 1024
│ │          sUnit = "GB"
│ │ -    msg = "Copying {dsize:3.2f} {dunit:s} of data based on selection " +\
│ │ -        "to create new {objkind:s} object on disk"
│ │ -    SPYInfo(msg.format(dsize=selectionSize, dunit=sUnit, objkind=data.__class__.__name__))
│ │ +        msg = "Copying {dsize:3.2f} {dunit:s} of data based on selection " +\
│ │ +            "to create new {objkind:s} object on disk"
│ │ +        SPYInfo(msg.format(dsize=selectionSize, dunit=sUnit, objkind=data.__class__.__name__))
│ │  
│ │      # Create inventory of all available selectors and actually provided values
│ │      # to create a bookkeeping dict for logging
│ │ -    log_dct = {"inplace": inplace, "clear": clear}
│ │ +    log_dct = {"inplace": inplace, "clear": clear, "latency": latency}
│ │      log_dct.update(selectDict)
│ │      log_dct.update(**kwargs)
│ │  
│ │      # Fire up `ComputationalRoutine`-subclass to do the actual selecting/copying
│ │      selectMethod = DataSelection()
│ │      selectMethod.initialize(data, out._stackingDim, chan_per_worker=kwargs.get("chan_per_worker"))
│ │      selectMethod.compute(data, out, parallel=kwargs.get("parallel"),
│ │                           log_dict=log_dct)
│ │  
│ │ +    # Handle selection of waveform for SpikeData objects
│ │ +    if type(data) == spy.SpikeData and data.waveform is not None:
│ │ +        if inplace:
│ │ +            spy.log("Inplace selection of SpikeData with waveform not supported for the waveform.", level="WARNING")
│ │ +        else:
│ │ +            fauxTrials = [data._preview_trial(trlno) for trlno in data.selection.trial_ids]
│ │ +            spikes_by_trial = [f.idx[0] for f in fauxTrials]
│ │ +            spike_idx = np.concatenate([np.array(x).ravel() for x in spikes_by_trial])
│ │ +
│ │ +            # Copy the proper subset of the waveform dataset to `out`, the new `SpikeData` object.
│ │ +            hdf5_file_in = data._get_backing_hdf5_file_handle()
│ │ +            hdf5_file_out = out._get_backing_hdf5_file_handle()
│ │ +
│ │ +            # Copy the waveform dataset into the new file, trial by trial to prevent memory issues.
│ │ +            ds = hdf5_file_out.create_dataset('waveform', shape=(len(spike_idx), *data.waveform.shape[1:]), dtype=data.waveform.dtype)
│ │ +            cur_new_idx = 0
│ │ +            for tidx, old_trial_indices in enumerate(spikes_by_trial):
│ │ +                num_spikes_this_trial = len(old_trial_indices)
│ │ +                new_indices = np.s_[cur_new_idx:cur_new_idx + num_spikes_this_trial]
│ │ +                ds[new_indices, :, :] = hdf5_file_in['/waveform'][old_trial_indices, :, :]
│ │ +                cur_new_idx = new_indices.stop
│ │ +
│ │ +            out.waveform = ds
│ │ +
│ │      # Wipe data-selection slot to not alter input object
│ │      data.selection = None
│ │  
│ │      # attach cfg
│ │      out.cfg.update(data.cfg)
│ │      out.cfg.update({'selectdata': new_cfg})
│ │  
│ │ @@ -356,16 +405,16 @@
│ │      return out
│ │  
│ │  
│ │  def _get_selection_size(data):
│ │      """
│ │      Local helper routine for computing the on-disk size of an active data-selection
│ │      """
│ │ -    fauxTrials = [data._preview_trial(trlno) for trlno in data.selection.trials]
│ │ -    fauxSizes = [np.prod(ftrl.shape)*ftrl.dtype.itemsize for ftrl in fauxTrials]
│ │ +    fauxTrials = [data._preview_trial(trlno) for trlno in data.selection.trial_ids]
│ │ +    fauxSizes = [np.prod(ftrl.shape) * ftrl.dtype.itemsize for ftrl in fauxTrials]
│ │      return sum(fauxSizes) / 1024**2
│ │  
│ │  
│ │  @process_io
│ │  def _selectdata(trl, noCompute=False, chunkShape=None):
│ │      if noCompute:
│ │          return trl.shape, trl.dtype
│ │   --- esi-syncopy-2022.8/syncopy/datatype/methods/show.py
│ ├── +++ esi_syncopy-2023.3/syncopy/datatype/methods/show.py
│ │┄ Files 3% similar despite different names
│ │ @@ -159,29 +159,34 @@
│ │      SPYInfo("Showing{}".format(msg))
│ │  
│ │      # catch totally out of range toi selection
│ │      has_time = True if 'time' in data.dimord else False
│ │  
│ │      # Use an object's `_preview_trial` method fetch required indexing tuples
│ │      idxList = []
│ │ -    for trlno in data.selection.trials:
│ │ -        # each dim has an entry
│ │ -        idxs = data._preview_trial(trlno).idx
│ │ +    for trlno in data.selection.trial_ids:
│ │ +        # each dim has an entry, list only needed for mutability
│ │ +        idxs = list(data._preview_trial(trlno).idx)
│ │  
│ │          # time/toi is a special case, all other dims get checked
│ │          # beforehand, e.g. foi, channel, ... but out of range toi's get mapped
│ │          # repeatedly to the last index, causing invalid hdf5 indexing
│ │          if has_time:
│ │              idx = idxs[data.dimord.index('time')]
│ │              if not isinstance(idx, slice) and (
│ │                      len(idx) != len(set(idx))):
│ │                  lgl = "valid `toi` selection"
│ │                  act = sel
│ │                  raise SPYValueError(lgl, 'show kwargs', act)
│ │ -        idxList.append(idxs)
│ │ +
│ │ +        for i, prop_idx in enumerate(idxs):            
│ │ +            if isinstance(prop_idx, list) and len(prop_idx) == 1:
│ │ +                idxs[i] = prop_idx[0]
│ │ +            
│ │ +        idxList.append(tuple(idxs))
│ │  
│ │      # Reset in-place subset selection
│ │      data.selection = None
│ │  
│ │      # single trial selected
│ │      if len(idxList) == 1:
│ │          return transform_out(data.data[idxList[0]])
│ │   --- esi-syncopy-2022.8/syncopy/io/__init__.py
│ ├── +++ esi_syncopy-2023.3/syncopy/io/__init__.py
│ │┄ Files 8% similar despite different names
│ │ @@ -6,23 +6,21 @@
│ │  # Import __all__ routines from local modules
│ │  from . import (
│ │      utils,
│ │      load_spy_container,
│ │      save_spy_container,
│ │      load_ft,
│ │      load_tdt,
│ │ -    _load_nwb
│ │ +    load_nwb
│ │  )
│ │  from .utils import *
│ │  from .load_spy_container import *
│ │  from .save_spy_container import *
│ │  from .load_ft import *
│ │  from .load_tdt import *
│ │ -from ._load_nwb import *
│ │ +from .load_nwb import *
│ │  
│ │  # Populate local __all__ namespace
│ │ -__all__ = []
│ │ +__all__ = ['load_ft_raw', 'load_tdt', 'load_nwb']
│ │  __all__.extend(utils.__all__)
│ │  __all__.extend(load_spy_container.__all__)
│ │  __all__.extend(save_spy_container.__all__)
│ │ -__all__.extend(load_ft.__all__)
│ │ -__all__.extend(_load_nwb.__all__)
│ │   --- esi-syncopy-2022.8/syncopy/io/_load_nwb.py
│ ├── +++ esi_syncopy-2023.3/syncopy/io/load_nwb.py
│ │┄ Files 6% similar despite different names
│ │ @@ -11,16 +11,16 @@
│ │  import numpy as np
│ │  from tqdm import tqdm
│ │  
│ │  # Local imports
│ │  from syncopy import __nwb__
│ │  from syncopy.datatype.continuous_data import AnalogData
│ │  from syncopy.datatype.discrete_data import EventData
│ │ -from syncopy.shared.errors import SPYError, SPYValueError, SPYWarning, SPYInfo
│ │ -from syncopy.shared.parsers import io_parser, scalar_parser
│ │ +from syncopy.shared.errors import SPYError, SPYTypeError, SPYValueError, SPYWarning, SPYInfo
│ │ +from syncopy.shared.parsers import io_parser, scalar_parser, filename_parser
│ │  
│ │  # Conditional imports
│ │  if __nwb__:
│ │      import pynwb
│ │  
│ │  # Global consistent error message if NWB is missing
│ │  nwbErrMsg = "\nSyncopy <core> WARNING: Could not import 'pynwb'. \n" +\
│ │ @@ -29,33 +29,35 @@
│ │            "\tconda install -c conda-forge pynwb\n" +\
│ │            "or using pip:\n" +\
│ │            "\tpip install pynwb"
│ │  
│ │  __all__ = ["load_nwb"]
│ │  
│ │  
│ │ -def load_nwb(filename, memuse=3000):
│ │ +def load_nwb(filename, memuse=3000, container=None):
│ │      """
│ │      Read contents of NWB files
│ │  
│ │      Parameters
│ │      ----------
│ │      filename : str
│ │          Name of (may include full path to) NWB file (e.g., `"/path/to/mydata.nwb"`).
│ │      memuse : scalar
│ │          Approximate in-memory cache size (in MB) for reading data from disk
│ │ +    container : str
│ │ +        Name of syncopy container folder to create the syncopy data in
│ │  
│ │      Returns
│ │      -------
│ │ -    angData : syncopy.AnalogData
│ │ +    objdict : dict
│ │          Any NWB `TimeSeries`-like data is imported into an :class:`~syncopy.AnalogData`
│ │ -        object
│ │ -    evtData : syncopy.EventData
│ │ -        If the NWB file contains TTL pulse data, an additional :class:`~syncopy.EventData`
│ │ -        object is instantiated
│ │ +        object. If the NWB file contains TTL pulse data, an additional
│ │ +        :class:`~syncopy.EventData` object is instantiated. The syncopy
│ │ +        objects are returned as a dictionary whose keys are the base-names
│ │ +        (sans path) of the corresponding files.
│ │      """
│ │  
│ │      # Abort if NWB is not installed
│ │      if not __nwb__:
│ │          raise SPYError(nwbErrMsg.format("read_nwb"))
│ │  
│ │      # Check if file exists
│ │ @@ -79,16 +81,14 @@
│ │      # Load NWB meta data from disk
│ │      nwbio = pynwb.NWBHDF5IO(nwbFullName, "r", load_namespaces=True)
│ │      nwbfile = nwbio.read()
│ │  
│ │      # Allocate lists for storing temporary NWB info: IMPORTANT use lists to preserve
│ │      # order of data chunks/channels
│ │      nSamples = 0
│ │ -    nChannels = 0
│ │ -    chanNames = []
│ │      tStarts = []
│ │      sRates = []
│ │      dTypes = []
│ │      angSeries = []
│ │      ttlVals = []
│ │      ttlChanStates = []
│ │      ttlChans = []
│ │ @@ -102,24 +102,21 @@
│ │  
│ │          # Actual extracellular analog time-series data
│ │          if isinstance(acqValue, pynwb.ecephys.ElectricalSeries):
│ │  
│ │              channels = acqValue.electrodes[:].location
│ │              if channels.unique().size == 1:
│ │                  SPYWarning("No channel names found for {}".format(acqName))
│ │ -            else:
│ │ -                chanNames += channels.to_list()
│ │  
│ │              dTypes.append(acqValue.data.dtype)
│ │              if acqValue.channel_conversion is not None:
│ │                  dTypes.append(acqValue.channel_conversion.dtype)
│ │  
│ │              tStarts.append(acqValue.starting_time)
│ │              sRates.append(acqValue.rate)
│ │ -            nChannels += acqValue.data.shape[1]
│ │              nSamples = max(nSamples, acqValue.data.shape[0])
│ │              angSeries.append(acqValue)
│ │  
│ │          # TTL event pulse data
│ │          elif ".TTLs" in str(acqValue.__class__):
│ │  
│ │              if acqValue.name == "TTL_PulseValues":
│ │ @@ -158,108 +155,130 @@
│ │          msg = "Found {} trials".format(trl.shape[0])
│ │      else:
│ │          trl = np.array([[0, nSamples, 0]])
│ │          msg = "No trial information found. Proceeding with single all-encompassing trial"
│ │  
│ │      # Print status update to inform user
│ │      SPYInfo(msg)
│ │ -    SPYInfo("Creating AnalogData object...")
│ │ +
│ │ +    # Check for filename
│ │ +    if container is not None:
│ │ +        if not isinstance(container, str):
│ │ +            raise SPYTypeError(container, varname="container", expected="str")
│ │ +        if not os.path.splitext(container)[1] == ".spy":
│ │ +            container += ".spy"
│ │ +        if not os.path.isdir(container):
│ │ +            os.makedirs(container)
│ │ +        fileInfo = filename_parser(container)
│ │ +        filebase = os.path.join(fileInfo["folder"],
│ │ +                                fileInfo["container"],
│ │ +                                fileInfo["basename"])
│ │  
│ │      # If TTL data was found, ensure we have exactly one set of values and associated
│ │      # channel markers
│ │      if max(len(ttlVals), len(ttlChans)) > min(len(ttlVals), len(ttlChans)):
│ │          lgl = "TTL pulse values and channel markers"
│ │          act = "pulses: {}, channels: {}".format(str(ttlVals), str(ttlChans))
│ │          raise SPYValueError(lgl, varname=ttlVals[0].name, actual=act)
│ │      if len(ttlVals) > 1:
│ │          lgl = "one set of TTL pulses"
│ │          act = "{} TTL data sets".format(len(ttlVals))
│ │          raise SPYValueError(lgl, varname=ttlVals[0].name, actual=act)
│ │  
│ │      # Use provided TTL data to initialize `EventData` object
│ │      evtData = None
│ │ +    objectDict = {}
│ │      if len(ttlVals) > 0:
│ │          msg = "Creating separate EventData object for embedded TTL pulse data..."
│ │          SPYInfo(msg)
│ │ -        evtData = EventData(dimord=["sample","eventid","chans"])
│ │ +        if container is not None:
│ │ +            filename = filebase + ".event"
│ │ +        else:
│ │ +            filename = None
│ │ +
│ │ +        evtData = EventData(dimord=["sample","eventid","chans"], filename=filename)
│ │          h5evt = h5py.File(evtData.filename, mode="w")
│ │ -        evtDset = h5evt.create_dataset("data", dtype=np.result_type(*ttlDtypes),
│ │ +        evtDset = h5evt.create_dataset("data", dtype=int,
│ │                                         shape=(ttlVals[0].data.size, 3))
│ │          # Column 1: sample indices
│ │          # Column 2: TTL pulse values
│ │          # Column 3: TTL channel markers
│ │          if 'resolution' in ttlChans[0].__nwbfields__:
│ │              ts_resolution = ttlChans[0].resolution
│ │          else:
│ │              ts_resolution = ttlChans[0].timestamps__resolution
│ │  
│ │          evtDset[:, 0] = ((ttlChans[0].timestamps[()] - tStarts[0]) / ts_resolution).astype(np.intp)
│ │ -        evtDset[:, 1] = ttlVals[0].data[()]
│ │ -        evtDset[:, 2] = ttlChans[0].data[()]
│ │ +        evtDset[:, 1] = ttlVals[0].data[()].astype(int)
│ │ +        evtDset[:, 2] = ttlChans[0].data[()].astype(int)
│ │          evtData.data = evtDset
│ │          evtData.samplerate = float(1 / ts_resolution)
│ │          if hasTrials:
│ │              evtData.trialdefinition = trl
│ │          else:
│ │              evtData.trialdefinition = np.array([[np.nanmin(evtDset[:,0]), np.nanmax(evtDset[:,0]), 0]])
│ │              msg = "No trial information found. Proceeding with single all-encompassing trial"
│ │  
│ │ -    # Allocate `AnalogData` object and use generated HDF5 file-name to manually
│ │ -    # allocate a target dataset for reading the NWB data
│ │ -    angData = AnalogData(dimord=AnalogData._defaultDimord)
│ │ -    angShape = [None, None]
│ │ -    angShape[angData.dimord.index("time")] = nSamples
│ │ -    angShape[angData.dimord.index("channel")] = nChannels
│ │ -    h5ang = h5py.File(angData.filename, mode="w")
│ │ -    angDset = h5ang.create_dataset("data", dtype=np.result_type(*dTypes), shape=angShape)
│ │ +        # Write logs
│ │ +        log_msg = "Read data from NWB file {}".format(nwbFullName)
│ │ +        evtData.log = log_msg
│ │ +        objectDict[os.path.basename(evtData.filename)] = evtData
│ │  
│ │ -    # Compute actually available memory (divide by 2 since we're working with an add'l tmp array)
│ │ +    # Compute actually available memory
│ │      pbarDesc = "Reading data in blocks of {} GB".format(round(memuse / 1000, 2))
│ │ -    memuse *= 1024**2 / 2
│ │ -    chanCounter = 0
│ │ +    memuse *= 1024**2
│ │  
│ │ -    # Process analog time series data and save stuff block by block (if necessary)
│ │ -    pbar = tqdm(angSeries, position=0)
│ │ +    # Process analog time series data and convert stuff block by block (if necessary)
│ │ +    pbar = tqdm(angSeries, position=0, disable=None)
│ │      for acqValue in pbar:
│ │ -
│ │          # Show dataset name in progress bar label
│ │          pbar.set_description("Loading {} from disk".format(acqValue.name))
│ │  
│ │ +        # Allocate `AnalogData` object and use generated HDF5 file-name to manually
│ │ +        # allocate a target dataset for reading the NWB data
│ │ +        if container is not None:
│ │ +            filename = filebase + "_" + acqValue.name + ".analog"
│ │ +        else:
│ │ +            filename = None
│ │ +
│ │ +        angData = AnalogData(dimord=AnalogData._defaultDimord, filename=filename)
│ │ +        angShape = [None, None]
│ │ +        angShape[angData.dimord.index("time")] = acqValue.data.shape[0]
│ │ +        angShape[angData.dimord.index("channel")] = acqValue.data.shape[1]
│ │ +        h5ang = h5py.File(angData.filename, mode="w")
│ │ +        angDset = h5ang.create_dataset("data", dtype=np.result_type(*dTypes), shape=angShape)
│ │ +
│ │ +        # If channel-specific gains are set, load them now
│ │ +        if acqValue.channel_conversion is not None:
│ │ +            gains = acqValue.channel_conversion[()]
│ │ +            if np.all(gains ==  gains[0]):
│ │ +                gains = gains[0]
│ │ +
│ │          # Given memory cap, compute how many data blocks can be grabbed per swipe:
│ │          # `nSamp` is the no. of samples that can be loaded into memory without exceeding `memuse`
│ │          # `rem` is the no. of remaining samples, s. t. ``nSamp + rem = angDset.shape[0]`
│ │          # `blockList` is a list of samples to load per swipe, i.e., `[nSamp, nSamp, ..., rem]`
│ │ -        nSamp = int(memuse / (np.prod(angDset.shape[1:]) * angDset.dtype.itemsize))
│ │ +        nSamp = int(memuse / (acqValue.data.shape[1] * angDset.dtype.itemsize))
│ │          rem = int(angDset.shape[0] % nSamp)
│ │          blockList = [nSamp] * int(angDset.shape[0] // nSamp) + [rem] * int(rem > 0)
│ │  
│ │ -        # If channel-specific gains are set, load them now
│ │ -        if acqValue.channel_conversion is not None:
│ │ -            gains = acqValue.channel_conversion[()]
│ │ -
│ │ -        # Write data block-wise to `angDset` (use `del` to wipe blocks from memory)
│ │ -        # Use 'unsafe' casting to allow `tmp` array conversion int -> float
│ │ -        endChan = chanCounter + acqValue.data.shape[1]
│ │ -        for m, M in enumerate(tqdm(blockList, desc=pbarDesc, position=1, leave=False)):
│ │ -            tmp = acqValue.data[m * nSamp: m * nSamp + M, :]
│ │ +        for m, M in enumerate(tqdm(blockList, desc=pbarDesc, position=1, leave=False, disable=None)):
│ │ +            st_samp, end_samp = m * nSamp, m * nSamp + M
│ │ +            angDset[st_samp : end_samp, :] = acqValue.data[st_samp : end_samp, :]
│ │              if acqValue.channel_conversion is not None:
│ │ -                np.multiply(tmp, gains, out=tmp, casting="unsafe")
│ │ -            angDset[m * nSamp: m * nSamp + M, chanCounter : endChan] = tmp
│ │ -            del tmp
│ │ -
│ │ -        # Update channel counter for next `acqValue``
│ │ -        chanCounter += acqValue.data.shape[1]
│ │ -
│ │ -    # Finalize angData
│ │ -    angData.data = angDset
│ │ -    angData.channel = chanNames
│ │ -    angData.samplerate = sRates[0]
│ │ -    angData.trialdefinition = trl
│ │ -    angData.info = {'starting_time' : tStarts[0]}
│ │ -
│ │ -    # Write logs
│ │ -    msg = "Read data from NWB file {}".format(nwbFullName)
│ │ -    angData.log = msg
│ │ -    if evtData is not None:
│ │ -        evtData.log = msg
│ │ +                angDset[st_samp : end_samp, :] *= gains
│ │ +
│ │ +        # Finalize angData
│ │ +        angData.data = angDset
│ │ +        channels = acqValue.electrodes[:].location
│ │ +        if channels.unique().size == 1:
│ │ +            SPYWarning("No channel names found for {}".format(acqName))
│ │ +            angData.channel = None
│ │ +        else:
│ │ +            angData.channel = channels.to_list()
│ │ +        angData.samplerate = sRates[0]
│ │ +        angData.trialdefinition = trl
│ │ +        angData.info = {'starting_time' : tStarts[0]}
│ │ +        angData.log = log_msg
│ │ +        objectDict[os.path.basename(angData.filename)] = angData
│ │  
│ │ -    return angData, evtData
│ │ +    return objectDict
│ │   --- esi-syncopy-2022.8/syncopy/io/load_ft.py
│ ├── +++ esi_syncopy-2023.3/syncopy/io/load_ft.py
│ │┄ Files 4% similar despite different names
│ │ @@ -1,68 +1,67 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │  # Load data from Field Trip .mat files
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │ -import sys
│ │  import re
│ │  import h5py
│ │  import numpy as np
│ │  from scipy import io as sio
│ │  from tqdm import tqdm
│ │  
│ │  # Local imports
│ │ -from syncopy.shared.errors import SPYValueError, SPYInfo, SPYWarning, SPYTypeError
│ │ +from syncopy.shared.errors import SPYValueError, SPYInfo, SPYWarning
│ │  from syncopy.shared.parsers import io_parser, sequence_parser, scalar_parser
│ │  from syncopy.datatype import AnalogData
│ │  
│ │  __all__ = ["load_ft_raw"]
│ │  
│ │  # Required fields for the ft_datatype_raw
│ │  req_fields_raw = ('time', 'trial', 'label')
│ │  
│ │  
│ │  def load_ft_raw(filename,
│ │                  list_only=False,
│ │                  select_structures=None,
│ │                  include_fields=None,
│ │ -                mem_use=2000):
│ │ +                mem_use=4000):
│ │  
│ │      """
│ │      Imports raw time-series data from Field Trip
│ │      into potentially multiple :class:`~syncopy.AnalogData` objects,
│ │      one for each structure found within the MAT-file.
│ │  
│ │ -    For MAT-File < v7.3 the MAT-file gets loaded completely
│ │ -    into RAM, but its size should be capped by Matlab at 2GB.
│ │ -    The v7.3 is in hdf5 format and will be read in trial-by-trial,
│ │ -    this should be the Matlab default for MAT-Files exceeding 2GB.
│ │ -
│ │      The aim is to parse each FT data structure, which
│ │      have the following fields (Syncopy analogon on the right):
│ │  
│ │      +--------------------+------------+
│ │      | FT                 | Syncopy    |
│ │      +====================+============+
│ │      | label              | channel    |
│ │      +--------------------+------------+
│ │      | trial              | trial      |
│ │      +--------------------+------------+
│ │      | time               | time       |
│ │      +--------------------+------------+
│ │ +    | trialinfo          | trialinfo  |
│ │ +    +--------------------+------------+
│ │      | fsample (optional) | samplerate |
│ │      +--------------------+------------+
│ │ -    | cfg                | ?          |
│ │ +    | cfg                | cfg        |
│ │      +--------------------+------------+
│ │  
│ │ +    Limitations:
│ │ +
│ │      The FT `cfg` contains a lot of meta data which at the
│ │ -    moment we don't import into Syncopy.
│ │ +    moment we don't import into Syncopy. Syncopy however has
│ │ +    it's own `cfg` mirroring FT's functionality (replay analyses)
│ │  
│ │ -    This is still experimental code, use with caution!!
│ │ +    FT's `sampleinfo` is not generally compatible with Syncopy
│ │  
│ │      Parameters
│ │      ----------
│ │      filename: str
│ │          Path to the MAT-file
│ │      list_only: bool, optional
│ │          Set to `True` to return only a list containing the names
│ │ @@ -81,14 +80,21 @@
│ │  
│ │      Returns
│ │      -------
│ │      out_dict: dict
│ │          Dictionary with the names of the structures as keys loaded from the MAT-File,
│ │          and :class:`~syncopy.AnalogData` datasets as values
│ │  
│ │ +    Notes
│ │ +    -----
│ │ +    For MAT-File < v7.3 the MAT-file gets loaded completely
│ │ +    into RAM using :func:`scipy.io.loadmat`, but its size should be capped by Matlab at 2GB.
│ │ +    The >v7.3 MAT-files are in hdf5 format and will be read in trial-by-trial,
│ │ +    this should be the Matlab default for MAT-files exceeding 2GB.
│ │ +
│ │      See also
│ │      --------
│ │      `MAT-File formats <https://de.mathworks.com/help/matlab/import_export/mat-file-versions.html>`_
│ │      `Field Trip datastructures <https://www.fieldtriptoolbox.org/development/datastructure/>`_
│ │  
│ │      Examples
│ │      --------
│ │ @@ -198,15 +204,14 @@
│ │              SPYWarning(msg)
│ │              continue
│ │  
│ │          structure = struct_container[skey]
│ │          _check_req_fields(req_fields_raw, structure)
│ │          # the AnalogData objs
│ │          adata = struct_reader(structure)
│ │ -        thisMethod = sys._getframe().f_code.co_name.replace("_", "")
│ │  
│ │          # Write log-entry
│ │          msg = f"loaded struct `{skey}` from Matlab file version {version}\n"
│ │          msg += f"\tsource file: {filename}"
│ │          adata.log = msg
│ │  
│ │          out_dict[skey] = adata
│ │ @@ -259,67 +264,78 @@
│ │          AData.samplerate = h5Group['fsample'][0, 0]
│ │      else:
│ │          AData.samplerate = _infer_fsample(h5File[time_refs[0]])
│ │  
│ │      # -- retrieve shape information --
│ │      nTrials = trl_refs.size
│ │  
│ │ -    # peek in 1st trial to determine single trial shape
│ │ -    # we only support equal trial lengths at this stage
│ │ -    nSamples, nChannels = h5File[trl_refs[0]].shape
│ │ -    nTotalSamples = nTrials * nSamples
│ │ +    # peek in 1st trial to determine the number of channels
│ │ +    # and one trial size for
│ │ +    nSamples1, nChannels = h5File[trl_refs[0]].shape
│ │ +    # compute total hdf5 shape
│ │ +    # we stack along 1st axis
│ │ +    trlSamples = [h5File[ref].shape[0] for ref in trl_refs]
│ │ +    # in samples
│ │ +    mean_trl_size = np.mean(trlSamples)
│ │ +    nTotalSamples = np.sum(trlSamples)
│ │ +    # get sample indices
│ │ +    si = np.r_[0, np.cumsum(trlSamples)]
│ │ +    sampleinfo = np.column_stack([si[:-1], si[1:]])
│ │  
│ │      itemsize = h5File[trl_refs[0]].dtype.itemsize
│ │      # in Mbyte
│ │ -    trl_size = itemsize * nSamples * nChannels / 1e6
│ │ +    trl_size = itemsize * mean_trl_size * nChannels / 1e6
│ │  
│ │      # assumption: single trial fits into RAM
│ │      if trl_size >= 0.4 * mem_use:
│ │          lgl = f'{2.5 * trl_size} or more MB'
│ │          actual = f"{mem_use}"
│ │          raise SPYValueError(lgl, varname='mem_use', actual=actual)
│ │  
│ │      # -- IO process --
│ │  
│ │      # create new hdf5 dataset for our AnalogData
│ │      # with the default dimord ['time', 'channel']
│ │      # and our default data type np.float32 -> implicit casting!
│ │ -    h5FileOut = h5py.File(AData.filename, mode="w")
│ │ -    ADset = h5FileOut.create_dataset("data",
│ │ -                                     dtype=np.float32,
│ │ -                                     shape=[nTotalSamples, nChannels])
│ │ -
│ │ -    pbar = tqdm(trl_refs, desc=f"{struct_name} - loading {nTrials} trials")
│ │ -    SampleCounter = 0   # trial stacking
│ │ +    with h5py.File(AData.filename, mode="w") as h5FileOut:
│ │ +        ADset = h5FileOut.create_dataset("data",
│ │ +                                         dtype=np.float32,
│ │ +                                         shape=[nTotalSamples, nChannels])
│ │ +
│ │ +        pbar = tqdm(trl_refs, desc=f"{struct_name} - loading {nTrials} trials", disable=None)
│ │ +
│ │ +        SampleCounter = 0   # trial stacking
│ │ +        # one swipe per trial
│ │ +        for tr in pbar:
│ │ +            trl_array = h5File[tr]
│ │ +            # in samples
│ │ +            trl_samples = trl_array.shape[0]
│ │ +            ADset[SampleCounter:SampleCounter + trl_samples, :] = trl_array
│ │ +            SampleCounter += trl_samples
│ │ +        pbar.close()
│ │  
│ │ -    # one swipe per trial
│ │ -    for tr in pbar:
│ │ -        trl_array = h5File[tr]
│ │ -        if trl_array.shape != (nSamples, nChannels):
│ │ -            lgl = 'trials of equal lenghts'
│ │ -            actual = 'trials of unequal lengths'
│ │ -            raise SPYValueError(lgl, actual=actual)
│ │ -        ADset[SampleCounter:SampleCounter + nSamples, :] = trl_array
│ │ -        SampleCounter += nSamples
│ │ -    pbar.close()
│ │ +        AData.data = ADset
│ │  
│ │ -    AData.data = ADset
│ │ +    AData._reopen()
│ │  
│ │      # -- trialdefinition --
│ │  
│ │ -    nTr_rng = np.arange(nTrials)
│ │ -    sampleinfo = np.vstack([nTr_rng, nTr_rng + 1]).T * nSamples
│ │ -
│ │      offsets = []
│ │      # we need to look into the time vectors for each trial
│ │      for time_r in time_refs:
│ │          offsets.append(h5File[time_r][0, 0])
│ │ -    offsets = np.array(offsets)
│ │ -
│ │ +    offsets = np.rint(np.array(offsets) * AData.samplerate)
│ │      trl_def = np.hstack([sampleinfo, offsets[:, None]])
│ │ +
│ │ +    # check if there is a 'trialinfo'
│ │ +    try:
│ │ +        trl_def = np.hstack([trl_def, h5Group['trialinfo']])
│ │ +    except KeyError:
│ │ +        pass
│ │ +
│ │      AData.trialdefinition = trl_def
│ │  
│ │      # each channel label is an integer array with shape (X, 1),
│ │      # where `X` is the number of ascii encoded characters
│ │      channels = [''.join(map(chr, h5File[cr][:, 0])) for cr in chan_refs]
│ │      AData.channel = channels
│ │  
│ │ @@ -384,52 +400,71 @@
│ │      | cfg                | X          |
│ │      +--------------------+------------+
│ │  
│ │      Each trial in FT has nChannels x nSamples ordering,
│ │      Syncopy has nSamples x nChannels
│ │      """
│ │  
│ │ -    # nTrials = structure["trial"].shape[0]
│ │ -    trials = []
│ │ -
│ │ -    # 1st trial as reference
│ │ -    nChannels, nSamples = structure['trial'][0].shape
│ │ -
│ │ -    # check equal trial lengths
│ │ -    for trl in structure['trial']:
│ │ -
│ │ -        if trl.shape[-1] != nSamples:
│ │ -            lgl = 'Trials of equal lengths'
│ │ -            actual = 'Trials of unequal lengths'
│ │ -            raise SPYValueError(lgl, varname="load .mat", actual=actual)
│ │ -
│ │ -        # channel x sample ordering in FT
│ │ -        # default data type np.float32 -> implicit casting!
│ │ -        trials.append(trl.T.astype(np.float32))
│ │  
│ │      # initialize AnalogData
│ │      if 'fsample' in structure:
│ │          samplerate = structure['fsample']
│ │      else:
│ │          samplerate = _infer_fsample(structure['time'][0])
│ │  
│ │ -    AData = AnalogData(trials, samplerate=samplerate)
│ │ +    AData = AnalogData(samplerate=samplerate)
│ │ +
│ │ +    # compute total hdf5 shape
│ │ +    # we use fixed stacking along 1st axis
│ │ +    # but channel x sample ordering in FT
│ │ +    nTotalSamples = np.sum([trl.shape[1] for trl in structure['trial']])
│ │ +    nChannels = structure['trial'][0].shape[0]
│ │ +    sampleinfo = []
│ │ +
│ │ +    with h5py.File(AData._filename, 'w') as h5file:
│ │ +
│ │ +        dset = h5file.create_dataset("data",
│ │ +                                     dtype=np.float32,
│ │ +                                     shape=[nTotalSamples, nChannels])
│ │ +
│ │ +        stack_count = 0
│ │ +        for trl in structure['trial']:
│ │ +            trl_size = trl.shape[1]
│ │ +            # default data type np.float32 -> implicit casting!
│ │ +            dset[stack_count:stack_count + trl_size] = trl.T.astype(np.float32)
│ │ +
│ │ +            # construct on the fly to cover all the trials
│ │ +            sampleinfo.append(np.array([stack_count, stack_count + trl_size]))
│ │ +
│ │ +            stack_count += trl_size
│ │ +
│ │ +        AData.data = dset
│ │ +
│ │ +    AData._reopen()
│ │ +
│ │ +    sampleinfo = np.array(sampleinfo)
│ │  
│ │      # get the channel ids
│ │      channels = structure['label']
│ │      # set the channel ids
│ │      AData.channel = list(channels.astype(str))
│ │  
│ │ -    # update trialdefinition
│ │ -    times_array = np.vstack(structure['time'])
│ │ -
│ │ -    # nTrials x nSamples
│ │ -    offsets = times_array[:, 0] * AData.samplerate
│ │ +    # get the offets
│ │ +    offsets = np.array([tvec[0] for tvec in structure['time']])
│ │ +    offsets *= AData.samplerate
│ │ +
│ │ +    # build trialdefinition
│ │ +    trl_def = np.column_stack([sampleinfo, offsets])
│ │ +
│ │ +    # check if there is a 'trialinfo'
│ │ +    try:
│ │ +        trl_def = np.hstack([trl_def, structure['trialinfo']])
│ │ +    except KeyError:
│ │ +        pass
│ │  
│ │ -    trl_def = np.hstack([AData.sampleinfo, offsets[:, None]])
│ │      AData.trialdefinition = trl_def
│ │  
│ │      # -- Additional Fields --
│ │  
│ │      if include_fields is not None:
│ │          AData.info = {}
│ │          # additional fields in MAT-File
│ │ @@ -466,15 +501,15 @@
│ │      with open(filename, 'rb') as matfile:
│ │          line1 = next(matfile)
│ │          # relevant information
│ │          header = line1[:76].decode()
│ │  
│ │      # matches for example 'MATLAB 5.01'
│ │      # with the version as only capture group
│ │ -    pattern = re.compile("^MATLAB\s(\d*\.\d*)")
│ │ +    pattern = re.compile(r"^MATLAB\s(\d*\.\d*)")
│ │      match = pattern.match(header)
│ │  
│ │      if not match:
│ │          lgl = 'recognizable .mat file'
│ │          actual = 'can not recognize .mat file'
│ │          raise SPYValueError(lgl, filename, actual)
│ │   --- esi-syncopy-2022.8/syncopy/io/load_spy_container.py
│ ├── +++ esi_syncopy-2023.3/syncopy/io/load_spy_container.py
│ │┄ Files 11% similar despite different names
│ │ @@ -13,16 +13,16 @@
│ │  
│ │  # Local imports
│ │  from syncopy.shared.filetypes import FILE_EXT
│ │  from syncopy.shared.parsers import io_parser, data_parser, filename_parser, array_parser
│ │  from syncopy.shared.errors import (SPYTypeError, SPYValueError, SPYIOError,
│ │                                     SPYError, SPYWarning)
│ │  from syncopy.io.utils import hash_file, startInfoDict
│ │ -
│ │  import syncopy.datatype as spd
│ │ +import syncopy as spy
│ │  
│ │  # to allow loading older spy containers
│ │  legacy_not_required = ['info']
│ │  
│ │  __all__ = ["load"]
│ │  
│ │  
│ │ @@ -34,15 +34,15 @@
│ │      multiple objects from a single '.spy'-container. Loading from containers can
│ │      be further controlled by imposing restrictions on object class(es) (via
│ │      `dataclass`) and file-name tag(s) (via `tag`).
│ │  
│ │      Parameters
│ │      ----------
│ │      filename : str
│ │ -        Either path to Syncopy container folder (\*.spy, if omitted, the extension
│ │ +        Either path to Syncopy container folder (`*.spy`, if omitted, the extension
│ │          '.spy' will be appended) or name of data or metadata file. If `filename`
│ │          points to a container and no further specifications are provided, the
│ │          entire contents of the container is loaded. Otherwise, specific objects
│ │          may be selected using the `dataclass` or `tag` keywords (see below).
│ │      tag : None or str or list
│ │          If `filename` points to a container, `tag` may be used to filter objects
│ │          by filename-`tag`. Multiple tags can be provided using a list, e.g.,
│ │ @@ -138,14 +138,15 @@
│ │  
│ │      # Ensure `filename` is either a valid .spy container or data file: if `filename`
│ │      # is a directory w/o '.spy' extension, append it
│ │      if not isinstance(filename, str):
│ │          raise SPYTypeError(filename, varname="filename", expected="str")
│ │      if len(os.path.splitext(os.path.abspath(os.path.expanduser(filename)))[1]) == 0:
│ │          filename += FILE_EXT["dir"]
│ │ +
│ │      try:
│ │          fileInfo = filename_parser(filename)
│ │      except Exception as exc:
│ │          raise exc
│ │  
│ │      if tag is not None:
│ │          if isinstance(tag, str):
│ │ @@ -178,15 +179,15 @@
│ │                  lgl = "extension(s) '" + "or '".join(ext + "' " for ext in FILE_EXT["data"])
│ │                  raise SPYValueError(legal=lgl, varname="dataclass", actual=str(dataclass))
│ │  
│ │      # Avoid any misunderstandings here...
│ │      if not isinstance(checksum, bool):
│ │          raise SPYTypeError(checksum, varname="checksum", expected="bool")
│ │  
│ │ -    # Abuse `AnalogData.mode`-setter to vet `mode`
│ │ +    # Abuse `AnalogData.mode`-setter to check `mode`
│ │      try:
│ │          spd.AnalogData().mode = mode
│ │      except Exception as exc:
│ │          raise exc
│ │  
│ │      # If `filename` points to a spy container, `glob` what's inside, otherwise just load
│ │      if fileInfo["filename"] is None:
│ │ @@ -283,16 +284,32 @@
│ │          out.dimord = dimord
│ │      else:
│ │          out = dataclass(dimord=dimord)
│ │          new_out = True
│ │  
│ │      # Access data on disk (error checking is done by setters)
│ │      out.mode = mode
│ │ +
│ │ +    # If the JSON contains `_hdfFileDatasetProperties`, load all datasets listed in there. Otherwise, load the ones
│ │ +    # already defined by `out._hdfFileDatasetProperties` and defined in the respective data class.
│ │ +    # This is needed to load both new files with, and legacy files without the `_hdfFileDatasetProperties` in the JSON.
│ │ +    json_hdfFileDatasetProperties = jsonDict.pop("_hdfFileDatasetProperties", None) # They may not be in there for legacy files, so allow None.
│ │ +    if json_hdfFileDatasetProperties is not None:
│ │ +        out._hdfFileDatasetProperties = tuple(json_hdfFileDatasetProperties) # It's a list in the JSON, so convert to tuple.
│ │      for datasetProperty in out._hdfFileDatasetProperties:
│ │ -        setattr(out, datasetProperty, h5py.File(hdfFile, mode="r")[datasetProperty])
│ │ +        targetProperty = datasetProperty if datasetProperty == "data" else "_" + datasetProperty
│ │ +        try:
│ │ +            setattr(out, targetProperty, h5py.File(hdfFile, mode="r")[datasetProperty])
│ │ +        except KeyError:
│ │ +            if datasetProperty == "data":
│ │ +                raise SPYError("Data file {file} does not contain a dataset named 'data'.".format(file=hdfFile))
│ │ +            else:
│ │ +                spy.log(f"Dataset '{datasetProperty}' not present in HDF5 file, cannot load it. Setting to None.", level="DEBUG")
│ │ +                # It is fine if an extra dataset is not present in the file, e.g., the SpikeData waveform dataset is not present when set to None.
│ │ +                setattr(out, targetProperty, None)
│ │  
│ │  
│ │      # Abuse ``definetrial`` to set trial-related props
│ │      trialdef = h5py.File(hdfFile, mode="r")["trialdefinition"][()]
│ │      out.definetrial(trialdef)
│ │  
│ │      # Assign metadata
│ │   --- esi-syncopy-2022.8/syncopy/io/load_tdt.py
│ ├── +++ esi_syncopy-2023.3/syncopy/io/load_tdt.py
│ │┄ Files 1% similar despite different names
│ │ @@ -20,15 +20,14 @@
│ │  
│ │  
│ │  # --- The user exposed function ---
│ │  
│ │  
│ │  def load_tdt(data_path, start_code=None, end_code=None,
│ │               subtract_median=False):
│ │ -
│ │      """
│ │      Imports TDT time series data and meta-information
│ │      into a single :class:`~syncopy.AnalogData` object.
│ │  
│ │      An ad-hoc trialdefinition will be attached if
│ │      both `start_code` and `end_code` are given.
│ │      Otherwise a single all-to-all trialdefinition
│ │ @@ -300,15 +299,15 @@
│ │                          "size": heads[0, unique_ind[counter]],
│ │                          "buddy": heads[3, unique_ind[counter]],
│ │                          "temp": heads[:, unique_ind[counter]],
│ │                      }
│ │                  )
│ │  
│ │              # Looking for only Mark, PDi\ and PDio
│ │ -            looking_for = ["Mark", "PDio", "LFPs", "PDi\\"]  #
│ │ +            looking_for = ["Mark", "PDio", "LFPs", "LFP1", "PDi\\"]  #
│ │              targets = StructDict()
│ │              for chk, content in enumerate(store_codes):
│ │                  if self.code_to_name(content["code"]) in looking_for:
│ │                      targets[self.code_to_name(content["code"])] = chk
│ │  
│ │              for tar in targets.items():
│ │                  store_code = store_codes[tar[1]]
│ │ @@ -678,15 +677,18 @@
│ │                      # remove channels field
│ │                      delattr(data[current_type_str][current_name], "chan")
│ │          tsq.close()
│ │          del epocs
│ │          del header
│ │          Data = StructDict()
│ │          Data.PDio = data.epocs.PDio
│ │ -        Data.LFPs = data.streams.LFPs
│ │ +        try:
│ │ +            Data.LFPs = data.streams.LFPs
│ │ +        except Exception:
│ │ +            Data.LFPs = data.streams.LFP1
│ │          Data.Mark = data.scalars.Mark
│ │          Data.info = data.info
│ │          return Data
│ │  
│ │  
│ │  class ESI_TDTdata:
│ │      def __init__(
│ │ @@ -723,26 +725,27 @@
│ │              for chunk in iter(lambda: f.read(128 * hash.block_size), b""):
│ │                  hash.update(chunk)
│ │          return hash.hexdigest()
│ │  
│ │      def data_aranging(self, Files, DataInfo_loaded):
│ │          AData = spy.AnalogData(dimord=['time', 'channel'])
│ │          hdf_out_path = AData.filename
│ │ +        LenOfData = self.read_data(Files[0]).shape[0]  # Lenght of the data is always set to the length of the first channel
│ │          with h5py.File(hdf_out_path, "w") as combined_data_file:
│ │              idxStartStop = [
│ │                  np.clip(np.array((jj, jj + self.chan_in_chunks)), a_min=None, a_max=len(Files))
│ │                  for jj in range(0, len(Files), self.chan_in_chunks)
│ │              ]
│ │              print(
│ │                  "Merging {0} files in {1} chunks each with {2} channels into \n   {3}".format(
│ │                      len(Files), len(idxStartStop), self.chan_in_chunks, hdf_out_path
│ │                  )
│ │              )
│ │ -            for (start, stop) in tqdm(iterable=idxStartStop, desc="chunk", unit="chunk"):
│ │ -                data = [self.read_data(Files[jj]) for jj in range(start, stop)]
│ │ +            for (start, stop) in tqdm(iterable=idxStartStop, desc="chunk", unit="chunk", disable=None):
│ │ +                data = [self.read_data(Files[jj])[:LenOfData] for jj in range(start, stop)]
│ │                  data = np.vstack(data).T
│ │                  if start == 0:
│ │                      # this is the actual dataset for the AnalogData
│ │                      target = combined_data_file.create_dataset(
│ │                          "data", shape=(data.shape[0], len(Files)), dtype="single"
│ │                      )
│ │                  if self.subtract_median:
│ │ @@ -782,15 +785,14 @@
│ │  
│ │          return AData
│ │  
│ │  
│ │  # --- Helpers ---
│ │  
│ │  def _mk_trialdef(adata, start_code, end_code):
│ │ -
│ │      """
│ │      Create a basic trialdefinition from the trial
│ │      start and end trigger codes
│ │      """
│ │  
│ │      # trigger codes and samples
│ │      trg_codes = np.array(adata.info['Trigger_code'], dtype=int)
│ │ @@ -824,14 +826,15 @@
│ │  
│ │      trldef = np.zeros((N, 3))
│ │      trldef[:, 0] = trl_starts[:N]
│ │      trldef[:, 1] = trl_ends[:N]
│ │  
│ │      return trldef
│ │  
│ │ +
│ │  def _get_source_paths(directory, ext=".sev"):
│ │      """
│ │      Returns all abs. paths in `directory`
│ │      for files which end with `ext`
│ │      """
│ │  
│ │      # get all data source file names
│ │   --- esi-syncopy-2022.8/syncopy/io/save_spy_container.py
│ ├── +++ esi_syncopy-2023.3/syncopy/io/save_spy_container.py
│ │┄ Files 9% similar despite different names
│ │ @@ -5,14 +5,15 @@
│ │  
│ │  # Builtin/3rd party package imports
│ │  import os
│ │  import json
│ │  import h5py
│ │  import numpy as np
│ │  from collections import OrderedDict
│ │ +import syncopy as spy
│ │  
│ │  # Local imports
│ │  from syncopy.shared.filetypes import FILE_EXT
│ │  from syncopy.shared.parsers import filename_parser, data_parser
│ │  from syncopy.shared.errors import SPYIOError, SPYTypeError, SPYError, SPYWarning
│ │  from syncopy.io.utils import hash_file, startInfoDict
│ │  from syncopy import __storage__
│ │ @@ -144,17 +145,17 @@
│ │  
│ │      if not isinstance(overwrite, bool):
│ │          raise SPYTypeError(overwrite, varname="overwrite", expected="bool")
│ │  
│ │      # Parse filename for validity and construct full path to HDF5 file
│ │      fileInfo = filename_parser(filename)
│ │      if fileInfo["extension"] != out._classname_to_extension():
│ │ -        raise SPYError("""Extension in filename ({ext}) does not match data
│ │ -                    class ({dclass})""".format(ext=fileInfo["extension"],
│ │ -                                               dclass=out.__class__.__name__))
│ │ +        raise SPYError("""Extension in filename ('{ext}') does not match data
│ │ +                    class ({dclass}), expected '{exp}'.""".format(ext=fileInfo["extension"],
│ │ +                                               dclass=out.__class__.__name__, exp=out._classname_to_extension()))
│ │      dataFile = os.path.join(fileInfo["folder"], fileInfo["filename"])
│ │  
│ │      # If `out` is to replace its own on-disk representation, be more careful
│ │      if overwrite and dataFile == out.filename:
│ │          replace = True
│ │      else:
│ │          replace = False
│ │ @@ -187,16 +188,20 @@
│ │                          raise SPYError(msg.format(dataFile, str(exc)))
│ │                  else:
│ │                      raise SPYIOError(dataFile, exists=True)
│ │          h5f = h5py.File(dataFile, mode="w")
│ │  
│ │          # Save each member of `_hdfFileDatasetProperties` in target HDF file
│ │          for datasetName in out._hdfFileDatasetProperties:
│ │ -            dataset = getattr(out, datasetName)
│ │ -            dat = h5f.create_dataset(datasetName, data=dataset)
│ │ +            dataset = getattr(out, "_" + datasetName)
│ │ +            if dataset is not None:
│ │ +                spy.log(f"Writing dataset '{datasetName}' ({len(out._hdfFileDatasetProperties)} datasets total) to HDF5 file '{dataFile}'.", level="DEBUG")
│ │ +                dat = h5f.create_dataset(datasetName, data=dataset)
│ │ +            else:
│ │ +                spy.log(f"Not writing 'None 'dataset '{datasetName}' ({len(out._hdfFileDatasetProperties)} datasets total) to HDF5 file '{dataFile}'.", level="DEBUG")
│ │  
│ │      # Now write trial-related information
│ │      trl_arr = np.array(out.trialdefinition)
│ │      if replace:
│ │          trl[()] = trl_arr
│ │          trl.flush()
│ │      else:
│ │ @@ -245,20 +250,27 @@
│ │                      "Please refer to {} for complete listing."
│ │                  info_fle = os.path.split(os.path.split(filename.format(ext=FILE_EXT["info"]))[0])[1]
│ │                  info_fle = os.path.join(info_fle, os.path.basename(
│ │                      filename.format(ext=FILE_EXT["info"])))
│ │                  SPYWarning(msg.format(key, info_fle))
│ │                  h5f.attrs[key] = [outDict[key][0], "...", outDict[key][-1]]
│ │  
│ │ +    # Save the dataset names that should be loaded later into the JSON.
│ │ +    outDict['_hdfFileDatasetProperties'] = list(out._hdfFileDatasetProperties)
│ │ +
│ │ +
│ │      # Re-assign filename after saving (and remove source in case it came from `__storage__`)
│ │      if not replace:
│ │          h5f.close()
│ │          if __storage__ in out.filename:
│ │              out.data.file.close()
│ │ -            os.unlink(out.filename)
│ │ +            try:
│ │ +                os.unlink(out.filename)
│ │ +            except PermissionError as ex:
│ │ +                print(f"Could not delete file '{out.filename}': {str(ex)}.")
│ │          out.data = dataFile
│ │  
│ │      # Compute checksum and finally write JSON (automatically overwrites existing)
│ │      outDict["file_checksum"] = hash_file(dataFile)
│ │  
│ │      with open(infoFile, 'w') as out_json:
│ │          json.dump(outDict, out_json, indent=4)
│ │   --- esi-syncopy-2022.8/syncopy/nwanalysis/AV_compRoutines.py
│ ├── +++ esi_syncopy-2023.3/syncopy/connectivity/AV_compRoutines.py
│ │┄ Files 6% similar despite different names
│ │ @@ -17,15 +17,16 @@
│ │  # backend method imports
│ │  from .csd import normalize_csd
│ │  from .wilson_sf import wilson_sf, regularize_csd
│ │  from .granger import granger
│ │  
│ │  # syncopy imports
│ │  from syncopy.shared.const_def import spectralDTypes
│ │ -from syncopy.shared.computational_routine import ComputationalRoutine
│ │ +from syncopy.shared.computational_routine import ComputationalRoutine, propagate_properties
│ │ +from syncopy.shared.metadata import metadata_from_hdf5_file, cast_0array
│ │  from syncopy.shared.kwarg_decorators import process_io
│ │  from syncopy.shared.errors import (
│ │      SPYValueError,
│ │  )
│ │  
│ │  
│ │  @process_io
│ │ @@ -47,32 +48,32 @@
│ │  
│ │      The coherence is now defined as either ``|C_ij|``
│ │      or ``|C_ij|^2``, this can be controlled with the `output`
│ │      parameter.
│ │  
│ │      Parameters
│ │      ----------
│ │ -    csd_av_dat : (1, nFreq, N, N) :class:`numpy.ndarray`
│ │ +    csd_av_dat : (nTime, nFreq, N, N) :class:`numpy.ndarray`
│ │          Cross-spectral densities for `N` x `N` channels
│ │          and `nFreq` frequencies averaged over trials.
│ │ -    output : {'abs', 'pow', 'fourier'}, default: 'abs'
│ │ +    output : {'abs', 'pow', 'fourier', 'real', 'imag'}, default: 'abs'
│ │          Also after normalization the coherency is still complex (`'complex'`),
│ │          to get the real valued coherence ``0 < C_ij(f) < 1`` one can either take the
│ │          absolute (`'abs'`) or the absolute squared (`'pow'`) values of the
│ │          coherencies. The definitions are not uniform in the literature,
│ │          hence multiple output types are supported. Additionally `'angle'`,
│ │          `'imag'` or `'real'` are supported.
│ │      noCompute : bool
│ │          Preprocessing flag. If `True`, do not perform actual calculation but
│ │          instead return expected shape and :class:`numpy.dtype` of output
│ │          array.
│ │  
│ │      Returns
│ │      -------
│ │ -    CS_ij : (1, nFreq, N, N) :class:`numpy.ndarray`
│ │ +    CS_ij : (nTime, nFreq, N, N) :class:`numpy.ndarray`
│ │          Coherence for all channel combinations ``i,j``.
│ │          `N` corresponds to number of input channels.
│ │  
│ │      Notes
│ │      -----
│ │  
│ │      This method is intended to be used as
│ │ @@ -102,18 +103,17 @@
│ │      if output in ['complex', 'fourier']:
│ │          fmt = spectralDTypes['fourier']
│ │      else:
│ │          fmt = spectralDTypes['abs']
│ │      if noCompute:
│ │          return outShape, fmt
│ │  
│ │ -    CS_ij = normalize_csd(csd_av_dat[0], output)
│ │ +    CS_ij = normalize_csd(csd_av_dat, output)
│ │  
│ │ -    # re-attach dummy time axis
│ │ -    return CS_ij[None, ...]
│ │ +    return CS_ij
│ │  
│ │  
│ │  class NormalizeCrossSpectra(ComputationalRoutine):
│ │  
│ │      """
│ │      Compute class that normalizes trial averaged csd's
│ │      of :class:`~syncopy.CrossSpectralData` objects
│ │ @@ -129,15 +129,15 @@
│ │      """
│ │  
│ │      # the hard wired dimord of the cF
│ │      dimord = ['time', 'freq', 'channel_i', 'channel_j']
│ │  
│ │      computeFunction = staticmethod(normalize_csd_cF)
│ │  
│ │ -    method = "" # there is no backend
│ │ +    method = ""  # there is no backend
│ │      # 1st argument,the data, gets omitted
│ │      valid_kws = list(signature(normalize_csd_cF).parameters.keys())[1:]
│ │  
│ │      def pre_check(self):
│ │          '''
│ │          Make sure we have a trial average,
│ │          so the input data only consists of `1 trial`.
│ │ @@ -152,40 +152,17 @@
│ │          if self.numTrials != 1:
│ │              lgl = "1 trial: normalizations can only be done on averaged quantities!"
│ │              act = f"DataSet contains {self.numTrials} trials"
│ │              raise SPYValueError(legal=lgl, varname="data", actual=act)
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │ -        # Some index gymnastics to get trial begin/end "samples"
│ │ -        if data.selection is not None:
│ │ -            chanSec_i = data.selection.channel_i
│ │ -            chanSec_j = data.selection.channel_j
│ │ -            trl = data.selection.trialdefinition
│ │ -            for row in range(trl.shape[0]):
│ │ -                trl[row, :2] = [row, row + 1]
│ │ -        else:
│ │ -            chanSec_i = slice(None)
│ │ -            chanSec_j = slice(None)
│ │ -            time = np.arange(len(data.trials))
│ │ -            time = time.reshape((time.size, 1))
│ │ -            trl = np.hstack((time, time + 1,
│ │ -                             np.zeros((len(data.trials), 1)),
│ │ -                             np.array(data.trialinfo)))
│ │ -
│ │ -        # Attach constructed trialdef-array (if even necessary)
│ │ -        if self.keeptrials:
│ │ -            out.trialdefinition = trl
│ │ -        else:
│ │ -            out.trialdefinition = np.array([[0, 1, 0]])
│ │ +        time_axis = np.any(np.diff(data.trialdefinition)[:,0] != 1)
│ │  
│ │ -        # Attach remaining meta-data
│ │ -        out.samplerate = data.samplerate
│ │ -        out.channel_i = np.array(data.channel_i[chanSec_i])
│ │ -        out.channel_j = np.array(data.channel_j[chanSec_j])
│ │ +        propagate_properties(data, out, self.keeptrials, time_axis)
│ │          out.freq = data.freq
│ │  
│ │  
│ │  @process_io
│ │  def normalize_ccov_cF(trl_av_dat,
│ │                        chunkShape=None,
│ │                        noCompute=False):
│ │ @@ -270,15 +247,15 @@
│ │      """
│ │  
│ │      # the hard wired dimord of the cF
│ │      dimord = ['time', 'freq', 'channel_i', 'channel_j']
│ │  
│ │      computeFunction = staticmethod(normalize_ccov_cF)
│ │  
│ │ -    method = "" # there is no backend
│ │ +    method = ""   # there is no backend
│ │      # 1st argument,the data, gets omitted
│ │      valid_kws = list(signature(normalize_ccov_cF).parameters.keys())[1:]
│ │  
│ │      def pre_check(self):
│ │          '''
│ │          Make sure we have a trial average,
│ │          so the input data only consists of `1 trial`.
│ │ @@ -312,15 +289,15 @@
│ │          out.samplerate = data.samplerate
│ │          out.channel_i = np.array(data.channel_i[chanSec_i])
│ │          out.channel_j = np.array(data.channel_j[chanSec_j])
│ │  
│ │  
│ │  @process_io
│ │  def granger_cF(csd_av_dat,
│ │ -               rtol=1e-8,
│ │ +               rtol=5e-6,
│ │                 nIter=100,
│ │                 cond_max=1e4,
│ │                 chunkShape=None,
│ │                 noCompute=False):
│ │  
│ │      """
│ │      Given the trial averaged cross spectral densities,
│ │ @@ -373,15 +350,14 @@
│ │          Spectral Granger-Geweke causality between all channel
│ │          combinations. Directionality follows array
│ │          notation: causality from ``i -> j`` is ``Granger[0,:,i,j]``,
│ │          causality from ``j -> i`` is ``Granger[0,:,j,i]``
│ │  
│ │      Notes
│ │      -----
│ │ -
│ │      This method is intended to be used as
│ │      :meth:`~syncopy.shared.computational_routine.ComputationalRoutine.computeFunction`
│ │      inside a :class:`~syncopy.shared.computational_routine.ComputationalRoutine`.
│ │      Thus, input parameters are presumed to be forwarded from a parent metafunction.
│ │      Consequently, this function does **not** perform any error checking and operates
│ │      under the assumption that all inputs have been externally validated and cross-checked.
│ │  
│ │ @@ -416,25 +392,34 @@
│ │          return outShape, spectralDTypes['abs']
│ │  
│ │      # strip off singleton time dimension
│ │      # for the backend calls
│ │      CSD = csd_av_dat[0]
│ │  
│ │      # auto-regularize to `cond_max` condition number
│ │ -    # maximal regularization factor is 1e-3, raises a ValueError
│ │ -    # if this is not enough!
│ │ -    CSDreg, factor = regularize_csd(CSD, cond_max=cond_max, eps_max=1e-3)
│ │ +    # maximal regularization factor is 1e-1
│ │ +    CSDreg, factor, ini_cn = regularize_csd(CSD, cond_max=cond_max, eps_max=1e-1)
│ │ +    # cast to 64bit for better precision
│ │ +    CSDreg = CSDreg.astype(np.complex128)
│ │ +
│ │      # call Wilson
│ │ -    H, Sigma, conv = wilson_sf(CSDreg, nIter=nIter, rtol=rtol)
│ │ +    H, Sigma, conv, err = wilson_sf(CSDreg, nIter=nIter, rtol=rtol)
│ │  
│ │      # calculate G-causality
│ │      Granger = granger(CSDreg, H, Sigma)
│ │  
│ │ +    # format is 'label--cast'
│ │ +    metadata = {'converged--bool': np.array(conv),
│ │ +                'max rel. err--float': np.array(err),
│ │ +                'reg. factor--float': np.array(factor),
│ │ +                'initial cond. num--float': np.array(ini_cn)
│ │ +                }
│ │ +
│ │      # reattach dummy time axis
│ │ -    return Granger[None, ...]
│ │ +    return Granger[None, ...], metadata
│ │  
│ │  
│ │  class GrangerCausality(ComputationalRoutine):
│ │  
│ │      """
│ │      Compute class that computes pairwise Granger causalities
│ │      of :class:`~syncopy.CrossSpectralData` objects.
│ │ @@ -444,20 +429,31 @@
│ │      classes and metafunctions.
│ │  
│ │      See also
│ │      --------
│ │      syncopy.connectivityanalysis : parent metafunction
│ │      """
│ │  
│ │ +    #: The keys available in the `info` property of the returned data instance.
│ │ +    #:
│ │ +    #: 'converged' : bool, ``True`` if the algoritm converged successfully
│ │ +    #:
│ │ +    #: 'max rel, err' : float, maximum relative error between the input CSD and the spectral factorization
│ │ +    #:
│ │ +    #: 'reg. factor' : float, brute force regularization factor in case the CSD is nearly singular
│ │ +    #:
│ │ +    #: 'initial cond. num' : float, condition number of the CSD, regularization kicks in if that is too high
│ │ +    metadata_keys = ("converged", "max rel. err", "reg. factor", "initial cond. num",)
│ │ +
│ │      # the hard wired dimord of the cF
│ │      dimord = ['time', 'freq', 'channel_i', 'channel_j']
│ │  
│ │      computeFunction = staticmethod(granger_cF)
│ │  
│ │ -    method = "" # there is no backend
│ │ +    method = ""   # there is no backend
│ │      # 1st argument,the data, gets omitted
│ │      valid_kws = list(signature(granger_cF).parameters.keys())[1:]
│ │  
│ │      def pre_check(self):
│ │          '''
│ │          Make sure we have a trial average,
│ │          so the input data only consists of `1 trial`.
│ │ @@ -472,34 +468,19 @@
│ │          if self.numTrials != 1:
│ │              lgl = "1 trial: Granger causality can only be computed on trial averages!"
│ │              act = f"DataSet contains {self.numTrials} trials"
│ │              raise SPYValueError(legal=lgl, varname="data", actual=act)
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │ -        # Some index gymnastics to get trial begin/end "samples"
│ │ -        if data.selection is not None:
│ │ -            chanSec_i = data.selection.channel_i
│ │ -            chanSec_j = data.selection.channel_j
│ │ -            trl = data.selection.trialdefinition
│ │ -            for row in range(trl.shape[0]):
│ │ -                trl[row, :2] = [row, row + 1]
│ │ -        else:
│ │ -            chanSec_i = slice(None)
│ │ -            chanSec_j = slice(None)
│ │ -            time = np.arange(len(data.trials))
│ │ -            time = time.reshape((time.size, 1))
│ │ -            trl = np.hstack((time, time + 1,
│ │ -                             np.zeros((len(data.trials), 1)),
│ │ -                             np.array(data.trialinfo)))
│ │ -
│ │ -        # Attach constructed trialdef-array (if even necessary)
│ │ -        if self.keeptrials:
│ │ -            out.trialdefinition = trl
│ │ -        else:
│ │ -            out.trialdefinition = np.array([[0, 1, 0]])
│ │ -
│ │ -        # Attach remaining meta-data
│ │ -        out.samplerate = data.samplerate
│ │ -        out.channel_i = np.array(data.channel_i[chanSec_i])
│ │ -        out.channel_j = np.array(data.channel_j[chanSec_j])
│ │ +        propagate_properties(data, out, self.keeptrials)
│ │          out.freq = data.freq
│ │ +
│ │ +        # digest metadata and attach to .info property
│ │ +        mdata = metadata_from_hdf5_file(out.filename)
│ │ +
│ │ +        for key, value in mdata.items():
│ │ +            # we always have a (single) trial average here
│ │ +            label_cast = key.split('__')[0]
│ │ +            # learn how to serialize
│ │ +            label, cast = label_cast.split('--')
│ │ +            out.info[label] = cast_0array(cast, value)
│ │   --- esi-syncopy-2022.8/syncopy/nwanalysis/ST_compRoutines.py
│ ├── +++ esi_syncopy-2023.3/syncopy/statistics/compRoutines.py
│ │┄ Files 27% similar despite different names
│ │ @@ -1,392 +1,378 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │ -# computeFunctions and -Routines for parallel calculation
│ │ -# of single trial measures needed for the averaged
│ │ -# measures like cross spectral densities
│ │ +# Computational Routines for statstical methods
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │ -import numpy as np
│ │ -from scipy.signal import fftconvolve, detrend
│ │  from inspect import signature
│ │ +import numpy as np
│ │ +from numpy.lib import stride_tricks
│ │  
│ │  # backend method imports
│ │ -from .csd import csd
│ │ +from .psth import psth
│ │  
│ │  # syncopy imports
│ │ -from syncopy.shared.const_def import spectralDTypes
│ │ -from syncopy.shared.tools import best_match
│ │ -from syncopy.shared.computational_routine import ComputationalRoutine
│ │ +from syncopy.shared.computational_routine import ComputationalRoutine, propagate_properties
│ │  from syncopy.shared.kwarg_decorators import process_io
│ │  
│ │  
│ │  @process_io
│ │ -def cross_spectra_cF(trl_dat,
│ │ -                     samplerate=1,
│ │ -                     nSamples=None,
│ │ -                     foi=None,
│ │ -                     taper="hann",
│ │ -                     taper_opt=None,
│ │ -                     demean_taper=False,
│ │ -                     polyremoval=False,
│ │ -                     timeAxis=0,
│ │ -                     chunkShape=None,
│ │ -                     noCompute=False):
│ │ -
│ │ -    """
│ │ -    Single trial Fourier cross spectral estimates between all channels
│ │ -    of the input data. First all the individual Fourier transforms
│ │ -    are calculated via a (multi-)tapered FFT, then the pairwise
│ │ -    cross-spectra are computed.
│ │ -
│ │ -    Averaging over tapers is done implicitly
│ │ -    for multi-taper analysis with `taper="dpss"`.
│ │ -
│ │ -    Output consists of all (nChannels x nChannels+1)/2 different complex
│ │ -    estimates arranged in a symmetric fashion (``CS_ij == CS_ji*``). The
│ │ -    elements on the main diagonal (`CS_ii`) are the (real) auto-spectra.
│ │ -
│ │ -    This is NOT the same as what is commonly referred to as
│ │ -    "cross spectral density" as there is no (time) averaging!!
│ │ -    Multi-tapering alone is not necessarily sufficient to get enough
│ │ -    statitstical power for a robust csd estimate. Yet for completeness
│ │ -    and testing the option `norm=True` will output a single-trial
│ │ -    coherence estimate.
│ │ +def npstats_cF(trl_dat, operation='mean', axis=0, noCompute=False, chunkShape=None):
│ │ +
│ │ +    """
│ │ +    Numpy summary statistics on single-trial arrays along indicated `axis`.
│ │  
│ │      Parameters
│ │      ----------
│ │ -    trl_dat : (K, N) :class:`numpy.ndarray`
│ │ -        Uniformly sampled multi-channel time-series data
│ │ -        The 1st dimension is interpreted as the time axis,
│ │ -        columns represent individual channels.
│ │ -        Dimensions can be transposed to `(N, K)` with the `timeAxis` parameter.
│ │ -    samplerate : float
│ │ -        Samplerate in Hz
│ │ -    nSamples : int or None
│ │ -        Absolute length of the (potentially to be padded) signal or
│ │ -        `None` for no padding
│ │ -    foi : 1D :class:`numpy.ndarray` or None, optional
│ │ -        Frequencies of interest  (Hz) for output. If desired frequencies
│ │ -        cannot be matched exactly the closest possible frequencies (respecting
│ │ -        data length and padding) are used.
│ │ -    taper : str or None
│ │ -        Taper function to use, one of scipy.signal.windows
│ │ -        Set to `None` for no tapering.
│ │ -    taper_opt : dict, optional
│ │ -        Additional keyword arguments passed to the `taper` function.
│ │ -        For multi-tapering with `taper='dpss'` set the keys
│ │ -        `'Kmax'` and `'NW'`.
│ │ -        For further details, please refer to the
│ │ -        `SciPy docs <https://docs.scipy.org/doc/scipy/reference/signal.windows.html>`_
│ │ -    demean_taper : bool
│ │ -        Set to `True` to perform de-meaning after tapering
│ │ -    polyremoval : int or None
│ │ -        Order of polynomial used for de-trending data in the time domain prior
│ │ -        to spectral analysis. A value of 0 corresponds to subtracting the mean
│ │ -        ("de-meaning"), ``polyremoval = 1`` removes linear trends (subtracting the
│ │ -        least squares fit of a linear polynomial).
│ │ -        If `polyremoval` is `None`, no de-trending is performed.
│ │ -    timeAxis : int, optional
│ │ -        Index of running time axis in `trl_dat` (0 or 1)
│ │ -    noCompute : bool
│ │ -        Preprocessing flag. If `True`, do not perform actual calculation but
│ │ -        instead return expected shape and :class:`numpy.dtype` of output
│ │ -        array.
│ │ +    trl_dat : :class:`numpy.ndarray`
│ │ +        Single trial data of arbitrary dimension
│ │ +    operation : {'mean', 'std', 'var', 'median'}
│ │ +        The statistical operation to perform
│ │ +    axis : int
│ │ +        The axis over which to calulate the average
│ │  
│ │      Returns
│ │      -------
│ │ -    CS_ij : (1, nFreq, N, N) :class:`numpy.ndarray`
│ │ -        Complex cross spectra for all channel combinations ``i,j``.
│ │ -        `N` corresponds to number of input channels.
│ │ +    res : :class:`numpy.ndarray`
│ │ +        Result of the average
│ │  
│ │      Notes
│ │      -----
│ │ -    This method is intended to be used as
│ │ -    :meth:`~syncopy.shared.computational_routine.ComputationalRoutine.computeFunction`
│ │ +    This method is intended to be used as :meth:`~syncopy.shared.computational_routine.ComputationalRoutine.computeFunction`
│ │      inside a :class:`~syncopy.shared.computational_routine.ComputationalRoutine`.
│ │      Thus, input parameters are presumed to be forwarded from a parent metafunction.
│ │      Consequently, this function does **not** perform any error checking and operates
│ │      under the assumption that all inputs have been externally validated and cross-checked.
│ │ +    """
│ │ +
│ │ +    if noCompute:
│ │ +        # initialize result array
│ │ +        out_shape = list(trl_dat.shape)
│ │ +        out_shape[axis] = 1   # this axis will be summed over
│ │ +
│ │ +        return out_shape, trl_dat.dtype
│ │ +
│ │ +    return NumpyStatDim.methods[operation](trl_dat, axis=axis, keepdims=True)
│ │ +
│ │ +
│ │ +class NumpyStatDim(ComputationalRoutine):
│ │ +
│ │ +    """
│ │ +    Simple compute class which applies basic numpy statistical funtions
│ │ +    along  a ``dim`` of a Syncopy data object. The resulting Syncopy object
│ │ +    is of the same type as the input object, having one of its
│ │ +    dimensions reduced to a singleton due to the summary statistical operation.
│ │ +
│ │ +    Notes
│ │ +    -----
│ │ +    If ``keeptrials`` is set to False, a trial average is additionally computed
│ │ +    as in any other CR. For a standalone trial average use the _trial_statistics function.
│ │  
│ │      See also
│ │      --------
│ │ -    csd : :func:`~syncopy.connectivity.csd.csd`
│ │ -             Cross-spectra backend function
│ │ -    normalize_csd : :func:`~syncopy.connectivity.csd.normalize_csd`
│ │ -             Coherence from trial averages
│ │ -    mtmfft : :func:`~syncopy.specest.mtmfft.mtmfft`
│ │ -             (Multi-)tapered Fourier analysis
│ │ -
│ │ +    _trial_statistics: Sequential computation of statistics over trials
│ │      """
│ │  
│ │ -    # Re-arrange array if necessary and get dimensional information
│ │ -    if timeAxis != 0:
│ │ -        dat = trl_dat.T       # does not copy but creates view of `trl_dat`
│ │ -    else:
│ │ -        dat = trl_dat
│ │ +    methods = {'mean': np.nanmean,
│ │ +               'std': np.nanstd,
│ │ +               'var': np.nanvar,
│ │ +               'median': np.nanmedian
│ │ +               }
│ │  
│ │ -    if nSamples is None:
│ │ -        nSamples = dat.shape[0]
│ │ +    computeFunction = staticmethod(npstats_cF)
│ │  
│ │ -    nChannels = dat.shape[1]
│ │ +    def process_metadata(self, in_data, out_data):
│ │ +
│ │ +        # the dimension over which the statistic got computed
│ │ +        dim = in_data.dimord[self.cfg['axis']]
│ │ +
│ │ +        out_data.samplerate = in_data.samplerate
│ │ +
│ │ +        # Get/set timing-related selection modifiers
│ │ +        # We've set a fallback all-to-all selection in any case
│ │ +
│ │ +        # time axis really gone, only one trial and time got averaged out
│ │ +        if dim == 'time' and not self.keeptrials:
│ │ +            trldef = np.array([[0, 1, 0]])
│ │ +
│ │ +        # trial average, needs equal trial lengths.. just copy from 1st
│ │ +        elif dim != 'time' and not self.keeptrials:
│ │ +            trldef = in_data.selection.trialdefinition[0, :][None, :]
│ │ +
│ │ +        # each trial has empty time axis, so we attach trivial trialdefinition:
│ │ +        # 1 sample per trial for stacking
│ │ +        elif dim == 'time' and self.keeptrials:
│ │ +            nTrials = len(in_data.selection.trials)
│ │ +            stacking_time = np.arange(nTrials)[:, None]
│ │ +            trldef = np.hstack((stacking_time, stacking_time + 1,
│ │ +                               np.zeros((nTrials, 1))))
│ │ +
│ │ +        # nothing happened on the time axis
│ │ +        else:
│ │ +            trldef = in_data.selection.trialdefinition
│ │ +
│ │ +        out_data.trialdefinition = trldef
│ │ +
│ │ +        # Get/set dimensional attributes changed by selection
│ │ +        for prop in in_data.selection._dimProps:
│ │ +            selection = getattr(in_data.selection, prop)
│ │ +            # due to fallback all-to-all selection this captures
│ │ +            # all existing dimensions
│ │ +            if selection is not None:
│ │ +                # propagate without change
│ │ +                if dim not in prop:
│ │ +                    if np.issubdtype(type(selection), np.number):
│ │ +                        selection = [selection]
│ │ +                    setattr(out_data, prop, getattr(in_data, prop)[selection])
│ │ +                # set to singleton or None
│ │ +                else:
│ │ +                    # numerical freq axis is gone after averaging
│ │ +                    if dim == 'freq':
│ │ +                        out_data.freq = None
│ │ +                    else:
│ │ +                        setattr(out_data, prop, [self.cfg['operation']])
│ │ +
│ │ +@process_io
│ │ +def cov_cF(trl_dat,
│ │ +           ddof=None,
│ │ +           statAxis=0,
│ │ +           noCompute=False,
│ │ +           chunkShape=None):
│ │ +
│ │ +    """
│ │ +    Covariance between channels via ``np.cov``
│ │  
│ │ -    freqs = np.fft.rfftfreq(nSamples, 1 / samplerate)
│ │ +    Parameters
│ │ +    ----------
│ │ +    trl_dat : :class:`numpy.ndarray`
│ │ +        Single trial multi-channel data
│ │ +    ddof : int, optional
│ │ +        Degrees of freedom
│ │ +    statAxis : int, optional
│ │ +        Index of axis holding the observations in `trl_dat` (0 or 1)
│ │ +    """
│ │  
│ │ -    if foi is not None:
│ │ -        _, freq_idx = best_match(freqs, foi, squash_duplicates=True)
│ │ -        nFreq = freq_idx.size
│ │ +    # our variables are put in columns (rowvar=False)
│ │ +    if statAxis != 0:
│ │ +        dat = trl_dat.T
│ │      else:
│ │ -        freq_idx = slice(None)
│ │ -        nFreq = freqs.size
│ │ +        dat = trl_dat
│ │  
│ │ -    # we always average over tapers here
│ │ -    outShape = (1, nFreq, nChannels, nChannels)
│ │ +    nChannels = dat.shape[1]
│ │ +
│ │ +    # mockup CrossSpectralData shape
│ │ +    outShape = (1, 1, nChannels, nChannels)
│ │  
│ │      # For initialization of computational routine,
│ │      # just return output shape and dtype
│ │ -    # cross spectra are complex!
│ │      if noCompute:
│ │ -        return outShape, spectralDTypes["fourier"]
│ │ -
│ │ -    # detrend
│ │ -    if polyremoval == 0:
│ │ -        # SciPy's overwrite_data not working for type='constant' :/
│ │ -        dat = detrend(dat, type='constant', axis=0, overwrite_data=True)
│ │ -    elif polyremoval == 1:
│ │ -        dat = detrend(dat, type='linear', axis=0, overwrite_data=True)
│ │ +        return outShape, np.float32
│ │  
│ │ -    CS_ij = csd(dat,
│ │ -                samplerate,
│ │ -                nSamples,
│ │ -                taper=taper,
│ │ -                taper_opt=taper_opt,
│ │ -                demean_taper=demean_taper)
│ │ +    cov = np.cov(trl_dat, ddof=ddof, rowvar=False)
│ │  
│ │ -    # where does freqs go/come from -
│ │ -    # we will eventually solve this issue..
│ │ -    return CS_ij[None, freq_idx, ...]
│ │ +    # attach dummy time and freq axes
│ │ +    return cov[None, None, ...]
│ │  
│ │  
│ │ -class ST_CrossSpectra(ComputationalRoutine):
│ │ +class Covariance(ComputationalRoutine):
│ │  
│ │      """
│ │ -    Compute class that calculates single-trial (multi-)tapered cross spectra
│ │ -    of :class:`~syncopy.AnalogData` objects
│ │ +    Compute class that calculates covariance of :class:`~syncopy.AnalogData` objects
│ │  
│ │      Sub-class of :class:`~syncopy.shared.computational_routine.ComputationalRoutine`,
│ │      see :doc:`/developer/compute_kernels` for technical details on Syncopy's compute
│ │      classes and metafunctions.
│ │  
│ │ +    Notes
│ │ +    -----
│ │ +    Outputs a :class:`~syncopy.CrossSpectralData` object with singleton time and freq
│ │ +    axis. The backing hdf5 dataset then gets stripped of the empty axes and attached
│ │ +    as additional ``.cov`` dataset to a :class:`~syncopy.TimeLockData` object in 
│ │ +    the respective frontend.
│ │ +
│ │      See also
│ │      --------
│ │ -    syncopy.connectivityanalysis : parent metafunction
│ │ +    syncopy.timelockanalysis : parent metafunction
│ │      """
│ │  
│ │ -    # the hard wired dimord of the cF
│ │ -    dimord = ['time', 'freq', 'channel_i', 'channel_j']
│ │ -
│ │ -    computeFunction = staticmethod(cross_spectra_cF)
│ │ +    computeFunction = staticmethod(cov_cF)
│ │  
│ │      # 1st argument,the data, gets omitted
│ │ -    valid_kws = list(signature(cross_spectra_cF).parameters.keys())[1:]
│ │ -    # hardcode some parameter names which got digested from the frontend
│ │ -    valid_kws += ['tapsmofrq', 'nTaper', 'pad_to_length']
│ │ +    valid_kws = list(signature(cov_cF).parameters.keys())[1:-1]
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │ -        # Some index gymnastics to get trial begin/end "samples"
│ │          if data.selection is not None:
│ │              chanSec = data.selection.channel
│ │ -            trl = data.selection.trialdefinition
│ │ -            for row in range(trl.shape[0]):
│ │ -                trl[row, :2] = [row, row + 1]
│ │ +            trldef = data.selection.trialdefinition
│ │ +            for row in range(trldef.shape[0]):
│ │ +                trldef[row, :2] = [row, row + 1]
│ │          else:
│ │              chanSec = slice(None)
│ │              time = np.arange(len(data.trials))
│ │              time = time.reshape((time.size, 1))
│ │ -            trl = np.hstack((time, time + 1,
│ │ -                             np.zeros((len(data.trials), 1)),
│ │ -                             np.array(data.trialinfo)))
│ │ +            trldef = np.hstack((time, time + 1,
│ │ +                                np.zeros((len(data.trials), 1)),
│ │ +                                np.array(data.trialinfo)))
│ │  
│ │ -        # Attach constructed trialdef-array (if even necessary)
│ │ +        # Attach constructed trialdef-array, time axis is gone 
│ │          if self.keeptrials:
│ │ -            out.trialdefinition = trl
│ │ +            out.trialdefinition = trldef
│ │          else:
│ │              out.trialdefinition = np.array([[0, 1, 0]])
│ │  
│ │          # Attach remaining meta-data
│ │          out.samplerate = data.samplerate
│ │          out.channel_i = np.array(data.channel[chanSec])
│ │          out.channel_j = np.array(data.channel[chanSec])
│ │ -        out.freq = self.cfg['foi']
│ │  
│ │  
│ │  @process_io
│ │ -def cross_covariance_cF(trl_dat,
│ │ -                        samplerate=1,
│ │ -                        polyremoval=0,
│ │ -                        timeAxis=0,
│ │ -                        norm=False,
│ │ -                        chunkShape=None,
│ │ -                        noCompute=False,
│ │ -                        fullOutput=False):
│ │ +def psth_cF(trl_dat,
│ │ +            trl_start,
│ │ +            onset,
│ │ +            trl_end,
│ │ +            chan_unit_combs=None,
│ │ +            tbins=None,
│ │ +            output='rate',
│ │ +            samplerate=1000,
│ │ +            noCompute=False,
│ │ +            chunkShape=None):
│ │  
│ │      """
│ │ -    Single trial covariance estimates between all channels
│ │ -    of the input data. Output consists of all ``(nChannels x nChannels+1)/2``
│ │ -    different estimates arranged in a symmetric fashion
│ │ -    (``COV_ij == COV_ji``). The elements on the
│ │ -    main diagonal (`CS_ii`) are the channel variances.
│ │ +    Peristimulus time histogram
│ │ +
│ │ +    Backend :func:`~syncopy.spikes.psth.psth` `method_kwargs`:
│ │ +
│ │ +        {'trl_start', 'onset', 'bins', 'samplerate'}
│ │  
│ │      Parameters
│ │      ----------
│ │ -    trl_dat : (K, N) :class:`numpy.ndarray`
│ │ -        Uniformly sampled multi-channel time-series data
│ │ -        The 1st dimension is interpreted as the time axis,
│ │ -        columns represent individual channels.
│ │ -        Dimensions can be transposed to `(N, K)` with the `timeAxis` parameter.
│ │ -    samplerate : float
│ │ -        Samplerate in Hz
│ │ -    polyremoval : int or None
│ │ -        Order of polynomial used for de-trending data in the time domain prior
│ │ -        to spectral analysis. A value of 0 corresponds to subtracting the mean
│ │ -        ("de-meaning"), ``polyremoval = 1`` removes linear trends (subtracting the
│ │ -        least squares fit of a linear polynomial).
│ │ -        If `polyremoval` is `None`, no de-trending is performed.
│ │ -    timeAxis : int, optional
│ │ -        Index of running time axis in `trl_dat` (0 or 1)
│ │ -    norm : bool, optional
│ │ -        Set to `True` to normalize for single-trial cross-correlation.
│ │ +    trl_dat : 2D :class:`numpy.ndarray`
│ │ +        Single trial spike data with shape (nEvents x 3)
│ │ +    trl_start : int
│ │ +        Start of the trial in sample units
│ │ +    onset : int
│ │ +        Trigger onset in samples units
│ │ +    trl_end : int
│ │ +        End of the trial in sample units
│ │ +    chan_unit_combs : :class:`~np.ndarray`
│ │ +        All (sorted) numeric channel-unit combinations to bin for
│ │ +        arangend in a (N, 2) shaped array, where each row is
│ │ +        one unique combination (say [4, 1] for channel4 - unit1)
│ │ +        If `None` will infer from the supplied SpikeData array.
│ │ +    tbins: :class:`~numpy.array` or None
│ │ +        An array of monotonically increasing PSTH bin edges
│ │ +        in seconds including the rightmost edge
│ │ +        Defaults with `None` to the Rice rule
│ │ +    output : {'rate', 'spikecount', 'proportion'}, optional
│ │      noCompute : bool
│ │          Preprocessing flag. If `True`, do not perform actual calculation but
│ │          instead return expected shape and :class:`numpy.dtype` of output
│ │          array.
│ │ -    fullOutput : bool
│ │ -        For backend testing or stand-alone applications, set to `True`
│ │ -        to return also the `lags` array.
│ │ +    chunkShape : None or tuple
│ │ +        If not `None`, represents shape of output `tl_data`
│ │  
│ │      Returns
│ │      -------
│ │ -    CC_ij : (K, 1, N, N) :class:`numpy.ndarray`
│ │ -        Cross covariance for all channel combinations ``i,j``.
│ │ -        `N` corresponds to number of input channels.
│ │ +    tl_data : 2D :class:`numpy.ndarray`
│ │ +        Spike counts for each unit
│ │ +        with shape (nBins, nUnits)
│ │  
│ │ -    lags : (M,) :class:`numpy.ndarray`
│ │ -        The lag times if `fullOutput=True`
│ │  
│ │      Notes
│ │      -----
│ │      This method is intended to be used as
│ │      :meth:`~syncopy.shared.computational_routine.ComputationalRoutine.computeFunction`
│ │      inside a :class:`~syncopy.shared.computational_routine.ComputationalRoutine`.
│ │      Thus, input parameters are presumed to be forwarded from a parent metafunction.
│ │      Consequently, this function does **not** perform any error checking and operates
│ │      under the assumption that all inputs have been externally validated and cross-checked.
│ │  
│ │ -    """
│ │ -
│ │ -    # Re-arrange array if necessary and get dimensional information
│ │ -    if timeAxis != 0:
│ │ -        dat = trl_dat.T       # does not copy but creates view of `trl_dat`
│ │ -    else:
│ │ -        dat = trl_dat
│ │ -
│ │ -    nSamples = dat.shape[0]
│ │ -    nChannels = dat.shape[1]
│ │ +    See also
│ │ +    --------
│ │ +    syncopy.spike_psth : parent metafunction
│ │ +    backend method : :func:`~syncopy.spikes.psth.psth`
│ │ +    PSTH : :class:`~syncopy.shared.computational_routine.ComputationalRoutine` instance
│ │ +                     that calls this method as :meth:`~syncopy.shared.computational_routine.ComputationalRoutine.computeFunction`
│ │  
│ │ -    # positive lags in time units
│ │ -    if nSamples % 2 == 0:
│ │ -        lags = np.arange(0, nSamples // 2)
│ │ -    else:
│ │ -        lags = np.arange(0, nSamples // 2 + 1)
│ │ -    lags = lags * 1 / samplerate
│ │ +    """
│ │  
│ │ -    outShape = (len(lags), 1, nChannels, nChannels)
│ │ +    nChanUnit = len(chan_unit_combs)
│ │ +    nBins = len(tbins) - 1
│ │  
│ │      # For initialization of computational routine,
│ │      # just return output shape and dtype
│ │ -    # cross covariances are real!
│ │      if noCompute:
│ │ -        return outShape, spectralDTypes["abs"]
│ │ +        outShape = (nBins, nChanUnit)
│ │ +        return outShape, np.float32
│ │  
│ │ -    # detrend, has to be done after noCompute!
│ │ -    if polyremoval == 0:
│ │ -        # SciPy's overwrite_data not working for type='constant' :/
│ │ -        dat = detrend(dat, type='constant', axis=0, overwrite_data=True)
│ │ -    elif polyremoval == 1:
│ │ -        detrend(dat, type='linear', axis=0, overwrite_data=True)
│ │ -
│ │ -    # re-normalize output for different effective overlaps
│ │ -    norm_overlap = np.arange(nSamples, nSamples // 2, step = -1)
│ │ -
│ │ -    CC = np.empty(outShape)
│ │ -    for i in range(nChannels):
│ │ -        for j in range(i + 1):
│ │ -            cc12 = fftconvolve(dat[:, i], dat[::-1, j], mode='same')
│ │ -            CC[:, 0, i, j] = cc12[nSamples // 2:] / norm_overlap
│ │ -            if i != j:
│ │ -                # cross-correlation is symmetric with C(tau) = C(-tau)^T
│ │ -                cc21 = cc12[::-1]
│ │ -                CC[:, 0, j, i] = cc21[nSamples // 2:] / norm_overlap
│ │ -
│ │ -    # normalize with products of std
│ │ -    if norm:
│ │ -        STDs = np.std(dat, axis=0)
│ │ -        N = STDs[:, None] * STDs[None, :]
│ │ -        CC = CC / N
│ │ -
│ │ -    if not fullOutput:
│ │ -        return CC
│ │ -    else:
│ │ -        return CC, lags
│ │ +    # call backend method
│ │ +    counts, bins = psth(trl_dat, trl_start, onset, trl_end,
│ │ +                        chan_unit_combs=chan_unit_combs,
│ │ +                        tbins=tbins, samplerate=samplerate, output=output)
│ │  
│ │ +    return counts
│ │  
│ │ -class ST_CrossCovariance(ComputationalRoutine):
│ │  
│ │ +class PSTH(ComputationalRoutine):
│ │      """
│ │ -    Compute class that calculates single-trial cross-covariances
│ │ -    of :class:`~syncopy.AnalogData` objects
│ │ +    Compute class that performs psth analysis of :class:`~syncopy.SpikeData` objects
│ │  
│ │      Sub-class of :class:`~syncopy.shared.computational_routine.ComputationalRoutine`,
│ │      see :doc:`/developer/compute_kernels` for technical details on Syncopy's compute
│ │      classes and metafunctions.
│ │  
│ │      See also
│ │      --------
│ │ -    syncopy.connectivityanalysis : parent metafunction
│ │ +    syncopy.spike_psth : parent metafunction
│ │      """
│ │  
│ │ -    # the hard wired dimord of the cF
│ │ -    dimord = ['time', 'freq', 'channel_i', 'channel_j']
│ │ -
│ │ -    computeFunction = staticmethod(cross_covariance_cF)
│ │ +    computeFunction = staticmethod(psth_cF)
│ │  
│ │      # 1st argument,the data, gets omitted
│ │ -    valid_kws = list(signature(cross_covariance_cF).parameters.keys())[1:]
│ │ +    valid_kws = list(signature(psth).parameters.keys())[1:]
│ │ +    valid_kws += list(signature(psth_cF).parameters.keys())[1:-1]
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │ -        # Get trialdef array + channels from source: note, since lags are encoded
│ │ -        # in time-axis, trial offsets etc. are bogus anyway: simply take max-sample
│ │ -        # counts / 2 to fit lags
│ │ +        tbins = self.cfg['tbins']
│ │ +        # compute new time axis / samplerate
│ │ +        bin_midpoints = stride_tricks.sliding_window_view(tbins, (2,)).mean(axis=1)
│ │ +        srate = 1 / np.diff(bin_midpoints).mean()
│ │ +
│ │ +        # each trial has the same length
│ │ +        # for "timelocked" (same bins) psth data
│ │ +        trl_len = len(tbins) - 1
│ │          if data.selection is not None:
│ │ -            chanSec = data.selection.channel
│ │ -            trl = np.ceil(data.selection.trialdefinition / 2)
│ │ +            nTrials = len(data.selection.trial_ids)
│ │          else:
│ │ -            chanSec = slice(None)
│ │ -            trl = np.ceil(data.trialdefinition / 2)
│ │ +            nTrials = len(data.trials)
│ │  
│ │ -        # If trial-averaging was requested, use the first trial as reference
│ │ -        # (all trials had to have identical lengths), and average onset timings
│ │ +        # create trialdefinition, offsets are all equal
│ │ +        # for timelocked data
│ │ +        trl = np.zeros((nTrials, 3))
│ │ +        sample_idx = np.arange(0, nTrials * trl_len + 1, trl_len)
│ │ +        trl[:, :2] = stride_tricks.sliding_window_view(sample_idx, (2,))
│ │ +        # negative relative time is pre-stimulus!
│ │ +        # note that bin edges are set on the input data (high-res) time axis
│ │ +        # we can only approximate atm with the new 1/srate time steps
│ │ +        offsets = np.rint(bin_midpoints[0] * srate)
│ │ +        trl[:, 2] = offsets
│ │  
│ │ -        if not self.keeptrials:
│ │ -            trl = trl[[0], :]
│ │ +        # Attach meta-data
│ │ +        if self.keeptrials:
│ │ +            out.trialdefinition = trl
│ │ +        else:
│ │ +            out.trialdefinition = trl[[0], :]
│ │  
│ │ -        # set 1st entry of time axis to the 0-lag
│ │ -        trl[:, 2] = 0
│ │ -        out.trialdefinition = trl
│ │ +        out.samplerate = srate
│ │ +        # join labels for final unitX_channelY channel labels
│ │ +        chan_str = "channel{}_unit{}"
│ │ +        out.channel = [chan_str.format(c, u) for c, u in self.cfg['chan_unit_combs']]
│ │  
│ │ -        # Attach remaining meta-data
│ │ -        out.samplerate = data.samplerate
│ │ -        out.channel_i = np.array(data.channel[chanSec])
│ │ -        out.channel_j = np.array(data.channel[chanSec])
│ │ +        if not self.keeptrials:
│ │ +            # the ad-hoc averaging does not work well here because of NaNs
│ │ +            # so we rather delete the data to 'not keep the trials'
│ │ +            out.data = None
│ │ +            # TODO: add real average operator here
│ │   --- esi-syncopy-2022.8/syncopy/nwanalysis/connectivity_analysis.py
│ ├── +++ esi_syncopy-2023.3/syncopy/shared/input_processors.py
│ │┄ Files 27% similar despite different names
│ │ @@ -1,378 +1,413 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │ -# Syncopy connectivity analysis methods
│ │ +# Processing of user submitted frontend arguments like foi, taper, etc.
│ │ +# The processors return values needed directly for the
│ │ +# downstream method calls.
│ │ +# Input args are the parameters to check for validity + auxiliary parameters
│ │ +# needed for the checks. Processors raise exceptions in case of invalid input.
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │ +import numbers
│ │ +from inspect import signature
│ │ +from scipy.signal import windows
│ │  
│ │ -# Syncopy imports
│ │ -from syncopy.shared.parsers import data_parser, scalar_parser
│ │ -from syncopy.shared.tools import get_defaults, best_match, get_frontend_cfg
│ │ -from syncopy.datatype import CrossSpectralData
│ │ -from syncopy.shared.errors import (
│ │ -    SPYValueError,
│ │ -    SPYWarning,
│ │ -    SPYInfo)
│ │ -from syncopy.shared.kwarg_decorators import (unwrap_cfg, unwrap_select,
│ │ -                                             detect_parallel_client)
│ │ -from syncopy.shared.input_processors import (
│ │ -    process_taper,
│ │ -    process_foi,
│ │ -    process_padding,
│ │ -    check_effective_parameters,
│ │ -    check_passed_kwargs
│ │ -)
│ │ -
│ │ -from .ST_compRoutines import ST_CrossSpectra, ST_CrossCovariance
│ │ -from .AV_compRoutines import NormalizeCrossSpectra, NormalizeCrossCov, GrangerCausality
│ │ -
│ │ -
│ │ -availableMethods = ("coh", "corr", "granger")
│ │ -coh_outputs = {"abs", "pow", "complex", "fourier", "angle", "real", "imag"}
│ │ -
│ │ -
│ │ -@unwrap_cfg
│ │ -@unwrap_select
│ │ -@detect_parallel_client
│ │ -def connectivityanalysis(data, method="coh", keeptrials=False, output="abs",
│ │ -                         foi=None, foilim=None, pad='maxperlen',
│ │ -                         polyremoval=None, tapsmofrq=None, nTaper=None,
│ │ -                         taper="hann", taper_opt=None, **kwargs):
│ │ +from syncopy.specest.mtmfft import _get_dpss_pars
│ │ +from syncopy.shared.errors import SPYValueError, SPYWarning, SPYInfo
│ │ +from syncopy.shared.parsers import scalar_parser, array_parser
│ │ +from syncopy.shared.const_def import availableTapers, generalParameters, availablePaddingOpt
│ │ +
│ │ +
│ │ +def process_padding(pad, lenTrials, samplerate):
│ │  
│ │      """
│ │ -    Perform connectivity analysis of Syncopy :class:`~syncopy.AnalogData` objects
│ │ +    Simplified padding interface, for all taper based methods
│ │ +    padding has to be done **after** tapering!
│ │  
│ │ -    **Usage Summary**
│ │ +    This function returns a number indicating the total
│ │ +    length in samples of all trials after padding. When
│ │ +    inputted into fft related methods, the actual padding
│ │ +    is then performed there.
│ │  
│ │ -    Options available in all analysis methods:
│ │ +    Parameters
│ │ +    ----------
│ │ +    pad : 'maxperlen', float or 'nextpow2'
│ │ +        For the frontend default `maxperlen`, no padding is to
│ │ +        be performed in case of equal length trials but unequal lengths
│ │ +        trials get padded to the max. trial length.
│ │ +        A float indicates the absolute length of
│ │ +        all trials after padding in seconds. `'nextpow2'` pads all trials
│ │ +        to the nearest power of two.
│ │ +    lenTrials : sequence of int_like
│ │ +        Sequence holding all individual trial lengths
│ │ +    samplerate : float
│ │ +        The sampling rate in Hz
│ │  
│ │ -    * **foi**/**foilim** : frequencies of interest; either array of frequencies or
│ │ -      frequency window (not both)
│ │ -    * **polyremoval** : de-trending method to use (0 = mean, 1 = linear or `None`)
│ │ +    Returns
│ │ +    -------
│ │ +    abs_pad : int
│ │ +        Absolute length of all trials after padding (in samples)
│ │  
│ │ -    List of available analysis methods and respective distinct options:
│ │ +    """
│ │ +    # supported padding options
│ │ +    not_valid = False
│ │ +    if not isinstance(pad, (numbers.Number, str)):
│ │ +        not_valid = True
│ │ +    elif isinstance(pad, str) and pad not in availablePaddingOpt:
│ │ +        not_valid = True
│ │ +        # bool is an int subclass, have to check for it separately...
│ │ +    if isinstance(pad, bool):
│ │ +        not_valid = True
│ │ +    if not_valid:
│ │ +        lgl = "'maxperlen', 'nextpow2' or a float number"
│ │ +        actual = f"{pad}"
│ │ +        raise SPYValueError(legal=lgl, varname="pad", actual=actual)
│ │  
│ │ -    "coh" : (Multi-) tapered coherency estimate
│ │ -        Compute the normalized cross spectral densities
│ │ -        between all channel combinations
│ │ +    # zero padding of ALL trials the same way
│ │ +    if isinstance(pad, numbers.Number):
│ │  
│ │ -        * **output** : one of ('abs', 'pow', 'complex', 'angle', 'imag' or 'real')
│ │ -        * **taper** : one of :data:`~syncopy.shared.const_def.availableTapers`
│ │ -        * **tapsmofrq** : spectral smoothing box for slepian tapers (in Hz)
│ │ -        * **nTaper** : (optional) number of orthogonal tapers for slepian tapers
│ │ -        * **pad**: either pad to an absolute length in seconds or set to `'nextpow2'`
│ │ +        scalar_parser(pad,
│ │ +                      varname='pad',
│ │ +                      lims=[lenTrials.max() / samplerate, np.inf])
│ │ +        abs_pad = int(pad * samplerate)
│ │ +
│ │ +    # or pad to optimal FFT lengths
│ │ +    elif pad == 'nextpow2':
│ │ +        abs_pad = _nextpow2(int(lenTrials.max()))
│ │ +
│ │ +    # no padding in case of equal length trials
│ │ +    elif pad == 'maxperlen':
│ │ +        abs_pad = int(lenTrials.max())
│ │ +        if lenTrials.min() != lenTrials.max():
│ │ +            msg = f"Unequal trial lengths present, padding all trials to {abs_pad} samples"
│ │ +            SPYInfo(msg)
│ │  
│ │ -    "corr" : Cross-correlations
│ │ -        Computes the one sided (positive lags) cross-correlations
│ │ -        between all channel combinations. The maximal lag is half
│ │ -        the trial lengths.
│ │ +    # `abs_pad` is now the (soon to be padded) signal length in samples
│ │  
│ │ -        * **keeptrials** : set to `True` for single trial cross-correlations
│ │ +    return abs_pad
│ │  
│ │ -    "granger" : Spectral Granger-Geweke causality
│ │ -        Computes linear causality estimates between
│ │ -        all channel combinations. The intermediate cross-spectral
│ │ -        densities can be computed via multi-tapering.
│ │  
│ │ -        * **taper** : one of :data:`~syncopy.shared.const_def.availableTapers`
│ │ -        * **tapsmofrq** : spectral smoothing box for slepian tapers (in Hz)
│ │ -        * **nTaper** : (optional, not recommended) number of slepian tapers
│ │ -        * **pad**: either pad to an absolute length in seconds or set to `'nextpow2'`
│ │ +def process_foi(foi, foilim, samplerate):
│ │  
│ │ +    """
│ │      Parameters
│ │      ----------
│ │ -    data : `~syncopy.AnalogData`
│ │ -        A non-empty Syncopy :class:`~syncopy.datatype.AnalogData` object
│ │ -    method : str
│ │ -        Connectivity estimation method, one of 'coh', 'corr', 'granger'
│ │ -    output : str
│ │ -        Relevant for cross-spectral density estimation (`method='coh'`)
│ │ -        Use `'pow'` for absolute squared coherence, `'abs'` for absolute value of coherence
│ │ -        , `'complex'` for the complex valued coherency or `'angle'`, `'imag'` or `'real'`
│ │ -        to extract the phase difference, imaginary or real part of the coherency respectively.
│ │ -    keeptrials : bool
│ │ -        Relevant for cross-correlations (`method='corr'`).
│ │ -        If `True` single-trial cross-correlations are returned.
│ │ -    foi : array-like or None
│ │ -        Frequencies of interest (Hz) for output. If desired frequencies cannot be
│ │ -        matched exactly, the closest possible frequencies are used. If `foi` is `None`
│ │ -        or ``foi = "all"``, all attainable frequencies (i.e., zero to Nyquist / 2)
│ │ -        are selected.
│ │ -    foilim : array-like (floats [fmin, fmax]) or None or "all"
│ │ -        Frequency-window ``[fmin, fmax]`` (in Hz) of interest. The
│ │ -        `foi` array will be constructed in 1Hz steps from `fmin` to
│ │ -        `fmax` (inclusive).
│ │ -    pad : 'maxperlen', float or 'nextpow2'
│ │ -        For the default `maxperlen`, no padding is performed in case of equal
│ │ -        length trials, while trials of varying lengths are padded to match the
│ │ -        longest trial. If `pad` is a number all trials are padded so that `pad` indicates
│ │ -        the absolute length of all trials after padding (in seconds). For instance
│ │ -        ``pad = 2`` pads all trials to an absolute length of 2000 samples, if and
│ │ -        only if the longest trial contains at maximum 2000 samples and the
│ │ -        samplerate is 1kHz. If `pad` is `'nextpow2'` all trials are padded to the
│ │ -        nearest power of two (in samples) of the longest trial.
│ │ -    tapsmofrq : float or None
│ │ -        Only valid if `method` is `'coh'` or `'granger'`.
│ │ -        Enables multi-tapering and sets the amount of spectral
│ │ -        smoothing with slepian tapers in Hz.
│ │ -    nTaper : int or None
│ │ -        Only valid if `method` is `'coh'` or `'granger'` and `tapsmofrq` is set.
│ │ -        Number of orthogonal tapers to use for multi-tapering. It is not recommended to set the number
│ │ -        of tapers manually! Leave at `None` for the optimal number to be set automatically.
│ │ -    taper : str or None, optional
│ │ -        Only valid if `method` is `'coh'` or `'granger'`. Windowing function,
│ │ -        one of :data:`~syncopy.specest.const_def.availableTapers`
│ │ -        For multi-tapering with slepian tapers use `tapsmofrq` directly.
│ │ -    taper_opt : dict or None
│ │ -        Dictionary with keys for additional taper parameters.
│ │ -        For example :func:`~scipy.signal.windows.kaiser` has
│ │ -        the additional parameter 'beta'. For multi-tapering use `tapsmofrq` directly.
│ │ +    foi : 'all' or array like or None
│ │ +        frequencies of interest
│ │ +    foilim : 2-element sequence or None
│ │ +        foi limits
│ │ +
│ │ +    Other Parameters
│ │ +    ----------------
│ │ +    samplerate : float
│ │ +        the samplerate in Hz
│ │  
│ │      Returns
│ │      -------
│ │ -    out : `~syncopy.CrossSpectralData`
│ │ -        The analyis result with dims ['time', 'freq', 'channel_i', channel_j']
│ │ +    foi, foilim : tuple
│ │ +        Either both are `None` or the
│ │ +        user submitted one is parsed and returned
│ │ +
│ │ +    Notes
│ │ +    -----
│ │ +    Setting both `foi` and `foilim` to `None` is valid, the
│ │ +    subsequent analysis methods should all have a default way to
│ │ +    select a standard set of frequencies (e.g. np.fft.fftfreq).
│ │ +    """
│ │  
│ │ -    Examples
│ │ -    --------
│ │ -    In the following `adata` is an instance of :class:`~syncopy.AnalogData`
│ │ +    if foi is not None and foilim is not None:
│ │ +        lgl = "either `foi` or `foilim` specification"
│ │ +        act = "both"
│ │ +        raise SPYValueError(legal=lgl, varname="foi/foilim", actual=act)
│ │ +
│ │ +    if foi is not None:
│ │ +        if isinstance(foi, str):
│ │ +            if foi == "all":
│ │ +                foi = None
│ │ +            else:
│ │ +                raise SPYValueError(legal="'all' or `None` or list/array",
│ │ +                                    varname="foi", actual=foi)
│ │ +        else:
│ │ +            try:
│ │ +                array_parser(foi, varname="foi", hasinf=False, hasnan=False,
│ │ +                             lims=[0, samplerate / 2], dims=(None,))
│ │ +            except Exception as exc:
│ │ +                raise exc
│ │ +            foi = np.array(foi, dtype="float")
│ │  
│ │ -    Calculate the coherence between all channels with 2Hz spectral smoothing,
│ │ -    and plot the results for two combinations between 30Hz and 90Hz:
│ │ +    if foilim is not None:
│ │ +        if isinstance(foilim, str):
│ │ +            if foilim == "all":
│ │ +                foilim = None
│ │ +            else:
│ │ +                raise SPYValueError(legal="'all' or `None` or `[fmin, fmax]`",
│ │ +                                    varname="foilim", actual=foilim)
│ │ +        else:
│ │ +            array_parser(foilim, varname="foilim", hasinf=False, hasnan=False,
│ │ +                         lims=[0, samplerate / 2], dims=(2,))
│ │  
│ │ -    >>> coh = spy.connectivityanalysis(adata, method='coh', tapsmofrq=2)
│ │ -    >>> coh.singlepanelplot(channel_i=0, channel_j=1, foilim=[30,90])
│ │ -    >>> coh.singlepanelplot(channel_i=1, channel_j=2, foilim=[30,90])
│ │ +            # QUICKFIX for #392
│ │ +            foilim = [float(f) for f in foilim]
│ │  
│ │ -    Compute the cross-correlation between channel 8 and 12 and
│ │ -    plot the results for the first 200ms:
│ │ +            # foilim is of shape (2,)
│ │ +            if foilim[0] > foilim[1]:
│ │ +                msg = "Sorting foilim low to high.."
│ │ +                SPYInfo(msg)
│ │ +                foilim = np.sort(foilim)
│ │ +
│ │ +    return foi, foilim
│ │ +
│ │ +
│ │ +def process_taper(taper,
│ │ +                  taper_opt,
│ │ +                  tapsmofrq,
│ │ +                  nTaper,
│ │ +                  keeptapers,
│ │ +                  foimax,
│ │ +                  samplerate,
│ │ +                  nSamples,
│ │ +                  output):
│ │  
│ │ -    >>> cfg = spy.StructDict()
│ │ -    >>> cfg.method = 'corr'
│ │ -    >>> cfg.select = {'channel': ['channel8', 'channel12']}
│ │ -    >>> corr = spy.connectivityanalysis(adata, cfg)
│ │ -    >>> corr.singlepanelplot(channel_i='channel8', channel_j='channel12', toilim=[0, 0.2])
│ │ +    """
│ │ +    General taper validation and Slepian/dpss input sanitization.
│ │  
│ │ -    Estimate Granger causality between the same channels (re-using the cfg from above):
│ │ +    For multi-tapering with slepian tapers the default is to max out
│ │ +    `nTaper` to achieve the desired frequency smoothing bandwidth.
│ │ +    For details about the Slepian settings see
│ │  
│ │ -    >>> cfg.method = 'granger'
│ │ -    >>> granger = spy.connectivityanalysis(adata, cfg)
│ │ +    "The Effective Bandwidth of a Multitaper Spectral Estimator,
│ │ +    A. T. Walden, E. J. McCoy and D. B. Percival"
│ │  
│ │ -    Plot the results between 15Hz and 30Hz:
│ │ +    Parameters
│ │ +    ----------
│ │ +    taper : str
│ │ +        Windowing function, one of :data:`~syncopy.shared.const_def.availableTapers`
│ │ +    taper_opt : dict or None
│ │ +        Dictionary holding additional keywords for tapers which have additional
│ │ +        parameters like for example :func:`~scipy.signal.windows.kaiser`
│ │ +    tapsmofrq : float or None
│ │ +        Taper smoothing bandwidth for multi-tapering with implicit 'dpss' window
│ │ +    nTaper : int_like or None
│ │ +        Number of tapers to use for multi-tapering (not recommended)
│ │ +
│ │ +    Other Parameters
│ │ +    ----------------
│ │ +    keeptapers : bool
│ │ +    foimax : float
│ │ +        Maximum frequency for the analysis
│ │ +    samplerate : float
│ │ +        the samplerate in Hz
│ │ +    nSamples : int
│ │ +        Number of samples
│ │ +    output : str, one of {'abs', 'pow', 'fourier'}
│ │ +        Fourier transformation output type
│ │  
│ │ -    >>> granger.singlepanelplot(channel_i='channel8', channel_j='channel12', foilim=[15, 25])
│ │ +    Returns
│ │ +    -------
│ │ +    taper : str or None
│ │ +        The user supplied taper
│ │ +    taper_opt : dict
│ │ +        For multi-tapering contains the
│ │ +        keys `NW` and `Kmax` for `scipy.signal.windows.dpss`.
│ │ +        For other tapers these are the additional parameters or
│ │ +        an empty dictionary in case selected taper has no further args.
│ │      """
│ │  
│ │ -    # Make sure our one mandatory input object can be processed
│ │ -    try:
│ │ -        data_parser(data, varname="data", dataclass="AnalogData",
│ │ -                    writable=None, empty=False)
│ │ -    except Exception as exc:
│ │ -        raise exc
│ │ -    timeAxis = data.dimord.index("time")
│ │ -
│ │ -    # Get everything of interest in local namespace
│ │ -    defaults = get_defaults(connectivityanalysis)
│ │ -    lcls = locals()
│ │ -    # check for ineffective additional kwargs
│ │ -    check_passed_kwargs(lcls, defaults, frontend_name="connectivity")
│ │ -
│ │ -    new_cfg = get_frontend_cfg(defaults, lcls, kwargs)
│ │ -
│ │ -    # Ensure a valid computational method was selected
│ │ -    if method not in availableMethods:
│ │ -        lgl = "'" + "or '".join(opt + "' " for opt in availableMethods)
│ │ -        raise SPYValueError(legal=lgl, varname="method", actual=method)
│ │ -
│ │ -    # if a subset selection is present
│ │ -    # get sampleinfo and check for equidistancy
│ │ -    if data.selection is not None:
│ │ -        sinfo = data.selection.trialdefinition[:, :2]
│ │ -        # user picked discrete set of time points
│ │ -        if isinstance(data.selection.time[0], list):
│ │ -            lgl = "equidistant time points (toi) or time slice (toilim)"
│ │ -            actual = "non-equidistant set of time points"
│ │ -            raise SPYValueError(legal=lgl, varname="select", actual=actual)
│ │ +    if taper == 'dpss':
│ │ +        lgl = "set `tapsmofrq` parameter directly for multi-tapering"
│ │ +        raise SPYValueError(legal=lgl, varname='taper', actual=taper)
│ │ +
│ │ +    # no tapering at all
│ │ +    if taper is None and tapsmofrq is None:
│ │ +        return None, {}
│ │ +
│ │ +    # See if taper choice is supported
│ │ +    if taper not in availableTapers:
│ │ +        lgl = "'" + "or '".join(opt + "' " for opt in availableTapers)
│ │ +        raise SPYValueError(legal=lgl, varname="taper", actual=taper)
│ │ +
│ │ +    if not isinstance(taper_opt, (dict, type(None))):
│ │ +        lgl = "dict or None"
│ │ +        actual = type(taper_opt)
│ │ +        raise SPYValueError(lgl, "taper_opt", actual)
│ │ +
│ │ +    # -- no multi-tapering --
│ │ +    if tapsmofrq is None:
│ │ +        if nTaper is not None:
│ │ +            msg = "`nTaper` is only used for multi-tapering!"
│ │ +            SPYWarning(msg)
│ │ +        if keeptapers:
│ │ +            msg = "`keeptapers` is only used for multi-tapering!"
│ │ +            SPYWarning(msg)
│ │ +
│ │ +        # availableTapers are given by windows.__all__
│ │ +        parameters = signature(getattr(windows, taper)).parameters
│ │ +        supported_kws = list(parameters.keys())
│ │ +        # 'M' is the kw for the window length
│ │ +        # for all of scipy's windows
│ │ +        supported_kws.remove('M')
│ │ +        supported_kws.remove('sym')
│ │ +
│ │ +        if taper_opt is not None:
│ │ +
│ │ +            if len(supported_kws) == 0:
│ │ +                lgl = f"`None`, taper '{taper}' has no additional parameters"
│ │ +                raise SPYValueError(lgl, varname='taper_opt', actual=taper_opt)
│ │ +
│ │ +            for key in taper_opt:
│ │ +                if key not in supported_kws:
│ │ +                    lgl = f"one of {supported_kws} for `taper='{taper}'`"
│ │ +                    raise SPYValueError(lgl, "taper_opt key", key)
│ │ +            for key in supported_kws:
│ │ +                if key not in taper_opt:
│ │ +                    lgl = f"additional parameter '{key}' for `taper='{taper}'`"
│ │ +                    raise SPYValueError(lgl, "taper_opt", None)
│ │ +            # all supplied keys are fine
│ │ +            return taper, taper_opt
│ │ +
│ │ +        elif len(supported_kws) > 0:
│ │ +            lgl = f"additional parameters for taper '{taper}': {supported_kws}"
│ │ +            raise SPYValueError(lgl, varname='taper_opt', actual=taper_opt)
│ │ +        else:
│ │ +            # taper_opt was None and taper needs no additional parameters
│ │ +            return taper, {}
│ │ +
│ │ +    # -- multi-tapering --
│ │      else:
│ │ -        sinfo = data.sampleinfo
│ │ -    lenTrials = np.diff(sinfo).squeeze()
│ │ +        if taper != 'hann':
│ │ +            lgl = "`None` for multi-tapering, just set `tapsmofrq`"
│ │ +            raise SPYValueError(lgl, varname='taper', actual=taper)
│ │  
│ │ -    # check polyremoval
│ │ -    if polyremoval is not None:
│ │ -        scalar_parser(polyremoval, varname="polyremoval", ntype="int_like", lims=[0, 1])
│ │ +        if taper_opt is not None:
│ │ +            msg = "For multi-tapering use `tapsmofrq` and `nTaper` to control frequency smoothing, `taper_opt` has no effect"
│ │ +            SPYWarning(msg)
│ │  
│ │ -    # --- Padding ---
│ │ +        # direct mtm estimate (averaging) only valid for spectral power
│ │ +        if not keeptapers and output != "pow":
│ │ +            lgl = (f"'pow'|False or '{output}'|True, set either keeptapers=True "
│ │ +                   "or `output='pow'`!")
│ │ +            raise SPYValueError(legal=lgl, varname="output|keeptapers", actual=f"'{output}'|{keeptapers}")
│ │ +
│ │ +        # --- minimal smoothing bandwidth ---
│ │ +        # --- such that Kmax/nTaper is at least 1
│ │ +        minBw = 2 * samplerate / nSamples
│ │ +        # -----------------------------------
│ │ +
│ │ +        # --- maximal smoothing bandwidth ---
│ │ +        # --- such that Kmax < nSamples and NW < nSamples / 2
│ │ +        maxBw = np.min([samplerate / 2 - 1 / nSamples,
│ │ +                        samplerate * (nSamples + 1) / (2 * nSamples)])
│ │ +        # -----------------------------------
│ │ +
│ │ +        try:
│ │ +            scalar_parser(tapsmofrq, varname="tapsmofrq", lims=[0, np.inf])
│ │ +        except Exception:
│ │ +            lgl = "smoothing bandwidth in Hz, typical values are in the range 1-10Hz"
│ │ +            raise SPYValueError(legal=lgl, varname="tapsmofrq", actual=tapsmofrq)
│ │  
│ │ -    if method == "corr" and pad != 'maxperlen':
│ │ -        lgl = "'maxperlen', no padding needed/allowed for cross-correlations"
│ │ -        actual = f"{pad}"
│ │ -        raise SPYValueError(legal=lgl, varname="pad", actual=actual)
│ │ +        if tapsmofrq < minBw:
│ │ +            msg = f'Setting tapsmofrq to the minimal attainable bandwidth of {minBw:.2f}Hz'
│ │ +            SPYInfo(msg)
│ │ +            tapsmofrq = minBw
│ │  
│ │ -    # the actual number of samples in case of later padding
│ │ -    nSamples = process_padding(pad, lenTrials, data.samplerate)
│ │ +        if tapsmofrq > maxBw:
│ │ +            msg = f'Setting tapsmofrq to the maximal attainable bandwidth of {maxBw:.2f}Hz'
│ │ +            SPYInfo(msg)
│ │ +            tapsmofrq = maxBw
│ │  
│ │ -    # --- Basic foi sanitization ---
│ │ +        # --------------------------------------------------------------
│ │ +        # set parameters for scipy.signal.windows.dpss
│ │ +        NW, Kmax = _get_dpss_pars(tapsmofrq, nSamples, samplerate)
│ │ +        # --------------------------------------------------------------
│ │ +
│ │ +        # tapsmofrq too large
│ │ +        # if Kmax > nSamples or NW > nSamples / 2:
│ │ +
│ │ +        # the recommended way:
│ │ +        # set nTaper automatically to achieve exact effective smoothing bandwidth
│ │ +        if nTaper is None:
│ │ +            msg = f'Using {Kmax} taper(s) for multi-tapering'
│ │ +            SPYInfo(msg)
│ │ +            dpss_opt = {'NW': NW, 'Kmax': Kmax}
│ │ +            return 'dpss', dpss_opt
│ │  
│ │ -    foi, foilim = process_foi(foi, foilim, data.samplerate)
│ │ +        elif nTaper is not None:
│ │  
│ │ -    # only now set foi array for foilim in 1Hz steps
│ │ -    if foilim is not None:
│ │ -        foi = np.arange(foilim[0], foilim[1] + 1, dtype=float)
│ │ +            scalar_parser(nTaper,
│ │ +                          varname="nTaper",
│ │ +                          ntype="int_like", lims=[1, np.inf])
│ │  
│ │ -    # Prepare keyword dict for logging (use `lcls` to get actually provided
│ │ -    # keyword values, not defaults set above)
│ │ -    log_dict = {"method": method,
│ │ -                "keeptrials": keeptrials,
│ │ -                "polyremoval": polyremoval,
│ │ -                "pad": pad}
│ │ -
│ │ -    # --- Setting up specific Methods ---
│ │ -    if method == 'granger':
│ │ -
│ │ -        if foi is not None or foilim is not None:
│ │ -            lgl = "no foi specification for Granger analysis"
│ │ -            actual = "foi or foilim specification"
│ │ -            raise SPYValueError(lgl, 'foi/foilim', actual)
│ │ -
│ │ -        nChannels = len(data.channel)
│ │ -        nTrials = len(lenTrials)
│ │ -        # warn user if this ratio is not small
│ │ -        if nChannels / nTrials > 0.1:
│ │ -            msg = "Multi-channel Granger analysis can be numerically unstable, it is recommended to have at least 10 times the number of trials compared to the number of channels. Try calculating in sub-groups of fewer channels!"
│ │ -            SPYWarning(msg)
│ │ +            if nTaper != Kmax:
│ │ +                msg = f'''
│ │ +                Manually setting the number of tapers is not recommended
│ │ +                and may (strongly) distort the effective smoothing bandwidth!\n
│ │ +                The optimal number of tapers is {Kmax}, you have chosen to use {nTaper}.
│ │ +                '''
│ │ +                SPYWarning(msg)
│ │  
│ │ -    if method in ['coh', 'granger']:
│ │ +            dpss_opt = {'NW': NW, 'Kmax': nTaper}
│ │ +            return 'dpss', dpss_opt
│ │  
│ │ -        # --- set up computation of the single trial CSDs ---
│ │  
│ │ -        if keeptrials is not False:
│ │ -            lgl = "False, trial averaging needed!"
│ │ -            act = keeptrials
│ │ -            raise SPYValueError(lgl, varname="keeptrials", actual=act)
│ │ -
│ │ -        # Construct array of maximally attainable frequencies
│ │ -        freqs = np.fft.rfftfreq(nSamples, 1 / data.samplerate)
│ │ -
│ │ -        # Match desired frequencies as close as possible to
│ │ -        # actually attainable freqs
│ │ -        # these are the frequencies attached to the SpectralData by the CR!
│ │ -        if foi is not None:
│ │ -            foi, _ = best_match(freqs, foi, squash_duplicates=True)
│ │ -        elif foilim is not None:
│ │ -            foi, _ = best_match(freqs, foilim, span=True, squash_duplicates=True)
│ │ -        elif foi is None and foilim is None:
│ │ -            # Construct array of maximally attainable frequencies
│ │ -            msg = (f"Setting frequencies of interest to {freqs[0]:.1f}-"
│ │ -                   f"{freqs[-1]:.1f}Hz")
│ │ -            SPYInfo(msg)
│ │ -            foi = freqs
│ │ +def check_effective_parameters(CR, defaults, lcls, besides=None):
│ │  
│ │ -        # sanitize taper selection and retrieve dpss settings
│ │ -        taper, taper_opt = process_taper(taper,
│ │ -                                         taper_opt,
│ │ -                                         tapsmofrq,
│ │ -                                         nTaper,
│ │ -                                         keeptapers=False,   # ST_CSD's always average tapers
│ │ -                                         foimax=foi.max(),
│ │ -                                         samplerate=data.samplerate,
│ │ -                                         nSamples=nSamples,
│ │ -                                         output="pow")   # ST_CSD's always have this unit/norm
│ │ -
│ │ -        log_dict["foi"] = foi
│ │ -        log_dict["taper"] = taper
│ │ -        if taper_opt and taper == 'dpss':
│ │ -            log_dict["nTaper"] = taper_opt["Kmax"]
│ │ -            log_dict["tapsmofrq"] = tapsmofrq
│ │ -        elif taper_opt:
│ │ -            log_dict["taper_opt"] = taper_opt
│ │ -
│ │ -        check_effective_parameters(ST_CrossSpectra, defaults, lcls)
│ │ -
│ │ -        # parallel computation over trials
│ │ -        st_compRoutine = ST_CrossSpectra(samplerate=data.samplerate,
│ │ -                                         nSamples=nSamples,
│ │ -                                         taper=taper,
│ │ -                                         taper_opt=taper_opt,
│ │ -                                         demean_taper=method == 'granger',
│ │ -                                         polyremoval=polyremoval,
│ │ -                                         timeAxis=timeAxis,
│ │ -                                         foi=foi)
│ │ -        # hard coded as class attribute
│ │ -        st_dimord = ST_CrossSpectra.dimord
│ │ -
│ │ -    if method == 'coh':
│ │ -
│ │ -        if output not in coh_outputs:
│ │ -            lgl = f"one of {coh_outputs}"
│ │ -            raise SPYValueError(lgl, varname="output", actual=output)
│ │ -        log_dict['output'] = output
│ │ -        
│ │ -        # final normalization after trial averaging
│ │ -        av_compRoutine = NormalizeCrossSpectra(output=output)
│ │ -
│ │ -    if method == 'granger':
│ │ -        # after trial averaging
│ │ -        # hardcoded numerical parameters
│ │ -        av_compRoutine = GrangerCausality(rtol=1e-8,
│ │ -                                          nIter=100,
│ │ -                                          cond_max=1e4
│ │ -                                          )
│ │ -
│ │ -    if method == 'corr':
│ │ -        if lcls['foi'] is not None:
│ │ -            msg = 'Parameter `foi` has no effect for `corr`'
│ │ -            SPYWarning(msg)
│ │ -        check_effective_parameters(ST_CrossCovariance, defaults, lcls)
│ │ +    """
│ │ +    For a given ComputationalRoutine, compare set parameters
│ │ +    (*lcls*) with the accepted parameters and the frontend
│ │ +    meta function *defaults* to warn if any ineffective parameters are set.
│ │  
│ │ -        # single trial cross-correlations
│ │ -        if keeptrials:
│ │ -            av_compRoutine = None   # no trial average
│ │ -            norm = True   # normalize individual trials within the ST CR
│ │ -        else:
│ │ -            av_compRoutine = NormalizeCrossCov()
│ │ -            norm = False
│ │ +    Parameters
│ │ +    ----------
│ │ +    CR : :class:`~syncopy.shared.computational_routine.ComputationalRoutine
│ │ +        Needs to have a `valid_kws` attribute
│ │ +    defaults : dict
│ │ +        Result of :func:`~syncopy.shared.tools.get_defaults`, the frontend
│ │ +        parameter names plus values with default values
│ │ +    lcls : dict
│ │ +        Result of `locals()`, all names and values of the local (frontend-)name space
│ │ +    besides : list or None
│ │ +        List of kws which don't get checked
│ │ +    """
│ │ +    # list of possible parameter names of the CR
│ │ +    expected = CR.valid_kws + ["parallel", "select"]
│ │ +    if besides is not None:
│ │ +        expected += besides
│ │ +
│ │ +    relevant = [name for name in defaults if name not in generalParameters]
│ │ +
│ │ +    for name in relevant:
│ │ +        if name not in expected and (lcls[name] != defaults[name]):
│ │ +            msg = f"option `{name}` has no effect for `{CR.__name__}`!"
│ │ +            SPYWarning(msg, caller=__name__.split('.')[-1])
│ │ +
│ │ +
│ │ +def check_passed_kwargs(lcls, defaults, frontend_name):
│ │ +    """
│ │ +    Catch additional kwargs passed to the frontends
│ │ +    which have no effect
│ │ +    """
│ │  
│ │ -        # parallel computation over trials
│ │ -        st_compRoutine = ST_CrossCovariance(samplerate=data.samplerate,
│ │ -                                            polyremoval=polyremoval,
│ │ -                                            timeAxis=timeAxis,
│ │ -                                            norm=norm)
│ │ -        # hard coded as class attribute
│ │ -        st_dimord = ST_CrossCovariance.dimord
│ │ -
│ │ -    # -------------------------------------------------
│ │ -    # Call the chosen single trial ComputationalRoutine
│ │ -    # -------------------------------------------------
│ │ -
│ │ -    # the single trial results need a new DataSet
│ │ -    st_out = CrossSpectralData(dimord=st_dimord)
│ │ -
│ │ -    # Perform the trial-parallelized computation of the matrix quantity
│ │ -    st_compRoutine.initialize(data,
│ │ -                              st_out._stackingDim,
│ │ -                              chan_per_worker=None,   # no parallelisation over channels possible
│ │ -                              keeptrials=keeptrials)  # we most likely need trial averaging!
│ │ -    st_compRoutine.compute(data, st_out, parallel=kwargs.get("parallel"), log_dict=log_dict)
│ │ -
│ │ -    # for single trial cross-corr results <-> keeptrials is True
│ │ -    if av_compRoutine is None:
│ │ -        return st_out
│ │ -
│ │ -    # ----------------------------------------------------------------------------------
│ │ -    # Sanitize output and call the chosen ComputationalRoutine on the averaged ST output
│ │ -    # ----------------------------------------------------------------------------------
│ │ -
│ │ -    out = CrossSpectralData(dimord=st_dimord)
│ │ -
│ │ -    # now take the trial average from the single trial CR as input
│ │ -    av_compRoutine.initialize(st_out, out._stackingDim, chan_per_worker=None)
│ │ -    av_compRoutine.pre_check()   # make sure we got a trial_average
│ │ -    av_compRoutine.compute(st_out, out, parallel=False, log_dict=log_dict)
│ │ -
│ │ -    # attach potential older cfg's from the input
│ │ -    # to support chained frontend calls..
│ │ -    out.cfg.update(data.cfg)
│ │ -    # attach frontend parameters for replay
│ │ -    out.cfg.update({'connectivityanalysis': new_cfg})
│ │ -    return out
│ │ +    # unpack **kwargs of frontend call which
│ │ +    # might contain arbitrary kws passed from the user
│ │ +    kw_dict = lcls.get("kwargs")
│ │ +
│ │ +    # nothing to do..
│ │ +    if not kw_dict:
│ │ +        return
│ │ +
│ │ +    relevant = list(kw_dict.keys())
│ │ +    expected = [name for name in defaults] + ['chan_per_worker']
│ │ +
│ │ +    for name in relevant:
│ │ +        if name not in expected:
│ │ +            msg = f"option `{name}` has no effect in `{frontend_name}`!"
│ │ +            SPYWarning(msg, caller=__name__.split('.')[-1])
│ │ +
│ │ +
│ │ +def _nextpow2(number):
│ │ +    """Find integer power of 2 greater than or equal to number."""
│ │ +    n = 1
│ │ +    while n < number:
│ │ +        n *= 2
│ │ +    return n
│ │   --- esi-syncopy-2022.8/syncopy/nwanalysis/csd.py
│ ├── +++ esi_syncopy-2023.3/syncopy/connectivity/csd.py
│ │┄ Files 7% similar despite different names
│ │ @@ -1,12 +1,11 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │ -# computeFunctions and -Routines for parallel calculation
│ │ -# of single trial measures needed for the averaged
│ │ -# measures like cross spectral densities
│ │ +# Backend functions for calculating cross spectral densities
│ │ +#
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │  
│ │  # syncopy imports
│ │  from syncopy.specest.mtmfft import mtmfft
│ │ @@ -16,16 +15,15 @@
│ │  
│ │  def csd(trl_dat,
│ │          samplerate=1,
│ │          nSamples=None,
│ │          taper="hann",
│ │          taper_opt=None,
│ │          demean_taper=False,
│ │ -        norm=False,
│ │ -        fullOutput=False):
│ │ +        norm=False):
│ │  
│ │      """
│ │      Single trial Fourier cross spectral estimates between all channels
│ │      of the input data. First all the individual Fourier transforms
│ │      are calculated via a (multi-)tapered FFT, then the pairwise
│ │      cross-spectra are computed.
│ │  
│ │ @@ -35,15 +33,15 @@
│ │      Output consists of all (``nChannels x nChannels + 1) / 2`` different complex
│ │      estimates arranged in a symmetric fashion (``CS_ij == CS_ji*``). The
│ │      elements on the main diagonal (`CS_ii`) are the (real) auto-spectra.
│ │  
│ │      This is NOT the same as what is commonly referred to as
│ │      "cross spectral density" as there is no (time) averaging!!
│ │      Multi-tapering alone is not necessarily sufficient to get enough
│ │ -    statitstical power for a robust csd estimate. Yet for completeness
│ │ +    statistical power for a robust csd estimate. Yet for completeness
│ │      and testing the option ``norm = True`` returns a single-trial
│ │      coherence estimate for ``taper = "dpss"``.
│ │  
│ │      Parameters
│ │      ----------
│ │      trl_dat : (N, K) :class:`numpy.ndarray`
│ │          Uniformly sampled multi-channel time-series data
│ │ @@ -65,26 +63,23 @@
│ │          `SciPy docs <https://docs.scipy.org/doc/scipy/reference/signal.windows.html>`_
│ │      demean_taper : bool
│ │          Set to `True` to perform de-meaning after tapering
│ │      norm : bool, optional
│ │          Set to `True` to normalize for a single-trial coherence measure.
│ │          Only meaningful in a multi-taper (``taper = "dpss"``) setup and if no
│ │          additional (trial-)averaging is performed afterwards.
│ │ -    fullOutput : bool
│ │ -        For backend testing or stand-alone applications, set to `True`
│ │ -        to return also the `freqs` array.
│ │  
│ │      Returns
│ │      -------
│ │      CS_ij : (nFreq, K, K) :class:`numpy.ndarray`
│ │          Complex cross spectra for all channel combinations ``i,j``.
│ │          `K` corresponds to number of input channels.
│ │  
│ │      freqs : (nFreq,) :class:`numpy.ndarray`
│ │ -        The Fourier frequencies if ``fullOutput = True``
│ │ +        The Fourier frequencies
│ │  
│ │      See also
│ │      --------
│ │      normalize_csd : :func:`~syncopy.connectivity.csd.normalize_csd`
│ │               Coherence from trial averages
│ │  
│ │      mtmfft : :func:`~syncopy.specest.mtmfft.mtmfft`
│ │ @@ -111,18 +106,15 @@
│ │              raise SPYValueError(legal=msg, varname="taper", actual=taper)
│ │          # main diagonal has shape (nChannels x nFreq): the auto spectra
│ │          diag = CS_ij.diagonal()
│ │          # get the needed product pairs of the autospectra
│ │          Ciijj = np.sqrt(diag[:, :, None] * diag[:, None, :]).T
│ │          CS_ij = CS_ij / Ciijj
│ │  
│ │ -    if fullOutput:
│ │ -        return CS_ij.transpose(2, 0, 1), freqs
│ │ -    else:
│ │ -        return CS_ij.transpose(2, 0, 1)
│ │ +    return CS_ij.transpose(2, 0, 1), freqs
│ │  
│ │  
│ │  def normalize_csd(csd_av_dat,
│ │                    output='abs'):
│ │  
│ │      """
│ │      Given the trial averaged cross spectral densities,
│ │ @@ -137,44 +129,43 @@
│ │  
│ │      The coherence is now defined as either ``|C_ij|``
│ │      or ``|C_ij|^2``, this can be controlled with the `output`
│ │      parameter.
│ │  
│ │      Parameters
│ │      ----------
│ │ -    csd_av_dat : (nFreq, N, N) :class:`numpy.ndarray`
│ │ +    csd_av_dat : (nTime, nFreq, N, N) :class:`numpy.ndarray`
│ │          Averaged cross-spectral densities for `N` x `N` channels
│ │          and `nFreq` frequencies averaged over trials.
│ │      output : {'abs', 'pow', 'fourier', 'complex', 'angle', 'imag' or 'real'}, default: 'abs'
│ │          After normalization the coherency is still complex (`'fourier'`);
│ │          to get the real valued coherence ``0 < C_ij(f) < 1`` one can either take the
│ │          absolute (`'abs'`) or the absolute squared (`'pow'`) values of the
│ │          coherencies. The definitions are not uniform in the literature,
│ │          hence multiple output types are supported. Use `'angle'`, `'imag'` or `'real'`
│ │          to extract the phase difference, imaginary or real part of the coherency respectively.
│ │  
│ │      Returns
│ │      -------
│ │ -    CS_ij : (nFreq, N, N) :class:`numpy.ndarray`
│ │ +    CS_ij : (nTime, nFreq, N, N) :class:`numpy.ndarray`
│ │          Coherence for all channel combinations ``i,j``.
│ │          `N` corresponds to number of input channels.
│ │  
│ │      Notes
│ │      -----
│ │      .. [1] Nolte, Guido, et al. "Identifying true brain interaction from EEG
│ │            data using the imaginary part of coherency."
│ │            Clinical neurophysiology 115.10 (2004): 2292-2307.
│ │      """
│ │ -    # re-shape to (nChannels x nChannels x nFreq)
│ │ -    CS_ij = csd_av_dat.transpose(1, 2, 0)
│ │  
│ │ -    # main diagonal has shape (nFreq x nChannels): the auto spectra
│ │ -    diag = CS_ij.diagonal()
│ │ +    CS_ij = csd_av_dat.view()
│ │ +
│ │ +    # channel diagonal has shape (nTime x nFreq x nChannels): the auto spectra
│ │ +    diag = CS_ij.diagonal(axis1=-2, axis2=-1)
│ │  
│ │      # get the needed product pairs of the autospectra
│ │ -    Ciijj = np.sqrt(diag[:, :, None] * diag[:, None, :]).T
│ │ +    Ciijj = np.sqrt(diag[..., None] * diag[..., None, :])
│ │      CS_ij = CS_ij / Ciijj
│ │  
│ │      CS_ij = spectralConversions[output](CS_ij)
│ │  
│ │ -    # re-shape to original form
│ │ -    return CS_ij.transpose(2, 0, 1)
│ │ +    return CS_ij
│ │   --- esi-syncopy-2022.8/syncopy/nwanalysis/granger.py
│ ├── +++ esi_syncopy-2023.3/syncopy/connectivity/granger.py
│ │┄ Files 5% similar despite different names
│ │ @@ -32,30 +32,30 @@
│ │          Spectral Granger-Geweke causality between all channel
│ │          combinations. Directionality follows array
│ │          notation: causality from ``i -> j`` is ``Granger[:,i,j]``,
│ │          causality from ``j -> i`` is ``Granger[:,j,i]``
│ │  
│ │      See also
│ │      --------
│ │ -    wilson_sf : :func:`~syncopy.connectivity.wilson_sf.wilson_sf
│ │ +    wilson_sf : :func:`~syncopy.connectivity.wilson_sf.wilson_sf`
│ │               Spectral matrix factorization that yields the
│ │               transfer functions and noise covariances
│ │               from a cross spectral density.
│ │  
│ │      Notes
│ │      -----
│ │ -    .. [1] Dhamala, Mukeshwar, Govindan Rangarajan, and Mingzhou Ding.
│ │ +    .. [1] Dhamala Mukeshwar, Govindan Rangarajan, and Mingzhou Ding.
│ │         "Estimating Granger causality from Fourier and wavelet transforms
│ │          of time series data." Physical review letters 100.1 (2008): 018701.
│ │  
│ │      """
│ │  
│ │      nChannels = CSD.shape[1]
│ │      auto_spectra = CSD.transpose(1, 2, 0).diagonal()
│ │ -    auto_spectra = np.abs(auto_spectra) # auto-spectra are real
│ │ +    auto_spectra = np.abs(auto_spectra)  # auto-spectra are real
│ │  
│ │      # we need the stacked auto-spectra of the form (nChannel=3):
│ │      #           S_11 S_22 S_33
│ │      # Smat(f) = S_11 S_22 S_33
│ │      #           S_11 S_22 S_33
│ │      Smat = auto_spectra[:, None, :] * np.ones(nChannels)[:, None]
│ │   --- esi-syncopy-2022.8/syncopy/nwanalysis/wilson_sf.py
│ ├── +++ esi_syncopy-2023.3/syncopy/connectivity/wilson_sf.py
│ │┄ Files 8% similar despite different names
│ │ @@ -9,15 +9,15 @@
│ │  # The Factorization of Matricial Spectral Densities, SIAM J. Appl. Math,
│ │  # Vol. 23, No. 4, pgs 420-426 December 1972 by G T Wilson).
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │  
│ │  
│ │ -def wilson_sf(CSD, nIter=100, rtol=1e-9, direct_inversion=True):
│ │ +def wilson_sf(CSD, nIter=100, rtol=1e-6, direct_inversion=True):
│ │      """
│ │      Wilsons spectral matrix factorization ("analytic method")
│ │  
│ │      Converges extremely fast, so the default number of
│ │      iterations should be more than enough in practical situations.
│ │  
│ │      This is a pure backend function and hence no input argument
│ │ @@ -46,14 +46,17 @@
│ │          The transfer function
│ │      Sigma : (N, N) :class:`numpy.ndarray`
│ │          Noise covariance
│ │      converged : bool
│ │          Indicates wether the algorithm converged.
│ │          If `False` result was returned after `nIter`
│ │          iterations.
│ │ +    err : float
│ │ +        Maximal final relative error
│ │ +        between input CSD and factorized CSD
│ │      """
│ │  
│ │      nFreq = CSD.shape[0]
│ │  
│ │      Ident = np.eye(*CSD.shape[1:])
│ │  
│ │      # attach negative frequencies
│ │ @@ -98,27 +101,28 @@
│ │          # the next step psi_{tau+1}
│ │          psi = psi @ (gplus + S)
│ │          psi0 = psi0 @ (gplus_0 + S)
│ │  
│ │          # max relative error
│ │          CSDfac = psi @ psi.conj().transpose(0, 2, 1)
│ │          err = max_rel_err(CSD, CSDfac)
│ │ +
│ │          # converged
│ │          if err < rtol:
│ │              converged = True
│ │              break
│ │  
│ │      # Noise Covariance
│ │      Sigma = psi0 @ psi0.T
│ │  
│ │      # Transfer function
│ │      psi0_inv = np.linalg.inv(psi0)
│ │      Hfunc = psi @ psi0_inv
│ │  
│ │ -    return Hfunc[:nFreq], Sigma, converged
│ │ +    return Hfunc[:nFreq], Sigma, converged, err
│ │  
│ │  
│ │  def _psi0_initial(CSD):
│ │  
│ │      """
│ │      Initialize Wilson's algorithm with the Cholesky
│ │      decomposition of the 1st Fourier series component
│ │ @@ -187,25 +191,25 @@
│ │  def max_rel_err(A, B):
│ │  
│ │      err = np.abs(A - B)
│ │      err = (err / np.abs(A)).max()
│ │      return err
│ │  
│ │  
│ │ -def regularize_csd(CSD, cond_max=1e6, eps_max=1e-3, nSteps=15):
│ │ +def regularize_csd(CSD, cond_max=1e3, eps_max=1e-3, nSteps=15):
│ │  
│ │      """
│ │      Brute force regularization of CSD matrix
│ │      by inspecting the maximal condition number
│ │      along the frequency axis.
│ │ -    Multiply with different ``epsilon * I``,
│ │ -    starting with ``epsilon = 1e-10`` until the
│ │ -    condition number is smaller than `cond_max`.
│ │ -    Raises a `ValueError` if the maximal regularization
│ │ -    factor `epx_max` was reached but `cond_max` still not met.
│ │ +    Multiply with ``epsilon * I``, starting with ``epsilon = 1e-10``
│ │ +    up to ``epsilon = eps_max`` on a log-scale of size ``nSteps``
│ │ +    until the condition number is smaller than `cond_max`.
│ │ +    If that can not be achieved, return the last regularization
│ │ +    result and `-1` as factor for downstream (error/warning) handling.
│ │  
│ │  
│ │      Parameters
│ │      ----------
│ │      CSD : 3D :class:`numpy.ndarray`
│ │          The cross spectral density matrix
│ │          with shape ``(nFreq, nChannel, nChannel)``
│ │ @@ -221,28 +225,31 @@
│ │      Returns
│ │      -------
│ │      CSDreg : 3D :class:`numpy.ndarray`
│ │          The regularized CSD matrix with a maximal
│ │          condition number of `cond_max`
│ │      eps : float
│ │          The regularization factor used
│ │ +    iniCondNum : float
│ │ +        The initial condition number of the CSD
│ │  
│ │      """
│ │  
│ │      epsilons = np.logspace(-10, np.log10(eps_max), nSteps)
│ │      I = np.eye(CSD.shape[1])
│ │  
│ │      CondNum = np.linalg.cond(CSD).max()
│ │ -
│ │ +    iniCondNum = CondNum
│ │ +    
│ │      # nothing to be done
│ │      if CondNum < cond_max:
│ │ -        return CSD, 0
│ │ +        return CSD, 0, iniCondNum
│ │  
│ │      for eps in epsilons:
│ │          CSDreg = CSD + eps * I
│ │          CondNum = np.linalg.cond(CSDreg).max()
│ │  
│ │          if CondNum < cond_max:
│ │ -            return CSDreg, eps
│ │ +            return CSDreg, eps, iniCondNum
│ │  
│ │ -    msg = f"CSD matrix not regularizable with a max epsilon of {eps_max}!"
│ │ -    raise ValueError(msg)
│ │ +    # regularization goal not achieved
│ │ +    return CSDreg, -1, iniCondNum
│ │   --- esi-syncopy-2022.8/syncopy/plotting/_helpers.py
│ ├── +++ esi_syncopy-2023.3/syncopy/plotting/helpers.py
│ │┄ Files 13% similar despite different names
│ │ @@ -2,33 +2,43 @@
│ │  #
│ │  # Helpers  to generate correct data, labels etc. for the plots
│ │  # from Syncopy dataypes
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │ +from copy import deepcopy
│ │  import re
│ │  import functools
│ │  
│ │  
│ │  def revert_selection(plotter):
│ │  
│ │      """
│ │      To extract 'meta-information' like time and freq axis
│ │      for a particular plot we use (implicit from the users
│ │      perspective) selections. To return to a clean slate
│ │      we revert/delete it afterwards.
│ │  
│ │ -    All plotting routines must have `data` as 1st (*arg) argument!
│ │ +    All plotting routines must have `data` as 1st argument!
│ │      """
│ │      @functools.wraps(plotter)
│ │ -    def wrapper_plot(*args, **kwargs):
│ │ +    def wrapper_plot(data, *args, **kwargs):
│ │ +
│ │ +        # to restore
│ │ +        select_backup = None if data.selection is None else deepcopy(data.selection.select)
│ │ +
│ │ +        res = plotter(data, *args, **kwargs)
│ │ +
│ │ +        # restore initial selection or wipe
│ │ +        if select_backup:
│ │ +            data.selectdata(select_backup, inplace=True)
│ │ +        else:
│ │ +            data.selection = None
│ │  
│ │ -        res = plotter(*args, **kwargs)
│ │ -        args[0].selection = None
│ │          return res
│ │  
│ │      return wrapper_plot
│ │  
│ │  
│ │  def parse_foi(dataobject, show_kwargs):
│ │  
│ │ @@ -69,15 +79,15 @@
│ │      show_kwargs : dict
│ │          The keywords provided to the `selectdata` method
│ │      """
│ │  
│ │      # apply the selection
│ │      dataobject.selectdata(inplace=True, **show_kwargs)
│ │  
│ │ -    # still have to index the single trial
│ │ +    # still have to index the only and single trial
│ │      idx = dataobject.selection.time[0]
│ │  
│ │      # index selection, again the single trial
│ │      time = dataobject.time[trl][idx]
│ │  
│ │      return time
│ │  
│ │ @@ -128,27 +138,34 @@
│ │          offsets = np.cumsum(np.r_[0, offsets] * 1.1)
│ │      else:
│ │          offsets = 0
│ │  
│ │      return offsets
│ │  
│ │  
│ │ -def get_method(dataobject):
│ │ +def get_method(dataobject, frontend_name):
│ │  
│ │      """
│ │      Returns the method string from
│ │ -    the log of a Syncopy data object
│ │ +    the cfg of a Syncopy data object
│ │ +    """
│ │ +
│ │ +    cfg_entry = dataobject.cfg[frontend_name]
│ │ +    return cfg_entry.get('method')
│ │ +
│ │ +
│ │ +def get_output(dataobject, frontend_name):
│ │ +
│ │ +    """
│ │ +    Returns the output string from
│ │ +    the cfg of a Syncopy data object
│ │      """
│ │  
│ │ -    # get the method string in a capture group
│ │ -    pattern = re.compile(r'[\s\w\D]+method = (\w+)')
│ │ -    match = pattern.match(dataobject._log)
│ │ -    if match:
│ │ -        meth_str = match.group(1)
│ │ -        return meth_str
│ │ +    cfg_entry = dataobject.cfg[frontend_name]
│ │ +    return cfg_entry.get('output')
│ │  
│ │  
│ │  def calc_multi_layout(nAx):
│ │  
│ │      """
│ │      Given the total numbers of
│ │      axes `nAx` create the nrows, ncols
│ │ @@ -178,7 +195,18 @@
│ │              nrows -= 1
│ │              ncols = int(nAx / nrows)
│ │      # just two axes
│ │      elif nAx == 2:
│ │          nrows, ncols = 1, 2
│ │  
│ │      return nrows, ncols
│ │ +
│ │ +
│ │ +def check_if_time_freq(data):
│ │ +    """
│ │ +    Looks into the first column of the trialdefinition
│ │ +    to determine if there is a real time axis, or it is
│ │ +    just trial stacking.
│ │ +    """
│ │ +    is_tf = np.any(np.diff(data.trialdefinition)[:, 0] != 1)
│ │ +
│ │ +    return is_tf
│ │   --- esi-syncopy-2022.8/syncopy/plotting/_plotting.py
│ ├── +++ esi_syncopy-2023.3/syncopy/plotting/_plotting.py
│ │┄ Files 2% similar despite different names
│ │ @@ -179,9 +179,10 @@
│ │      """
│ │  
│ │      # extent is defined in xy order
│ │      df = freqs[1] - freqs[0]
│ │      extent = [times[0], times[-1],
│ │                freqs[0] - df / 2, freqs[-1] - df / 2]
│ │  
│ │ -    ax.imshow(data_yx[::-1], aspect='auto', cmap=pltConfig['cmap'],
│ │ +    cmap = pkwargs.pop('cmap', pltConfig['cmap'])
│ │ +    ax.imshow(data_yx[::-1], aspect='auto', cmap=cmap,
│ │                extent=extent, **pkwargs)
│ │   --- esi-syncopy-2022.8/syncopy/plotting/config.py
│ ├── +++ esi_syncopy-2023.3/syncopy/plotting/config.py
│ │┄ Files 17% similar despite different names
│ │ @@ -1,24 +1,29 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │  # Syncopy plotting setup
│ │  #
│ │  
│ │  from syncopy import __plt__
│ │ +from packaging.version import parse
│ │  
│ │  foreground = "#2E3440"  # nord0
│ │  background = '#fcfcfc'  # hint of gray
│ │  
│ │  # dark mode
│ │  # foreground = "#D8DEE9"  # nord4
│ │  # background = "#2E3440"  # nord0
│ │  
│ │  if __plt__:
│ │      import matplotlib as mpl
│ │ -    mpl.style.use('seaborn-colorblind')
│ │ +    # to allow both older and newer matplotlib versions
│ │ +    if parse(mpl.__version__) < parse("3.6"):
│ │ +        mpl.style.use('seaborn-colorblind')
│ │ +    else:
│ │ +        mpl.style.use('seaborn-v0_8-colorblind')
│ │      # a hint of gray
│ │      rc_props = {
│ │          'patch.edgecolor': foreground,
│ │          'text.color': foreground,
│ │          'axes.facecolor': background,
│ │          'axes.facecolor': background,
│ │          'figure.facecolor': background,
│ │   --- esi-syncopy-2022.8/syncopy/plotting/mp_plotting.py
│ ├── +++ esi_syncopy-2023.3/syncopy/plotting/mp_plotting.py
│ │┄ Files 16% similar despite different names
│ │ @@ -9,15 +9,15 @@
│ │  import numpy as np
│ │  from numbers import Number
│ │  
│ │  # Syncopy imports
│ │  from syncopy import __plt__
│ │  from syncopy.shared.errors import SPYWarning, SPYValueError
│ │  from syncopy.plotting import _plotting
│ │ -from syncopy.plotting import _helpers as plot_helpers
│ │ +from syncopy.plotting import helpers as plot_helpers
│ │  from syncopy.plotting.config import pltErrMsg, pltConfig
│ │  
│ │  
│ │  @plot_helpers.revert_selection
│ │  def plot_AnalogData(data, shifted=True, **show_kwargs):
│ │  
│ │      """
│ │ @@ -44,14 +44,17 @@
│ │      # only 1 trial so no explicit selection needed
│ │      elif len(data.trials) == 1:
│ │          trl = 0
│ │  
│ │      # get the data to plot
│ │      data_x = plot_helpers.parse_toi(data, trl, show_kwargs)
│ │      data_y = data.show(**show_kwargs)
│ │ +    # 'time' and 'channel' are the only axes
│ │ +    if data._defaultDimord != data.dimord:
│ │ +        data_y = data_y.T
│ │  
│ │      if data_y.size == 0:
│ │          lgl = "Selection with non-zero size"
│ │          act = "got zero samples"
│ │          raise SPYValueError(lgl, varname="show_kwargs", actual=act)
│ │  
│ │      # multiple channels?
│ │ @@ -59,15 +62,15 @@
│ │      nAx = 1 if isinstance(labels, str) else len(labels)
│ │  
│ │      if nAx < 2:
│ │          SPYWarning("Please select at least two channels for a multipanelplot!")
│ │          return
│ │  
│ │      elif nAx > pltConfig['mMaxAxes']:
│ │ -        SPYWarning("Please select max. {pltConfig['mMaxAxes']} channels for a multipanelplot!")
│ │ +        SPYWarning(f"Please select max. {pltConfig['mMaxAxes']} channels for a multipanelplot!")
│ │          return
│ │      else:
│ │          # determine axes layout, prefer columns over rows due to display aspect ratio
│ │          nrows, ncols = plot_helpers.calc_multi_layout(nAx)
│ │  
│ │      fig, axs = _plotting.mk_multi_line_figax(nrows, ncols)
│ │  
│ │ @@ -108,30 +111,31 @@
│ │      trl = show_kwargs.get('trials', None)
│ │      if not isinstance(trl, Number) and len(data.trials) > 1:
│ │          SPYWarning("Please select a single trial for plotting!")
│ │          return
│ │      elif len(data.trials) == 1:
│ │          trl = 0
│ │  
│ │ -    labels = plot_helpers.parse_channel(data, show_kwargs)
│ │ -    nAx = 1 if isinstance(labels, str) else len(labels)
│ │ +    channels = plot_helpers.parse_channel(data, show_kwargs)
│ │ +    nAx = 1 if isinstance(channels, str) else len(channels)
│ │  
│ │      if nAx < 2:
│ │          SPYWarning("Please select at least two channels for a multipanelplot!")
│ │          return
│ │      elif nAx > pltConfig['mMaxAxes']:
│ │          SPYWarning("Please select max. {pltConfig['mMaxAxes']} channels for a multipanelplot!")
│ │          return
│ │      else:
│ │          # determine axes layout, prefer columns over rows due to display aspect ratio
│ │          nrows, ncols = plot_helpers.calc_multi_layout(nAx)
│ │  
│ │ -    # how got the spectrum computed
│ │ -    method = plot_helpers.get_method(data)
│ │ -    if method in ('wavelet', 'superlet', 'mtmconvol'):
│ │ +    # -- check if it is a time-frequency spectrum ----------
│ │ +    is_tf = np.any(np.diff(data.trialdefinition)[:, 0] != 1)
│ │ +    # ------------------------------------------------------
│ │ +    if is_tf:
│ │          fig, axs = _plotting.mk_multi_img_figax(nrows, ncols)
│ │  
│ │          # this could be more elegantly solve by
│ │          # an in-place selection?!
│ │          time = plot_helpers.parse_toi(data, trl, show_kwargs)
│ │          freqs = plot_helpers.parse_foi(data, show_kwargs)
│ │  
│ │ @@ -140,15 +144,15 @@
│ │          data_cyx = data.show(**show_kwargs).T
│ │          if data_cyx.size == 0:
│ │              lgl = "Selection with non-zero size"
│ │              act = "got zero samples"
│ │              raise SPYValueError(lgl, varname="show_kwargs", actual=act)
│ │  
│ │          maxP = data_cyx.max()
│ │ -        for data_yx, ax, label in zip(data_cyx, axs.flatten(), labels):
│ │ +        for data_yx, ax, label in zip(data_cyx, axs.flatten(), channels):
│ │              _plotting.plot_tfreq(ax, data_yx, time, freqs, vmax=maxP)
│ │              ax.set_title(label, fontsize=pltConfig['mTitleSize'])
│ │          fig.tight_layout()
│ │          fig.subplots_adjust(wspace=0.05)
│ │  
│ │      # just a line plot
│ │      else:
│ │ @@ -162,21 +166,43 @@
│ │          if msg:
│ │              msg = ("Line spectra don't have a time axis, "
│ │                     "ignoring `toi/toilim` selection!")
│ │              SPYWarning(msg)
│ │  
│ │          # get the data to plot
│ │          data_x = plot_helpers.parse_foi(data, show_kwargs)
│ │ -        data_y = np.log10(data.show(**show_kwargs))
│ │ +        output = plot_helpers.get_output(data, 'freqanalysis')
│ │ +
│ │ +        # only log10 the absolute squared spectra
│ │ +        if output == 'pow':
│ │ +            data_y = np.log10(data.show(**show_kwargs))
│ │ +            ylabel = 'power (dB)'
│ │ +        elif output in ['fourier', 'complex']:
│ │ +            SPYWarning("Can't plot complex valued spectra, choose 'real' or 'imag' as output! Aborting plotting.")
│ │ +            return
│ │ +        else:
│ │ +            data_y = data.show(**show_kwargs)
│ │ +            ylabel = f'{output}'
│ │ +
│ │ +        taper_labels = None
│ │ +        if len(data.taper) != 1:   
│ │ +            taper = show_kwargs.get('taper')
│ │ +            # multiple tapers are to be plotted
│ │ +            if not isinstance(taper, (Number, str)):
│ │ +                taper_labels = data.taper
│ │  
│ │          fig, axs = _plotting.mk_multi_line_figax(nrows, ncols, xlabel='frequency (Hz)',
│ │ -                                                 ylabel='power (dB)')
│ │ +                                                 ylabel=ylabel)
│ │  
│ │ -        for chan_dat, ax, label in zip(data_y.T, axs.flatten(), labels):
│ │ -            _plotting.plot_lines(ax, data_x, chan_dat, label=label, leg_fontsize=pltConfig['mLegendSize'])
│ │ +        for chan_dat, ax, label in zip(data_y.T, axs.flatten(), channels):
│ │ +            if taper_labels is not None:
│ │ +                _plotting.plot_lines(ax, data_x, chan_dat, label=taper_labels, leg_fontsize=pltConfig['mLegendSize'])
│ │ +            else:
│ │ +                _plotting.plot_lines(ax, data_x, chan_dat)
│ │ +            ax.set_title(label, fontsize=pltConfig['mTitleSize'])
│ │  
│ │          # delete empty plot due to grid extension
│ │          # because of prime nAx -> can be maximally 1 plot
│ │          if ncols * nrows > nAx:
│ │              axs.flatten()[-1].remove()
│ │          fig.tight_layout()
│ │  
│ │ @@ -210,36 +236,40 @@
│ │      # what channel combination
│ │      if 'channel_i' not in show_kwargs or 'channel_j' not in show_kwargs:
│ │          SPYWarning("Please select a channel combination for plotting!")
│ │          return
│ │      chi, chj = show_kwargs['channel_i'], show_kwargs['channel_j']
│ │  
│ │      # what data do we have?
│ │ -    method = plot_helpers.get_method(data)
│ │ +    method = plot_helpers.get_method(data, 'connectivityanalysis')
│ │ +    output = plot_helpers.get_output(data, 'connectivityanalysis')
│ │ +
│ │      if method == 'granger':
│ │          xlabel = 'frequency (Hz)'
│ │          ylabel = 'Granger causality'
│ │          label = rf"channel{chi} $\rightarrow$ channel{chj}"
│ │          data_x = plot_helpers.parse_foi(data, show_kwargs)
│ │      elif method == 'coh':
│ │          xlabel = 'frequency (Hz)'
│ │ -        ylabel = 'coherence'
│ │ +        ylabel = f'{output} coherence'
│ │          label = rf"channel{chi} - channel{chj}"
│ │          data_x = plot_helpers.parse_foi(data, show_kwargs)
│ │      elif method == 'corr':
│ │          xlabel = 'lag'
│ │          ylabel = 'correlation'
│ │          label = rf"channel{chi} - channel{chj}"
│ │          data_x = plot_helpers.parse_toi(data, show_kwargs)
│ │      # that's all the methods we got so far
│ │      else:
│ │          raise NotImplementedError
│ │  
│ │      # get the data to plot
│ │      data_y = data.show(**show_kwargs)
│ │  
│ │ -    # create the axes and figure if needed
│ │ -    # persisten axes allows for plotting different
│ │ -    # channel combinations into the same figure
│ │ +    # Create the axes and figure if needed.
│ │ +    # Persistent axes allow for plotting different
│ │ +    # channel combinations into the same figure.
│ │      if not hasattr(data, 'ax'):
│ │          fig, data.ax = _plotting.mk_line_figax(xlabel, ylabel)
│ │      _plotting.plot_lines(data.ax, data_x, data_y, label=label)
│ │ +
│ │ +    return fig, data.ax
│ │   --- esi-syncopy-2022.8/syncopy/plotting/spy_plotting.py
│ ├── +++ esi_syncopy-2023.3/syncopy/plotting/spy_plotting.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/preproc/compRoutines.py
│ ├── +++ esi_syncopy-2023.3/syncopy/preproc/compRoutines.py
│ │┄ Files 2% similar despite different names
│ │ @@ -3,18 +3,19 @@
│ │  # computeFunctions and -Routines for parallel calculation
│ │  # of FIR and IIR Filter operations
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │  import scipy.signal as sci
│ │ +import logging, platform
│ │  from inspect import signature
│ │  
│ │  # syncopy imports
│ │ -from syncopy.shared.computational_routine import ComputationalRoutine, propagate_metadata
│ │ +from syncopy.shared.computational_routine import ComputationalRoutine, propagate_properties
│ │  from syncopy.shared.const_def import spectralConversions, spectralDTypes
│ │  from syncopy.shared.kwarg_decorators import process_io
│ │  
│ │  # backend imports
│ │  from .firws import design_wsinc, apply_fir, minphaserceps
│ │  from .resampling import downsample, resample
│ │  
│ │ @@ -151,15 +152,15 @@
│ │      computeFunction = staticmethod(sinc_filtering_cF)
│ │  
│ │      # 1st argument,the data, gets omitted
│ │      valid_kws = list(signature(sinc_filtering_cF).parameters.keys())[1:]
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │ -        propagate_metadata(data, out)
│ │ +        propagate_properties(data, out)
│ │  
│ │  
│ │  @process_io
│ │  def but_filtering_cF(dat,
│ │                       samplerate=1,
│ │                       filter_type='lp',
│ │                       freq=None,
│ │ @@ -274,15 +275,15 @@
│ │      computeFunction = staticmethod(but_filtering_cF)
│ │  
│ │      # 1st argument,the data, gets omitted
│ │      valid_kws = list(signature(but_filtering_cF).parameters.keys())[1:]
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │ -        propagate_metadata(data, out)
│ │ +        propagate_properties(data, out)
│ │  
│ │  
│ │  @process_io
│ │  def rectify_cF(dat, noCompute=False, chunkShape=None):
│ │  
│ │      """
│ │      Provides straightforward rectification via `np.abs`.
│ │ @@ -336,15 +337,15 @@
│ │      computeFunction = staticmethod(rectify_cF)
│ │  
│ │      # 1st argument,the data, gets omitted
│ │      valid_kws = list(signature(rectify_cF).parameters.keys())[1:]
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │ -        propagate_metadata(data, out)
│ │ +        propagate_properties(data, out)
│ │  
│ │  
│ │  @process_io
│ │  def hilbert_cF(dat, output='abs', timeAxis=0, noCompute=False, chunkShape=None):
│ │  
│ │      """
│ │      Provides Hilbert transformation with various outputs, band-pass filtering
│ │ @@ -387,14 +388,17 @@
│ │      # operation does not change the shape
│ │      # but may change the number format
│ │      outShape = dat.shape
│ │      fmt = spectralDTypes["fourier"] if output == 'complex' else spectralDTypes["abs"]
│ │      if noCompute:
│ │          return outShape, fmt
│ │  
│ │ +    logger = logging.getLogger("syncopy_" + platform.node())
│ │ +    logger.debug(f"Computing Hilbert transformation on data chunk with shape {dat.shape} along axis 0.")
│ │ +
│ │      trafo = sci.hilbert(dat, axis=0)
│ │  
│ │      return spectralConversions[output](trafo)
│ │  
│ │  
│ │  class Hilbert(ComputationalRoutine):
│ │  
│ │ @@ -414,15 +418,15 @@
│ │      computeFunction = staticmethod(hilbert_cF)
│ │  
│ │      # 1st argument,the data, gets omitted
│ │      valid_kws = list(signature(hilbert_cF).parameters.keys())[1:]
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │ -        propagate_metadata(data, out)
│ │ +        propagate_properties(data, out)
│ │  
│ │  
│ │  @process_io
│ │  def downsample_cF(dat,
│ │                    samplerate=1,
│ │                    new_samplerate=1,
│ │                    timeAxis=0,
│ │ @@ -469,14 +473,17 @@
│ │      if noCompute:
│ │          # we need integers for slicing
│ │          skipped = int(samplerate // new_samplerate)
│ │          outShape = list(dat.shape)
│ │          outShape[0] = int(np.ceil(dat.shape[0] / skipped))
│ │          return tuple(outShape), dat.dtype
│ │  
│ │ +    logger = logging.getLogger("syncopy_" + platform.node())
│ │ +    logger.debug(f"Downsampling data chunk with shape {dat.shape} from samplerate {samplerate} to {new_samplerate}.")
│ │ +
│ │      resampled = downsample(dat, samplerate, new_samplerate)
│ │  
│ │      return resampled
│ │  
│ │  
│ │  class Downsample(ComputationalRoutine):
│ │  
│ │ @@ -497,22 +504,23 @@
│ │  
│ │      # 1st argument,the data, gets omitted
│ │      valid_kws = list(signature(downsample_cF).parameters.keys())[1:]
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │          # we need to re-calculate the downsampling factor
│ │ -        factor = int(data.samplerate // self.cfg['new_samplerate'])
│ │ +        # that it actually is an 1 / integer gets checked in the frontend
│ │ +        factor = self.cfg['new_samplerate'] / data.samplerate
│ │  
│ │          if data.selection is not None:
│ │              chanSec = data.selection.channel
│ │ -            trl = data.selection.trialdefinition // factor
│ │ +            trl = _resampling_trl_definition(data.selection.trialdefinition, factor)
│ │          else:
│ │              chanSec = slice(None)
│ │ -            trl = data.trialdefinition // factor
│ │ +            trl = _resampling_trl_definition(data.trialdefinition, factor)
│ │  
│ │          out.trialdefinition = trl
│ │          # now set new samplerate
│ │          out.samplerate = self.cfg['new_samplerate']
│ │          out.channel = np.array(data.channel[chanSec])
│ │  
│ │  
│ │ @@ -578,14 +586,17 @@
│ │      nSamples = dat.shape[0]
│ │      fs_ratio = new_samplerate / samplerate
│ │  
│ │      if noCompute:
│ │          new_nSamples = int(np.ceil(nSamples * fs_ratio))
│ │          return (new_nSamples, dat.shape[1]), dat.dtype
│ │  
│ │ +    logger = logging.getLogger("syncopy_" + platform.node())
│ │ +    logger.debug(f"Resampling data chunk with shape {dat.shape} from samplerate {samplerate} to {new_samplerate} with lpfreq={lpfreq}, order={order}.")
│ │ +
│ │      resampled = resample(dat,
│ │                           samplerate,
│ │                           new_samplerate,
│ │                           lpfreq=lpfreq,
│ │                           order=order)
│ │  
│ │      return resampled
│ │ @@ -611,36 +622,36 @@
│ │      # 1st argument,the data, gets omitted
│ │      valid_kws = list(signature(downsample_cF).parameters.keys())[1:]
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │          # we need to re-calculate the resampling factor
│ │          factor = self.cfg['new_samplerate'] / data.samplerate
│ │ -        trafo_trl = lambda trldef: np.ceil(trldef * factor)
│ │ +        trafo_trl = _resampling_trl_definition
│ │  
│ │          if data.selection is not None:
│ │              chanSec = data.selection.channel
│ │ -            trl = trafo_trl(data.selection.trialdefinition)
│ │ +            trl = trafo_trl(data.selection.trialdefinition, factor)
│ │          else:
│ │              chanSec = slice(None)
│ │ -            trl = trafo_trl(data.trialdefinition)
│ │ +            trl = trafo_trl(data.trialdefinition, factor)
│ │  
│ │          out.trialdefinition = trl
│ │  
│ │          # now set new samplerate
│ │          out.samplerate = self.cfg['new_samplerate']
│ │          out.channel = np.array(data.channel[chanSec])
│ │  
│ │  
│ │  @process_io
│ │  def detrending_cF(dat, polyremoval=None, timeAxis=0, noCompute=False, chunkShape=None):
│ │  
│ │      """
│ │      Simple cF to wire SciPy's `detrend` to our CRs,
│ │ -    supported are constant and linear detrending
│ │ +    supported are constant and linear detrending.
│ │  
│ │      Parameters
│ │      ----------
│ │      dat : (N, K) :class:`numpy.ndarray`
│ │          Uniformly sampled multi-channel time-series data
│ │          The 1st dimension is interpreted as the time axis,
│ │          columns represent individual channels.
│ │ @@ -683,14 +694,17 @@
│ │          dat = dat
│ │  
│ │      # detrending does not change the shape
│ │      outShape = dat.shape
│ │      if noCompute:
│ │          return outShape, np.float32
│ │  
│ │ +    logger = logging.getLogger("syncopy_" + platform.node())
│ │ +    logger.debug(f"Detrending data chunk with shape {dat.shape} with polyremoval={polyremoval}.")
│ │ +
│ │      # detrend
│ │      if polyremoval == 0:
│ │          dat = sci.detrend(dat, type='constant', axis=0, overwrite_data=True)
│ │      elif polyremoval == 1:
│ │          dat = sci.detrend(dat, type='linear', axis=0, overwrite_data=True)
│ │  
│ │      # renaming
│ │ @@ -716,15 +730,15 @@
│ │      computeFunction = staticmethod(detrending_cF)
│ │  
│ │      # 1st argument,the data, gets omitted
│ │      valid_kws = list(signature(detrending_cF).parameters.keys())[1:]
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │ -        propagate_metadata(data, out)
│ │ +        propagate_properties(data, out)
│ │  
│ │  
│ │  @process_io
│ │  def standardize_cF(dat, polyremoval=None, timeAxis=0, noCompute=False, chunkShape=None):
│ │  
│ │      """
│ │      Yet another simple cF to z-score ('standardize') signals:
│ │ @@ -777,14 +791,17 @@
│ │  
│ │      # detrend
│ │      if polyremoval == 0:
│ │          dat = sci.detrend(dat, type='constant', axis=0, overwrite_data=True)
│ │      elif polyremoval == 1:
│ │          dat = sci.detrend(dat, type='linear', axis=0, overwrite_data=True)
│ │  
│ │ +    logger = logging.getLogger("syncopy_" + platform.node())
│ │ +    logger.debug(f"Standardizing data chunk with shape {dat.shape} (prior polyremoval was {polyremoval}).")
│ │ +
│ │      # standardize
│ │      dat = (dat - np.mean(dat, axis=0)) / np.std(dat, axis=0)
│ │  
│ │      # renaming
│ │      standardized = dat
│ │      return standardized
│ │  
│ │ @@ -807,8 +824,36 @@
│ │      computeFunction = staticmethod(standardize_cF)
│ │  
│ │      # 1st argument,the data, gets omitted
│ │      valid_kws = list(signature(standardize_cF).parameters.keys())[1:]
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │ -        propagate_metadata(data, out)
│ │ +        propagate_properties(data, out)
│ │ +
│ │ +
│ │ +def _resampling_trl_definition(orig_trl, factor):
│ │ +
│ │ +    """
│ │ +    Construct new trialdefinition from original
│ │ +    trialdefinition and the resampling factor
│ │ +    """
│ │ +
│ │ +    # start from input trial lengths and scale
│ │ +    # and ceil them to arrive at new trial lengths
│ │ +    # important is 1st diff then ceil..
│ │ +    sinfo = orig_trl[:, :2]
│ │ +    trl_len = np.ceil(np.diff(sinfo * factor, axis=1)).squeeze()
│ │ +
│ │ +    # use ceil again to define new trial start
│ │ +    # and offset samples
│ │ +    trl_scaled = np.ceil(orig_trl * factor)
│ │ +    trl_starts = trl_scaled[:, 0]
│ │ +    offsets = trl_scaled[:, 2]
│ │ +
│ │ +    # now add new trl_len to get new trial ends
│ │ +    trl_ends = trl_starts + trl_len
│ │ +
│ │ +    # finally stack everything back together
│ │ +    trldef = np.column_stack([trl_starts, trl_ends, offsets])
│ │ +
│ │ +    return trldef
│ │   --- esi-syncopy-2022.8/syncopy/preproc/firws.py
│ ├── +++ esi_syncopy-2023.3/syncopy/preproc/firws.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/preproc/preprocessing.py
│ ├── +++ esi_syncopy-2023.3/syncopy/preproc/preprocessing.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/preproc/resampledata.py
│ ├── +++ esi_syncopy-2023.3/syncopy/preproc/resampledata.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/preproc/resampling.py
│ ├── +++ esi_syncopy-2023.3/syncopy/preproc/resampling.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/shared/__init__.py
│ ├── +++ esi_syncopy-2023.3/syncopy/shared/__init__.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/shared/computational_routine.py
│ ├── +++ esi_syncopy-2023.3/syncopy/shared/computational_routine.py
│ │┄ Files 10% similar despite different names
│ │ @@ -15,27 +15,31 @@
│ │  from tqdm.auto import tqdm
│ │  if sys.platform == "win32":
│ │      # tqdm breaks term colors on Windows - fix that (tqdm issue #446)
│ │      import colorama
│ │      colorama.deinit()
│ │      colorama.init(strip=False)
│ │  
│ │ +import dask.distributed as dd
│ │ +import dask_jobqueue as dj
│ │ +
│ │  # Local imports
│ │  import syncopy as spy
│ │  from .tools import get_defaults
│ │ +from .dask_helpers import check_slurm_available
│ │  from syncopy import __storage__, __acme__
│ │  from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYParallelError, SPYWarning
│ │  if __acme__:
│ │      from acme import ParallelMap
│ │ -    import dask.distributed as dd
│ │ -    import dask_jobqueue as dj
│ │      # # In case of problems w/worker-stealing, uncomment the following lines
│ │      # import dask
│ │      # dask.config.set(distributed__scheduler__work_stealing=False)
│ │  
│ │ +from syncopy.shared.metadata import parse_cF_returns, h5_add_metadata
│ │ +
│ │  __all__ = []
│ │  
│ │  
│ │  class ComputationalRoutine(ABC):
│ │      """Abstract class for encapsulating sequential/parallel algorithms
│ │  
│ │      A Syncopy compute class consists of a
│ │ @@ -135,15 +139,15 @@
│ │  
│ │          # full shape of final output dataset (all trials, all chunks, etc.)
│ │          self.outputShape = None
│ │  
│ │          # numerical type of output dataset
│ │          self.dtype = None
│ │  
│ │ -        # list of trial numbers to process (either `data.trials` or `data.selection.trials`)
│ │ +        # list of trial numbers to process (either list(range(len(`data.trials`))) or `data.selection.trial_ids`)
│ │          self.trialList = None
│ │  
│ │          # number of trials to process (shortcut for `len(self.trialList)`)
│ │          self.numTrials = None
│ │  
│ │          # number of channel blocks to process per Trial (1 by default, only > 1 if
│ │          # `chan_per_worker` is not `None`)
│ │ @@ -263,15 +267,15 @@
│ │  
│ │          # First store `keeptrial` keyword value (important for output shapes below)
│ │          self.keeptrials = keeptrials
│ │  
│ │          # Determine if data-selection was provided; if so, extract trials and check
│ │          # whether selection requires fancy array indexing
│ │          if data.selection is not None:
│ │ -            self.trialList = data.selection.trials
│ │ +            self.trialList = data.selection.trial_ids
│ │              self.useFancyIdx = data.selection._useFancy
│ │          else:
│ │              self.trialList = list(range(len(data.trials)))
│ │              self.useFancyIdx = False
│ │          self.numTrials = len(self.trialList)
│ │  
│ │          # Prepare dryrun arguments and determine geometry of trials in output
│ │ @@ -394,15 +398,15 @@
│ │          # Simple: consume all channels simultaneously, i.e., just take the entire trial
│ │          else:
│ │              targetLayout.append(tuple(lyt))
│ │              targetShapes.append(chunkShape0)
│ │              sourceLayout.append(trial.idx)
│ │              sourceShapes.append(trial.shape)
│ │  
│ │ -        # Construct dimensional layout of output
│ │ +        # Construct dimensional layout of output and append remaining trials to input layout
│ │          stacking = targetLayout[0][stackingDim].stop
│ │          for tk in range(1, self.numTrials):
│ │              trial = trials[tk]
│ │              trlArg = tuple(arg[tk] if isinstance(arg, (list, tuple, np.ndarray)) and
│ │                             len(arg) == self.numTrials
│ │                             else arg for arg in self.argv)
│ │              chkshp = chk_list[tk]
│ │ @@ -440,15 +444,15 @@
│ │          # If the determined source layout contains unordered lists and/or index
│ │          # repetitions, set `self.useFancyIdx` to `True` and prepare a separate
│ │          # `sourceSelectors` list that is used in addition to `sourceLayout` for
│ │          # data extraction.
│ │          # In this case `sourceLayout` uses ABSOLUTE indices (indices wrt to size
│ │          # of ENTIRE DATASET) that are SORTED W/O REPS to extract a NumPy array
│ │          # of appropriate size from HDF5.
│ │ -        # Then `sourceLayout` uses RELATIVE indices (indices wrt to size of CURRENT
│ │ +        # Then `sourceSelectors` uses RELATIVE indices (indices wrt to size of CURRENT
│ │          # TRIAL) that can be UNSORTED W/REPS to actually perform the requested
│ │          # selection on the NumPy array extracted w/`sourceLayout`.
│ │          for grd in sourceLayout:
│ │              if any([np.diff(sel).min() <= 0 if isinstance(sel, list)
│ │                      and len(sel) > 1 else False for sel in grd]):
│ │                  self.useFancyIdx = True
│ │                  break
│ │ @@ -592,39 +596,57 @@
│ │  
│ │          # By default, use VDS storage for parallel computing
│ │          if parallel_store is None:
│ │              parallel_store = parallel
│ │  
│ │          # Do not spill trials on disk if they're supposed to be removed anyway
│ │          if parallel_store and not self.keeptrials:
│ │ -            msg = "trial-averaging only supports sequential writing!"
│ │ +            msg = "trial-averaging only supports sequential storage, disabling parallel storage mode!"
│ │              SPYWarning(msg)
│ │              parallel_store = False
│ │  
│ │          # Create HDF5 dataset of appropriate dimension
│ │          self.preallocate_output(out, parallel_store=parallel_store)
│ │  
│ │ -        # Concurrent processing requires some additional prep-work...
│ │ +        log_dict = {} if log_dict is None else log_dict
│ │ +        log_dict["used_parallel"] = str(parallel)
│ │ +
│ │          if parallel:
│ │  
│ │              # Construct list of dicts that will be passed on to workers: in the
│ │              # parallel case, `trl_dat` is a dictionary!
│ │ +
│ │ +            # Use the trial IDs from the selection, so we have absolute trial indices.
│ │ +            trial_ids = data.selection.trial_ids if data.selection is not None else range(self.numTrials)
│ │ +
│ │ +            # Use trial_ids and chunk_ids to turn into a unique index.
│ │ +            if self.numBlocksPerTrial == 1:  # The simple case: 1 call per trial. We add a chunk id (the trailing `_0` to be consistent with
│ │ +                                             # the more complex case, but the chunk index is always `0`).
│ │ +                unique_key = ["__" + str(trial_id) + "_0" for trial_id in trial_ids]
│ │ +            else:  # The more complex case: channel parallelization is active, we need to add the chunk to the
│ │ +                   # trial ID for the key to be unique, as a trial will be split into several chunks.
│ │ +                trial_ids = np.repeat(trial_ids, self.numBlocksPerTrial)
│ │ +                chunk_ids = np.tile(np.arange(self.numBlocksPerTrial), self.numTrials)
│ │ +                unique_key = ["__" + str(trial_id) + "_" + str(chunk_id) for trial_id, chunk_id in zip(trial_ids, chunk_ids)]
│ │ +
│ │              workerDicts = [{"keeptrials": self.keeptrials,
│ │                              "infile": data.filename,
│ │                              "indset": data.data.name,
│ │                              "ingrid": self.sourceLayout[chk],
│ │                              "inshape": self.sourceShapes[chk],
│ │                              "sigrid": self.sourceSelectors[chk],
│ │                              "fancy": self.useFancyIdx,
│ │                              "vdsdir": self.virtualDatasetDir,
│ │                              "outfile": self.outFileName.format(chk),
│ │                              "outdset": self.tmpDsetName,
│ │                              "outgrid": self.targetLayout[chk],
│ │                              "outshape": self.targetShapes[chk],
│ │ -                            "dtype": self.dtype} for chk in range(self.numCalls)]
│ │ +                            "dtype": self.dtype,
│ │ +                            "call_id": unique_key[chk] } for chk in range(self.numCalls)]
│ │ +
│ │  
│ │              # If channel-block parallelization has been set up, positional args of
│ │              # `computeFunction` need to be massaged: any list whose elements represent
│ │              # trial-specific args, needs to be expanded (so that each channel-block
│ │              # per trial receives the correct number of pos. args)
│ │              ArgV = list(self.argv)
│ │              if self.numBlocksPerTrial > 1:
│ │ @@ -636,80 +658,38 @@
│ │                                  ArgV[ak] = list(unrolled)
│ │                              else:
│ │                                  ArgV[ak] = tuple(unrolled)
│ │                      elif isinstance(arg, np.ndarray):
│ │                          if len(arg.squeeze().shape) == 1 and arg.squeeze().size == self.numTrials:
│ │                              ArgV[ak] = np.array(chain.from_iterable([[ag] * self.numBlocksPerTrial for ag in arg]))
│ │  
│ │ -            # Positional args for computeFunctions consist of `trl_dat` + others
│ │ +            # Positional args for `process_io` wrapped computeFunctions consist of `trl_dat` + others
│ │              # (stored in `ArgV`). Account for this when seeting up `ParallelMap`
│ │              if len(ArgV) == 0:
│ │ -                inargs = (workerDicts, )
│ │ -            else:
│ │ -                inargs = (workerDicts, *ArgV)
│ │ -
│ │ -            # Let ACME take care of argument distribution and memory checks: note
│ │ -            # that `cfg` is trial-independent, i.e., we can simply throw it in here!
│ │ -            self.pmap = ParallelMap(self.computeFunction,
│ │ -                                    *inargs,
│ │ -                                    n_inputs=self.numCalls,
│ │ -                                    write_worker_results=False,
│ │ -                                    write_pickle=False,
│ │ -                                    partition="auto",
│ │ -                                    n_jobs="auto",
│ │ -                                    mem_per_job="auto",
│ │ -                                    setup_timeout=60,
│ │ -                                    setup_interactive=False,
│ │ -                                    stop_client="auto",
│ │ -                                    verbose=None,
│ │ -                                    logfile=None,
│ │ -                                    **self.cfg)
│ │ -
│ │ -            # Edge-case correction: if by chance, any array-like element `x` of `cfg`
│ │ -            # satisfies `len(x) = numCalls`, `ParallelMap` attempts to tear open `x` and
│ │ -            # distribute its elements across workers. Prevent this!
│ │ -            if self.numCalls > 1:
│ │ -                for key, value in self.pmap.kwargv.items():
│ │ -                    if isinstance(value, (list, tuple)):
│ │ -                        if len(value) == self.numCalls:
│ │ -                            self.pmap.kwargv[key] = [value]
│ │ -                    elif isinstance(value, np.ndarray):
│ │ -                        if len(value.squeeze().shape) == 1 and value.squeeze().size == self.numCalls:
│ │ -                            self.pmap.kwargv[key] = [value]
│ │ -
│ │ -            # Check if trials actually fit into memory before we start computation
│ │ -            client = self.pmap.daemon.client
│ │ -            if isinstance(client.cluster, (dd.LocalCluster, dj.SLURMCluster)):
│ │ -                workerMem = [w["memory_limit"] for w in client.cluster.scheduler_info["workers"].values()]
│ │ -                if len(workerMem) == 0:
│ │ -                    raise SPYParallelError("no online workers found", client=client)
│ │ -                workerMemMax = max(workerMem)
│ │ -                if self.chunkMem >= mem_thresh * workerMemMax:
│ │ -                    self.chunkMem /= 1024**3
│ │ -                    workerMemMax /= 1000**3
│ │ -                    msg = "Single-trial result sizes ({0:2.2f} GB) larger than available " +\
│ │ -                        "worker memory ({1:2.2f} GB) currently not supported"
│ │ -                    raise NotImplementedError(msg.format(self.chunkMem, workerMemMax))
│ │ +                self.inargs = (workerDicts, )
│ │              else:
│ │ -                msg = "`ComputationalRoutine` only supports `LocalCluster` and " +\
│ │ -                    "`SLURMCluster` dask cluster objects. Proceed with caution. "
│ │ -                SPYWarning(msg)
│ │ -
│ │ -            # Store provided debugging state
│ │ +                self.inargs = (workerDicts, *ArgV)
│ │ +            # Store provided debugging state for ACME
│ │              self.parallelDebug = parallel_debug
│ │  
│ │ -        # For sequential processing, just ensure enough memory is available
│ │ +            # store mem_thresh
│ │ +            self.mem_thresh = mem_thresh
│ │ +
│ │ +        # Now for the sequential processing case.
│ │          else:
│ │ +
│ │ +            # We only check memory
│ │              memSize = psutil.virtual_memory().available
│ │              if self.chunkMem >= mem_thresh * memSize:
│ │                  self.chunkMem /= 1024**3
│ │                  memSize /= 1024**3
│ │ -                msg = "Single-trial result sizes ({0:2.2f} GB) larger than available " +\
│ │ -                      "memory ({1:2.2f} GB) currently not supported"
│ │ -                raise NotImplementedError(msg.format(self.chunkMem, memSize))
│ │ +                msg = ("Single-trial processing requires {0:2.2f} GB of memory "
│ │ +                       "which is larger than the available "
│ │ +                       "memory ({1:2.2f} GB)")
│ │ +                raise SPYParallelError(msg.format(2 * self.chunkMem, memSize))
│ │  
│ │          # The `method` keyword can be used to override the `parallel` flag
│ │          if method is None:
│ │              if parallel:
│ │                  computeMethod = self.compute_parallel
│ │              else:
│ │                  computeMethod = self.compute_sequential
│ │ @@ -791,15 +771,18 @@
│ │      def compute_parallel(self, data, out):
│ │          """
│ │          Concurrent computing kernel
│ │  
│ │          Parameters
│ │          ----------
│ │          data : syncopy data object
│ │ -           Syncopy data object to be processed
│ │ +           Syncopy data object, ignored for parallel case. The input data information
│ │ +           is already contained in the workerdicts, which are stored in `self.pmap` (expanded
│ │ +           from the `inargs` in `compute()`) and can be accessed from it.
│ │ +
│ │          out : syncopy data object
│ │             Empty object for holding results
│ │  
│ │          Returns
│ │          -------
│ │          Nothing : None
│ │  
│ │ @@ -810,31 +793,115 @@
│ │  
│ │          See also
│ │          --------
│ │          compute : management routine invoking parallel/sequential compute kernels
│ │          compute_sequential : serial processing counterpart of this method
│ │          """
│ │  
│ │ -        # Let ACME do the heavy lifting
│ │ -        with self.pmap as pm:
│ │ -            pm.compute(debug=self.parallelDebug)
│ │ +        # Let ACME take care of argument distribution and memory checks: note
│ │ +        # that `cfg` is trial-independent, i.e., we can simply throw it in here!
│ │ +        if __acme__ and check_slurm_available():
│ │ +
│ │ +            self.pmap = ParallelMap(self.computeFunction,
│ │ +                                    *self.inargs,
│ │ +                                    n_inputs=self.numCalls,
│ │ +                                    write_worker_results=False,
│ │ +                                    write_pickle=False,
│ │ +                                    partition="auto",
│ │ +                                    n_workers="auto",
│ │ +                                    mem_per_worker="auto",
│ │ +                                    setup_timeout=60,
│ │ +                                    setup_interactive=False,
│ │ +                                    stop_client="auto",
│ │ +                                    verbose=None,
│ │ +                                    logfile=None,
│ │ +                                    **self.cfg)
│ │ +
│ │ +            # Edge-case correction: if by chance, any array-like element `x` of `cfg`
│ │ +            # satisfies `len(x) = numCalls`, `ParallelMap` attempts to tear open `x` and
│ │ +            # distribute its elements across workers. Prevent this!
│ │ +            if self.numCalls > 1:
│ │ +                for key, value in self.pmap.kwargv.items():
│ │ +                    if isinstance(value, (list, tuple)):
│ │ +                        if len(value) == self.numCalls:
│ │ +                            self.pmap.kwargv[key] = [value]
│ │ +                    elif isinstance(value, np.ndarray):
│ │ +                        if len(value.squeeze().shape) == 1 and value.squeeze().size == self.numCalls:
│ │ +                            self.pmap.kwargv[key] = [value]
│ │ +
│ │ +            # Check if trials actually fit into memory before we start computation
│ │ +            client = self.pmap.daemon.client
│ │ +
│ │ +        # fallback to any client we can connect to
│ │ +        else:
│ │ +            self.pmap = None
│ │ +            try:
│ │ +                client = dd.get_client()
│ │ +            except ValueError:
│ │ +                raise SPYParallelError("Could not connect to a running Dask client")
│ │ +
│ │ +        # --- some sanity checks before computations ---
│ │ +
│ │ +        if isinstance(client.cluster, (dd.LocalCluster, dj.SLURMCluster)):
│ │ +            workerMem = [w["memory_limit"] for w in client.cluster.scheduler_info["workers"].values()]
│ │ +            if len(workerMem) == 0:
│ │ +                raise SPYParallelError("no online workers found", client=client)
│ │ +            workerMemMax = max(workerMem)
│ │ +            if self.chunkMem >= self.mem_thresh * workerMemMax:
│ │ +                self.chunkMem /= 1024**3
│ │ +                workerMemMax /= 1000**3
│ │ +                msg = ("Single-trial processing requires {0:2.2f} GB of memory "
│ │ +                       "which is larger than the available "
│ │ +                       "worker memory ({1:2.2f} GB)")
│ │ +                raise SPYParallelError(msg.format(2 * self.chunkMem, workerMemMax))
│ │ +
│ │ +        # --- trigger actual computation ---
│ │ +
│ │ +        if self.pmap is not None:
│ │ +            # Let ACME do the heavy lifting
│ │ +            with self.pmap as pm:
│ │ +                pm.compute(debug=self.parallelDebug)
│ │ +
│ │ +        # use our own client and map over workerdicts + cfg
│ │ +        else:
│ │ +            # we have to prepare a well behaved iterable where for each call n
│ │ +            # we have a tuple like (wdict[n], argv1_seq[n], argv2) passed to the cF
│ │ +            # by the Dask client mapping
│ │ +            workerDicts = self.inargs[0]
│ │ +            if len(self.inargs) > 1:
│ │ +                iterables = []
│ │ +                ArgV = self.inargs[1:]
│ │ +                for nblock, wdict in enumerate(workerDicts):
│ │ +                    argv = tuple(arg[nblock]
│ │ +                                 if isinstance(arg, (list, tuple, np.ndarray)) and  # these are the argv_seq
│ │ +                                 len(arg) == len(workerDicts)  # each call gets one element of a sequence type argv
│ │ +                                 else arg for arg in ArgV)
│ │ +                    iterables.append((wdict, *argv))
│ │ +            # no *args for the cF
│ │ +            else:
│ │ +                iterables = workerDicts
│ │ +
│ │ +            futures = client.map(self.computeFunction, iterables, **self.cfg)
│ │ +            # similar to tqdm progress bar
│ │ +            dd.progress(futures)
│ │ +            # actual results get handled by hdf5 operations inside `process_io`
│ │ +            client.gather(futures)
│ │  
│ │          # When writing concurrently, now's the time to finally create the virtual dataset
│ │ +        # by referencing the individual hdf5 datasets created by the IO decorator
│ │          if self.virtualDatasetDir is not None:
│ │              with h5py.File(out.filename, mode="w") as h5f:
│ │                  h5f.create_virtual_dataset(self.outDatasetName, self.VirtualDatasetLayout)
│ │  
│ │          # If trial-averaging was requested, normalize computed sum to get mean
│ │          if not self.keeptrials:
│ │              with h5py.File(out.filename, mode="r+") as h5f:
│ │                  h5f[self.outDatasetName][()] /= self.numTrials
│ │                  h5f.flush()
│ │  
│ │ -        return
│ │ -
│ │      def compute_sequential(self, data, out):
│ │          """
│ │          Sequential computing kernel
│ │  
│ │          Parameters
│ │          ----------
│ │          data : syncopy data object
│ │ @@ -856,22 +923,21 @@
│ │          is avoided and memory usage is kept to a minimum.
│ │  
│ │          See also
│ │          --------
│ │          compute : management routine invoking parallel/sequential compute kernels
│ │          compute_parallel : concurrent processing counterpart of this method
│ │          """
│ │ -
│ │          sourceObj = h5py.File(data.filename, mode="r")[data.data.name]
│ │  
│ │          # Iterate over (selected) trials and write directly to target HDF5 dataset
│ │          with h5py.File(out.filename, "r+") as h5fout:
│ │              target = h5fout[self.outDatasetName]
│ │  
│ │ -            for nblock in tqdm(range(self.numTrials), bar_format=self.tqdmFormat):
│ │ +            for nblock in tqdm(range(self.numTrials), bar_format=self.tqdmFormat, disable=None):
│ │  
│ │                  # Extract respective indexing tuples from constructed lists
│ │                  ingrid = self.sourceLayout[nblock]
│ │                  sigrid = self.sourceSelectors[nblock]
│ │                  outgrid = self.targetLayout[nblock]
│ │                  argv = tuple(arg[nblock]
│ │                               if isinstance(arg, (list, tuple, np.ndarray)) and
│ │ @@ -892,39 +958,41 @@
│ │  
│ │                      # Ensure input array shape was not inflated by scalar selection
│ │                      # tuple, e.g., ``e=np.ones((2,2)); e[0,:].shape = (2,)`` not ``(1,2)``
│ │                      # (use an explicit `shape` assignment here to avoid copies)
│ │                      arr.shape = self.sourceShapes[nblock]
│ │  
│ │                      # Perform computation
│ │ -                    res = self.computeFunction(arr, *argv, **self.cfg)
│ │ +                    res, details = parse_cF_returns(self.computeFunction(arr, *argv, **self.cfg))
│ │  
│ │                      # In case scalar selections have been performed, explicitly assign
│ │                      # desired output shape to re-create "lost" singleton dimensions
│ │                      # (use an explicit `shape` assignment here to avoid copies)
│ │                      res.shape = self.targetShapes[nblock]
│ │  
│ │ +                    trial_idx = data.selection.trial_ids[nblock] if data.selection is not None else nblock
│ │ +
│ │ +                    h5_add_metadata(h5fout, details, unique_key_suffix=trial_idx)
│ │ +
│ │                  # Either write result to `outgrid` location in `target` or add it up
│ │                  if self.keeptrials:
│ │                      target[outgrid] = res
│ │                  else:
│ │ -                    target[()] = np.nansum([target, res], axis=0)
│ │ +                    target[()] += res
│ │  
│ │                  # Flush every iteration to avoid memory leakage
│ │                  h5fout.flush()
│ │  
│ │              # If trial-averaging was requested, normalize computed sum to get mean
│ │              if not self.keeptrials:
│ │                  target[()] /= self.numTrials
│ │  
│ │          # If source was HDF5 file, close it to prevent access errors
│ │          sourceObj.file.close()
│ │  
│ │ -        return
│ │ -
│ │      def write_log(self, data, out, log_dict=None):
│ │          """
│ │          Processing of output log
│ │  
│ │          Parameters
│ │          ----------
│ │          data : syncopy data object
│ │ @@ -992,63 +1060,120 @@
│ │          write_log : Logging of calculation parameters
│ │          """
│ │          pass
│ │  
│ │  
│ │  # --- metadata helper functions ---
│ │  
│ │ -def propagate_metadata(in_data, out_data):
│ │ +def propagate_properties(in_data, out_data, keeptrials=True, time_axis=False):
│ │  
│ │      """
│ │ -    Propagating metadata/selections (channels, trials, time, ...)
│ │ +    Propagating data class properties (channels, trials, time, ...)
│ │      from the input object `in_data` to the output
│ │ -    object `out_data` is the task of
│ │ -    concrete (overloaded) `process_metadata` implementations.
│ │ +    object `out_data` and respecting selections.
│ │  
│ │ -    Depending on the concrete CR, different propagations are needed.
│ │ +    Depending on the CR in question, different propagations are needed.
│ │      For example
│ │  
│ │          AnalogData -> CrossSpectralData (connectivityanalysis)
│ │  
│ │      needs a mapping .channel -> .channel_i, .channel_j
│ │  
│ │      Whereas
│ │  
│ │          AnalogData -> AnalogData (preprocessing)
│ │  
│ │ -    directly propagates all (including selections)
│ │ -    attributes from the input to the output
│ │ -    (same channels, time and so on)
│ │ +    directly propagates the properties from the input to the output
│ │ +    (same channels, trialdefinition/time and so on)
│ │  
│ │      This function is a general approach to unify the
│ │ -    propagation / adaptation of all needed attributes.
│ │ +    propagation / manipulation of all needed properties
│ │ +    done in the `process_metadata` implementations of the
│ │ +    respective ComputationalRoutines.
│ │  
│ │      Parameters
│ │      ----------
│ │      in_data : Syncopy data object
│ │          Input object to get attributes from
│ │      out_data : Syncopy data object
│ │          Output object to set attributes
│ │ +    keeptrials : bool
│ │ +    time_axis : bool
│ │ +        If `True` `out_data` has a time axis (not just trial stacking)
│ │      """
│ │  
│ │      # instance checkers
│ │ -    is_AD = lambda data: isinstance(data, spy.AnalogData)
│ │ -
│ │ -    # simplest case, direct propagation between two AnalogData objects
│ │ -    if is_AD(in_data) and is_AD(out_data):
│ │ +    is_Analog = lambda data: isinstance(data, spy.AnalogData)
│ │ +    is_Spectral = lambda data: isinstance(data, spy.SpectralData)
│ │ +    is_CrossSpectral = lambda data: isinstance(data, spy.CrossSpectralData)
│ │ +
│ │ +    # attach a dummy selection for easier propagation
│ │ +    selection_cleanup = False
│ │ +    if in_data.selection is None:
│ │ +        in_data.selectdata(inplace=True)
│ │ +        selection_cleanup = True
│ │ +
│ │ +    # if preserving the data type, propagation is straightforward
│ │ +    if in_data.__class__ == out_data.__class__:
│ │ +
│ │ +        # Get/set dimensional attributes changed by selection
│ │ +        for prop in in_data.selection._dimProps:
│ │ +            selection = getattr(in_data.selection, prop)
│ │ +            if selection is not None:
│ │ +                if np.issubdtype(type(selection), np.number):
│ │ +                    selection = [selection]
│ │ +                setattr(out_data, prop, getattr(in_data, prop)[selection])
│ │  
│ │ -        # get channels and trial selections
│ │ -        if in_data.selection is not None:
│ │ -            chanSec = in_data.selection.channel
│ │ -            # captures toi selections
│ │ -            trl = in_data.selection.trialdefinition
│ │ +        if keeptrials:
│ │ +            out_data.trialdefinition = in_data.selection.trialdefinition
│ │          else:
│ │ -            chanSec = slice(None)
│ │ -            trl = in_data.trialdefinition
│ │ +            # trial average requires equal length trials, so just copy the 1st
│ │ +            out_data.trialdefinition = in_data.trialdefinition[0, :][None, :]
│ │  
│ │ -        out_data.trialdefinition = trl
│ │ +        out_data.samplerate = in_data.samplerate
│ │ +        if selection_cleanup:
│ │ +            in_data.selection = None
│ │ +        return
│ │ +
│ │ +    # --- propagate only channels and deal with the rest below---
│ │ +
│ │ +    elif is_Spectral(out_data):
│ │ +        chanSec = in_data.selection.channel
│ │          out_data.channel = np.array(in_data.channel[chanSec])
│ │ +
│ │ +    # from one channel to cross-channel data
│ │ +    elif (is_Analog(in_data) or is_Spectral(in_data)) and is_CrossSpectral(out_data):
│ │ +        chanSec = in_data.selection.channel
│ │ +        out_data.channel_i = np.array(in_data.channel[chanSec])
│ │ +        out_data.channel_j = np.array(in_data.channel[chanSec])
│ │ +
│ │ +    # --- time and trialdefinition ---
│ │ +
│ │ +    if not time_axis:
│ │ +        # Note that here the `time` axis is only(!) used as stacking dimension
│ │ +        if keeptrials:
│ │ +            trldef = in_data.selection.trialdefinition
│ │ +
│ │ +            for row in range(trldef.shape[0]):
│ │ +                trldef[row, :2] = [row, row + 1]
│ │ +            out_data.trialdefinition = trldef
│ │ +
│ │ +        else:
│ │ +            # only single trial on the time stacking dim.
│ │ +            out_data.trialdefinition = np.array([[0, 1, 0]])
│ │ +
│ │          out_data.samplerate = in_data.samplerate
│ │  
│ │ -    # nothing else supported atm
│ │ +    # this captures active in-place selections on the time axis
│ │      else:
│ │ -        raise NotImplementedError
│ │ +        if keeptrials:
│ │ +            out_data.trialdefinition = in_data.selection.trialdefinition
│ │ +        # just copy 1st trial, have to be all equal anyways
│ │ +        else:
│ │ +            out_data.trialdefinition = in_data.selection.trialdefinition[0, :][None, :]
│ │ +
│ │ +        # this might bite us
│ │ +        out_data.samplerate = in_data.samplerate
│ │ +
│ │ +    if selection_cleanup:
│ │ +        in_data.selection = None
│ │ +        in_data.cfg.pop('selectdata')
│ │   --- esi-syncopy-2022.8/syncopy/shared/errors.py
│ ├── +++ esi_syncopy-2023.3/syncopy/shared/errors.py
│ │┄ Files 20% similar despite different names
│ │ @@ -2,18 +2,21 @@
│ │  #
│ │  # Collection of utility classes/functions for Syncopy
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import sys
│ │  import traceback
│ │ +import logging
│ │  from collections import OrderedDict
│ │  
│ │  # Local imports
│ │  from syncopy import __tbcount__
│ │ +from syncopy.shared.log import get_logger, get_parallel_logger, loglevels
│ │ +import syncopy
│ │  
│ │  # Custom definition of bold ANSI for formatting errors/warnings in iPython/Jupyter
│ │  ansiBold = "\033[1m"
│ │  
│ │  __all__ = []
│ │  
│ │  
│ │ @@ -125,15 +128,17 @@
│ │              preamble = ""
│ │          err = "{preamble:s}{msg:s}"
│ │          return err.format(preamble=preamble, msg=self.msg)
│ │  
│ │  
│ │  def SPYExceptionHandler(*excargs, **exckwargs):
│ │      """
│ │ -    Docstring coming soon(ish)...
│ │ +    Syncopy custom ExceptionHandler.
│ │ +
│ │ +    Prints formatted and colored messages and stack traces, and starts debugging if `%pdb` is enabled in Jupyter/iPython.
│ │      """
│ │  
│ │      # Depending on the number of input arguments, we're either in Jupyter/iPython
│ │      # or "regular" Python - this matters for coloring error messages
│ │      if len(excargs) == 3:
│ │          isipy = False
│ │          etype, evalue, etb = excargs
│ │ @@ -185,15 +190,16 @@
│ │                                                             cols.Normal if isipy else "")
│ │              else:
│ │                  emsg += "{}{}{}".format(cols.line if isipy else "",
│ │                                          eline,
│ │                                          cols.Normal if isipy else "")
│ │  
│ │          # Show generated message and leave (or kick-off debugging in Jupyer/iPython if %pdb is on)
│ │ -        print(emsg)
│ │ +        logger = get_parallel_logger()
│ │ +        logger.critical(emsg)
│ │          if isipy:
│ │              if ipy.call_pdb:
│ │                  ipy.InteractiveTB.debugger()
│ │          return
│ │  
│ │      # Build an ordered(!) dictionary that encodes separators for traceback components
│ │      sep = OrderedDict({"filename": ", line ",
│ │ @@ -265,25 +271,29 @@
│ │                                  exc_name,
│ │                                  cols.bold if isipy else "",
│ │                                  exc_msg,
│ │                                  cols.Normal if isipy else "",)
│ │  
│ │  
│ │      # Show generated message and get outta here
│ │ -    print(emsg)
│ │ +    logger = get_parallel_logger()
│ │ +    logger.critical(emsg)
│ │  
│ │      # Kick-start debugging in case %pdb is enabled in Jupyter/iPython
│ │      if isipy:
│ │          if ipy.call_pdb:
│ │              ipy.InteractiveTB.debugger()
│ │  
│ │  
│ │  def SPYWarning(msg, caller=None):
│ │      """
│ │ -    Standardized Syncopy warning message
│ │ +    Log a standardized Syncopy warning message.
│ │ +
│ │ +    .. note::
│ │ +        Depending on the currently active log level, this may or may not produce any output.
│ │  
│ │      Parameters
│ │      ----------
│ │      msg : str
│ │          Warning message to be printed
│ │      caller : None or str
│ │          Issuer of warning message. If `None`, name of calling method is
│ │ @@ -307,24 +317,93 @@
│ │          normCol = ""
│ │          boldEm = ""
│ │  
│ │      # Plug together message string and print it
│ │      if caller is None:
│ │          caller = sys._getframe().f_back.f_code.co_name
│ │      PrintMsg = "{coloron:s}{bold:s}Syncopy{caller:s} WARNING: {msg:s}{coloroff:s}"
│ │ -    print(PrintMsg.format(coloron=warnCol,
│ │ +    logger = get_logger()
│ │ +    logger.warning(PrintMsg.format(coloron=warnCol,
│ │                            bold=boldEm,
│ │                            caller=" <" + caller + ">" if len(caller) else caller,
│ │                            msg=msg,
│ │                            coloroff=normCol))
│ │  
│ │  
│ │ -def SPYInfo(msg, caller=None):
│ │ +def SPYParallelLog(msg, loglevel="INFO", caller=None):
│ │ +    """Log a message in parallel code run via slurm.
│ │ +
│ │ +    This uses the parallel logger and one file per machine.
│ │ +
│ │ +    Returns
│ │ +    -------
│ │ +    Nothing : None
│ │ +    """
│ │ +    numeric_level = getattr(logging, loglevel.upper(), None)
│ │ +    if not isinstance(numeric_level, int):  # Invalid string was set.
│ │ +        raise SPYValueError(legal=f"one of: {loglevels}", varname="loglevel", actual=loglevel)
│ │ +    if caller is None:
│ │ +        caller = sys._getframe().f_back.f_code.co_name
│ │ +    PrintMsg = "{caller:s} {msg:s}"
│ │ +    logger = get_parallel_logger()
│ │ +    logfunc = getattr(logger, loglevel.lower())
│ │ +    logfunc(PrintMsg.format(caller=" <" + caller + ">" if len(caller) else caller,
│ │ +                          msg=msg))
│ │ +
│ │ +def SPYLog(msg, loglevel="INFO", caller=None):
│ │ +    """Log a message in sequential code.
│ │ +
│ │ +    This uses the standard logger that logs to console and a local log file by default.
│ │ +
│ │ +    Returns
│ │ +    -------
│ │ +    Nothing : None
│ │ +    """
│ │ +    numeric_level = getattr(logging, loglevel.upper(), None)
│ │ +    if not isinstance(numeric_level, int):  # Invalid string was set.
│ │ +        raise SPYValueError(legal=f"one of: {loglevels}", varname="loglevel", actual=loglevel)
│ │ +    if caller is None:
│ │ +        caller = sys._getframe().f_back.f_code.co_name
│ │ +    PrintMsg = "{caller:s} {msg:s}"
│ │ +    logger = get_logger()
│ │ +    logfunc = getattr(logger, loglevel.lower())
│ │ +    logfunc(PrintMsg.format(caller=" <" + caller + ">" if len(caller) else caller,
│ │ +                          msg=msg))
│ │ +
│ │ +def log(msg, level="IMPORTANT", par=False, caller=None):
│ │      """
│ │ -    Standardized Syncopy info message
│ │ +    Log a message using the Syncopy logging setup.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    msg    : str
│ │ +        The message to be logged.
│ │ +    level  : str
│ │ +        One of 'DEBUG', 'IMPORTANT', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'.
│ │ +    caller : None or str
│ │ +        Issuer of warning message. If `None`, name of calling method is
│ │ +        automatically fetched and pre-pended to `msg`.
│ │ +    par    : bool, whether to use the parallel logger.
│ │ +
│ │ +    Returns
│ │ +    -------
│ │ +    Nothing : None
│ │ +    """
│ │ +    if par:
│ │ +        SPYParallelLog(msg, loglevel=level, caller=caller)
│ │ +    else:
│ │ +        SPYLog(msg, loglevel=level, caller=caller)
│ │ +
│ │ +
│ │ +def SPYInfo(msg, caller=None, tag="INFO"):
│ │ +    """
│ │ +    Log a standardized Syncopy info message.
│ │ +
│ │ +    .. note::
│ │ +        Depending on the currently active log level, this may or may not produce any output.
│ │  
│ │      Parameters
│ │      ----------
│ │      msg : str
│ │          Info message to be printed
│ │      caller : None or str
│ │          Issuer of info message. If `None`, name of calling method is
│ │ @@ -347,14 +426,38 @@
│ │          infoCol = ""
│ │          normCol = ""
│ │          boldEm = ""
│ │  
│ │      # Plug together message string and print it
│ │      if caller is None:
│ │          caller = sys._getframe().f_back.f_code.co_name
│ │ -    PrintMsg = "{coloron:s}{bold:s}Syncopy{caller:s} INFO: {msg:s}{coloroff:s}"
│ │ -    print(PrintMsg.format(coloron=infoCol,
│ │ +    PrintMsg = "{coloron:s}{bold:s}Syncopy{caller:s} {tag}: {msg:s}{coloroff:s}"
│ │ +    logger = get_logger()
│ │ +    logger.info(PrintMsg.format(coloron=infoCol,
│ │                            bold=boldEm,
│ │                            caller=" <" + caller + ">" if len(caller) else caller,
│ │ +                          tag=tag,
│ │                            msg=msg,
│ │                            coloroff=normCol))
│ │  
│ │ +def SPYDebug(msg, caller=None):
│ │ +    """
│ │ +    Log a standardized Syncopy debug message.
│ │ +
│ │ +    .. note::
│ │ +        Depending on the currently active log level, this may or may not produce any output.
│ │ +
│ │ +    Parameters
│ │ +    ----------
│ │ +    msg : str
│ │ +        Debug message to be printed
│ │ +    caller : None or str
│ │ +        Issuer of debug message. If `None`, name of calling method is
│ │ +        automatically fetched and pre-pended to `msg`.
│ │ +
│ │ +    Returns
│ │ +    -------
│ │ +    Nothing : None
│ │ +    """
│ │ +    if caller is None:
│ │ +        caller = sys._getframe().f_back.f_code.co_name
│ │ +    SPYInfo(msg, caller=caller, tag="DEBUG")
│ │   --- esi-syncopy-2022.8/syncopy/shared/filetypes.py
│ ├── +++ esi_syncopy-2023.3/syncopy/shared/filetypes.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/shared/kwarg_decorators.py
│ ├── +++ esi_syncopy-2023.3/syncopy/shared/kwarg_decorators.py
│ │┄ Files 5% similar despite different names
│ │ @@ -4,23 +4,26 @@
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import functools
│ │  import h5py
│ │  import inspect
│ │  import numpy as np
│ │ +import dask.distributed as dd
│ │  
│ │ -# Local imports
│ │ +
│ │ +import syncopy as spy
│ │  from syncopy.shared.errors import (SPYTypeError, SPYValueError,
│ │ -                                   SPYError, SPYWarning)
│ │ +                                   SPYError, SPYWarning, SPYInfo)
│ │  from syncopy.shared.tools import StructDict
│ │ -import syncopy as spy
│ │ -if spy.__acme__:
│ │ -    import dask.distributed as dd
│ │ -    from acme.dask_helpers import esi_cluster_setup, cluster_cleanup
│ │ +from syncopy.shared.metadata import h5_add_metadata, parse_cF_returns
│ │ +
│ │ +# Local imports
│ │ +from .dask_helpers import check_slurm_available, check_workers_available
│ │ +from .log import get_logger
│ │  
│ │  __all__ = []
│ │  
│ │  
│ │  def unwrap_cfg(func):
│ │      """
│ │      Decorator that unwraps `cfg` "structure" in metafunction call
│ │ @@ -53,17 +56,17 @@
│ │             (a) process its "linguistic" boolean keys (convert any "yes"/"no" entries
│ │             to `True` /`False`) and then (b) extract any existing "data"/"dataset"
│ │             entry/entries. Raises a :class:`~syncopy.shared.errors.SPYValueError`
│ │             if `cfg` contains both a "data" and "dataset" entry.
│ │          4. Perform the actual unwrapping: at this point, a provided `cfg` only
│ │             contains keyword arguments of `func`. If the (first) input object `data`
│ │             was provided as `cfg` entry, it already exists in the local namespace.
│ │ -           If not, then by convention, `data` makes up the first elements of the
│ │ +           If not, then by convention, `data` is the first element of the
│ │             (remaining) positional argument list. Thus, the metafunction can now
│ │ -           be called via ``func(*data, *args, **kwargs)``.
│ │ +           be called via ``func(data, *args, **kwargs)``.
│ │          5. Amend the docstring of `func`: add a one-liner mentioning the possibility
│ │             of using `cfg` when calling `func` to the header of its docstring.
│ │             Append a paragraph to the docstrings' "Notes" section illustrating
│ │             how to call `func` with a `cfg` option "structure" that specifically
│ │             uses `func` and its input parameters. Note: both amendments are only
│ │             inserted in `func`'s docstring if the respective sections already exist.
│ │  
│ │ @@ -76,37 +79,28 @@
│ │      to safely use standard Python ``*args`` and ``**kwargs`` input arguments.
│ │  
│ │      Supported call signatures:
│ │  
│ │      * ``func(cfg, data)``: `cfg` exclusively contains keyword arguments of `func`,
│ │        `data` is a Syncopy data object.
│ │      * ``func(data, cfg)``: same as above
│ │ -    * ``func(data1, data2, data3, cfg)``: same as above with multiple Syncopy
│ │ -      data objects. **Note**: the `cfg` object has to be valid for *all* provided
│ │ -      input objects!
│ │      * ``func(data, cfg=cfg)``: same as above, but `cfg` itself is provided as
│ │        keyword argument
│ │ -    * ``func(data1, data2, cfg=cfg)``: same as above, but `cfg` itself is provided as
│ │ -      keyword argument for multiple input objects.
│ │      * ``func(cfg)``: `cfg` contains a field `data` or `dataset` (not both!)
│ │        holding one or more Syncopy data objects used as input of `func`
│ │      * ``func(cfg=cfg)``: same as above with `cfg` being provided as keyword
│ │      * ``func(data, kw1=val1, kw2=val2)``: standard Python call style with keywords
│ │        being provided explicitly
│ │ -    * ``func(data1, data2, kw1=val1, kw2=val2)``: same as above with multiple input
│ │ -      objects
│ │      * ``func(data, cfg, kw2=val2)``: valid if `cfg` does NOT contain `'kw2'`
│ │  
│ │  
│ │      Invalid call signatures:
│ │  
│ │      * ``func(data, cfg, cfg=cfg)``: `cfg` must not be provided as positional and
│ │        keyword argument
│ │ -    * ``func(data, cfg, kw1=val1)``: if `cfg` is provided, any non-default
│ │ -      keyword-values must be provided as `cfg` entries
│ │      * ``func(cfg, {})``: every dict in `func`'s positional argument list is interpreted
│ │        as `cfg` "structure"
│ │      * ``func(data, cfg=value)``: `cfg` must be a Python dict or :class:`~syncopy.StructDict`
│ │      * ``func(data, cfg, kw1=val1)``: invalid if keyword `'kw1'` also appears in `cfg`
│ │  
│ │      See also
│ │      --------
│ │ @@ -228,43 +222,47 @@
│ │                  data = kwargs.pop("dataset")
│ │  
│ │          # If Syncopy data object(s) were provided convert single objects to one-element
│ │          # lists, ensure positional args do *not* contain add'l objects; ensure keyword
│ │          # args (besides `cfg`) do *not* contain add'l objects; ensure `data` exclusively
│ │          # contains Syncopy data objects. Finally, rename remaining positional arguments
│ │          if data:
│ │ -            if not isinstance(data, (tuple, list)):
│ │ -                data = [data]
│ │              if any([isinstance(arg, spy.datatype.base_data.BaseData) for arg in args]):
│ │ -                lgl = "Syncopy data object(s) provided either via `cfg`/keyword or " +\
│ │ +                lgl = "Syncopy data object provided either via `cfg`/keyword or " +\
│ │                      "positional arguments, not both"
│ │                  raise SPYValueError(legal=lgl, varname="cfg/data")
│ │              if kwargs.get("data") or kwargs.get("dataset"):
│ │ -                lgl = "Syncopy data object(s) provided either via `cfg` or as " +\
│ │ +                lgl = "Syncopy data object provided either via `cfg` or as " +\
│ │                      "keyword argument, not both"
│ │                  raise SPYValueError(legal=lgl, varname="cfg.data")
│ │ -            if any([not isinstance(obj, spy.datatype.base_data.BaseData) for obj in data]):
│ │ -                raise SPYError("`data` must be Syncopy data object(s)!")
│ │ +            if not isinstance(data, spy.datatype.base_data.BaseData):
│ │ +                raise SPYError("`data` must be Syncopy data object!")
│ │              posargs = args
│ │  
│ │          # If `data` was not provided via `cfg` or as kw-arg, parse positional arguments
│ │          if data is None:
│ │ -            data = []
│ │              posargs = []
│ │              while args:
│ │                  arg = args.pop(0)
│ │ +                if data is not None and isinstance(arg, spy.datatype.base_data.BaseData):
│ │ +                    lgl = "only one Syncopy data object"
│ │ +                    raise SPYValueError(lgl, varname='data')
│ │                  if isinstance(arg, spy.datatype.base_data.BaseData):
│ │ -                    data.append(arg)
│ │ +                    data = arg
│ │                  else:
│ │                      posargs.append(arg)
│ │  
│ │ +        # if there was no Syncopy data found at this point, we have to give up
│ │ +        if data is None:
│ │ +            raise SPYError("Found no Syncopy data object as input")
│ │ +
│ │          # Call function with unfolded `data` + modified positional/keyword args
│ │ -        return func(*data, *posargs, **cfg)
│ │ +        return func(data, *posargs, **cfg)
│ │  
│ │ -    # Append one-liner to docstring header mentioning the use of `cfg`
│ │ +    # Append two-liner to docstring header mentioning the use of `cfg`
│ │      introEntry = \
│ │      "    \n" +\
│ │      "    The parameters listed below can be provided as is or a via a `cfg`\n" +\
│ │      "    configuration 'structure', see Notes for details. \n"
│ │      wrapper_cfg.__doc__ = _append_docstring(wrapper_cfg,
│ │                                              introEntry,
│ │                                              insert_in="Header",
│ │ @@ -363,62 +361,87 @@
│ │      detect_parallel_client : controls parallel processing engine via `parallel` keyword
│ │      """
│ │  
│ │      @functools.wraps(func)
│ │      def wrapper_select(*args, **kwargs):
│ │  
│ │          # Either extract `select` from input kws and cycle through positional
│ │ -        # argument to apply in-place selection to all Syncopy objects, or clean
│ │ -        # any unintended leftovers in `selection` if no `select` keyword was provided
│ │ +        # argument to apply in-place selection to the Syncopy object, or raise
│ │ +        # an error if a selection is already present and `select` is not None
│ │          select = kwargs.get("select", None)
│ │ +        attached_selection = False
│ │          for obj in args:
│ │ +            # this hits all Syncopy data objects
│ │              if hasattr(obj, "selection"):
│ │ -                obj.selection = select
│ │ +                if obj.selection is None and select is not None:
│ │ +                    obj.selection = select
│ │ +                    attached_selection = True
│ │ +                    # we have one and only one input data object
│ │ +                    break
│ │ +                else:
│ │ +                    if select is not None:
│ │ +                        raise SPYError(f"Selection found both in kwarg 'selection' ({select}) and in \npassed Syncopy Data object of type '{type(obj)}' ({obj.selection})")
│ │ +
│ │  
│ │          # Call function with modified data object(s)
│ │          res = func(*args, **kwargs)
│ │  
│ │          # Wipe data-selection slot to not alter user objects
│ │ +        # if the selection got attached by this wrapper here
│ │          for obj in args:
│ │ -            if hasattr(obj, "selection"):
│ │ +            if hasattr(obj, "selection") and attached_selection:
│ │                  obj.selection = None
│ │  
│ │          return res
│ │  
│ │      # Append `select` keyword entry to wrapped function's docstring and signature
│ │      selectDocEntry = \
│ │      "    select : dict or :class:`~syncopy.shared.tools.StructDict` or str\n" +\
│ │      "        In-place selection of subset of input data for processing. Please refer\n" +\
│ │ -    "        to :func:`syncopy.selectdata` for further usage details. \n"
│ │ +    "        to :func:`syncopy.selectdata` for further usage details."
│ │      wrapper_select.__doc__ = _append_docstring(func, selectDocEntry)
│ │      wrapper_select.__signature__ = _append_signature(func, "select")
│ │  
│ │      return wrapper_select
│ │  
│ │  
│ │  def detect_parallel_client(func):
│ │      """
│ │      Decorator for handling parallelization via `parallel` keyword/client detection
│ │  
│ │ +    Any already initialized Dask cluster always takes precedence
│ │ +    with both `parallel=True` and `parallel=None`. This gets checked via `dd.get_client()`,
│ │ +    and hence if a Dask cluster was set up before, Syncopy (and also potentially ACME later) will just
│ │ +    pass-through this one to the compute classes.
│ │ +    In case no cluster is running, only a dedicated `parallel=True` will spawn either a new
│ │ +    Dask cluster down the road via ACME (if on a slurm cluster) or a new LocalCluster as a default fallback.
│ │ +    The LocalCluster gets closed again after the wrapped function exited.
│ │ +
│ │ +    If `parallel` is `None`:
│ │ +        First attempts to connect to a running dask parallel processing client. If successful,
│ │ +        `parallel` is set to `True` and updated in `func`'s keyword argument dict.
│ │ +        If no client is found `parallel` is set to `False`
│ │ +    If `parallel` is True and ACME is installed AND we are on a slurm cluster:
│ │ +        Do nothing and forward all the parallelization setup with `parallel=True`
│ │ +        to the CR and ultimately ACME
│ │ +    If `parallel` is True and ACME is NOT installed OR we ar NOT on a slurm cluster:
│ │ +        Fire up a standard dask LocalCluster and forward `parallel=True` to func
│ │ +
│ │      Parameters
│ │      ----------
│ │      func : callable
│ │          Typically a Syncopy metafunction such as :func:`~syncopy.freqanalysis`
│ │  
│ │      Returns
│ │      -------
│ │      parallel_client_detector : callable
│ │          Wrapped function; `parallel_client_detector` attempts to extract `parallel`
│ │ -        from keywords provided to `func`. If `parallel` is `True` or `None`
│ │ -        (i.e., was not provided) and dask is available, `parallel_client_detector`
│ │ -        attempts to connect to a running dask parallel processing client. If successful,
│ │ -        `parallel` is set to `True` and updated in `func`'s keyword argument dict.
│ │ -        If no client is found or dask is not available, a corresponding warning
│ │ -        message is printed and `parallel` is set to `False` and updated in `func`'s
│ │ -        keyword argument dict. After successfully calling `func` with the modified
│ │ +        from keywords provided to `func`.
│ │ +
│ │ +        After successfully calling `func` with the modified
│ │          input arguments, `parallel_client_detector` modifies `func` itself:
│ │  
│ │          1. The "Parameters" section in the docstring of `func` is amended by an
│ │             entry explaining the usage of `parallel`. Note: `func`'s docstring is
│ │             only extended if it has a "Parameters" section.
│ │          2. If not already present, `parallel` is added as optional keyword (with
│ │             default value `None`) to the signature of `func`.
│ │ @@ -435,88 +458,120 @@
│ │  
│ │      See also
│ │      --------
│ │      unwrap_select : extract `select` keyword and process in-place data-selections
│ │      unwrap_cfg : Decorator for processing `cfg` "structs"
│ │      """
│ │  
│ │ +    # timeout in seconds for dask worker allocation
│ │ +    dask_timeout = 600
│ │ +
│ │      @functools.wraps(func)
│ │      def parallel_client_detector(*args, **kwargs):
│ │  
│ │ +        logger = get_logger()
│ │ +
│ │          # Extract `parallel` keyword: if `parallel` is `False`, nothing happens
│ │          parallel = kwargs.get("parallel")
│ │ +        kill_spawn = False
│ │ +        has_slurm = check_slurm_available()
│ │  
│ │ -        # Determine if multiple or a single object was supplied for processing
│ │ -        objList = []
│ │ -        argList = list(args)
│ │ -        nTrials = 0
│ │ -        for arg in args:
│ │ -            if hasattr(arg, "trials"):
│ │ -                objList.append(arg)
│ │ -                nTrials = max(nTrials, len(arg.trials))
│ │ -                argList.remove(arg)
│ │ -        nObs = len(objList)
│ │ -
│ │ -        # If parallel processing was requested but ACME is not installed, show
│ │ -        # warning but keep going
│ │ -        if parallel is True and not spy.__acme__:
│ │ -            wrng = "ACME seems not to be installed on this system. " +\
│ │ -                   "Parallel processing capabilities cannot be used. "
│ │ -            SPYWarning(wrng)
│ │ -            parallel = False
│ │ -
│ │ -        # If ACME is available but `parallel` was not set *and* no active
│ │ -        # dask client is found, set `parallel` to`False` (i.e., sequential
│ │ -        # processing as default)
│ │ -        clientRunning = False
│ │ -        if parallel is None and spy.__acme__:
│ │ +        # warning only emitted if slurm available but no ACME or Dask client
│ │ +        slurm_msg = ""
│ │ +
│ │ +        # if acme is around, let it manage everything assuming we are on the ESI cluster
│ │ +        if spy.__acme__ and parallel is not False:
│ │              try:
│ │ -                dd.get_client()
│ │ +                client = dd.get_client()
│ │                  parallel = True
│ │ -                clientRunning = True
│ │              except ValueError:
│ │ +                if parallel:
│ │ +                    msg = (f"Could not find a running dask cluster!\n"
│ │ +                           "Try `esi_cluster_setup` from ACME to set up a cluster on the ESI HPC\n"
│ │ +                           "..computing sequentially"
│ │ +                           )
│ │ +                    logger.important(msg)
│ │                  parallel = False
│ │  
│ │ -        # If only one object was supplied, a dask client is set up in `ComputationalRoutine`;
│ │ -        # for multiple objects, count the total number of trials and start a "global"
│ │ -        # computing client with `n_jobs = nTrials` in here (unless a client is already running)
│ │ -        cleanup = False
│ │ -        if parallel and not clientRunning and nObs > 1:
│ │ -            msg = "Syncopy <{fname:s}> Launching parallel computing client " +\
│ │ -                    "to process {no:d} objects..."
│ │ -            print(msg.format(fname=func.__name__, no=nObs))
│ │ -            client = esi_cluster_setup(n_jobs=nTrials, interactive=False)
│ │ -            cleanup = True
│ │ +        # This effectively searches for a global dask cluster, and sets
│ │ +        # parallel=True if one was found. If no cluster was found, parallel is set to False,
│ │ +        # so no automatic spawning of a LocalCluster this needs explicit `parallel=True`.
│ │ +        elif parallel is None:
│ │ +            # w/o acme interface dask directly
│ │ +            try:
│ │ +                client = dd.get_client()
│ │ +                # wait for at least 1 worker
│ │ +                check_workers_available(client, timeout=dask_timeout, n_workers=1)
│ │ +                msg = f"..attaching to running Dask client:\n\t{client}"
│ │ +                logger.important(msg)
│ │ +                parallel = True
│ │ +            except ValueError:
│ │ +                parallel = False
│ │ +
│ │ +        # If parallel processing was requested but ACME is not installed
│ │ +        # and no other Dask cluster is running,
│ │ +        # initialize a local dask cluster as fallback for local machines
│ │ +        elif parallel is True:
│ │ +            # if already one cluster is reachable do nothing
│ │ +            try:
│ │ +                client = dd.get_client()
│ │ +                # wait for at least 1 worker
│ │ +                check_workers_available(client, timeout=dask_timeout, n_workers=1)
│ │ +                msg = f"..attaching to running Dask client:\n{client}"
│ │ +                logger.important(msg)
│ │ +            except ValueError:
│ │ +                # we are on a HPC but ACME and/or Dask client are missing,
│ │ +                # LocalCluster still gets created
│ │ +                if has_slurm and not spy.__acme__:
│ │ +                    slurm_msg = ("We are apparently on a slurm cluster but\n"
│ │ +                                 "Syncopy could not find a Dask client.\n"
│ │ +                                 "Syncopy does not provide an "
│ │ +                                 "automatic Dask SLURMCluster on its own!"
│ │ +                                 "\nPlease consider configuring your own dask cluster "
│ │ +                                 "via `dask_jobqueue.SLURMCluster()`"
│ │ +                                 "\n\nCreating a LocalCluster as fallback.."
│ │ +                           )
│ │ +                    SPYWarning(slurm_msg)
│ │ +
│ │ +                # -- spawn fallback local cluster --
│ │ +
│ │ +                cluster = dd.LocalCluster()
│ │ +                # attaches to local cluster residing in global namespace
│ │ +                dd.Client(cluster)
│ │ +                kill_spawn = True
│ │ +                msg = ("No running Dask cluster found, created a local instance:\n"
│ │ +                       f"\t{cluster.scheduler}")
│ │ +                logger.important(msg)
│ │  
│ │          # Add/update `parallel` to/in keyword args
│ │          kwargs["parallel"] = parallel
│ │  
│ │ -        # Process provided object(s)
│ │ -        if nObs <= 1:
│ │ -            results = func(*args, **kwargs)
│ │ -        else:
│ │ -            results = []
│ │ -            for obj in objList:
│ │ -                results.append(func(obj, *argList, **kwargs))
│ │ -
│ │ -        # Kill "global" cluster started in here
│ │ -        if cleanup:
│ │ -            cluster_cleanup(client=client)
│ │ +        results = func(*args, **kwargs)
│ │ +
│ │ +        # kill local cluster
│ │ +        if kill_spawn:
│ │ +            # disconnect
│ │ +            dd.get_client().close()
│ │ +            # and kill
│ │ +            cluster.close()
│ │ +        # print again in case it got drowned
│ │ +        if slurm_msg:
│ │ +            SPYWarning(slurm_msg)
│ │  
│ │          return results
│ │  
│ │      # Append `parallel` keyword entry to wrapped function's docstring and signature
│ │      parallelDocEntry = \
│ │      "    parallel : None or bool\n" +\
│ │      "        If `None` (recommended), processing is automatically performed in \n" +\
│ │      "        parallel (i.e., concurrently across trials/channel-groups), provided \n" +\
│ │      "        a dask parallel processing client is running and available. \n" +\
│ │      "        Parallel processing can be manually disabled by setting `parallel` \n" +\
│ │      "        to `False`. If `parallel` is `True` but no parallel processing client\n" +\
│ │ -    "        is running, computing will be performed sequentially. \n"
│ │ +    "        is running, computing will be performed sequentially."
│ │      parallel_client_detector.__doc__ = _append_docstring(func, parallelDocEntry)
│ │      parallel_client_detector.__signature__ = _append_signature(func, "parallel")
│ │  
│ │      return parallel_client_detector
│ │  
│ │  
│ │  def process_io(func):
│ │ @@ -578,19 +633,25 @@
│ │      """
│ │  
│ │      @functools.wraps(func)
│ │      def wrapper_io(trl_dat, *wrkargs, **kwargs):
│ │  
│ │          # `trl_dat` is a NumPy array or `FauxTrial` object: execute the wrapped
│ │          # function and return its result
│ │ -        if not isinstance(trl_dat, dict):
│ │ +        if not isinstance(trl_dat, (dict, tuple)):
│ │ +            # Adding the metadata is done in compute_sequential(), nothing to do here.
│ │ +            # Note that the return value of 'func' in the next line may be a tuple containing
│ │ +            # both the ndarray for 'data', and the 'details'.
│ │              return func(trl_dat, *wrkargs, **kwargs)
│ │  
│ │ -        ### Idea: hook .compute_sequential() from CR into here
│ │ -
│ │ +        # compatibility to adhere to the inargs the CRs produces: ill-formatted tuples
│ │ +        # which mix dicts, lists and even slices
│ │ +        if isinstance(trl_dat, tuple):
│ │ +            wrkargs = trl_dat[1:]
│ │ +            trl_dat = trl_dat[0]
│ │  
│ │          # The fun part: `trl_dat` is a dictionary holding components for parallelization
│ │          keeptrials = trl_dat["keeptrials"]
│ │          infilename = trl_dat["infile"]
│ │          indset = trl_dat["indset"]
│ │          ingrid = trl_dat["ingrid"]
│ │          inshape = trl_dat["inshape"]
│ │ @@ -598,69 +659,72 @@
│ │          fancy = trl_dat["fancy"]
│ │          vdsdir = trl_dat["vdsdir"]
│ │          outfilename = trl_dat["outfile"]
│ │          outdset = trl_dat["outdset"]
│ │          outgrid = trl_dat["outgrid"]
│ │          outshape = trl_dat["outshape"]
│ │          outdtype = trl_dat["dtype"]
│ │ +        call_id = trl_dat["call_id"]
│ │  
│ │          # === STEP 1 === read data into memory
│ │          # Catch empty source-array selections; this workaround is not
│ │          # necessary for h5py version 2.10+ (see https://github.com/h5py/h5py/pull/1174)
│ │          if any([not sel for sel in ingrid]):
│ │ -            res = np.empty(outshape, dtype=outdtype)
│ │ +            res, details = np.empty(outshape, dtype=outdtype), {}
│ │          else:
│ │ -            try:
│ │ -                with h5py.File(infilename, mode="r") as h5fin:
│ │ -                        if fancy:
│ │ -                            arr = np.array(h5fin[indset][ingrid])[np.ix_(*sigrid)]
│ │ -                        else:
│ │ -                            arr = np.array(h5fin[indset][ingrid])
│ │ -            except Exception as exc:  # TODO: aren't these 2 lines superfluous?
│ │ -                raise exc
│ │ +            with h5py.File(infilename, mode="r") as h5fin:
│ │ +                if fancy:
│ │ +                    arr = np.array(h5fin[indset][ingrid])[np.ix_(*sigrid)]
│ │ +                else:
│ │ +                    arr = np.array(h5fin[indset][ingrid])
│ │  
│ │              # === STEP 2 === perform computation
│ │              # Ensure input array shape was not inflated by scalar selection
│ │              # tuple, e.g., ``e=np.ones((2,2)); e[0,:].shape = (2,)`` not ``(1,2)``
│ │              # (use an explicit `shape` assignment here to avoid copies)
│ │              arr.shape = inshape
│ │  
│ │              # Now, actually call wrapped function
│ │              # Put new outputs here!
│ │ -            res = func(arr, *wrkargs, **kwargs)
│ │ +            res, details = parse_cF_returns(func(arr, *wrkargs, **kwargs))
│ │ +            # User-supplied cFs may return a single numpy.ndarray, or a 2-tuple of type (ndarray, sdict) where
│ │ +            # 'ndarray' is a numpy.ndarray containing computation results to be stored in the Syncopy
│ │ +            # data type (like AnalogData),
│ │ +            #  and 'sdict' is a shallow dictionary containing meta data that will be temporarily
│ │ +            # attached to the hdf5 container(s)
│ │ +            # during the compute run, but removed/collected and returned as separate return values
│ │ +            # to the user in the frontend.
│ │  
│ │              # In case scalar selections have been performed, explicitly assign
│ │              # desired output shape to re-create "lost" singleton dimensions
│ │              # (use an explicit `shape` assignment here to avoid copies)
│ │              res.shape = outshape
│ │  
│ │          # === STEP 3 === write result to disk
│ │          # Write result to multiple stand-alone HDF files or use a mutex to write to a
│ │          # common single file (sequentially)
│ │          if vdsdir is not None:
│ │              with h5py.File(outfilename, "w") as h5fout:
│ │                  h5fout.create_dataset(outdset, data=res)
│ │ -                # add new dataset/attribute to capture new outputs
│ │ +                h5_add_metadata(h5fout, details, unique_key_suffix=call_id)
│ │                  h5fout.flush()
│ │          else:
│ │  
│ │              # Create distributed lock (use unique name so it's synced across workers)
│ │              lock = dd.lock.Lock(name='sequential_write')
│ │ -
│ │              # Either (continue to) compute average or write current chunk
│ │              lock.acquire()
│ │              with h5py.File(outfilename, "r+") as h5fout:
│ │ -                # get unique id (from outgrid?)
│ │ -                # to attach additional outputs
│ │ -                # to the one and only hdf5 container
│ │ -                target = h5fout[outdset]
│ │ +                main_dset = h5fout[outdset]
│ │                  if keeptrials:
│ │ -                    target[outgrid] = res
│ │ +                    main_dset[outgrid] = res
│ │                  else:
│ │ -                    target[()] = np.nansum([target, res], axis=0)
│ │ +                    main_dset[()] += res
│ │ +
│ │ +                h5_add_metadata(h5fout, details, unique_key_suffix=call_id)
│ │                  h5fout.flush()
│ │              lock.release()
│ │  
│ │          return None  # result has already been written to disk
│ │  
│ │      return wrapper_io
│ │  
│ │ @@ -704,39 +768,49 @@
│ │      --------
│ │      _append_signature : extend a function's signature
│ │      """
│ │  
│ │      if func.__doc__ is None:
│ │          return
│ │  
│ │ +    # these are the 4 whitespaces right in front of every doc string line
│ │ +    space4 = '    '
│ │ +
│ │      # "Header" insertions always work (an empty docstring is enough to do this).
│ │      # Otherwise ensure the provided `insert_in` section already exists, i.e.,
│ │      # partitioned `sectionHeading` == queried `sectionTitle`
│ │      if insert_in == "Header":
│ │          sectionText, sectionDivider, rest = func.__doc__.partition("Parameters\n")
│ │          textBefore = ""
│ │          sectionHeading = ""
│ │      else:
│ │          sectionTitle = insert_in + "\n"
│ │          textBefore, sectionHeading, textAfter = func.__doc__.partition(sectionTitle)
│ │          if sectionHeading != sectionTitle:  # `insert_in` was not found in docstring
│ │              return func.__doc__
│ │          sectionText, sectionDivider, rest = textAfter.partition("\n\n")
│ │ -    sectionText = sectionText.splitlines(keepends=True)
│ │ +    sectionTextList = sectionText.splitlines(keepends=True)
│ │  
│ │      if at_end:
│ │          insertAtLine = -1
│ │ -        while sectionText[insertAtLine].isspace():
│ │ +        while sectionTextList[insertAtLine].isspace():
│ │              insertAtLine -= 1
│ │          insertAtLine = min(-1, insertAtLine + 1)
│ │ +
│ │ +        # to avoid clipping the last line of a parameter description
│ │ +        if sectionTextList[-1] != space4:
│ │ +            sectionTextList.append('\n')
│ │ +            sectionTextList.append(space4)
│ │      else:
│ │ +        # this is the 1st line break or the '    --------'
│ │          insertAtLine = 1
│ │ -    sectionText = "".join(sectionText[:insertAtLine]) +\
│ │ -                  supplement +\
│ │ -                  "".join(sectionText[insertAtLine:])
│ │ +
│ │ +    sectionText = "".join(sectionTextList[:insertAtLine])
│ │ +    sectionText += supplement
│ │ +    sectionText += "".join(sectionTextList[insertAtLine:])
│ │  
│ │      newDocString = textBefore +\
│ │                     sectionHeading +\
│ │                     sectionText +\
│ │                     sectionDivider +\
│ │                     rest
│ │   --- esi-syncopy-2022.8/syncopy/shared/parsers.py
│ ├── +++ esi_syncopy-2023.3/syncopy/shared/parsers.py
│ │┄ Files 0% similar despite different names
│ │ @@ -411,15 +411,15 @@
│ │              amax = max(fi_arr.real.max(), fi_arr.imag.max())
│ │          else:
│ │              amin = fi_arr.min()
│ │              amax = fi_arr.max()
│ │          if amin < lims[0] or amax > lims[1]:
│ │              legal = "all array elements to be bounded by {lb:s} and {ub:s}"
│ │              raise SPYValueError(legal.format(lb=str(lims[0]), ub=str(lims[1])),
│ │ -                                varname=varname)
│ │ +                                varname=varname, actual=f"array with range {amin} to {amax}")
│ │  
│ │      # If required parse dimensional layout of array
│ │      if dims is not None:
│ │  
│ │          # Account for the special case of 1d character arrays (that
│ │          # collapse to 0d-arrays when squeezed)
│ │          ischar = int(np.issubdtype(arr.dtype, np.dtype("str").type))
│ │   --- esi-syncopy-2022.8/syncopy/shared/queries.py
│ ├── +++ esi_syncopy-2023.3/syncopy/shared/queries.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/shared/tools.py
│ ├── +++ esi_syncopy-2023.3/syncopy/shared/tools.py
│ │┄ Files 26% similar despite different names
│ │ @@ -1,19 +1,22 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │  # Auxiliaries used across all of Syncopy
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │ +from numbers import Number
│ │ +from copy import deepcopy
│ │  import inspect
│ │  import json
│ │  
│ │  # Local imports
│ │  from syncopy.shared.errors import SPYValueError, SPYWarning, SPYTypeError
│ │ +from syncopy.shared.parsers import sequence_parser
│ │  
│ │  __all__ = ["StructDict", "get_defaults"]
│ │  
│ │  
│ │  class StructDict(dict):
│ │      """Child-class of dict for emulating MATLAB structs
│ │  
│ │ @@ -43,14 +46,53 @@
│ │              for key, value in self.items():
│ │                  ppStr += printString.format(key, str(value))
│ │              ppStr += "\nUse `dict(cfg)` for copy-paste-friendly format"
│ │          else:
│ │              ppStr = "{}"
│ │          return ppStr
│ │  
│ │ +    def copy(self, deep=True):
│ │ +        """
│ │ +        Create a copy of this StructDict instance.
│ │ +
│ │ +        Note: Overwrites the `.copy` method of the parent `dict` class, otherwise `copy()` will return a `dict` instead of a `StructDict`.
│ │ +
│ │ +        Parameters
│ │ +        ---------
│ │ +        deep: bool
│ │ +            Whether to produce a deep copy. Defaults to `True`.
│ │ +
│ │ +        Returns
│ │ +        -------
│ │ +        Copy of StructDict.
│ │ +        """
│ │ +        if(deep):
│ │ +            return self.deepcopy()
│ │ +        else:
│ │ +            obj = type(self).__new__(self.__class__)
│ │ +            obj.__dict__.update(self.__dict__)
│ │ +            return obj
│ │ +
│ │ +    def deepcopy(self):
│ │ +        """
│ │ +        Return a deep copy of this StructDict.
│ │ +
│ │ +        Notes
│ │ +        -----
│ │ +        Call the `.copy()` method instead to get a shallow copy, though that seems rather uncommon.
│ │ +        """
│ │ +        return deepcopy(self)
│ │ +
│ │ +    def __deepcopy__(self, memo):
│ │ +        result = type(self).__new__(self.__class__)
│ │ +        memo[id(self)] = result
│ │ +        for k, v in self.__dict__.items():
│ │ +            setattr(result, k, deepcopy(v, memo))
│ │ +        return result
│ │ +
│ │  
│ │  class SerializableDict(dict):
│ │  
│ │      """
│ │      It's a dict which checks newly inserted
│ │      values for serializability, keys should always be serializable
│ │      """
│ │ @@ -66,26 +108,63 @@
│ │          dict.__setitem__(self, key, value)
│ │  
│ │      def is_json(self, key, value):
│ │          try:
│ │              json.dumps(value)
│ │          except TypeError:
│ │              lgl = "serializable data type, e.g. floats, lists, tuples, ... "
│ │ -            raise SPYTypeError(value, f"value for key '{key}'", lgl)
│ │ +            raise SPYTypeError(value, f"value {value} for key '{key}'", lgl)
│ │          try:
│ │              json.dumps(key)
│ │          except TypeError:
│ │              lgl = "serializable data type, e.g. floats, lists, tuples, ... "
│ │              raise SPYTypeError(value, f"key '{key}'", lgl)
│ │  
│ │  
│ │ -def get_frontend_cfg(defaults, lcls, kwargs):
│ │ +def _serialize_value(value):
│ │ +    """
│ │ +    Helper to serialize 1-level deep sequences (lists, arrays, ranges) or
│ │ +    single numbers/strings as ``value``s.
│ │ +
│ │ +    Main task is to get rid of numpy data types which are not
│ │ +    serializable (e.i. np.int64).
│ │ +    """
│ │  
│ │ +    if isinstance(value, np.ndarray):
│ │ +        value = value.tolist()
│ │ +
│ │ +    if isinstance(value, range):
│ │ +        value = list(value)
│ │ +
│ │ +    # unpack the list, if ppl mix types this will go wrong
│ │ +    if isinstance(value, list) and len(value) != 0:
│ │ +        if hasattr(value[0], 'is_integer'):
│ │ +            value = [float(v) for v in value]
│ │ +        # should only be the integers
│ │ +        elif isinstance(value[0], Number) and not isinstance(value[0], bool):
│ │ +            value = [int(v) for v in value]
│ │ +
│ │ +    # singleton/non-sequence type entries
│ │ +    if isinstance(value, Number) and not isinstance(value, bool):
│ │ +        # all floating types have this method
│ │ +        if hasattr(value, 'is_integer'):
│ │ +            # get rid of np.int64 or np.float32
│ │ +            value = int(value) if value.is_integer() else float(value)
│ │ +        else:
│ │ +            value = int(value)
│ │ +
│ │ +    return value
│ │ +
│ │ +
│ │ +def get_frontend_cfg(defaults, lcls, kwargs):
│ │      """
│ │ -    Assemble cfg dict to allow direct replay of frontend calls
│ │ +    Assemble serializable cfg dict to allow direct replay of frontend calls
│ │ +
│ │ +    Most parsing is done in the respective frontends, the config values
│ │ +    should be straightforward to serialize.
│ │  
│ │      Parameters
│ │      ----------
│ │  
│ │      defaults : dict
│ │          The result of :func:`~get_defaults`, holding all frontend specific
│ │          parameter names and default values
│ │ @@ -102,21 +181,37 @@
│ │          Holds all (default and non-default) parameter key-value
│ │          pairs passed to the frontend
│ │  
│ │      """
│ │  
│ │      # create new cfg dict
│ │      new_cfg = StructDict()
│ │ +
│ │      for par_name in defaults:
│ │ -        # check only needed for injected kwargs like `parallel`
│ │ +        # check only set parameters
│ │          if par_name in lcls:
│ │ -            new_cfg[par_name] = lcls[par_name]
│ │ -    # attach additional kwargs (like select)
│ │ +            value = _serialize_value(lcls[par_name])
│ │ +            new_cfg[par_name] = value
│ │ +
│ │ +    # 'select' only allowed dictionary parameter within kwargs
│ │ +    # we can 'pop' here as selection got digested beforehand by @unwrap_select
│ │ +    sdict = kwargs.pop('select', None)
│ │ +    if sdict is not None:
│ │ +        # serialized selection dict
│ │ +        ser_sdict = dict()
│ │ +        for sel_key in sdict:
│ │ +            ser_sdict[sel_key] = _serialize_value(sdict[sel_key])
│ │ +        new_cfg['select'] = ser_sdict
│ │ +
│ │ +    # should only be 'parallel' and 'chan_per_worker'
│ │      for key in kwargs:
│ │ -        new_cfg[key] = kwargs[key]
│ │ +        new_cfg[key] = _serialize_value(kwargs[key])
│ │ +
│ │ +    # use instantiation for a final check
│ │ +    SerializableDict(new_cfg)
│ │  
│ │      return new_cfg
│ │  
│ │  
│ │  def best_match(source, selection, span=False, tol=None, squash_duplicates=False):
│ │      """
│ │      Find matching elements in a given 1d-array/list
│ │ @@ -265,7 +360,10 @@
│ │      """
│ │  
│ │      if not callable(obj):
│ │          raise SPYTypeError(obj, varname="obj", expected="SyNCoPy function or class")
│ │      dct = {k: v.default for k, v in inspect.signature(obj).parameters.items()\
│ │             if v.default != v.empty and v.name != "cfg"}
│ │      return StructDict(dct)
│ │ +
│ │ +
│ │ +
│ │   --- esi-syncopy-2022.8/syncopy/specest/README.md
│ ├── +++ esi_syncopy-2023.3/syncopy/specest/README.md
│ │┄ Files 7% similar despite different names
│ │ @@ -13,8 +13,8 @@
│ │  
│ │  ## Usage Examples (TODO..)
│ │  
│ │  ...
│ │  
│ │  ## Sources
│ │  
│ │ -- [Wavelet core library](./specest/wavelets/) from GitHub: https://github.com/aaren/wavelets
│ │ +- [Wavelet core library](./wavelets/) from GitHub: https://github.com/aaren/wavelets
│ │   --- esi-syncopy-2022.8/syncopy/specest/_norm_spec.py
│ ├── +++ esi_syncopy-2023.3/syncopy/specest/_norm_spec.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/specest/compRoutines.py
│ ├── +++ esi_syncopy-2023.3/syncopy/specest/compRoutines.py
│ │┄ Files 18% similar despite different names
│ │ @@ -17,42 +17,52 @@
│ │  #
│ │  # the backend method name als gets explicitly attached as a class constant:
│ │  # method: backend method name
│ │  
│ │  # Builtin/3rd party package imports
│ │  from inspect import signature
│ │  import numpy as np
│ │ +from hashlib import blake2b
│ │ +
│ │  from scipy import signal
│ │  
│ │  # backend method imports
│ │  from .mtmfft import mtmfft
│ │  from .mtmconvol import mtmconvol
│ │  from .superlet import superlet
│ │  from .wavelet import wavelet
│ │  from .fooofspy import fooofspy
│ │  
│ │  
│ │  # Local imports
│ │ -from syncopy.shared.errors import SPYWarning
│ │ +from syncopy.shared.errors import SPYValueError, SPYWarning, SPYParallelLog
│ │  from syncopy.shared.tools import best_match
│ │ -from syncopy.shared.computational_routine import ComputationalRoutine
│ │ +from syncopy.shared.computational_routine import ComputationalRoutine, propagate_properties
│ │  from syncopy.shared.kwarg_decorators import process_io
│ │ +from syncopy.shared.metadata import (
│ │ +    encode_unique_md_label,
│ │ +    decode_unique_md_label,
│ │ +    metadata_from_hdf5_file,
│ │ +    check_freq_hashes,
│ │ +    metadata_nest
│ │ +)
│ │ +
│ │  from syncopy.shared.const_def import (
│ │      spectralConversions,
│ │      spectralDTypes
│ │  )
│ │  
│ │ -
│ │  # -----------------------
│ │  # MultiTaper FFT
│ │  # -----------------------
│ │  
│ │ +
│ │  @process_io
│ │  def mtmfft_cF(trl_dat, foi=None, timeAxis=0, keeptapers=True,
│ │ -              polyremoval=None, output_fmt="pow",
│ │ +              polyremoval=None, output="pow",
│ │                noCompute=False, chunkShape=None, method_kwargs=None):
│ │  
│ │      """
│ │      Compute (multi-)tapered Fourier transform of multi-channel time series data
│ │  
│ │      Parameters
│ │      ----------
│ │ @@ -63,15 +73,15 @@
│ │          cannot be matched exactly the closest possible frequencies (respecting
│ │          data length and padding) are used.
│ │      timeAxis : int
│ │          Index of running time axis in `trl_dat` (0 or 1)
│ │      keeptapers : bool
│ │          If `True`, return spectral estimates for each taper.
│ │          Otherwise power spectrum is averaged across tapers,
│ │ -        only valid spectral estimate if `output_fmt` is `pow`.
│ │ +        only valid spectral estimate if `output` is `pow`.
│ │      pad : str
│ │          Padding mode; one of `'absolute'`, `'relative'`, `'maxlen'`, or `'nextpow2'`.
│ │          See :func:`syncopy.padding` for more information.
│ │      padtype : str
│ │          Values to be used for padding. Can be 'zero', 'nan', 'mean',
│ │          'localmean', 'edge' or 'mirror'. See :func:`syncopy.padding` for
│ │          more information.
│ │ @@ -80,15 +90,15 @@
│ │          See :func:`syncopy.padding` for more information.
│ │      polyremoval : int or None
│ │          Order of polynomial used for de-trending data in the time domain prior
│ │          to spectral analysis. A value of 0 corresponds to subtracting the mean
│ │          ("de-meaning"), ``polyremoval = 1`` removes linear trends (subtracting the
│ │          least squares fit of a linear polynomial).
│ │          If `polyremoval` is `None`, no de-trending is performed.
│ │ -    output_fmt : str
│ │ +    output : str
│ │          Output of spectral estimation; one of :data:`~syncopy.specest.const_def.availableOutputs`
│ │      noCompute : bool
│ │          Preprocessing flag. If `True`, do not perform actual calculation but
│ │          instead return expected shape and :class:`numpy.dtype` of output
│ │          array.
│ │      chunkShape : None or tuple
│ │          If not `None`, represents shape of output `spec` (respecting provided
│ │ @@ -117,15 +127,14 @@
│ │      See also
│ │      --------
│ │      syncopy.freqanalysis : parent metafunction
│ │      MultiTaperFFT : :class:`~syncopy.shared.computational_routine.ComputationalRoutine` instance
│ │                       that calls this method as :meth:`~syncopy.shared.computational_routine.ComputationalRoutine.computeFunction`
│ │      numpy.fft.rfft : NumPy's FFT implementation
│ │      """
│ │ -
│ │      # Re-arrange array if necessary and get dimensional information
│ │      if timeAxis != 0:
│ │          dat = trl_dat.T       # does not copy but creates view of `trl_dat`
│ │      else:
│ │          dat = trl_dat
│ │  
│ │      if method_kwargs['nSamples'] is None:
│ │ @@ -142,34 +151,40 @@
│ │      nFreq = freq_idx.size
│ │      nTaper = method_kwargs["taper_opt"].get('Kmax', 1)
│ │      outShape = (1, max(1, nTaper * keeptapers), nFreq, nChannels)
│ │  
│ │      # For initialization of computational routine,
│ │      # just return output shape and dtype
│ │      if noCompute:
│ │ -        return outShape, spectralDTypes[output_fmt]
│ │ +        return outShape, spectralDTypes[output]
│ │  
│ │      # detrend, does not work with 'FauxTrial' data..
│ │      if polyremoval == 0:
│ │          dat = signal.detrend(dat, type='constant', axis=0, overwrite_data=True)
│ │      elif polyremoval == 1:
│ │          dat = signal.detrend(dat, type='linear', axis=0, overwrite_data=True)
│ │  
│ │      # call actual specest method
│ │ -    res, _ = mtmfft(dat, **method_kwargs)
│ │ +    res, freqs = mtmfft(dat, **method_kwargs)
│ │  
│ │ -    # attach time-axis and convert to output_fmt
│ │ +    # attach time-axis and convert to output
│ │      spec = res[np.newaxis, :, freq_idx, :]
│ │ -    spec = spectralConversions[output_fmt](spec)
│ │ +    spec = spectralConversions[output](spec)
│ │ +
│ │ +    # Hash the freqs and add to second return value.
│ │ +    freqs_hash = blake2b(freqs).hexdigest().encode('utf-8')
│ │ +    metadata = {'freqs_hash': np.array(freqs_hash)}  # Will have dtype='|S128'
│ │ +
│ │      # Average across tapers if wanted
│ │      # averaging is only valid spectral estimate
│ │ -    # if output_fmt == 'pow'! (gets checked in parent meta)
│ │ +    # if output == 'pow'! (gets checked in parent meta)
│ │      if not keeptapers:
│ │ -        return spec.mean(axis=1, keepdims=True)
│ │ -    return spec
│ │ +        return spec.mean(axis=1, keepdims=True), metadata
│ │ +
│ │ +    return spec, metadata
│ │  
│ │  
│ │  class MultiTaperFFT(ComputationalRoutine):
│ │      """
│ │      Compute class that calculates (multi-)tapered Fourier transfrom of :class:`~syncopy.AnalogData` objects
│ │  
│ │      Sub-class of :class:`~syncopy.shared.computational_routine.ComputationalRoutine`,
│ │ @@ -187,41 +202,32 @@
│ │      valid_kws = list(signature(mtmfft).parameters.keys())[1:]
│ │      valid_kws += list(signature(mtmfft_cF).parameters.keys())[1:]
│ │      # hardcode some parameter names which got digested from the frontend
│ │      valid_kws += ['tapsmofrq', 'nTaper', 'pad', 'fooof_opt']
│ │  
│ │      def process_metadata(self, data, out):
│ │  
│ │ -        # Some index gymnastics to get trial begin/end "samples"
│ │ -        if data.selection is not None:
│ │ -            chanSec = data.selection.channel
│ │ -            trl = data.selection.trialdefinition
│ │ -            for row in range(trl.shape[0]):
│ │ -                trl[row, :2] = [row, row + 1]
│ │ -        else:
│ │ -            chanSec = slice(None)
│ │ -            time = np.arange(len(data.trials))
│ │ -            time = time.reshape((time.size, 1))
│ │ -            trl = np.hstack((time, time + 1,
│ │ -                             np.zeros((len(data.trials), 1)),
│ │ -                             np.array(data.trialinfo)))
│ │ -
│ │ -        # Attach constructed trialdef-array (if even necessary)
│ │ -        if self.keeptrials:
│ │ -            out.trialdefinition = trl
│ │ -        else:
│ │ -            out.trialdefinition = np.array([[0, 1, 0]])
│ │ +        # General-purpose loading of metadata.
│ │ +        metadata = metadata_from_hdf5_file(out.filename)
│ │  
│ │ -        # Attach remaining meta-data
│ │ -        out.samplerate = data.samplerate
│ │ -        out.channel = np.array(data.channel[chanSec])
│ │ -        if self.cfg["method_kwargs"]["taper"] is None:
│ │ +        check_freq_hashes(metadata, out)
│ │ +
│ │ +        # channels and trialdefinition
│ │ +        propagate_properties(data, out, self.keeptrials)
│ │ +
│ │ +        taper_kw = self.cfg["method_kwargs"]["taper"]
│ │ +        if taper_kw is None:
│ │              out.taper = np.array(['None'])
│ │ +        # multi-tapering
│ │ +        elif taper_kw == 'dpss':
│ │ +            nTaper =  self.outputShape[out.dimord.index("taper")]
│ │ +            out.taper = np.array([taper_kw + str(i) for i in range(nTaper)])
│ │ +        # just a single taper
│ │          else:
│ │ -            out.taper = np.array([self.cfg["method_kwargs"]["taper"]] * self.outputShape[out.dimord.index("taper")])
│ │ +            out.taper = np.array([taper_kw])
│ │          out.freq = self.cfg["foi"]
│ │  
│ │  
│ │  # -----------------------
│ │  # MultiTaper Windowed FFT
│ │  # -----------------------
│ │  
│ │ @@ -232,15 +238,15 @@
│ │          trl_dat,
│ │          soi,
│ │          postselect,
│ │          equidistant=True,
│ │          toi=None,
│ │          foi=None,
│ │          nTaper=1, tapsmofrq=None, timeAxis=0,
│ │ -        keeptapers=True, polyremoval=0, output_fmt="pow",
│ │ +        keeptapers=True, polyremoval=0, output="pow",
│ │          noCompute=False, chunkShape=None, method_kwargs=None):
│ │      """
│ │      Perform time-frequency analysis on multi-channel time series data using a sliding window FFT
│ │  
│ │      Parameters
│ │      ----------
│ │      trl_dat : 2D :class:`numpy.ndarray`
│ │ @@ -284,15 +290,15 @@
│ │          otherwise spectrum is averaged across tapers.
│ │      polyremoval : int
│ │          Order of polynomial used for de-trending data in the time domain prior
│ │          to spectral analysis. A value of 0 corresponds to subtracting the mean
│ │          ("de-meaning"), ``polyremoval = 1`` removes linear trends (subtracting the
│ │          least squares fit of a linear polynomial). Detrending is done on each segment!
│ │          If `polyremoval` is `None`, no de-trending is performed.
│ │ -    output_fmt : str
│ │ +    output : str
│ │          Output of spectral estimation; one of :data:`~syncopy.specest.const_def.availableOutputs`
│ │      noCompute : bool
│ │          Preprocessing flag. If `True`, do not perform actual calculation but
│ │          instead return expected shape and :class:`numpy.dtype` of output
│ │          array.
│ │      chunkShape : None or tuple
│ │          If not `None`, represents shape of output object `spec` (respecting provided
│ │ @@ -345,15 +351,15 @@
│ │          stftPad = True
│ │      nFreq = foi.size
│ │      taper_opt = method_kwargs['taper_opt']
│ │      if taper_opt:
│ │          nTaper = taper_opt.get("Kmax", 1)
│ │      outShape = (nTime, max(1, nTaper * keeptapers), nFreq, nChannels)
│ │      if noCompute:
│ │ -        return outShape, spectralDTypes[output_fmt]
│ │ +        return outShape, spectralDTypes[output]
│ │  
│ │      # detrending options for each segment
│ │      if polyremoval == 0:
│ │          detrend = 'constant'
│ │      elif polyremoval == 1:
│ │          detrend = 'linear'
│ │      else:
│ │ @@ -364,41 +370,40 @@
│ │                            "padded": stftPad,
│ │                            "detrend": detrend})
│ │  
│ │      if equidistant:
│ │          ftr, freqs = mtmconvol(dat[soi, :], **method_kwargs)
│ │          _, fIdx = best_match(freqs, foi, squash_duplicates=True)
│ │          spec = ftr[postselect, :, fIdx, :]
│ │ -        spec = spectralConversions[output_fmt](spec)
│ │ +        spec = spectralConversions[output](spec)
│ │  
│ │      else:
│ │          # in this case only a single window gets centered on
│ │          # every individual soi, so we can use mtmfft!
│ │          samplerate = method_kwargs['samplerate']
│ │          taper = method_kwargs['taper']
│ │  
│ │          # In case tapers aren't preserved allocate `spec` "too big"
│ │          # and average afterwards
│ │ -        spec = np.full((nTime, nTaper, nFreq, nChannels), np.nan, dtype=spectralDTypes[output_fmt])
│ │ +        spec = np.full((nTime, nTaper, nFreq, nChannels), np.nan, dtype=spectralDTypes[output])
│ │  
│ │          ftr, freqs = mtmfft(dat[soi[0], :], samplerate, taper=taper, taper_opt=taper_opt)
│ │          _, fIdx = best_match(freqs, foi, squash_duplicates=True)
│ │ -        spec[0, ...] = spectralConversions[output_fmt](ftr[:, fIdx, :])
│ │ +        spec[0, ...] = spectralConversions[output](ftr[:, fIdx, :])
│ │          # loop over remaining soi to center windows on
│ │          for tk in range(1, len(soi)):
│ │              ftr, freqs = mtmfft(dat[soi[tk], :], samplerate, taper=taper, taper_opt=taper_opt)
│ │ -            spec[tk, ...] = spectralConversions[output_fmt](ftr[:, fIdx, :])
│ │ +            spec[tk, ...] = spectralConversions[output](ftr[:, fIdx, :])
│ │  
│ │      # Average across tapers if wanted
│ │ -    # only valid if output_fmt='pow' !
│ │ +    # only valid if output='pow' !
│ │      if not keeptapers:
│ │          return np.nanmean(spec, axis=1, keepdims=True)
│ │      return spec
│ │  
│ │ -
│ │  class MultiTaperFFTConvol(ComputationalRoutine):
│ │      """
│ │      Compute class that performs time-frequency analysis of :class:`~syncopy.AnalogData` objects
│ │  
│ │      Sub-class of :class:`~syncopy.shared.computational_routine.ComputationalRoutine`,
│ │      see :doc:`/developer/compute_kernels` for technical details on Syncopy's compute
│ │      classes and metafunctions.
│ │ @@ -436,18 +441,26 @@
│ │              trl = trl[[0], :]
│ │              trl[:, 2] = t0
│ │  
│ │          # Attach meta-data
│ │          out.trialdefinition = trl
│ │          out.samplerate = srate
│ │          out.channel = np.array(data.channel[chanSec])
│ │ -        if self.cfg["method_kwargs"]["taper"] is None:
│ │ +
│ │ +        taper_kw = self.cfg["method_kwargs"]["taper"]
│ │ +        if taper_kw is None:
│ │              out.taper = np.array(['None'])
│ │ +        # multi-tapering
│ │ +        elif taper_kw == 'dpss':
│ │ +            nTaper =  self.outputShape[out.dimord.index("taper")]
│ │ +            out.taper = np.array([taper_kw + str(i) for i in range(nTaper)])
│ │ +        # just a single taper
│ │          else:
│ │ -            out.taper = np.array([self.cfg["method_kwargs"]["taper"]] * self.outputShape[out.dimord.index("taper")])
│ │ +            out.taper = np.array([taper_kw])
│ │ +
│ │          out.freq = self.cfg["foi"]
│ │  
│ │  
│ │  # -----------------
│ │  # WaveletTransform
│ │  # -----------------
│ │  
│ │ @@ -456,15 +469,15 @@
│ │  def wavelet_cF(
│ │      trl_dat,
│ │      preselect,
│ │      postselect,
│ │      toi=None,
│ │      timeAxis=0,
│ │      polyremoval=0,
│ │ -    output_fmt="pow",
│ │ +    output="pow",
│ │      noCompute=False,
│ │      chunkShape=None,
│ │      method_kwargs=None,
│ │  ):
│ │      """
│ │      This is the middleware for the :func:`~syncopy.specest.wavelet.wavelet`
│ │      spectral estimation method.
│ │ @@ -487,15 +500,15 @@
│ │          Index of running time axis in `trl_dat` (0 or 1)
│ │      polyremoval : int
│ │          Order of polynomial used for de-trending data in the time domain prior
│ │          to spectral analysis. A value of 0 corresponds to subtracting the mean
│ │          ("de-meaning"), ``polyremoval = 1`` removes linear trends (subtracting the
│ │          least squares fit of a linear polynomial).
│ │          If `polyremoval` is `None`, no de-trending is performed.
│ │ -    output_fmt : str
│ │ +    output : str
│ │          Output of spectral estimation; one of :data:`~syncopy.specest.const_def.availableOutputs`
│ │      noCompute : bool
│ │          Preprocessing flag. If `True`, do not perform actual calculation but
│ │          instead return expected shape and :class:`numpy.dtype` of output
│ │          array.
│ │      chunkShape : None or tuple
│ │          If not `None`, represents shape of output object `spec` (respecting provided
│ │ @@ -545,15 +558,15 @@
│ │      if isinstance(toi, np.ndarray):  # `toi` is an array of time-points
│ │          nTime = toi.size
│ │      else:  # `toi` is 'all'
│ │          nTime = dat.shape[0]
│ │      nScales = method_kwargs["scales"].size
│ │      outShape = (nTime, 1, nScales, nChannels)
│ │      if noCompute:
│ │ -        return outShape, spectralDTypes[output_fmt]
│ │ +        return outShape, spectralDTypes[output]
│ │  
│ │      # detrend, does not work with 'FauxTrial' data..
│ │      if polyremoval == 0:
│ │          dat = signal.detrend(dat, type='constant', axis=0, overwrite_data=True)
│ │      elif polyremoval == 1:
│ │          dat = signal.detrend(dat, type='linear', axis=0, overwrite_data=True)
│ │  
│ │ @@ -561,15 +574,15 @@
│ │      # actual method call
│ │      # ------------------
│ │      # Compute wavelet transform with given data/time-selection
│ │      spec = wavelet(dat[preselect, :], **method_kwargs)
│ │      # the cwt stacks the scales on the 1st axis, move to 2nd
│ │      spec = spec.transpose(1, 0, 2)[postselect, :, :]
│ │  
│ │ -    return spectralConversions[output_fmt](spec[:, np.newaxis, :, :])
│ │ +    return spectralConversions[output](spec[:, np.newaxis, :, :])
│ │  
│ │  
│ │  class WaveletTransform(ComputationalRoutine):
│ │      """
│ │      Compute class that performs time-frequency analysis of :class:`~syncopy.AnalogData` objects
│ │  
│ │      Sub-class of :class:`~syncopy.shared.computational_routine.ComputationalRoutine`,
│ │ @@ -612,14 +625,15 @@
│ │          # Attach meta-data
│ │          out.trialdefinition = trl
│ │          out.samplerate = srate
│ │          out.channel = np.array(data.channel[chanSec])
│ │          out.freq = 1 / self.cfg["method_kwargs"]["wavelet"].fourier_period(
│ │              self.cfg["method_kwargs"]["scales"]
│ │          )
│ │ +        out.taper = np.array(['None'])
│ │  
│ │  
│ │  # -----------------
│ │  # SuperletTransform
│ │  # -----------------
│ │  
│ │  
│ │ @@ -627,15 +641,15 @@
│ │  def superlet_cF(
│ │      trl_dat,
│ │      preselect,
│ │      postselect,
│ │      toi=None,
│ │      timeAxis=0,
│ │      polyremoval=0,
│ │ -    output_fmt="pow",
│ │ +    output="pow",
│ │      noCompute=False,
│ │      chunkShape=None,
│ │      method_kwargs=None,
│ │  ):
│ │  
│ │      """
│ │      This is the middleware for the :func:`~syncopy.specest.superlet.superlet`
│ │ @@ -644,41 +658,41 @@
│ │      Parameters
│ │      ----------
│ │      trl_dat : 2D :class:`numpy.ndarray`
│ │          Uniformly sampled multi-channel time-series
│ │      preselect : slice
│ │          Begin- to end-samples to perform analysis on (trim data to interval).
│ │          See Notes for details.
│ │ -    postselect : list of slices or list of 1D NumPy arrays
│ │ +    postselect : list of slices or list of 1D numpy arrays
│ │          Actual time-points of interest within interval defined by `preselect`
│ │          See Notes for details.
│ │      toi : 1D :class:`numpy.ndarray` or str
│ │          Either array of equidistant time-points
│ │          or `"all"` to perform analysis on all samples in `trl_dat`. Please refer to
│ │          :func:`~syncopy.freqanalysis` for further details.
│ │      timeAxis : int
│ │          Index of running time axis in `trl_dat` (0 or 1)
│ │      polyremoval : int or None
│ │          Order of polynomial used for de-trending data in the time domain prior
│ │          to spectral analysis. A value of 0 corresponds to subtracting the mean
│ │          ("de-meaning"), ``polyremoval = 1`` removes linear trends (subtracting the
│ │          least squares fit of a linear polynomial).
│ │          If `polyremoval` is `None`, no de-trending is performed.
│ │ -    output_fmt : str
│ │ +    output : str
│ │          Output of spectral estimation; one of
│ │          :data:`~syncopy.specest.const_def.availableOutputs`
│ │      noCompute : bool
│ │          Preprocessing flag. If `True`, do not perform actual calculation but
│ │          instead return expected shape and :class:`numpy.dtype` of output
│ │          array.
│ │      chunkShape : None or tuple
│ │          If not `None`, represents shape of output object `gmean_spec`
│ │          (respecting provided values of `scales`, `preselect`, `postselect` etc.)
│ │      method_kwargs : dict
│ │ -        Keyword arguments passed to :func:`~syncopy.specest.superlet.superlet
│ │ +        Keyword arguments passed to :func:`~syncopy.specest.superlet.superlet`
│ │          controlling the spectral estimation method
│ │  
│ │      Returns
│ │      -------
│ │      gmean_spec : :class:`numpy.ndarray`
│ │          Complex time-frequency representation of the input data.
│ │          Shape is ``(nTime, 1, nScales, nChannels)``.
│ │ @@ -712,30 +726,30 @@
│ │      if isinstance(toi, np.ndarray):  # `toi` is an array of time-points
│ │          nTime = toi.size
│ │      else:  # `toi` is 'all'
│ │          nTime = dat.shape[0]
│ │      nScales = method_kwargs["scales"].size
│ │      outShape = (nTime, 1, nScales, nChannels)
│ │      if noCompute:
│ │ -        return outShape, spectralDTypes[output_fmt]
│ │ +        return outShape, spectralDTypes[output]
│ │  
│ │      # detrend, does not work with 'FauxTrial' data..
│ │      if polyremoval == 0:
│ │          dat = signal.detrend(dat, type='constant', axis=0, overwrite_data=True)
│ │      elif polyremoval == 1:
│ │          dat = signal.detrend(dat, type='linear', axis=0, overwrite_data=True)
│ │  
│ │      # ------------------
│ │      # actual method call
│ │      # ------------------
│ │      gmean_spec = superlet(dat[preselect, :], **method_kwargs)
│ │      # the cwtSL stacks the scales on the 1st axis
│ │      gmean_spec = gmean_spec.transpose(1, 0, 2)[postselect, :, :]
│ │  
│ │ -    return spectralConversions[output_fmt](gmean_spec[:, np.newaxis, :, :])
│ │ +    return spectralConversions[output](gmean_spec[:, np.newaxis, :, :])
│ │  
│ │  
│ │  class SuperletTransform(ComputationalRoutine):
│ │      """
│ │      Compute class that performs time-frequency analysis of :class:`~syncopy.AnalogData` objects
│ │  
│ │      Sub-class of :class:`~syncopy.shared.computational_routine.ComputationalRoutine`,
│ │ @@ -775,29 +789,30 @@
│ │  
│ │          # Attach meta-data
│ │          out.trialdefinition = trl
│ │          out.samplerate = srate
│ │          out.channel = np.array(data.channel[chanSec])
│ │          # for the SL Morlets the conversion is straightforward
│ │          out.freq = 1 / (2 * np.pi * self.cfg["method_kwargs"]["scales"])
│ │ +        out.taper = np.array(['None'])
│ │  
│ │  
│ │  def _make_trialdef(cfg, trialdefinition, samplerate):
│ │      """
│ │      Local helper to construct trialdefinition arrays for time-frequency
│ │      :class:`~syncopy.SpectralData` objects
│ │  
│ │      Parameters
│ │      ----------
│ │      cfg : dict
│ │          Config dictionary attribute of `ComputationalRoutine` subclass
│ │      trialdefinition : 2D :class:`numpy.ndarray`
│ │ -        Provisional trialdefnition array either directly copied from the
│ │ +        Provisional trialdefinition array either directly copied from the
│ │          :class:`~syncopy.AnalogData` input object or computed by the
│ │ -        :class:`~syncopy.datatype.base_data.Selector` class.
│ │ +        :class:`~syncopy.datatype.selector.Selector` class.
│ │      samplerate : float
│ │          Original sampling rate of :class:`~syncopy.AnalogData` input object
│ │  
│ │      Returns
│ │      -------
│ │      trialdefinition : 2D :class:`numpy.ndarray`
│ │          Updated trialdefinition array reflecting provided `toi`/`toilim` selection
│ │ @@ -871,29 +886,29 @@
│ │  
│ │  # -----------------------
│ │  # FOOOF
│ │  # -----------------------
│ │  
│ │  @process_io
│ │  def fooofspy_cF(trl_dat, foi=None, timeAxis=0,
│ │ -                output_fmt='fooof', fooof_settings=None, noCompute=False, chunkShape=None, method_kwargs=None):
│ │ +                output='fooof', fooof_settings=None, noCompute=False, chunkShape=None, method_kwargs=None):
│ │      """
│ │      Run FOOOF
│ │  
│ │      Parameters
│ │      ----------
│ │      trl_dat : 2D :class:`numpy.ndarray`
│ │          Uniformly sampled multi-channel time-series
│ │      foi : 1D :class:`numpy.ndarray`
│ │          Frequencies of interest  (Hz) for output. If desired frequencies
│ │          cannot be matched exactly the closest possible frequencies (respecting
│ │          data length and padding) are used.
│ │      timeAxis : int
│ │          Index of running time axis in `trl_dat` (0 or 1)
│ │ -    output_fmt : str
│ │ +    output : str
│ │          Output of FOOOF; one of :data:`~syncopy.specest.const_def.availableFOOOFOutputs`
│ │      fooof_settings: dict or None
│ │          Can contain keys `'in_freqs'` (the frequency axis for the data) and `'freq_range'` (post-processing range for fooofed spectrum).
│ │      noCompute : bool
│ │          Preprocessing flag. If `True`, do not perform actual calculation but
│ │          instead return expected shape and :class:`numpy.dtype` of output
│ │          array.
│ │ @@ -918,30 +933,39 @@
│ │      Consequently, this function does **not** perform any error checking and operates
│ │      under the assumption that all inputs have been externally validated and cross-checked.
│ │  
│ │      See also
│ │      --------
│ │      syncopy.freqanalysis : parent metafunction
│ │      """
│ │ +    if timeAxis != 0:
│ │ +        raise SPYValueError("timeaxis of input spectral data to be 0. Non-standard axes not supported with FOOOF.", actual=timeAxis)
│ │ +
│ │      outShape = trl_dat.shape
│ │      # For initialization of computational routine,
│ │      # just return output shape and dtype
│ │      if noCompute:
│ │          return outShape, spectralDTypes['pow']
│ │  
│ │ +
│ │ +
│ │      # Call actual fooof method
│ │ -    res, _ = fooofspy(trl_dat[0, 0, :, :], in_freqs=fooof_settings['in_freqs'], freq_range=fooof_settings['freq_range'], out_type=output_fmt,
│ │ +    res, metadata = fooofspy(trl_dat[0, 0, :, :], in_freqs=fooof_settings['in_freqs'], freq_range=fooof_settings['freq_range'], out_type=output,
│ │                        fooof_opt=method_kwargs)
│ │  
│ │ -    # TODO (later): get the 'details' from the unused _ return
│ │ -    #  value and pass them on. This cannot be done right now due
│ │ -    #  to lack of support for several return values, see #140.
│ │ +    if 'settings_used' in metadata:
│ │ +        del metadata['settings_used']  # We like to keep this in the return value of the
│ │ +    # backend functions for now (the vast majority of unit tests rely on it), but
│ │ +    # nested dicts are not allowed in the additional return value of cFs, so we remove
│ │ +    # it before passing the return value on.
│ │ +
│ │ +    metadata = FooofSpy.encode_singletrial_metadata_fooof_for_hdf5(metadata)
│ │  
│ │      res = res[np.newaxis, np.newaxis, :, :]  # Re-add omitted axes.
│ │ -    return res
│ │ +    return res, metadata
│ │  
│ │  
│ │  class FooofSpy(ComputationalRoutine):
│ │      """
│ │      Compute class that checks parameters and adds metadata to output spectral data.
│ │  
│ │      Sub-class of :class:`~syncopy.shared.computational_routine.ComputationalRoutine`,
│ │ @@ -957,21 +981,104 @@
│ │  
│ │      # 1st argument,the data, gets omitted
│ │      valid_kws = list(signature(fooofspy).parameters.keys())[1:]
│ │      valid_kws += list(signature(fooofspy_cF).parameters.keys())[1:]
│ │      # hardcode some parameter names which got digested from the frontend
│ │      valid_kws += ["fooof_settings"]
│ │  
│ │ +    #: The keys available in the metadata returned by this function. These come from `fooof` and correspond
│ │ +    #: to the attributes of the `fooof.FOOOF` instance (with an `'_'` suffix in `fooof`, e.g., `aperiodic_params` corresponds
│ │ +    #: to `fooof.FOOOF.aperiodic_params_`).
│ │ +    #: Please
│ │ +    #: refer to the `FOOOF docs <https://fooof-tools.github.io/fooof/generated/fooof.FOOOF.html#fooof.FOOOF>`_
│ │ +    #: for the meanings.
│ │ +    metadata_keys = ('aperiodic_params', 'error', 'gaussian_params', 'n_peaks', 'peak_params', 'r_squared',)
│ │ +
│ │ +
│ │      # To attach metadata to the output of the CF
│ │      def process_metadata(self, data, out):
│ │  
│ │ +        SPYParallelLog(f"Fetching FOOOF output metadata from file '{out.filename}'.", loglevel="DEBUG")
│ │ +
│ │ +        # General-purpose loading of metadata.
│ │ +        mdata = metadata_from_hdf5_file(out.filename)
│ │ +
│ │ +        # Note that FOOOF never sees absolute trial indices if a selection was
│ │ +        # made in the call to `freqanalysis`, because the mtmfft run before will have
│ │ +        # consumed them. So the trial indices are always relative.
│ │ +
│ │ +        SPYParallelLog(f"Decoding FOOOF output metadata from HDF5 datastructures.", loglevel="DEBUG")
│ │ +
│ │ +        # Backend-specific post-processing. May or may not be needed, depending on what
│ │ +        # you need to do in the cF to fit the return values into hdf5.
│ │ +        out.metadata = metadata_nest(FooofSpy.decode_metadata_fooof_alltrials_from_hdf5(mdata))
│ │ +
│ │ +        SPYParallelLog(f"Copying recording information to output syncopy data instance.", loglevel="DEBUG")
│ │ +
│ │          # Some index gymnastics to get trial begin/end "samples"
│ │          if data.selection is not None:
│ │              chanSec = data.selection.channel
│ │          else:
│ │              chanSec = slice(None)
│ │  
│ │          # Attach remaining meta-data
│ │          out.samplerate = data.samplerate
│ │          out.channel = np.array(data.channel[chanSec])
│ │          out.freq = data.freq
│ │          out._trialdefinition = data._trialdefinition
│ │ +
│ │ +    @staticmethod
│ │ +    def encode_singletrial_metadata_fooof_for_hdf5(metadata_fooof_backend):
│ │ +        """ Reformat the gaussian and peak params for inclusion in the 2nd return value and hdf5 file.
│ │ +
│ │ +        For several channels, the number of peaks may differ, and thus we cannot simply
│ │ +        call something like `np.array(gaussian_params)` in that case, as that will create
│ │ +        an array of dtype 'object', which is not supported by hdf5. We could use one return
│ │ +        value (entry in the `'metadata_fooof_backend'` dict below) per channel to solve that, but in this
│ │ +        case, we decided to `vstack` the arrays instead. When extracting the data again
│ │ +        (in `process_metadata()`), we need to revert this. That is possible because we can
│ │ +        see from the `n_peaks` return value how many (and thus which) rows belong to
│ │ +        which channel.
│ │ +        """
│ │ +        metadata_fooof_backend['gaussian_params'] = np.vstack(metadata_fooof_backend['gaussian_params'])
│ │ +        metadata_fooof_backend['peak_params'] = np.vstack(metadata_fooof_backend['peak_params'])
│ │ +        return metadata_fooof_backend
│ │ +
│ │ +    @staticmethod
│ │ +    def decode_metadata_fooof_alltrials_from_hdf5(metadata_fooof_hdf5):
│ │ +        """This reverts the special packaging applied to the fooof backend
│ │ +        function return values to fit them into the hdf5 container.
│ │ +
│ │ +        In the case of FOOOF, we had to `np.vstack` the `gaussian_params`
│ │ +        and `peak_params`, and we now revert this.
│ │ +
│ │ +        Of course, you do not have to undo things if you are fine
│ │ +        with passing them to the frontend the way they are stored in the hdf5.
│ │ +
│ │ +        Keep in mind that this is not directly the inverse of the
│ │ +        function called in the cF, because:
│ │ +        - that function prepares data from a single backend function call,
│ │ +        while this function has to unpack the data from *all* cF function calls.
│ │ +        - the input metadata to this function is a standard dict that has already
│ │ +        been pre-processed by the general-purpose metadata extraction
│ │ +        function `metadata_from_hdf5_file()`.
│ │ +        """
│ │ +        for unique_attr_label, v in metadata_fooof_hdf5.items():
│ │ +            label, trial_idx, call_idx = decode_unique_md_label(unique_attr_label)
│ │ +            if label == "n_peaks":
│ │ +                n_peaks = v
│ │ +                SPYParallelLog(f"FOOOF detected {n_peaks} peaks in data of trial {trial_idx} call {call_idx}.", loglevel="DEBUG")
│ │ +                gaussian_params_out = list()
│ │ +                peak_params_out = list()
│ │ +                start_idx = 0
│ │ +                unique_attr_label_gaussian_params = encode_unique_md_label('gaussian_params', trial_idx, call_idx)
│ │ +                unique_attr_label_peak_params = encode_unique_md_label('peak_params', trial_idx, call_idx)
│ │ +                gaussian_params_in = metadata_fooof_hdf5[unique_attr_label_gaussian_params]
│ │ +                peak_params_in = metadata_fooof_hdf5[unique_attr_label_peak_params]
│ │ +                for trial_idx in range(len(n_peaks)):
│ │ +                    end_idx = start_idx + n_peaks[trial_idx]
│ │ +                    gaussian_params_out.append(gaussian_params_in[start_idx:end_idx, :])
│ │ +                    peak_params_out.append(peak_params_in[start_idx:end_idx, :])
│ │ +
│ │ +                metadata_fooof_hdf5[unique_attr_label_gaussian_params] = gaussian_params_out
│ │ +                metadata_fooof_hdf5[unique_attr_label_peak_params] = peak_params_out
│ │ +        return metadata_fooof_hdf5
│ │   --- esi-syncopy-2022.8/syncopy/specest/fooofspy.py
│ ├── +++ esi_syncopy-2023.3/syncopy/specest/fooofspy.py
│ │┄ Files 7% similar despite different names
│ │ @@ -4,14 +4,16 @@
│ │  #
│ │  #
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │  from fooof import FOOOF
│ │ +import logging
│ │ +import platform
│ │  
│ │  # Constants
│ │  available_fooof_out_types = ['fooof', 'fooof_aperiodic', 'fooof_peaks']
│ │  default_fooof_opt = {'peak_width_limits': (0.5, 12.0), 'max_n_peaks': np.inf,
│ │                       'min_peak_height': 0.0, 'peak_threshold': 2.0,
│ │                       'aperiodic_mode': 'fixed', 'verbose': False}
│ │  available_fooof_options = list(default_fooof_opt)
│ │ @@ -50,15 +52,15 @@
│ │      -------
│ │      Depends on the value of parameter ``'out_type'``.
│ │      out_spectra: 2D :class:`numpy.ndarray`
│ │          The fooofed spectrum (for out_type ``'fooof'``), the aperiodic part of the
│ │          spectrum (for ``'fooof_aperiodic'``) or the peaks (for ``'fooof_peaks'``).
│ │          Each row corresponds to a row in the input `data_arr`, i.e., a channel.
│ │          The data is in linear space.
│ │ -    details : dictionary
│ │ +    metadata : dictionary
│ │          Details on the model fit and settings used. Contains the following keys:
│ │              `aperiodic_params` 2D :class:`numpy.ndarray`, the aperiodoc parameters of the fits, in log10.
│ │              `gaussian_params` list of 2D nx3 :class:`numpy.ndarray`, the Gaussian parameters of the fits, in log10.
│ │                                Each column describes the mean, height and width of a Gaussian fit to a peak.
│ │              `peak_params` list of 2D xn3 :class:`numpy.ndarray`, the peak parameters (a modified version of the
│ │                            Gaussian parameters, see FOOOF docs) of the fits, in log10. Each column describes the
│ │                            mean, height over aperiodic and 2-sided width of a Gaussian fit to a peak.
│ │ @@ -69,15 +71,15 @@
│ │  
│ │      Examples
│ │      --------
│ │      Run fooof on a generated power spectrum:
│ │      >>> from syncopy.specest.fooofspy import fooofspy
│ │      >>> from fooof.sim.gen import gen_power_spectrum
│ │      >>> freqs, powers = gen_power_spectrum([3, 40], [1, 1], [[10, 0.2, 1.25], [30, 0.15, 2]])
│ │ -    >>> spectra, details = fooofspy(powers, freqs, out_type='fooof')
│ │ +    >>> spectra, metadata = fooofspy(powers, freqs, out_type='fooof')
│ │  
│ │      References
│ │      -----
│ │      Donoghue T, Haller M, Peterson EJ, Varma P, Sebastian P, Gao R, Noto T, Lara AH, Wallis JD,
│ │      Knight RT, Shestyuk A, & Voytek B (2020). Parameterizing neural power spectra into periodic
│ │      and aperiodic components. Nature Neuroscience, 23, 1655-1665.
│ │      DOI: 10.1038/s41593-020-00744-x
│ │ @@ -90,14 +92,17 @@
│ │          fooof_opt = default_fooof_opt
│ │      else:
│ │          fooof_opt = {**default_fooof_opt, **fooof_opt}
│ │  
│ │      if in_freqs is None:
│ │          raise ValueError('infreqs: The input frequencies are required and must not be None.')
│ │  
│ │ +    logger = logging.getLogger("syncopy_" + platform.node())
│ │ +    logger.debug(f"Running FOOOF backend function on data chunk with shape {data_arr.shape}.")
│ │ +
│ │      invalid_fooof_opts = [i for i in fooof_opt.keys() if i not in available_fooof_options]
│ │      if invalid_fooof_opts:
│ │          raise ValueError("fooof_opt: invalid keys: '{inv}', allowed keys are: '{lgl}'.".format(inv=invalid_fooof_opts, lgl=fooof_opt.keys()))
│ │  
│ │      if out_type not in available_fooof_out_types:
│ │          raise ValueError("out_type: invalid value '{inv}', expected one of '{lgl}'.".format(inv=out_type, lgl=available_fooof_out_types))
│ │  
│ │ @@ -153,10 +158,19 @@
│ │          n_peaks[channel_idx] = fm.n_peaks_
│ │          r_squared[channel_idx] = fm.r_squared_
│ │          error[channel_idx] = fm.error_
│ │          gaussian_params.append(fm.gaussian_params_)
│ │          peak_params.append(fm.peak_params_)
│ │  
│ │      settings_used = {'fooof_opt': fooof_opt, 'out_type': out_type, 'freq_range': freq_range}
│ │ -    details = {'aperiodic_params': aperiodic_params, 'gaussian_params': gaussian_params, 'peak_params': peak_params, 'n_peaks': n_peaks, 'r_squared': r_squared, 'error': error, 'settings_used': settings_used}
│ │ +    #  Note: we add the 'settings_used' here in the backend, but they get stripped in the middle layer
│ │ +    #       (in the 'compRoutines.py/fooofspy_cF()'), so they do not reach the frontend.
│ │ +    #        The reason for removing them there is that we/h5py do not support nested dicts as
│ │ +    #        dataset/group attributes, and thus we cannot encode them in hdf5. We could work around
│ │ +    #        that, but due to our log, we do not really need to.
│ │ +    #        Returning them from here still has the benefit that we can test for them in backend tests.
│ │ +    metadata = {'aperiodic_params': aperiodic_params, 'gaussian_params': gaussian_params,
│ │ +               'peak_params': peak_params, 'n_peaks': n_peaks, 'r_squared': r_squared,
│ │ +               'error': error, 'settings_used': settings_used}
│ │ +
│ │ +    return out_spectra, metadata
│ │  
│ │ -    return out_spectra, details
│ │   --- esi-syncopy-2022.8/syncopy/specest/freqanalysis.py
│ ├── +++ esi_syncopy-2023.3/syncopy/specest/freqanalysis.py
│ │┄ Files 6% similar despite different names
│ │ @@ -6,19 +6,20 @@
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │  
│ │  # Syncopy imports
│ │  from syncopy.shared.parsers import data_parser, scalar_parser, array_parser
│ │  from syncopy.shared.tools import get_defaults, get_frontend_cfg
│ │  from syncopy.datatype import SpectralData
│ │ -from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYWarning, SPYInfo
│ │ +from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYWarning, SPYInfo, SPYLog
│ │  from syncopy.shared.kwarg_decorators import (unwrap_cfg, unwrap_select,
│ │                                               detect_parallel_client)
│ │  from syncopy.shared.tools import best_match
│ │  from syncopy.shared.const_def import spectralConversions
│ │ +import syncopy as spy
│ │  
│ │  from syncopy.shared.input_processors import (
│ │      process_taper,
│ │      process_foi,
│ │      process_padding,
│ │      check_effective_parameters,
│ │      check_passed_kwargs
│ │ @@ -39,27 +40,27 @@
│ │      MultiTaperFFTConvol,
│ │      FooofSpy
│ │  )
│ │  
│ │  availableFooofOutputs = ['fooof', 'fooof_aperiodic', 'fooof_peaks']
│ │  availableOutputs = tuple(spectralConversions.keys())
│ │  availableWavelets = ("Morlet", "Paul", "DOG", "Ricker", "Marr", "Mexican_hat")
│ │ -availableMethods = ("mtmfft", "mtmconvol", "wavelet", "superlet")
│ │ +availableMethods = ("mtmfft", "mtmconvol", "wavelet", "superlet", "welch")
│ │  
│ │  
│ │  @unwrap_cfg
│ │  @unwrap_select
│ │  @detect_parallel_client
│ │  def freqanalysis(data, method='mtmfft', output='pow',
│ │                   keeptrials=True, foi=None, foilim=None,
│ │ -                 pad='maxperlen', polyremoval=0, taper="hann",
│ │ +                 pad='maxperlen', polyremoval=0, taper="hann", demean_taper=False,
│ │                   taper_opt=None, tapsmofrq=None, nTaper=None, keeptapers=False,
│ │                   toi="all", t_ftimwin=None, wavelet="Morlet", width=6, order=None,
│ │                   order_max=None, order_min=1, c_1=3, adaptive=False,
│ │ -                 out=None, fooof_opt=None, **kwargs):
│ │ +                 out=None, fooof_opt=None, ft_compat=False, **kwargs):
│ │      """
│ │      Perform (time-)frequency analysis of Syncopy :class:`~syncopy.AnalogData` objects
│ │  
│ │      **Usage Summary**
│ │  
│ │      Options available in all analysis methods:
│ │  
│ │ @@ -80,18 +81,18 @@
│ │  
│ │          * **taper** : one of :data:`~syncopy.shared.const_def.availableTapers`
│ │          * **tapsmofrq** : spectral smoothing box for slepian tapers (in Hz)
│ │          * **nTaper** : number of orthogonal tapers for slepian tapers
│ │          * **keeptapers** : return individual tapers or average
│ │          * **pad**: either pad to an absolute length or set to `'nextpow2'`
│ │  
│ │ -        Post-processing of the resulting spectra with FOOOOF is available
│ │ +        Post-processing of the resulting spectra with FOOOF is available
│ │          via setting `output` to one of `'fooof'`, `'fooof_aperiodic'` or
│ │          `'fooof_peaks'`, see below for details. The returned spectrum represents
│ │ -        the full foofed spectrum for `'fooof'`, the aperiodic
│ │ +        the full fooofed spectrum for `'fooof'`, the aperiodic
│ │          fit for `'fooof_aperiodic'`, and the peaks (Gaussians fit to them) for
│ │          `'fooof_peaks'`. Returned data is in linear scale. Noisy input
│ │          data will most likely lead to fitting issues with fooof, always inspect
│ │          your results!
│ │  
│ │      "mtmconvol" : (Multi-)tapered sliding window Fourier transform
│ │          Perform time-frequency analysis on time-series trial data based on a sliding
│ │ @@ -104,14 +105,29 @@
│ │          * **keeptapers** : return individual tapers or average
│ │          * **toi** : time-points of interest; can be either an array representing
│ │            analysis window centroids (in sec), a scalar between 0 and 1 encoding
│ │            the percentage of overlap between adjacent windows or "all" to center
│ │            a window on every sample in the data.
│ │          * **t_ftimwin** : sliding window length (in sec)
│ │  
│ │ +    "welch" : Welch's method for the estimation of power spectra based on
│ │ +        time-averaging over short, modified periodograms. Here, *modified* means that
│ │ +        a taper is applied.
│ │ +        See [Welch1967]_ for details.
│ │ +
│ │ +        * **toi** : time-points of interest; a scalar between 0 and 1 encoding
│ │ +          the percentage of overlap between adjacent windows.
│ │ +        * **t_ftimwin** : sliding window length (in sec)
│ │ +        * **taper** : one of :data:`~syncopy.shared.const_def.availableTapers`
│ │ +        * **tapsmofrq** : spectral smoothing box for slepian tapers (in Hz)
│ │ +        * **nTaper** : number of orthogonal tapers for slepian tapers
│ │ +        * **keeptapers** : must be `False` with Welch. For multi-tapering,
│ │ +          taper averaging happens as part of the modified periodogram computation,
│ │ +           i.e., before the window averaging performed by Welch.
│ │ +
│ │      "wavelet" : (Continuous non-orthogonal) wavelet transform
│ │          Perform time-frequency analysis on time-series trial data using a non-orthogonal
│ │          continuous wavelet transform.
│ │  
│ │          * **wavelet** : one of :data:`~syncopy.specest.freqanalysis.availableWavelets`
│ │          * **toi** : time-points of interest; can be either an array representing
│ │            time points (in sec) or "all"(pre-trimming and subsampling of results)
│ │ @@ -159,15 +175,15 @@
│ │          but may be unbounded (e.g., ``[-np.inf, 60.5]`` is valid). Edges `fmin`
│ │          and `fmax` are included in the selection. If `foilim` is `None` or
│ │          ``foilim = "all"``, all frequencies are selected.
│ │      pad : 'maxperlen', float or 'nextpow2'
│ │          For the default `maxperlen`, no padding is performed in case of equal
│ │          length trials, while trials of varying lengths are padded to match the
│ │          longest trial. If `pad` is a number all trials are padded so that `pad` indicates
│ │ -        the absolute length of all trials after padding (in seconds). For instance
│ │ +        the absolute length of all trials after padding in seconds. For instance
│ │          ``pad = 2`` pads all trials to an absolute length of 2000 samples, if and
│ │          only if the longest trial contains at maximum 2000 samples and the
│ │          samplerate is 1kHz. If `pad` is `'nextpow2'` all trials are padded to the
│ │          nearest power of two (in samples) of the longest trial.
│ │      polyremoval : int or None
│ │          Order of polynomial used for de-trending data in the time domain prior
│ │          to spectral analysis. A value of 0 corresponds to subtracting the mean
│ │ @@ -181,17 +197,20 @@
│ │          Enables multi-tapering and sets the amount of one-sided spectral
│ │          smoothing with slepian tapers in Hz.
│ │      nTaper : int or None
│ │          Only valid if `method` is `'mtmfft'` or `'mtmconvol'` and `tapsmofrq` is set.
│ │          Number of orthogonal tapers to use for multi-tapering. It is not recommended to set the number
│ │          of tapers manually! Leave at `None` for the optimal number to be set automatically.
│ │      taper : str or None, optional
│ │ -        Only valid if `method` is `'mtmfft'` or `'mtmconvol'`. Windowing function,
│ │ +        Only valid if ``method`` is ``'mtmfft'`` or ``'mtmconvol'``. Windowing function,
│ │          one of :data:`~syncopy.shared.const_def.availableTapers`
│ │          For multi-tapering with slepian tapers use `tapsmofrq` directly.
│ │ +    demean_taper : bool
│ │ +        Set to `True` to perform de-meaning after tapering. Recommended for later Granger
│ │ +        analysis with :func:`~syncopy.connectivityanalysis`. Only valid for ``method='mtmfft'``.
│ │      taper_opt : dict or None
│ │          Dictionary with keys for additional taper parameters.
│ │          For example :func:`~scipy.signal.windows.kaiser` has
│ │          the additional parameter 'beta'. For multi-tapering use `tapsmofrq` directly.
│ │      keeptapers : bool
│ │          Only valid if `method` is `'mtmfft'` or `'mtmconvol'` and multi-tapering enabled
│ │          via  setting `tapsmofrq`.
│ │ @@ -206,15 +225,15 @@
│ │          valid if `method` is `'mtmconvol'`).
│ │          If `toi` is an array it explicitly selects the centroids of analysis
│ │          windows (in seconds), if `toi` is `"all"`, analysis windows are centered
│ │          on all samples in the data for `method="mtmconvol"`. For wavelet based
│ │          methods (`"wavelet"` or `"superlet"`) toi needs to be either an
│ │          equidistant array of time points or "all".
│ │      t_ftimwin : positive float
│ │ -        Only valid if `method` is `'mtmconvol'`. Sliding window length (in seconds).
│ │ +        Only valid if `method` is `'mtmconvol'` or `'welch'`. Sliding window length (in seconds).
│ │      wavelet : str
│ │          Only valid if `method` is `'wavelet'`. Wavelet function to use, one of
│ │          :data:`~syncopy.specest.freqanalysis.availableWavelets` (see below).
│ │      width : positive float
│ │          Only valid if `method` is `'wavelet'` and `wavelet` is `'Morlet'`. Nondimensional
│ │          frequency constant of Morlet wavelet function. This number should be >= 6,
│ │          which corresponds to 6 cycles within the analysis window to ensure sufficient
│ │ @@ -260,54 +279,82 @@
│ │          `'fooof'`, `'fooof_aperiodic'`, or `'fooof_peaks'`.
│ │          Additional keyword arguments passed to the `FOOOF` constructor. Available
│ │          arguments include ``'peak_width_limits'``, ``'max_n_peaks'``, ``'min_peak_height'``,
│ │          ``'peak_threshold'``, and ``'aperiodic_mode'``.
│ │          Please refer to the
│ │          `FOOOF docs <https://fooof-tools.github.io/fooof/generated/fooof.FOOOF.html#fooof.FOOOF>`_
│ │          for the meanings and the defaults.
│ │ -        The FOOOF reference is: Donoghue et al. 2020, DOI 10.1038/s41593-020-00744-x.
│ │ -    out : Must be `None`.
│ │ -
│ │ +        See the FOOOF reference [Donoghue2020]_ for details.
│ │ +    ft_compat : bool, optional
│ │ +        Set to `True` to use Field Trip's spectral normalization for FFT based methods
│ │ +        (``method='mtmfft'`` and ``method='mtmconvol'``). So spectral power is NOT
│ │ +        independent of the padding size!
│ │  
│ │      Returns
│ │      -------
│ │      spec : :class:`~syncopy.SpectralData`
│ │ -        (Time-)frequency spectrum of input data
│ │ +        (Time-)frequency spectrum of input data. The `spec` may contain additional metadata,
│ │ +        based on the `method` used to compute it:
│ │ +
│ │ +        * For `method='mtmfft'` when `output` is one of
│ │ +          `'fooof'`, `'fooof_aperiodic'`, or `'fooof_peaks'`, the `spec.metadata` property contains
│ │ +          the keys listed and explained in :data:`~syncopy.specest.compRoutines.FooofSpy.metadata_keys`.
│ │ +
│ │  
│ │      Notes
│ │      -----
│ │      .. [Moca2021] Moca, Vasile V., et al. "Time-frequency super-resolution with superlets."
│ │         Nature communications 12.1 (2021): 1-18.
│ │ +    .. [Donoghue2020] Donoghue et al. 2020, DOI 10.1038/s41593-020-00744-x.
│ │ +    .. [Welch1967] Welch. "The use of fast Fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms.", 1976, DOI 10.1109/TAU.1967.1161901
│ │  
│ │      **Options**
│ │  
│ │      .. autodata:: syncopy.specest.freqanalysis.availableMethods
│ │  
│ │      .. autodata:: syncopy.specest.freqanalysis.availableOutputs
│ │  
│ │      .. autodata:: syncopy.shared.const_def.availableTapers
│ │  
│ │      .. autodata:: syncopy.specest.freqanalysis.availableWavelets
│ │  
│ │      Examples
│ │      --------
│ │ -    Coming soon...
│ │  
│ │ +    Generate 10 seconds of white noise, sampled at 1000 Hz:
│ │  
│ │ +    >>> import syncopy.tests.synth_data as synth_data
│ │ +    >>> wn = synth_data.white_noise(nTrials=2, nChannels=3, nSamples=10000, samplerate=1000)
│ │ +
│ │ +    Configure Welch's method to estimate the power spectral density with a window length of 0.25 seconds
│ │ +    and an overlap of 50 percent between the windows:
│ │ +
│ │ +    >>> cfg = spy.get_defaults(spy.freqanalysis)
│ │ +    >>> cfg.method = "welch"
│ │ +    >>> cfg.t_ftimwin = 0.25  # Window length in seconds.
│ │ +    >>> cfg.toi = 0.5         # Overlap between periodograms (0.5 = 50 percent overlap).
│ │ +
│ │ +    Run Welch:
│ │ +
│ │ +    >>> psd = spy.freqanalysis(cfg, wn)
│ │ +
│ │ +    Visualize the result for the first trial:
│ │ +
│ │ +    >>> _, ax = res.singlepanelplot(trials=0, logscale=False)
│ │ +    >>> ax.set_title("Welch result")
│ │  
│ │      See also
│ │      --------
│ │      syncopy.specest.mtmfft.mtmfft : (multi-)tapered Fourier transform of multi-channel time series data
│ │      syncopy.specest.mtmconvol.mtmconvol : time-frequency analysis of multi-channel time series data with a sliding window FFT
│ │      syncopy.specest.wavelet.wavelet : time-frequency analysis of multi-channel time series data using a wavelet transform
│ │      syncopy.specest.fooofspy.fooofspy : parameterization of neural power spectra with the 'fitting oscillations & one over f' method
│ │      numpy.fft.fft : NumPy's reference FFT implementation
│ │      scipy.signal.stft : SciPy's Short Time Fourier Transform
│ │      """
│ │ -
│ │      # Make sure our one mandatory input object can be processed
│ │      try:
│ │          data_parser(data, varname="data", dataclass="AnalogData",
│ │                      writable=None, empty=False)
│ │      except Exception as exc:
│ │          raise exc
│ │      timeAxis = data.dimord.index("time")
│ │ @@ -340,27 +387,27 @@
│ │      # Ensure a valid output format was selected
│ │      valid_outputs = spectralConversions.keys()
│ │      if output not in valid_outputs:
│ │          lgl = "'" + "or '".join(opt + "' " for opt in valid_outputs)
│ │          raise SPYValueError(legal=lgl, varname="output", actual=output)
│ │  
│ │      # Parse all Boolean keyword arguments
│ │ -    for vname in ["keeptrials", "keeptapers"]:
│ │ +    for vname in ["keeptrials", "keeptapers", "demean_taper", "ft_compat"]:
│ │          if not isinstance(lcls[vname], bool):
│ │              raise SPYTypeError(lcls[vname], varname=vname, expected="Bool")
│ │  
│ │      # If only a subset of `data` is to be processed, make some necessary adjustments
│ │      # of the sampleinfo and trial lengths
│ │      if data.selection is not None:
│ │          # Refuse to go ahead with active time selection and provided `toi` on top`
│ │          if any(tsel != slice(None) for tsel in data.selection.time) and isinstance(toi, (np.ndarray, list)):
│ │              lgl = "no `toi` specification due to active in-place time-selection in input dataset"
│ │              raise SPYValueError(legal=lgl, varname="toi", actual=toi)
│ │          sinfo = data.selection.trialdefinition[:, :2]
│ │ -        trialList = data.selection.trials
│ │ +        trialList = data.selection.trial_ids
│ │      else:
│ │          trialList = list(range(len(data.trials)))
│ │          sinfo = data.sampleinfo
│ │      lenTrials = np.diff(sinfo).squeeze()
│ │      if not lenTrials.shape:
│ │          lenTrials = lenTrials[None]
│ │      numTrials = len(trialList)
│ │ @@ -368,16 +415,16 @@
│ │      # check polyremoval
│ │      if polyremoval is not None:
│ │          scalar_parser(polyremoval, varname="polyremoval", ntype="int_like", lims=[0, 1])
│ │  
│ │      # --- Padding ---
│ │  
│ │      # Sliding window FFT does not support "fancy" padding
│ │ -    if method == "mtmconvol" and isinstance(pad, str):
│ │ -        msg = "method 'mtmconvol' only supports in-place padding for windows " +\
│ │ +    if method in ["mtmconvol", "welch"] and isinstance(pad, str) and pad != defaults['pad']:
│ │ +        msg = "methods 'mtmconvol' and 'welch' only support in-place padding for windows " +\
│ │              "exceeding trial boundaries. Your choice of `pad = '{}'` will be ignored. "
│ │          SPYWarning(msg.format(pad))
│ │  
│ │      if method == 'mtmfft':
│ │          # the actual number of samples in case of later padding
│ │          minSampleNum = process_padding(pad, lenTrials, data.samplerate)
│ │      else:
│ │ @@ -403,20 +450,24 @@
│ │      log_dct = {"method": method,
│ │                 "output": output_fooof if is_fooof else output,
│ │                 "keeptapers": keeptapers,
│ │                 "keeptrials": keeptrials,
│ │                 "polyremoval": polyremoval,
│ │                 "pad": pad}
│ │  
│ │ +    SPYLog(f"Running specest method '{method}'.", loglevel="DEBUG")
│ │ +
│ │      # --------------------------------
│ │      # 1st: Check time-frequency inputs
│ │      # to prepare/sanitize `toi`
│ │      # --------------------------------
│ │  
│ │ -    if method in ["mtmconvol", "wavelet", "superlet"]:
│ │ +
│ │ +
│ │ +    if method in ["mtmconvol", "wavelet", "superlet", "welch"]:
│ │  
│ │          # Get start/end timing info respecting potential in-place selection
│ │          if toi is None:
│ │              raise SPYTypeError(toi, varname="toi", expected="scalar or array-like or 'all'")
│ │          if data.selection is not None:
│ │              tStart = data.selection.trialdefinition[:, 2] / data.samplerate
│ │          else:
│ │ @@ -476,28 +527,36 @@
│ │          log_dct["toi"] = lcls["toi"]
│ │  
│ │      # --------------------------------------------
│ │      # Check options specific to mtm*-methods
│ │      # (particularly tapers and foi/freqs alignment)
│ │      # --------------------------------------------
│ │  
│ │ -    if "mtm" in method:
│ │ +    if "mtm" in method or method == "welch":
│ │  
│ │ -        if method == "mtmconvol":
│ │ +        if method in ["mtmconvol", "welch"]:
│ │              # get the sliding window size
│ │              try:
│ │                  scalar_parser(t_ftimwin, varname="t_ftimwin",
│ │                                lims=[dt, minTrialLength])
│ │              except Exception as exc:
│ │                  SPYInfo("Please specify 't_ftimwin' parameter.. exiting!")
│ │                  raise exc
│ │  
│ │              # this is the effective sliding window FFT sample size
│ │              minSampleNum = int(t_ftimwin * data.samplerate)
│ │  
│ │ +            if method == "welch":
│ │ +                if keeptapers:
│ │ +                    raise SPYValueError(legal="keeptapers='False' with method='welch'", varname="keeptapers", actual=keeptapers)
│ │ +
│ │ +                if output != "pow":
│ │ +                    raise SPYValueError(legal="output='pow' with method='welch'", varname="output", actual=output)
│ │ +
│ │ +
│ │          # Construct array of maximally attainable frequencies
│ │          freqs = np.fft.rfftfreq(minSampleNum, dt)
│ │  
│ │          # Match desired frequencies as close as possible to
│ │          # actually attainable freqs
│ │          # these are the frequencies attached to the SpectralData by the CR!
│ │          if foi is not None:
│ │ @@ -545,61 +604,64 @@
│ │          check_effective_parameters(MultiTaperFFT, defaults, lcls)
│ │  
│ │          # method specific parameters
│ │          method_kwargs = {
│ │              'samplerate': data.samplerate,
│ │              'taper': taper,
│ │              'taper_opt': taper_opt,
│ │ -            'nSamples': minSampleNum
│ │ +            'nSamples': minSampleNum,
│ │ +            'demean_taper': demean_taper,
│ │ +            'ft_compat': ft_compat
│ │          }
│ │  
│ │          # Set up compute-class
│ │          specestMethod = MultiTaperFFT(
│ │              foi=foi,
│ │              timeAxis=timeAxis,
│ │              keeptapers=keeptapers,
│ │              polyremoval=polyremoval,
│ │ -            output_fmt=output,
│ │ +            output=output,
│ │              method_kwargs=method_kwargs)
│ │  
│ │ -    elif method == "mtmconvol":
│ │ +    elif method in ["mtmconvol", "welch"]:
│ │  
│ │          check_effective_parameters(MultiTaperFFTConvol, defaults, lcls)
│ │  
│ │          # Process `toi` for sliding window multi taper fft,
│ │          # we have to account for three scenarios: (1) center sliding
│ │          # windows on all samples in (selected) trials (2) `toi` was provided as
│ │          # percentage indicating the degree of overlap b/w time-windows and (3) a set
│ │          # of discrete time points was provided. These three cases are encoded in
│ │          # `overlap, i.e., ``overlap > 1` => all, `0 < overlap < 1` => percentage,
│ │          # `overlap < 0` => discrete `toi`
│ │  
│ │          # overlap = None
│ │          if isinstance(toi, str):
│ │ +            if method == "welch":
│ │ +                lgl = "toi to be a float in range [0, 1] for method='welch'"
│ │ +                raise SPYValueError(legal=lgl, varname="toi", actual=toi)
│ │              if toi != "all":
│ │                  lgl = "`toi = 'all'` to center analysis windows on all time-points"
│ │                  raise SPYValueError(legal=lgl, varname="toi", actual=toi)
│ │              equidistant = True
│ │              overlap = np.inf
│ │  
│ │          elif np.issubdtype(type(toi), np.number):
│ │ -            try:
│ │ -                scalar_parser(toi, varname="toi", lims=[0, 1])
│ │ -            except Exception as exc:
│ │ -                raise exc
│ │ +            scalar_parser(toi, varname="toi", lims=[0, 1])
│ │              overlap = toi
│ │              equidistant = True
│ │          # this captures all other cases, e.i. toi is of sequence type
│ │          else:
│ │ +            if method == "welch":
│ │ +                lgl = "toi to be a float in range [0, 1] for method='welch'"
│ │ +                raise SPYValueError(legal=lgl, varname="toi", actual=toi)
│ │ +
│ │              overlap = -1
│ │ -            try:
│ │ -                array_parser(toi, varname="toi", hasinf=False, hasnan=False,
│ │ +            array_parser(toi, varname="toi", hasinf=False, hasnan=False,
│ │                               lims=[tStart.min(), tEnd.max()], dims=(None,))
│ │ -            except Exception as exc:
│ │ -                raise exc
│ │              toi = np.array(toi)
│ │              tSteps = np.diff(toi)
│ │              if (tSteps < 0).any():
│ │                  lgl = "ordered list/array of time-points"
│ │                  act = "unsorted list/array"
│ │                  raise SPYValueError(legal=lgl, varname="toi", actual=act)
│ │              # Account for round-off errors: if toi spacing is almost at sample interval
│ │ @@ -694,15 +756,15 @@
│ │              postSelect,
│ │              equidistant=equidistant,
│ │              toi=toi,
│ │              foi=foi,
│ │              timeAxis=timeAxis,
│ │              keeptapers=keeptapers,
│ │              polyremoval=polyremoval,
│ │ -            output_fmt=output,
│ │ +            output=output,
│ │              method_kwargs=method_kwargs)
│ │  
│ │      elif method == "wavelet":
│ │  
│ │          check_effective_parameters(WaveletTransform, defaults, lcls)
│ │  
│ │          # Check wavelet selection
│ │ @@ -779,15 +841,15 @@
│ │          # Set up compute-class
│ │          specestMethod = WaveletTransform(
│ │              preSelect,
│ │              postSelect,
│ │              toi=toi,
│ │              timeAxis=timeAxis,
│ │              polyremoval=polyremoval,
│ │ -            output_fmt=output,
│ │ +            output=output,
│ │              method_kwargs=method_kwargs)
│ │  
│ │      elif method == "superlet":
│ │  
│ │          check_effective_parameters(SuperletTransform, defaults, lcls)
│ │  
│ │          # check and parse superlet specific arguments
│ │ @@ -858,15 +920,15 @@
│ │          # Set up compute-class
│ │          specestMethod = SuperletTransform(
│ │              preSelect,
│ │              postSelect,
│ │              toi=toi,
│ │              timeAxis=timeAxis,
│ │              polyremoval=polyremoval,
│ │ -            output_fmt=output,
│ │ +            output=output,
│ │              method_kwargs=method_kwargs)
│ │  
│ │      # -------------------------------------------------
│ │      # Sanitize output and call the ComputationalRoutine
│ │      # -------------------------------------------------
│ │  
│ │      out = SpectralData(dimord=SpectralData._defaultDimord)
│ │ @@ -887,48 +949,53 @@
│ │          fooof_out = SpectralData(dimord=SpectralData._defaultDimord)
│ │  
│ │          # method specific parameters
│ │          if fooof_opt is None:
│ │              fooof_opt = default_fooof_opt
│ │  
│ │          # These go into the FOOOF constructor, so we keep them separate from the fooof_settings below.
│ │ -        fooof_kwargs = {**default_fooof_opt, **fooof_opt}  # Join the ones from fooof_opt (the user) into fooof_kwargs.
│ │ +        fooof_kwargs = {**default_fooof_opt, **fooof_opt}  # Join the ones from fooof_opt (the user) into the default fooof_kwargs.
│ │  
│ │          # Settings used during the FOOOF analysis (that are NOT passed to FOOOF constructor).
│ │          # The user cannot influence these: in_freqs is derived from mtmfft output, freq_range is always None (=full mtmfft output spectrum).
│ │          # We still define them here, and they are passed through to the backend and actually used there.
│ │          fooof_settings = {
│ │              'in_freqs': fooof_data.freq,
│ │              'freq_range': None  # or something like [2, 40] to limit frequency range (post processing). Currently not exposed to user.
│ │          }
│ │  
│ │          if fooof_data.freq[0] == 0:
│ │              # FOOOF does not work with input frequency zero in the data.
│ │              raise SPYValueError(legal="a frequency range that does not include zero. Use 'foi' or 'foilim' to restrict.", varname="foi/foilim", actual="Frequency range from {} to {}.".format(min(fooof_data.freq), max(fooof_data.freq)))
│ │  
│ │          # Set up compute-class
│ │ -        #  - the output_fmt must be one of 'fooof', 'fooof_aperiodic',
│ │ +        #  - the output must be one of 'fooof', 'fooof_aperiodic',
│ │          #    or 'fooof_peaks'.
│ │          #  - everything passed as method_kwargs is passed as arguments
│ │ -        #    to the foooof.FOOOF() constructor or functions, the other args are
│ │ +        #    to the fooof.FOOOF() constructor or functions, the other args are
│ │          #    used elsewhere.
│ │ -        fooofMethod = FooofSpy(output_fmt=output_fooof, fooof_settings=fooof_settings, method_kwargs=fooof_kwargs)
│ │ +        fooofMethod = FooofSpy(output=output_fooof, fooof_settings=fooof_settings, method_kwargs=fooof_kwargs)
│ │  
│ │          # Update `log_dct` w/method-specific options
│ │          log_dct["fooof_method"] = output_fooof
│ │          log_dct["fooof_opt"] = fooof_kwargs
│ │  
│ │          # Perform actual computation
│ │          fooofMethod.initialize(fooof_data,
│ │                                 fooof_out._stackingDim,
│ │                                 chan_per_worker=kwargs.get("chan_per_worker"),
│ │                                 keeptrials=keeptrials)
│ │          fooofMethod.compute(fooof_data, fooof_out, parallel=kwargs.get("parallel"), log_dict=log_dct)
│ │          out = fooof_out
│ │  
│ │ -     # attach potential older cfg's from the input
│ │ -    # to support chained frontend calls..
│ │ +    # Perform mtmconvolv post-processing for `method='welch'`.
│ │ +    if method == "welch":
│ │ +        welch_data = out
│ │ +        out = spy.mean(welch_data, dim='time')
│ │ +
│ │ +    # Attach potential older cfg's from the input
│ │ +    # to support chained frontend calls.
│ │      out.cfg.update(data.cfg)
│ │  
│ │ -    # attach frontend parameters for replay
│ │ +    # Attach frontend parameters for replay.
│ │      out.cfg.update({'freqanalysis': new_cfg})
│ │      return out
│ │   --- esi-syncopy-2022.8/syncopy/specest/mtmconvol.py
│ ├── +++ esi_syncopy-2023.3/syncopy/specest/mtmconvol.py
│ │┄ Files 7% similar despite different names
│ │ @@ -1,23 +1,25 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │  # Time-frequency analysis based on a short-time Fourier transform
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │ +import logging
│ │ +import platform
│ │  from scipy import signal
│ │  
│ │  # local imports
│ │  from .stft import stft
│ │  from ._norm_spec import _norm_taper
│ │  
│ │  
│ │  def mtmconvol(data_arr, samplerate, nperseg, noverlap=None, taper="hann",
│ │ -              taper_opt={}, boundary='zeros', padded=True, detrend=False):
│ │ +              taper_opt=None, boundary='zeros', padded=True, detrend=False):
│ │  
│ │      """
│ │      (Multi-)tapered short time fast Fourier transform. Returns
│ │      full complex Fourier transform for each taper.
│ │      Multi-tapering only supported with Slepian windwows (`taper="dpss"`).
│ │  
│ │      Parameters
│ │ @@ -31,15 +33,15 @@
│ │          Sliding window size in sample units
│ │      noverlap : int
│ │          Overlap between consecutive windows, set to ``nperseg - 1``
│ │          to cover the whole signal
│ │      taper : str or None
│ │          Taper function to use, one of `scipy.signal.windows`
│ │          Set to `None` for no tapering.
│ │ -    taper_opt : dict
│ │ +    taper_opt : dict or None
│ │          Additional keyword arguments passed to the `taper` function.
│ │          For multi-tapering with ``taper='dpss'`` set the keys
│ │          `'Kmax'` and `'NW'`.
│ │          For further details, please refer to the
│ │          `SciPy docs <https://docs.scipy.org/doc/scipy/reference/signal.windows.html>`_
│ │      boundary : str or None
│ │          Wether or not to auto-pad the signal such that a window is centered on each
│ │ @@ -85,14 +87,17 @@
│ │      dFreq = freqs[1] - freqs[0]
│ │  
│ │      if taper is None:
│ │          taper = 'boxcar'
│ │  
│ │      taper_func = getattr(signal.windows, taper)
│ │  
│ │ +    if taper_opt is None:
│ │ +        taper_opt = {}
│ │ +
│ │      # this parameter mitigates the sum-to-zero problem for the odd slepians
│ │      # as signal.stft has hardcoded scaling='spectrum'
│ │      # -> normalizes with win.sum() :/
│ │      # see also https://github.com/scipy/scipy/issues/14740
│ │      if taper == 'dpss':
│ │          taper_opt['sym'] = False
│ │  
│ │ @@ -110,14 +115,17 @@
│ │          # the signal is padded on each side as to cover
│ │          # the whole signal
│ │          nTime = int(np.ceil(nSamples / (nperseg - noverlap)))
│ │  
│ │      # Short time Fourier transforms (nTime x nTapers x nFreq x nChannels)
│ │      ftr = np.zeros((nTime, windows.shape[0], nFreq, nChannels), dtype='complex64')
│ │  
│ │ +    logger = logging.getLogger("syncopy_" + platform.node())
│ │ +    logger.debug(f"Running mtmconvol on {len(windows)} windows, data chunk has {nSamples} samples and {nChannels} channels.")
│ │ +
│ │      for taperIdx, win in enumerate(windows):
│ │          # ftr has shape (nFreq, nChannels, nTime)
│ │          pxx, _, _ = stft(data_arr, samplerate, window=win,
│ │                           nperseg=nperseg, noverlap=noverlap,
│ │                           boundary=boundary, padded=padded,
│ │                           axis=0, detrend=detrend)
│ │   --- esi-syncopy-2022.8/syncopy/specest/mtmfft.py
│ ├── +++ esi_syncopy-2023.3/syncopy/specest/mtmfft.py
│ │┄ Files 13% similar despite different names
│ │ @@ -2,25 +2,28 @@
│ │  #
│ │  # Spectral estimation with (multi-)tapered FFT
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │  from scipy import signal
│ │ +import logging
│ │ +import platform
│ │  
│ │  # local imports
│ │  from ._norm_spec import _norm_spec, _norm_taper
│ │  
│ │  
│ │  def mtmfft(data_arr,
│ │             samplerate,
│ │             nSamples=None,
│ │             taper="hann",
│ │             taper_opt=None,
│ │ -           demean_taper=False):
│ │ +           demean_taper=False,
│ │ +           ft_compat=False):
│ │      """
│ │      (Multi-)tapered fast Fourier transform. Returns
│ │      full complex Fourier transform for each taper.
│ │      Multi-tapering only supported with Slepian windwows (`taper="dpss"`).
│ │  
│ │      Parameters
│ │      ----------
│ │ @@ -39,14 +42,17 @@
│ │          Additional keyword arguments passed to the `taper` function.
│ │          For multi-tapering with ``taper='dpss'`` set the keys
│ │          `'Kmax'` and `'NW'`.
│ │          For further details, please refer to the
│ │          `SciPy docs <https://docs.scipy.org/doc/scipy/reference/signal.windows.html>`_
│ │      demean_taper : bool
│ │          Set to `True` to perform de-meaning after tapering
│ │ +    ft_compat : bool
│ │ +        Set to `True` to use Field Trip's normalization,
│ │ +        which is NOT independent of the padding size
│ │  
│ │      Returns
│ │      -------
│ │      ftr : 3D :class:`numpy.ndarray`
│ │           Complex output has shape ``(nTapers x nFreq x nChannels)``.
│ │      freqs : 1D :class:`numpy.ndarray`
│ │           Array of Fourier frequencies
│ │ @@ -91,29 +97,37 @@
│ │      windows = np.atleast_2d(taper_func(signal_length, **taper_opt))
│ │      # normalize window with total (after padding) length
│ │      windows = _norm_taper(taper, windows, nSamples)
│ │  
│ │      # Fourier transforms (nTapers x nFreq x nChannels)
│ │      ftr = np.zeros((windows.shape[0], nFreq, nChannels), dtype='complex64')
│ │  
│ │ +    logger = logging.getLogger("syncopy_" + platform.node())
│ │ +    logger.debug(f"Running mtmfft on {len(windows)} windows, data chunk has {nSamples} samples and {nChannels} channels.")
│ │ +
│ │      for taperIdx, win in enumerate(windows):
│ │          win = np.tile(win, (nChannels, 1)).T
│ │          win *= data_arr
│ │          # de-mean again after tapering - needed for Granger!
│ │          if demean_taper:
│ │              win -= win.mean(axis=0)
│ │          ftr[taperIdx] = np.fft.rfft(win, n=nSamples, axis=0)
│ │ -        # FT uses potentially padded length `nSamples`
│ │ -        ftr[taperIdx] = _norm_spec(ftr[taperIdx], nSamples, samplerate)
│ │ +        # FT uses potentially padded length `nSamples`, which dilutes the power
│ │ +        if ft_compat:            
│ │ +            ftr[taperIdx] = _norm_spec(ftr[taperIdx], nSamples, samplerate)
│ │ +        # here the normalization adapts such that padding is NOT changing power
│ │ +        else:
│ │ +            ftr[taperIdx] = _norm_spec(ftr[taperIdx], signal_length * np.sqrt(nSamples / signal_length), samplerate)
│ │  
│ │      return ftr, freqs
│ │  
│ │  
│ │  def _get_dpss_pars(tapsmofrq, nSamples, samplerate):
│ │  
│ │      """ Helper function to retrieve dpss parameters from tapsmofrq """
│ │  
│ │ +    # taper width parameter in sample units
│ │      NW = tapsmofrq * nSamples / samplerate
│ │      # from the minBw setting NW always is at least 1
│ │      Kmax = int(2 * NW - 1)  # optimal number of tapers
│ │  
│ │      return NW, Kmax
│ │   --- esi-syncopy-2022.8/syncopy/specest/stft.py
│ ├── +++ esi_syncopy-2023.3/syncopy/specest/stft.py
│ │┄ Files 3% similar despite different names
│ │ @@ -2,14 +2,16 @@
│ │  #
│ │  # Short-time Fourier transform, uses np.fft as backend
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │  import scipy.signal as sci_sig
│ │ +import logging
│ │ +import platform
│ │  
│ │  # local imports
│ │  from ._norm_spec import _norm_spec
│ │  
│ │  
│ │  def stft(dat,
│ │           fs=1.,
│ │ @@ -128,14 +130,18 @@
│ │      if detrend:
│ │          dat = sci_sig.detrend(dat, type=detrend, overwrite_data=True)
│ │  
│ │      if window is not None:
│ │          # Apply window by multiplication
│ │          dat = dat * window
│ │  
│ │ +    logger = logging.getLogger("syncopy_" + platform.node())
│ │ +    pad_status = "with padding" if padded else "without padding"
│ │ +    logger.debug(f"Running short time Fourier transform {pad_status}, detrend={detrend} and overlap of {noverlap}.")
│ │ +
│ │      times = np.arange(nperseg / 2, dat.shape[-1] - nperseg / 2 + 1,
│ │                        nperseg - noverlap) / fs
│ │      if boundary is not None:
│ │          times -= (nperseg / 2) / fs
│ │  
│ │      freqs = np.fft.rfftfreq(nperseg, 1 / fs)
│ │   --- esi-syncopy-2022.8/syncopy/specest/superlet.py
│ ├── +++ esi_syncopy-2023.3/syncopy/specest/superlet.py
│ │┄ Files 2% similar despite different names
│ │ @@ -3,14 +3,16 @@
│ │  # Time-frequency analysis with superlets
│ │  # Based on 'Time-frequency super-resolution with superlets'
│ │  # by Moca et al., 2021 Nature Communications
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │ +import logging
│ │ +import platform
│ │  from scipy.signal import fftconvolve
│ │  
│ │  
│ │  def superlet(
│ │      data_arr,
│ │      samplerate,
│ │      scales,
│ │ @@ -20,15 +22,15 @@
│ │      adaptive=False,
│ │  ):
│ │  
│ │      """
│ │      Performs Superlet Transform (SLT) according to Moca et al. [1]_
│ │      Both multiplicative SLT and fractional adaptive SLT are available.
│ │      The former is recommended for a narrow frequency band of interest,
│ │ -    whereas the  is better suited for the analysis of a broad range
│ │ +    whereas the latter is better suited for the analysis of a broad range
│ │      of frequencies.
│ │  
│ │      A superlet (SL) is a set of Morlet wavelets with increasing number
│ │      of cycles within the Gaussian envelope. Hence the bandwith
│ │      is constrained more and more with more cycles yielding a sharper
│ │      frequency resolution. Complementary the low cycle numbers will give a
│ │      high time resolution. The SLT then is the geometric mean
│ │ @@ -57,15 +59,15 @@
│ │          with the `c_1` parameter: ``c_min = c_1 * order_min``
│ │          Note that for admissability reasons c_min should be at least 3!
│ │      c_1 : int
│ │          Number of cycles of the base Morlet wavelet. If set to lower
│ │          than 3 increase `order_min` as to never have less than 3 cycles
│ │          in a wavelet!
│ │      adaptive : bool
│ │ -        Wether to perform multiplicative SLT or fractional adaptive SLT.
│ │ +        Whether to perform fractional adaptive SLT or multiplicative SLT.
│ │          If set to True, the order of the wavelet set will increase
│ │          linearly with the frequencies of interest from `order_min`
│ │          to `order_max`. If set to False the same SL will be used for
│ │          all frequencies.
│ │  
│ │      Returns
│ │      -------
│ │ @@ -76,28 +78,33 @@
│ │      Notes
│ │      -----
│ │      .. [1] Moca, Vasile V., et al. "Time-frequency super-resolution with superlets."
│ │         Nature communications 12.1 (2021): 1-18.
│ │  
│ │  
│ │      """
│ │ +    logger = logging.getLogger("syncopy_" + platform.node())
│ │  
│ │      # adaptive SLT
│ │      if adaptive:
│ │  
│ │ +        logger.debug(f"Running fractional adaptive superlet transform with order_min={order_min}, order_max={order_max} and c_1={c_1} on data with shape {data_arr.shape}.")
│ │ +
│ │          gmean_spec = FASLT(data_arr,
│ │                             samplerate,
│ │                             scales,
│ │                             order_max,
│ │                             order_min,
│ │                             c_1)
│ │  
│ │      # multiplicative SLT
│ │      else:
│ │  
│ │ +        logger.debug(f"Running multiplicative superlet transform with order_min={order_min}, order_max={order_max} and c_1={c_1} on data with shape {data_arr.shape}.")
│ │ +
│ │          gmean_spec = multiplicativeSLT(data_arr,
│ │                                         samplerate,
│ │                                         scales,
│ │                                         order_max,
│ │                                         order_min,
│ │                                         c_1)
│ │   --- esi-syncopy-2022.8/syncopy/specest/wavelet.py
│ ├── +++ esi_syncopy-2023.3/syncopy/specest/wavelet.py
│ │┄ Files 5% similar despite different names
│ │ @@ -1,14 +1,16 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │  # Time-frequency analysis with wavelets
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import numpy as np
│ │ +import logging
│ │ +import platform
│ │  
│ │  # Local imports
│ │  from syncopy.specest.wavelets import cwt
│ │  
│ │  
│ │  def wavelet(data_arr, samplerate, scales, wavelet):
│ │  
│ │ @@ -33,14 +35,17 @@
│ │      Returns
│ │      -------
│ │      spec : :class:`numpy.ndarray`
│ │          Complex time-frequency representation of the input data.
│ │          Shape is (len(scales),) + data_arr.shape
│ │      """
│ │  
│ │ +    logger = logging.getLogger("syncopy_" + platform.node())
│ │ +    logger.debug(f"Running wavelet transform on data with shape {data_arr.shape} and samplerate {samplerate}.")
│ │ +
│ │      spec = cwt(data_arr, wavelet=wavelet, widths=scales, dt=1 / samplerate, axis=0)
│ │  
│ │      return spec
│ │  
│ │  
│ │  def get_optimal_wavelet_scales(scale_from_period, nSamples, dt, dj=0.25, s0=None):
│ │      """
│ │   --- esi-syncopy-2022.8/syncopy/specest/wavelets/transform.py
│ ├── +++ esi_syncopy-2023.3/syncopy/specest/wavelets/transform.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/specest/wavelets/wavelets.py
│ ├── +++ esi_syncopy-2023.3/syncopy/specest/wavelets/wavelets.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/tests/backend/run_tests.sh
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/backend/run_tests.sh
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/tests/backend/test_conn.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/backend/test_conn.py
│ │┄ Files 6% similar despite different names
│ │ @@ -1,23 +1,23 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │ -# syncopy.nwanalysis backend method tests
│ │ +# syncopy.connectivity backend method tests
│ │  #
│ │  import numpy as np
│ │  import matplotlib.pyplot as ppl
│ │  
│ │  from syncopy.tests import synth_data
│ │ -from syncopy.nwanalysis import csd
│ │ -from syncopy.nwanalysis import ST_compRoutines as stCR
│ │ -from syncopy.nwanalysis.wilson_sf import (
│ │ +from syncopy.connectivity import csd
│ │ +from syncopy.connectivity import ST_compRoutines as stCR
│ │ +from syncopy.connectivity.wilson_sf import (
│ │      wilson_sf,
│ │      regularize_csd,
│ │      max_rel_err
│ │  )
│ │ -from syncopy.nwanalysis.granger import granger
│ │ +from syncopy.connectivity.granger import granger
│ │  
│ │  
│ │  def test_coherence():
│ │  
│ │      """
│ │      Tests the csd normalization to
│ │      arrive at the coherence given
│ │ @@ -44,16 +44,15 @@
│ │                     for ps in phase_shifts]
│ │          trl_dat = np.array(trl_dat).T
│ │          trl_dat = np.array(trl_dat) + np.random.randn(nSamples, len(phase_shifts))
│ │  
│ │          # process every trial individually
│ │          CSD, freqs = csd.csd(trl_dat, fs,
│ │                               taper='hann',
│ │ -                             norm=False, # this is important!
│ │ -                             fullOutput=True)
│ │ +                             norm=False)  # this is important!
│ │  
│ │          assert avCSD.shape == CSD.shape
│ │          avCSD += CSD
│ │  
│ │      # this is the trial average
│ │      avCSD /= nTrials
│ │  
│ │ @@ -65,15 +64,15 @@
│ │  
│ │      # coherence between channel 0 and 1
│ │      coh = Cij[:, 0, 1]
│ │  
│ │      fig, ax = ppl.subplots(figsize=(6, 4), num=None)
│ │      ax.set_xlabel('frequency (Hz)')
│ │      ax.set_ylabel('coherence')
│ │ -    ax.set_ylim((-.02,1.05))
│ │ +    ax.set_ylim((-.02, 1.05))
│ │      ax.set_title(f'{nTrials} trials averaged coherence,  SNR=1')
│ │  
│ │      ax.plot(freqs, coh, lw=1.5, alpha=0.8, c='cornflowerblue')
│ │  
│ │      # we test for the highest peak sitting at
│ │      # the vicinity (± 5Hz) of the harmonic
│ │      peak_val = np.max(coh)
│ │ @@ -108,33 +107,32 @@
│ │  
│ │      # 1 phase phase shifted harmonics + white noise, SNR = 1
│ │      data = [np.cos(harm_freq * 2 * np. pi * tvec + ps)
│ │              for ps in phase_shifts]
│ │      data = np.array(data).T
│ │      data = np.array(data) + np.random.randn(nSamples, len(phase_shifts))
│ │  
│ │ -    bw = 8 #Hz
│ │ +    bw = 8  # Hz
│ │      NW = nSamples * bw / (2 * fs)
│ │ -    Kmax = int(2 * NW - 1) # multiple tapers for single trial coherence
│ │ +    Kmax = int(2 * NW - 1)   # multiple tapers for single trial coherence
│ │      CSD, freqs = csd.csd(data, fs,
│ │                           taper='dpss',
│ │ -                         taper_opt={'Kmax' : Kmax, 'NW' : NW},
│ │ -                         norm=True,
│ │ -                         fullOutput=True)
│ │ +                         taper_opt={'Kmax': Kmax, 'NW': NW},
│ │ +                         norm=True)
│ │  
│ │ -    # output has shape (1, nFreq, nChannels, nChannels)
│ │ +    # output has shape (nFreq, nChannels, nChannels)
│ │      assert CSD.shape == (len(freqs), data.shape[1], data.shape[1])
│ │  
│ │      # single trial coherence between channel 0 and 1
│ │      coh = np.abs(CSD[:, 0, 1])
│ │  
│ │ -    fig, ax = ppl.subplots(figsize=(6,4), num=None)
│ │ +    fig, ax = ppl.subplots(figsize=(6, 4), num=None)
│ │      ax.set_xlabel('frequency (Hz)')
│ │      ax.set_ylabel('coherence')
│ │ -    ax.set_ylim((-.02,1.05))
│ │ +    ax.set_ylim((-.02, 1.05))
│ │      ax.set_title(f'MTM coherence, {Kmax} tapers, SNR=1')
│ │  
│ │      ax.plot(freqs, coh, lw=1.5, alpha=0.8, c='cornflowerblue')
│ │  
│ │      # we test for the highest peak sitting at
│ │      # the vicinity (± 5Hz) of one the harmonic
│ │      peak_val = np.max(coh)
│ │ @@ -189,27 +187,27 @@
│ │      fs = 200
│ │      nChannels = 2
│ │      nSamples = 1000
│ │      nTrials = 150
│ │      CSDav = np.zeros((nSamples // 2 + 1, nChannels, nChannels), dtype=np.complex64)
│ │      for _ in range(nTrials):
│ │  
│ │ -        sol = synth_data.AR2_network(nSamples=nSamples)
│ │ +        sol = synth_data.AR2_network(nSamples=nSamples, seed=None)
│ │          # --- get the (single trial) CSD ---
│ │  
│ │          CSD, freqs = csd.csd(sol, fs,
│ │ -                             norm=False,
│ │ -                             fullOutput=True)
│ │ +                             norm=False)
│ │ +
│ │          CSDav += CSD
│ │  
│ │      CSDav /= nTrials
│ │  
│ │      # --- factorize CSD with Wilson's algorithm ---
│ │  
│ │ -    H, Sigma, conv = wilson_sf(CSDav, rtol=1e-6)
│ │ +    H, Sigma, conv, err = wilson_sf(CSDav, rtol=1e-6)
│ │      # converged - \Psi \Psi^* \approx CSD,
│ │      # with relative error <= rtol?
│ │      assert conv
│ │  
│ │      # reconstitute
│ │      CSDfac = H @ Sigma @ H.conj().transpose(0, 2, 1)
│ │      err = max_rel_err(CSDav, CSDfac)
│ │ @@ -226,34 +224,40 @@
│ │              '-o', label='factorized CSD', ms=3)
│ │      # ax.set_xlim((350, 450))
│ │      ax.legend()
│ │  
│ │  
│ │  def test_regularization():
│ │  
│ │ -    # dyadic product of random matrices has rank 1
│ │ -    CSD = np.zeros((50, 50))
│ │ -    for _ in range(40):
│ │ -        A = np.random.randn(50)
│ │ +    """
│ │ +    The dyadic product of single random matrices has rank 1
│ │ +    and condition number --> np.inf.
│ │ +    By averaging "many trials" we interestingly "fill the space" and
│ │ +    one can achieve full rank for white noise.
│ │ +    However here purposefully we ill-condition to test the regularization,
│ │ +    """
│ │ +    nChannels = 20
│ │ +    nTrials = 10
│ │ +    CSD = np.zeros((nChannels, nChannels))
│ │ +    for _ in range(nTrials):
│ │ +        A = np.random.randn(nChannels)
│ │          CSD += np.outer(A, A)
│ │  
│ │ -    # get CSD condition number, which is way too large!
│ │ -    CN = np.linalg.cond(CSD).max()
│ │ -    print(CN)
│ │ -    cmax = 1e6
│ │ -    assert CN > cmax
│ │ -
│ │      # --- regularize CSD ---
│ │  
│ │ -    CSDreg, fac = regularize_csd(CSD, cond_max=cmax, nSteps=25)
│ │ +    cmax = 1e4
│ │ +    eps_max = 1e-1
│ │ +    CSDreg, fac, iniCN = regularize_csd(CSD, cond_max=cmax, eps_max=eps_max)
│ │ +    # check initial CSD condition number, which is way too large!
│ │ +    assert iniCN > cmax, f"intial condition number is {iniCN}"
│ │  
│ │      CNreg = np.linalg.cond(CSDreg).max()
│ │ -    assert CNreg < 1e6
│ │ +    assert CNreg < cmax, f"regularized condition number is {CNreg}"
│ │      # check that 'small' regularization factor is enough
│ │ -    assert fac < 1e-3
│ │ +    assert fac < eps_max
│ │  
│ │  
│ │  def test_granger():
│ │  
│ │      """
│ │      Test the granger causality measure
│ │      with uni-directionally coupled AR(2)
│ │ @@ -277,23 +281,22 @@
│ │          # --- get CSD ---
│ │          bw = 2
│ │          NW = bw * nSamples / (2 * fs)
│ │          Kmax = int(2 * NW - 1)  # optimal number of tapers
│ │          CSD, freqs = csd.csd(sol, fs,
│ │                               taper='dpss',
│ │                               taper_opt={'Kmax': Kmax, 'NW': NW},
│ │ -                             fullOutput=True,
│ │                               demean_taper=True)
│ │  
│ │          CSDav += CSD
│ │  
│ │      CSDav /= nTrials
│ │      # with only 2 channels this CSD is well conditioned
│ │      assert np.linalg.cond(CSDav).max() < 1e2
│ │ -    H, Sigma, conv = wilson_sf(CSDav, direct_inversion=True)
│ │ +    H, Sigma, conv, err = wilson_sf(CSDav, direct_inversion=True)
│ │  
│ │      G = granger(CSDav, H, Sigma)
│ │      assert G.shape == CSDav.shape
│ │  
│ │      fig, ax = ppl.subplots(figsize=(6, 4))
│ │      ax.set_xlabel('frequency (Hz)')
│ │      ax.set_ylabel(r'Granger causality(f)')
│ │ @@ -307,15 +310,15 @@
│ │  
│ │      # check low to no causality for 1->2
│ │      assert G[freq_idx, 0, 1] < 0.1
│ │      # check high causality for 2->1
│ │      assert G[freq_idx, 1, 0] > 0.7
│ │  
│ │      # repeat test with least-square solution
│ │ -    H, Sigma, conv = wilson_sf(CSDav, direct_inversion=False)
│ │ +    H, Sigma, conv, err = wilson_sf(CSDav, direct_inversion=False)
│ │      G2 = granger(CSDav, H, Sigma)
│ │  
│ │      # check low to no causality for 1->2
│ │      assert G2[freq_idx, 0, 1] < 0.1
│ │      # check high causality for 2->1
│ │      assert G2[freq_idx, 1, 0] > 0.7
│ │   --- esi-syncopy-2022.8/syncopy/tests/backend/test_fooofspy.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/backend/test_fooofspy.py
│ │┄ Files 1% similar despite different names
│ │ @@ -19,15 +19,15 @@
│ │  
│ │      # use len 2 for fixed, 3 for knee. order is: offset, (knee), exponent.
│ │      aperiodic_params = [1, 1]
│ │  
│ │      # the Gaussians: Mean (Center Frequency), height (Power), and standard deviation (Bandwidth).
│ │      periodic_params = [[10, 0.2, 1.25], [30, 0.15, 2]]
│ │  
│ │ -    noise_level = 0.005
│ │ +    noise_level = 0.001
│ │      freqs, powers = gen_power_spectrum(freq_range, aperiodic_params,
│ │                                         periodic_params, nlv=noise_level, freq_res=freq_res)
│ │      return freqs, powers
│ │  
│ │  
│ │  def AR1_plus_harm_spec(nTrials=30, hfreq=30, ratio=0.7):
│ │  
│ │ @@ -52,15 +52,15 @@
│ │  
│ │  class TestSpfooof():
│ │  
│ │      freqs, powers = _power_spectrum()
│ │  
│ │      def test_output_fooof_single_channel(self, freqs=freqs, powers=powers):
│ │          """
│ │ -        Tests spfooof with output 'fooof' and a single input signal/channel.
│ │ +        Tests spfooof with output 'fooof' and a single input spectrum/channel.
│ │          This will return the full, fooofed spectrum.
│ │          """
│ │          spectra, details = fooofspy(powers, freqs, out_type='fooof', fooof_opt={'peak_width_limits': (1.0, 12.0)})
│ │  
│ │          assert spectra.shape == (freqs.size, 1)
│ │          assert details['settings_used']['out_type'] == 'fooof'
│ │          assert all(key in details for key in ("aperiodic_params", "gaussian_params",
│ │ @@ -158,15 +158,15 @@
│ │          plt.plot(freqs, fooofed_spectrum, label="Fooofed spectrum")
│ │          plt.plot(freqs, fooof_aperiodic, label="Fooof aperiodic fit")
│ │          plt.plot(freqs, fooof_peaks, label="Fooof peaks fit")
│ │          plt.xlabel('Frequency (Hz)')
│ │          plt.ylabel('Power (Db)')
│ │          plt.legend()
│ │          plt.title("Comparison of raw data and fooof results, linear scale.")
│ │ -        plt.show()
│ │ +        # plt.show()
│ │  
│ │      def test_the_fooof_opt_settings_are_used(self, freqs=freqs, powers=powers):
│ │          """
│ │          Tests spfooof with output 'fooof_peaks' and a single input signal.
│ │          This will return the Gaussian fit of the periodic part of the spectrum.
│ │          """
│ │          fooof_opt = {'peak_threshold': 3.0, 'peak_width_limits': (1.0, 12.0)}
│ │   --- esi-syncopy-2022.8/syncopy/tests/backend/test_resampling.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/backend/test_resampling.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/tests/backend/test_timefreq.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/backend/test_timefreq.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/tests/conftest.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/conftest.py
│ │┄ Files 22% similar despite different names
│ │ @@ -4,37 +4,52 @@
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import sys
│ │  import pytest
│ │  from syncopy import __acme__
│ │  import syncopy.tests.test_packagesetup as setupTestModule
│ │ -
│ │ -# If dask is available, either launch a SLURM cluster on a cluster node or
│ │ -# create a `LocalCluster` object if tests are run on a single machine. If dask
│ │ -# is not installed, return a dummy None-valued cluster object (tests will be
│ │ -# skipped anyway)
│ │ +import dask.distributed as dd
│ │ +import dask_jobqueue as dj
│ │ +from syncopy.tests.misc import is_slurm_node
│ │ +
│ │ +# If acme is available, either launch a SLURM cluster on a cluster node or
│ │ +# create a `LocalCluster` object if tests are run on a single machine. If
│ │ +# acme is not available, launch a custom SLURM cluster or again just a local
│ │ +# cluster as fallback
│ │ +cluster = None
│ │  if __acme__:
│ │ -    import dask.distributed as dd
│ │      from acme.dask_helpers import esi_cluster_setup
│ │ -    from syncopy.tests.misc import is_slurm_node
│ │      if sys.platform != "win32":
│ │          import resource
│ │          if max(resource.getrlimit(resource.RLIMIT_NOFILE)) < 1024:
│ │              msg = "Not enough open file descriptors allowed. Consider increasing " +\
│ │                  "the limit using, e.g., `ulimit -Sn 1024`"
│ │              raise ValueError(msg)
│ │      if is_slurm_node():
│ │ -        cluster = esi_cluster_setup(partition="8GB", n_jobs=10,
│ │ +        cluster = esi_cluster_setup(partition="8GB", n_jobs=4,
│ │                                      timeout=360, interactive=False,
│ │                                      start_client=False)
│ │      else:
│ │ -        cluster = dd.LocalCluster(n_workers=2)
│ │ +        cluster = dd.LocalCluster(n_workers=4)
│ │  else:
│ │ -    cluster = None
│ │ +    # manually start slurm cluster
│ │ +    if is_slurm_node():
│ │ +        n_jobs = 3
│ │ +        reqMem = 32
│ │ +        ESIQueue = 'S'
│ │ +        slurm_wdir = "/cs/slurm/syncopy/"
│ │ +
│ │ +        cluster = dj.SLURMCluster(cores=1, memory=f'{reqMem} GB', processes=1,
│ │ +                                  local_directory=slurm_wdir,
│ │ +                                  queue=f'{reqMem}GB{ESIQueue}',
│ │ +                                  python=sys.executable)
│ │ +        cluster.scale(n_jobs)
│ │ +    else:
│ │ +        cluster = dd.LocalCluster(n_workers=4)
│ │  
│ │  # Set up a pytest fixture `testcluster` that uses the constructed cluster object
│ │  @pytest.fixture
│ │  def testcluster():
│ │      return cluster
│ │  
│ │  # Re-order tests to first run stuff in test_packagesetup.py, then everything else
│ │ @@ -51,20 +66,7 @@
│ │          if testFirst in allTests:
│ │              newOrder.append(allTests.index(testFirst))
│ │      newOrder += [allTests.index(testFunc) for testFunc in allTests
│ │                   if testFunc not in setupTests]
│ │  
│ │      # Save potentially re-ordered test sequence
│ │      items[:] = [items[idx] for idx in newOrder]
│ │ -
│ │ -# Define custom command-line argument `--full`
│ │ -def pytest_addoption(parser):
│ │ -    parser.addoption(
│ │ -        "--full",
│ │ -        action="store_true",
│ │ -        help="run exhaustive test suite",
│ │ -    )
│ │ -
│ │ -# Build corresponding fixture for `--full`
│ │ -@pytest.fixture
│ │ -def fulltests(request):
│ │ -    return request.config.getoption("--full")
│ │   --- esi-syncopy-2022.8/syncopy/tests/helpers.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/helpers.py
│ │┄ Files 27% similar despite different names
│ │ @@ -5,19 +5,19 @@
│ │  # The `run_` function signatures take a callable,
│ │  # the `method_call`, as 1st argument
│ │  #
│ │  
│ │  # 3rd party imports
│ │  import itertools
│ │  import numpy as np
│ │ -
│ │ +import matplotlib.pyplot as plt
│ │  from syncopy.shared.errors import SPYValueError, SPYTypeError
│ │  
│ │  # fix random generators
│ │ -np.random.seed(40203)
│ │ +test_seed = 42
│ │  
│ │  
│ │  def run_padding_test(method_call, pad_length):
│ │      """
│ │      The callable should test a solution and support
│ │      a single keyword argument `pad`
│ │      """
│ │ @@ -70,112 +70,53 @@
│ │  
│ │      try:
│ │          method_call(polyremoval=np.array([1000]))
│ │      except SPYTypeError as err:
│ │          assert 'Wrong type of `polyremoval`' in str(err)
│ │  
│ │  
│ │ -def run_foi_test(method_call, foilim, positivity=True):
│ │ -
│ │ -    # only positive frequencies
│ │ -    assert np.min(foilim) >= 0
│ │ -    assert np.max(foilim) <= 500
│ │ -
│ │ -    # fois
│ │ -    foi1 = np.arange(foilim[0], foilim[1])  # 1Hz steps
│ │ -    foi2 = np.arange(foilim[0], foilim[1], 0.25)  # 0.5Hz steps
│ │ -    foi3 = 'all'
│ │ -    fois = [foi1, foi2, foi3, None]
│ │ -
│ │ -    for foi in fois:
│ │ -        result = method_call(foi=foi, foilim=None)
│ │ -        # check here just for finiteness and positivity
│ │ -        assert np.all(np.isfinite(result.data))
│ │ -        if positivity:
│ │ -            assert np.all(result.data[0, ...] >= -1e-10)
│ │ -
│ │ -    # 2 foilims
│ │ -    foilims = [[2, 60], [7.65, 45.1234], None]
│ │ -    for foil in foilims:
│ │ -        result = method_call(foilim=foil, foi=None)
│ │ -        # check here just for finiteness and positivity
│ │ -        assert np.all(np.isfinite(result.data))
│ │ -        if positivity:
│ │ -            assert np.all(result.data[0, ...] >= -1e-10)
│ │ -
│ │ -    # make sure specification of both foi and foilim triggers a
│ │ -    # Syncopy ValueError
│ │ -    try:
│ │ -        result = method_call(foi=foi, foilim=foil)
│ │ -    except SPYValueError as err:
│ │ -        assert 'foi/foilim' in str(err)
│ │ -
│ │ -    # make sure out-of-range foi selections are detected
│ │ -    try:
│ │ -        result = method_call(foilim=[-1, 70], foi=None)
│ │ -    except SPYValueError as err:
│ │ -        assert 'foilim' in str(err)
│ │ -        assert 'bounded by' in str(err)
│ │ -
│ │ -    try:
│ │ -        result = method_call(foi=np.arange(550, 700), foilim=None)
│ │ -    except SPYValueError as err:
│ │ -        assert 'foi' in str(err)
│ │ -        assert 'bounded by' in str(err)
│ │ -
│ │ -
│ │  def mk_selection_dicts(nTrials, nChannels, toi_min, toi_max, min_len=0.25):
│ │  
│ │      """
│ │ -    Takes 5 numbers, the last three descibing a `toilim/toi` time-interval
│ │ +    Takes 5 numbers, the last three descibing a `latency` time-interval
│ │      and creates cartesian product like `select` keyword
│ │ -    arguments.
│ │ +    arguments. One random selection is enough!
│ │  
│ │      Returns
│ │      -------
│ │      selections : list
│ │          The list of dicts holding the keys and values for
│ │          Syncopy selections.
│ │      """
│ │ -
│ │      # at least 10 trials
│ │      assert nTrials > 9
│ │      # at least 2 channels
│ │      assert nChannels > 1
│ │      # at least 250ms
│ │      assert (toi_max - toi_min) > 0.25
│ │  
│ │ -    # create 3 random trial and channel selections
│ │ +    # create 1 random trial and channel selections
│ │      trials, channels = [], []
│ │ -    for _ in range(3):
│ │ +    for _ in range(1):
│ │  
│ │          sizeTr = np.random.randint(10, nTrials + 1)
│ │          trials.append(list(np.random.choice(
│ │              nTrials, size=sizeTr
│ │          )
│ │          ))
│ │  
│ │          sizeCh = np.random.randint(2, nChannels + 1)
│ │          channels.append(['channel' + str(i + 1)
│ │                           for i in
│ │                           np.random.choice(
│ │                               nChannels, size=sizeCh, replace=False)])
│ │  
│ │ -    # create toi selections, signal length is toi_max
│ │ -    # with -1s as offset (from synthetic data instantiation)
│ │ -    # subsampling does NOT WORK due to precision issues :/
│ │ -    # toi1 = np.linspace(-.4, 2, 100)
│ │ -    tois = [None, 'all']
│ │ -    toi_combinations = itertools.product(trials,
│ │ -                                         channels,
│ │ -                                         tois)
│ │ -
│ │ -    # 2 random toilims
│ │ +    # 1 random toilim
│ │      toilims = []
│ │ -    while len(toilims) < 2:
│ │ +    while len(toilims) < 1:
│ │  
│ │          toil = np.sort(np.random.rand(2)) * (toi_max - toi_min) + toi_min
│ │          # at least min_len (250ms)
│ │          if np.diff(toil) < min_len:
│ │              continue
│ │          else:
│ │              toilims.append(toil)
│ │ @@ -183,25 +124,21 @@
│ │      # combinatorics of all selection options
│ │      # order matters to assign the selection dict keys!
│ │      toilim_combinations = itertools.product(trials,
│ │                                              channels,
│ │                                              toilims)
│ │  
│ │      selections = []
│ │ -    # digest generators to create all selection dictionaries
│ │ -    for comb in toi_combinations:
│ │ -
│ │ -        sel_dct = {}
│ │ -        sel_dct['trials'] = comb[0]
│ │ -        sel_dct['channel'] = comb[1]
│ │ -        sel_dct['toi'] = comb[2]
│ │ -        selections.append(sel_dct)
│ │ -
│ │      for comb in toilim_combinations:
│ │  
│ │          sel_dct = {}
│ │          sel_dct['trials'] = comb[0]
│ │          sel_dct['channel'] = comb[1]
│ │ -        sel_dct['toilim'] = comb[2]
│ │ +        sel_dct['latency'] = comb[2]
│ │          selections.append(sel_dct)
│ │  
│ │      return selections
│ │ +
│ │ +def teardown():
│ │ +    """Cleanup to run at the end of a set of tests, typically at the end of a Test class."""
│ │ +    # Close matplotlib plot windows:
│ │ +    plt.close('all')
│ │   --- esi-syncopy-2022.8/syncopy/tests/local_spy.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/local_spy.py
│ │┄ Files 10% similar despite different names
│ │ @@ -50,7 +50,17 @@
│ │      # show new plotting
│ │      # adata.singlepanelplot(trials=12, toilim=[0, 0.35])
│ │  
│ │      # mtmfft spectrum
│ │      # spec.singlepanelplot()
│ │      # coh.singlepanelplot(channel_i=0, channel_j=1)
│ │  
│ │ +    specf2 = spy.freqanalysis(adata, tapsmofrq=2, keeptrials=False, foi=foi,
│ │ +                              output="fooof_peaks", fooof_opt={'max_n_peaks': 2})
│ │ +
│ │ +    # print("Start: Testing parallel computation of mtmfft")
│ │ +    # spec4 = spy.freqanalysis(adata, tapsmofrq=2, keeptrials=True, foi=foi, parallel=True, output="pow")
│ │ +    # print("End: Testing parallel computation of mtmfft")
│ │ +
│ │ +    #spec.singlepanelplot()
│ │ +    #specf.singlepanelplot()
│ │ +    #specf2.singlepanelplot()S
│ │   --- esi-syncopy-2022.8/syncopy/tests/misc.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/misc.py
│ │┄ Files 1% similar despite different names
│ │ @@ -7,14 +7,15 @@
│ │  import subprocess
│ │  import sys
│ │  import os
│ │  import h5py
│ │  import tempfile
│ │  import time
│ │  import numpy as np
│ │ +import dask.distributed as dd
│ │  
│ │  # Local imports
│ │  from syncopy.datatype import AnalogData
│ │  from syncopy.shared.filetypes import _data_classname_to_extension, FILE_EXT
│ │  from syncopy import __plt__, __acme__
│ │  if __plt__:
│ │      import matplotlib.pyplot as plt
│ │ @@ -60,15 +61,15 @@
│ │                                  text=True, shell=True).communicate()
│ │      if len(out) > 0:
│ │          return True
│ │      else:
│ │          return False
│ │  
│ │  
│ │ -def generate_artificial_data(nTrials=2, nChannels=2, equidistant=True, seed=None,
│ │ +def generate_artificial_data(nTrials=2, nChannels=2, equidistant=True, seed=42,
│ │                               overlapping=False, inmemory=True, dimord="default"):
│ │      """
│ │      Create :class:`~syncopy.AnalogData` object with synthetic harmonic signal(s)
│ │  
│ │      Parameters
│ │      ----------
│ │      nTrials : int
│ │   --- esi-syncopy-2022.8/syncopy/tests/run_tests.cmd
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/run_tests.cmd
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/tests/run_tests.sh
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/run_tests.sh
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/tests/synth_data.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/synth_data.py
│ │┄ Files 22% similar despite different names
│ │ @@ -3,233 +3,281 @@
│ │  # Synthetic data generators for testing and tutorials
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  from inspect import signature
│ │  import numpy as np
│ │  import functools
│ │ -import random
│ │  
│ │  from syncopy import AnalogData, SpikeData
│ │  
│ │ +
│ │  _2pi = np.pi * 2
│ │  
│ │  
│ │  def collect_trials(trial_generator):
│ │ -
│ │      """
│ │      Decorator to wrap around a (nSamples x nChannels) shaped np.array producing
│ │      synthetic data routine, and returning an :class:`~syncopy.AnalogData`
│ │      object.
│ │  
│ │      All backend single trial generating functions (`trial_generator`) should
│ │      accept `nChannels` and `nSamples` as keyword arguments, OR provide
│ │      other means to define those numbers, e.g.
│ │      `AdjMat` for :func:`~syncopy.synth_data.AR2_network`
│ │  
│ │      If the underlying trial generating function also accepts
│ │      a `samplerate`, forward this directly.
│ │  
│ │ +    If the underlying trial generating function also accepts
│ │ +    a `seed`, forward this directly. One can set `seed_per_trial=False` to use
│ │ +    the same seed for all trials, or leave `seed_per_trial=True` (the default),
│ │ +    to have this function internally generate a list
│ │ +    of seeds with len equal to `nTrials` from the given seed, with one seed per trial.
│ │ +
│ │ +    One can set the `seed` to `None`, which will select a random seed each time,
│ │ +    (and it will differ between trials).
│ │ +
│ │      The default `nTrials=None` is the identity wrapper and
│ │      just returns the output of the trial generating function
│ │      directly, so a single trial :class:`numpy.ndarray`.
│ │      """
│ │  
│ │      @functools.wraps(trial_generator)
│ │ -    def wrapper_synth(nTrials=None, samplerate=1000, **tg_kwargs):
│ │ +    def wrapper_synth(nTrials=None, samplerate=1000, seed=None, seed_per_trial=True, **tg_kwargs):
│ │ +
│ │ +        seed_array = None  # One seed per trial.
│ │ +        if nTrials is not None and seed is not None and seed_per_trial:  # Use the single seed to create one seed per trial.
│ │ +            rng = np.random.default_rng(seed)
│ │ +            seed_array = rng.integers(1000000, size=nTrials)
│ │  
│ │          # append samplerate parameter if also needed by the generator
│ │          if 'samplerate' in signature(trial_generator).parameters.keys():
│ │              tg_kwargs['samplerate'] = samplerate
│ │  
│ │ -        # do nothing
│ │ +        # do nothing (may pass on the scalar seed if the function supports it)
│ │          if nTrials is None:
│ │ +            if 'seed' in signature(trial_generator).parameters.keys():
│ │ +                tg_kwargs['seed'] = seed
│ │              return trial_generator(**tg_kwargs)
│ │          # collect trials
│ │          else:
│ │              trl_list = []
│ │  
│ │ -            for _ in range(nTrials):
│ │ +            for trial_idx in range(nTrials):
│ │ +                if 'seed' in signature(trial_generator).parameters.keys():
│ │ +                    if seed_array is not None:
│ │ +                        tg_kwargs['seed'] = seed_array[trial_idx]
│ │ +                    else:
│ │ +                        tg_kwargs['seed'] = seed
│ │                  trl_arr = trial_generator(**tg_kwargs)
│ │                  trl_list.append(trl_arr)
│ │  
│ │              data = AnalogData(trl_list, samplerate=samplerate)
│ │          return data
│ │  
│ │      return wrapper_synth
│ │  
│ │  
│ │  # ---- Synthetic AnalogData ----
│ │  
│ │  
│ │  @collect_trials
│ │ -def white_noise(nSamples=1000, nChannels=2):
│ │ -
│ │ +def white_noise(nSamples=1000, nChannels=2, seed=42):
│ │      """
│ │ -    Plain white noise with unity standard deviation
│ │ +    Plain white noise with unity standard deviation.
│ │ +
│ │ +    Pass an extra `nTrials` `int` Parameter to generate multi-trial data using the `@collect_trials` decorator.
│ │      """
│ │ -    return np.random.randn(nSamples, nChannels)
│ │ +    rng = np.random.default_rng(seed)
│ │ +    return rng.normal(size=(nSamples, nChannels))
│ │  
│ │  
│ │  @collect_trials
│ │  def linear_trend(y_max, nSamples=1000, nChannels=2):
│ │ -
│ │ -    """
│ │ -    A linear trend  on all channels from 0 to `y_max` in `nSamples`
│ │      """
│ │ +    A linear trend  on all channels from 0 to `y_max` in `nSamples`.
│ │  
│ │ +    Pass an extra `nTrials` `int` Parameter to generate multi-trial data using the `@collect_trials` decorator.
│ │ +    """
│ │      trend = np.linspace(0, y_max, nSamples)
│ │      return np.column_stack([trend for _ in range(nChannels)])
│ │  
│ │  
│ │  @collect_trials
│ │  def harmonic(freq, samplerate, nSamples=1000, nChannels=2):
│ │ -
│ │      """
│ │ -    A harmonic with frequency `freq`
│ │ +    A harmonic with frequency `freq`.
│ │ +
│ │ +    Pass an extra `nTrials` `int` Parameter to generate multi-trial data using the `@collect_trials` decorator.
│ │      """
│ │      # the sampling times vector needed for construction
│ │      tvec = np.arange(nSamples) * 1 / samplerate
│ │      # the  harmonic
│ │      harm = np.cos(2 * np.pi * freq * tvec)
│ │      return np.column_stack([harm for _ in range(nChannels)])
│ │  
│ │  
│ │  # noisy phase evolution <-> phase diffusion
│ │  @collect_trials
│ │  def phase_diffusion(freq,
│ │                      eps=.1,
│ │ -                    fs=1000,
│ │ +                    samplerate=1000,
│ │                      nChannels=2,
│ │                      nSamples=1000,
│ │ -                    return_phase=False):
│ │ +                    rand_ini=False,
│ │ +                    return_phase=False,
│ │ +                    seed=None):
│ │  
│ │      """
│ │      Linear (harmonic) phase evolution + a Brownian noise term
│ │      inducing phase diffusion around the deterministic phase drift with
│ │      slope ``2pi * freq`` (angular frequency).
│ │  
│ │ -    The linear phase increments are given by ``dPhase = 2pi * freq/fs``,
│ │ +    The linear phase increments are given by ``dPhase = 2pi * freq/samplerate``,
│ │      the Brownian increments are scaled with `eps` relative to these
│ │      phase increments.
│ │  
│ │      Parameters
│ │      ----------
│ │      freq : float
│ │          Harmonic frequency in Hz
│ │      eps : float
│ │          Scaled Brownian increments
│ │          `1` means the single Wiener step
│ │          has on average the size of the
│ │          harmonic increments
│ │ -    fs : float
│ │ +    samplerate : float
│ │          Sampling rate in Hz
│ │      nChannels : int
│ │          Number of channels
│ │      nSamples : int
│ │          Number of samples in time
│ │ +    rand_ini : bool, optional
│ │ +        If set to ``True`` initial phases are randomized
│ │      return_phase : bool, optional
│ │          If set to true returns the phases in radians
│ │ +    seed: None or int, passed on to `np.random.default_rng`.
│ │ +          Set to an `int` to get reproducible results, or `None` for random ones.
│ │ +    nTrials: int, number of trials to generate using the `@collect_trials` decorator.
│ │  
│ │      Returns
│ │      -------
│ │      phases : numpy.ndarray
│ │          Synthetic `nSamples` x `nChannels` data array simulating noisy phase
│ │          evolution/diffusion
│ │      """
│ │  
│ │      # white noise
│ │ -    wn = np.random.randn(nSamples, nChannels)
│ │ +    wn = white_noise(nSamples=nSamples, nChannels=nChannels, seed=seed)
│ │  
│ │ -    delta_ts = np.ones(nSamples) * 1 / fs
│ │ +    tvec = np.linspace(0, nSamples / samplerate, nSamples)
│ │      omega0 = 2 * np.pi * freq
│ │ -    lin_incr = np.tile(omega0 * delta_ts, (nChannels, 1)).T
│ │ +    lin_phase = np.tile(omega0 * tvec, (nChannels, 1)).T
│ │ +
│ │ +    # randomize initial phase
│ │ +    if rand_ini:
│ │ +        rng = np.random.default_rng(seed)
│ │ +        ps0 = 2 * np.pi * rng.uniform(size=nChannels)
│ │ +        lin_phase += ps0
│ │  
│ │      # relative Brownian increments
│ │ -    rel_eps = np.sqrt(omega0 / fs * eps)
│ │ +    rel_eps = np.sqrt(omega0 / samplerate * eps)
│ │      brown_incr = rel_eps * wn
│ │ -    phases = np.cumsum(lin_incr + brown_incr, axis=0)
│ │ +
│ │ +    # combine harmonic and diffusive dyncamics
│ │ +    phases = lin_phase + np.cumsum(brown_incr, axis=0)
│ │      if not return_phase:
│ │          return np.cos(phases)
│ │      else:
│ │          return phases
│ │  
│ │  
│ │  @collect_trials
│ │ -def AR2_network(AdjMat=None, nSamples=1000, alphas=[0.55, -0.8]):
│ │ +def AR2_network(AdjMat=None, nSamples=1000, alphas=[0.55, -0.8], seed=None):
│ │  
│ │      """
│ │      Simulation of a network of coupled AR(2) processes
│ │  
│ │      With the default parameters the individual processes
│ │      (as in Dhamala 2008) have a spectral peak at 40Hz
│ │      with a sampling frequency of 200Hz.
│ │  
│ │      NOTE: There is no check for stability: setting the
│ │            `alphas` ad libitum and/or defining large
│ │            and dense (many connections) systems will
│ │            almost surely lead to an unstable system
│ │  
│ │ +    NOTE: One can set the number of channels via the shape
│ │ +          of the supplied `AdjMat`. Defaults to 2.
│ │ +
│ │      Parameters
│ │      ----------
│ │      AdjMat : np.ndarray or None
│ │          `nChannel` x `nChannel` adjacency matrix where
│ │          entry ``(i,j)`` is the coupling strength
│ │          from channel ``i -> j``.
│ │          If left at `None`, the default 2 Channel system
│ │          with unidirectional ``2 -> 1`` coupling is generated.
│ │ +        See also `mk_RandomAdjMat`.
│ │      nSamples : int, optional
│ │          Number of samples in time
│ │      alphas : 2-element sequence, optional
│ │          The AR(2) parameters for lag1 and lag2
│ │ +    seed : None or int.
│ │ +        Random seed to init random number generator, passed on to `np.random.default_rng` function.
│ │ +        When using this function with an `nTrials` argument (`@collect_trials` wrapper), and you *do*
│ │ +        want the data of all trials to be identical (and reproducible),
│ │ +        pass a single scalar seed and set 'seed_per_trial=False'.
│ │ +    nTrials: int, number of trials to generate using the `@collect_trials` decorator.
│ │  
│ │      Returns
│ │      -------
│ │      sol : numpy.ndarray
│ │          The `nSamples` x `nChannel`
│ │          solution of the network dynamics
│ │      """
│ │  
│ │ -
│ │      # default system layout as in Dhamala 2008:
│ │      # unidirectional (2->1) coupling
│ │      if AdjMat is None:
│ │ -        AdjMat = np.zeros((2, 2))
│ │ +        AdjMat = np.zeros((2, 2), dtype=np.float32)
│ │          AdjMat[1, 0] = .25
│ │ +    else:
│ │ +        # cast to our standard type
│ │ +        AdjMat = AdjMat.astype(np.float32)
│ │  
│ │      nChannels = AdjMat.shape[0]
│ │      alpha1, alpha2 = alphas
│ │      # diagonal 'self-interaction' with lag 1
│ │      DiagMat = np.diag(nChannels * [alpha1])
│ │  
│ │ -    sol = np.zeros((nSamples, nChannels))
│ │ +    sol = np.zeros((nSamples, nChannels), dtype=np.float32)
│ │      # pick the 1st values at random
│ │ -    sol[:2, :] = np.random.randn(2, nChannels)
│ │ +    rng = np.random.default_rng(seed)
│ │ +    sol[:2, :] = rng.normal(size=(2, nChannels))
│ │  
│ │      for i in range(2, nSamples):
│ │ -
│ │          sol[i, :] = (DiagMat + AdjMat.T) @ sol[i - 1, :] + alpha2 * sol[i - 2, :]
│ │ -        sol[i, :] += np.random.randn(nChannels)
│ │ +        sol[i, :] += rng.normal(size=(nChannels))
│ │  
│ │      return sol
│ │  
│ │  
│ │ -def AR2_peak_freq(a1, a2, fs=1):
│ │ -
│ │ +def AR2_peak_freq(a1, a2, samplerate=1):
│ │      """
│ │      Helper function to tune spectral peak of AR(2) process
│ │      """
│ │ -
│ │      if np.any((a1**2 + 4 * a2) > 0):
│ │          raise ValueError("No complex roots!")
│ │  
│ │ -    return np.arccos(a1 * (a2 - 1) / (4 * a2)) * 1 / _2pi * fs
│ │ +    return np.arccos(a1 * (a2 - 1) / (4 * a2)) * 1 / _2pi * samplerate
│ │  
│ │  
│ │ -def mk_RandomAdjMat(nChannels=3, conn_thresh=0.25, max_coupling=0.25):
│ │ +def mk_RandomAdjMat(nChannels=3, conn_thresh=0.25, max_coupling=0.25, seed=None):
│ │      """
│ │      Create a random adjacency matrix
│ │      for the network of AR(2) processes
│ │      where entry ``(i,j)`` is the coupling
│ │      strength from channel ``i -> j``
│ │  
│ │      Parameters
│ │ @@ -241,24 +289,26 @@
│ │          sampling of the network connections. Setting
│ │          ``conn_thresh = 1`` yields a fully connected network
│ │          (not recommended).
│ │      max_coupling : float < 0.5, optional
│ │          Total input into single channel
│ │          normalized by number of couplings
│ │          (for stability).
│ │ +    seed: None or int, passed on to `np.random.default_rng`.
│ │ +          Set to an int to get reproducible results.
│ │  
│ │      Returns
│ │      -------
│ │      AdjMat : numpy.ndarray
│ │          `nChannels` x `nChannels` adjacency matrix where
│ │      """
│ │  
│ │ -
│ │      # random numbers in [0,1)
│ │ -    AdjMat = np.random.random_sample((nChannels, nChannels))
│ │ +    rng = np.random.default_rng(seed)
│ │ +    AdjMat = rng.random((nChannels, nChannels))
│ │  
│ │      # all smaller than threshold elements get set to 1 (coupled)
│ │      AdjMat = (AdjMat < conn_thresh).astype(float)
│ │  
│ │      # set diagonal to 0 to easier identify coupling
│ │      np.fill_diagonal(AdjMat, 0)
│ │  
│ │ @@ -275,76 +325,118 @@
│ │  
│ │  
│ │  def poisson_noise(nTrials=10,
│ │                    nSpikes=10000,
│ │                    nChannels=3,
│ │                    nUnits=10,
│ │                    intensity=.1,
│ │ -                  samplerate=30000
│ │ -                  ):
│ │ +                  samplerate=10000,
│ │ +                  seed=None):
│ │  
│ │      """
│ │ -    Poisson (Shot-) noise generator
│ │ +    Poisson (Shot-)noise generator
│ │ +
│ │ +    The expected trial length in samples is given by:
│ │ +
│ │ +        ``nSpikes`` / (``intensity`` * ``nTrials``)
│ │ +
│ │ +    Dividing again by the ``samplerate` gives the
│ │ +    expected trial length in seconds.
│ │ +
│ │ +    Individual trial lengths get randomly
│ │ +    shortened by up to 10% of this expected length.
│ │  
│ │ +    The trigger offsets are also
│ │ +    randomized between 5% and 20% of the shortest
│ │ +    trial length.
│ │ +
│ │ +    Lastly, the distribution of the Poisson ``intensity`` along channels and units
│ │ +    has uniformly randomized weights, meaning that typically
│ │ +    you get very active channels/units and some which are almost quiet.
│ │  
│ │      Parameters
│ │      ----------
│ │      nTrials : int
│ │          Number of trials
│ │      nSpikes : int
│ │ -        The total number of spikes to generate
│ │ +        The total number of spikes over all trials to generate
│ │      nChannels : int
│ │          Number of channels
│ │      nUnits : int
│ │          Number of units
│ │      intensity : int
│ │ -        Average number of spikes per sampling interval
│ │ +        Expected number of spikes per sampling interval
│ │      samplerate : float
│ │          Sampling rate in Hz
│ │ +    seed: None or int, passed on to `np.random.default_rng`.
│ │ +          Set to an int to get reproducible results.
│ │  
│ │      Returns
│ │      -------
│ │      sdata : :class:`~syncopy.SpikeData`
│ │          The generated spike data
│ │  
│ │      Notes
│ │      -----
│ │      Originally conceived by `Alejandro Tlaie Boria https://github.com/atlaie_`
│ │  
│ │ +    Examples
│ │ +    --------
│ │ +    With `nSpikes=20_000`, `samplerate=10_000`, `nTrials=10` and the default `intensity=0.1`
│ │ +    we can expect a trial length of about 2 seconds:
│ │ +
│ │ +    >>> spike_data = poisson_noise(nTrials=10, nSpikes=20_000, samplerate=10_000)
│ │ +
│ │ +    Example output of the 1st trial [start, end] in seconds:
│ │ +
│ │ +    >>> spike_data.trialintervals[0]
│ │ +    >>> array([-0.3004, 1.6459])
│ │ +
│ │ +    Which is close to 2 seconds.
│ │ +
│ │      """
│ │  
│ │ -    def get_rdm_weights(size):
│ │ -        pvec = np.random.uniform(size=size)
│ │ +    # uniform random weights
│ │ +    def get_rdm_weights(size, seed=seed):
│ │ +        rng = np.random.default_rng(seed)
│ │ +        pvec = rng.uniform(size=size)
│ │          return pvec / pvec.sum()
│ │  
│ │ -    T_max = int(1 / intensity * nSpikes)
│ │ -    spike_times = np.sort(random.sample(range(T_max), nSpikes))
│ │ -    channels = np.random.choice(
│ │ +    # total length of all trials combined
│ │ +    rng = np.random.default_rng(seed)
│ │ +    T_max = int(nSpikes / intensity)
│ │ +
│ │ +    spike_samples = np.sort(rng.choice(range(T_max), size=nSpikes, replace=False))
│ │ +    channels = rng.choice(
│ │          np.arange(nChannels), p=get_rdm_weights(nChannels),
│ │          size=nSpikes, replace=True
│ │      )
│ │  
│ │      uvec = np.arange(nUnits)
│ │      pvec = get_rdm_weights(nUnits)
│ │ -    units = np.random.choice(uvec, p=pvec, size=nSpikes, replace=True)
│ │ -    # units = np.r_[units, np.random.choice(uvec, size=nSpikes // 2, replace=True)]
│ │ -    # if nSpikes % 2 == 1:
│ │ -    #     units = np.r_[units, [np.random.choice(uvec)]]
│ │ +    units = rng.choice(uvec, p=pvec, size=nSpikes, replace=True)
│ │ +
│ │ +    # originally fixed trial size
│ │ +    step = T_max // nTrials
│ │ +    trl_intervals = np.arange(T_max + 1, step=step)
│ │  
│ │ -    trl_intervals = np.sort(random.sample(range(T_max), nTrials + 1))
│ │      # 1st trial
│ │      idx_start = trl_intervals[:-1]
│ │      idx_end = trl_intervals[1:] - 1
│ │  
│ │ -    idx_offset = -np.random.choice(
│ │ -        np.arange(1, np.min(idx_end - idx_start)), size=nTrials, replace=True
│ │ +    # now randomize trial length a bit, max 10% size difference
│ │ +    idx_end = idx_end - np.r_[rng.integers(step // 10, size=nTrials - 1), 0]
│ │ +
│ │ +    shortest_trial = np.min(idx_end - idx_start)
│ │ +    idx_offset = -rng.choice(
│ │ +        np.arange(0.05 * shortest_trial, 0.2 * shortest_trial, dtype=int), size=nTrials, replace=True
│ │      )
│ │  
│ │      trldef = np.vstack([idx_start, idx_end, idx_offset]).T
│ │ -    data = np.vstack([spike_times, channels, units]).T
│ │ +    data = np.vstack([spike_samples, channels, units]).T
│ │      sdata = SpikeData(
│ │          data=data,
│ │          trialdefinition=trldef,
│ │          dimord=["sample", "channel", "unit"],
│ │          samplerate=samplerate,
│ │      )
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_basedata.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_basedata.py
│ │┄ Files 8% similar despite different names
│ │ @@ -57,30 +57,29 @@
│ │      data["CrossSpectralData"] = np.arange(1, nChannels * nChannels * nSamples * nFreqs + 1).reshape(nSamples, nFreqs, nChannels, nChannels)
│ │      trl["CrossSpectralData"] = trl["AnalogData"]
│ │  
│ │      # Use a fixed random number generator seed to simulate a 2D SpikeData array
│ │      seed = np.random.RandomState(13)
│ │      data["SpikeData"] = np.vstack([seed.choice(nSamples, size=nSpikes),
│ │                                     seed.choice(nChannels, size=nSpikes),
│ │ -                                   seed.choice(int(nChannels / 2), size=nSpikes)]).T
│ │ +                                   seed.choice(int(nChannels / 2), size=nSpikes)]).T.astype(int)
│ │      trl["SpikeData"] = trl["AnalogData"]
│ │  
│ │      # Use a simple binary trigger pattern to simulate EventData
│ │      data["EventData"] = np.vstack([np.arange(0, nSamples, 5),
│ │ -                                   np.zeros((int(nSamples / 5), ))]).T
│ │ +                                   np.zeros((int(nSamples / 5), ))]).T.astype(int)
│ │      data["EventData"][1::2, 1] = 1
│ │      trl["EventData"] = trl["AnalogData"]
│ │  
│ │      # Define data classes to be used in tests below
│ │      classes = ["AnalogData", "SpectralData", "CrossSpectralData", "SpikeData", "EventData"]
│ │  
│ │      # Allocation to `data` property is tested with all members of `classes`
│ │      def test_data_alloc(self):
│ │          with tempfile.TemporaryDirectory() as tdir:
│ │ -            fname = os.path.join(tdir, "dummy.npy")
│ │              hname = os.path.join(tdir, "dummy.h5")
│ │  
│ │              for dclass in self.classes:
│ │  
│ │                  # allocation with HDF5 file
│ │                  h5f = h5py.File(hname, mode="w")
│ │                  h5f.create_dataset("dummy", data=self.data[dclass])
│ │ @@ -89,33 +88,27 @@
│ │                  # allocation using HDF5 dataset directly
│ │                  dset = h5py.File(hname, mode="r+")["dummy"]
│ │                  dummy = getattr(spd, dclass)(data=dset)
│ │                  assert np.array_equal(dummy.data, self.data[dclass])
│ │                  assert dummy.mode == "r+", dummy.data.file.mode
│ │                  del dummy
│ │  
│ │ -                # allocation using array + filename
│ │ -                dummy = getattr(spd, dclass)(data=self.data[dclass], filename=fname)
│ │ -                assert dummy.filename == fname
│ │ -                assert np.array_equal(dummy.data, self.data[dclass])
│ │ -                del dummy
│ │ -
│ │                  # attempt allocation using HDF5 dataset of wrong shape
│ │                  h5f = h5py.File(hname, mode="r+")
│ │                  del h5f["dummy"]
│ │                  dset = h5f.create_dataset("dummy", data=np.ones((self.nChannels,)))
│ │                  with pytest.raises(SPYValueError):
│ │                      getattr(spd, dclass)(data=dset)
│ │  
│ │                  # allocate with valid dataset of "illegal" file
│ │                  del h5f["dummy"]
│ │                  h5f.create_dataset("dummy1", data=self.data[dclass])
│ │                  h5f.close()
│ │                  dset = h5py.File(hname, mode="r")["dummy1"]
│ │ -                dummy = getattr(spd, dclass)(data=dset, filename=fname)
│ │ +                dummy = getattr(spd, dclass)(data=dset, filename=hname)
│ │  
│ │                  # attempt data access after backing file of dataset has been closed
│ │                  dset.file.close()
│ │                  with pytest.raises(SPYValueError):
│ │                      dummy.data[0, ...]
│ │  
│ │                  # attempt allocation with HDF5 dataset of closed file
│ │ @@ -157,14 +150,51 @@
│ │              dummy = getattr(spd, dclass)(self.data[dclass],
│ │                                           samplerate=self.samplerate)
│ │              dummy.trialdefinition = self.trl[dclass]
│ │              assert np.array_equal(dummy.sampleinfo, self.trl[dclass][:, :2])
│ │              assert np.array_equal(dummy._t0, self.trl[dclass][:, 2])
│ │              assert np.array_equal(dummy.trialinfo.flatten(), self.trl[dclass][:, 3])
│ │  
│ │ +    def test_trials_property(self):
│ │ +
│ │ +        # 3 trials, trial index = data values
│ │ +        data = AnalogData([i * np.ones((2,2)) for i in range(3)], samplerate=1)
│ │ +
│ │ +        # single index access
│ │ +        assert np.all(data.trials[0] == 0)
│ │ +        assert np.all(data.trials[1] == 1)
│ │ +        assert np.all(data.trials[2] == 2)
│ │ +
│ │ +        # iterator
│ │ +        all_trials = [trl for trl in data.trials]
│ │ +        assert len(all_trials) == 3
│ │ +        assert all([np.all(all_trials[i] == i) for i in range(3)])
│ │ +
│ │ +        # selection
│ │ +        data.selectdata(trials=[0, 2], inplace=True)
│ │ +        all_selected_trials = [trl for trl in data.selection.trials]
│ │ +        assert data.selection.trial_ids == [0, 2]
│ │ +        assert len(all_selected_trials) == 2
│ │ +        assert all([np.all(data.selection.trials[i] == i) for i in data.selection.trial_ids])
│ │ +
│ │ +        # check that non-existing trials get catched
│ │ +        with pytest.raises(SPYValueError, match='existing trials'):
│ │ +            data.trials[999]
│ │ +        # selections have absolute trial indices!
│ │ +        with pytest.raises(SPYValueError, match='existing trials'):
│ │ +            data.selection.trials[1]
│ │ +
│ │ +        # check that invalid trial indexing gets catched
│ │ +        with pytest.raises(SPYTypeError, match='trial index'):
│ │ +            data.trials[range(4)]
│ │ +        with pytest.raises(SPYTypeError, match='trial index'):
│ │ +            data.trials[2:3]
│ │ +        with pytest.raises(SPYTypeError, match='trial index'):
│ │ +            data.trials[np.arange(3)]
│ │ +
│ │      # Test ``_gen_filename`` with `AnalogData` only - method is independent from concrete data object
│ │      def test_filename(self):
│ │          # ensure we're salting sufficiently to create at least `numf`
│ │          # distinct pseudo-random filenames in `__storage__`
│ │          numf = 1000
│ │          dummy = AnalogData()
│ │          fnames = []
│ │ @@ -174,21 +204,22 @@
│ │  
│ │      # Object copying is tested with all members of `classes`
│ │      def test_copy(self):
│ │  
│ │          # test (deep) copies HDF5 files
│ │          with tempfile.TemporaryDirectory() as tdir:
│ │              for dclass in self.classes:
│ │ +
│ │                  hname = os.path.join(tdir, "dummy.h5")
│ │                  h5f = h5py.File(hname, mode="w")
│ │ -                h5f.create_dataset("dummy", data=self.data[dclass])
│ │ +                h5f.create_dataset("data", data=self.data[dclass])
│ │                  h5f.close()
│ │  
│ │                  # hash-matching of shallow-copied HDF5 dataset
│ │ -                dummy = getattr(spd, dclass)(data=h5py.File(hname, 'r')["dummy"],
│ │ +                dummy = getattr(spd, dclass)(data=h5py.File(hname, 'r')["data"],
│ │                                               samplerate=self.samplerate)
│ │  
│ │                  # attach some aux. info
│ │                  dummy.info = {'sth': 4, 'important': [1, 2],
│ │                                'to-remember': {'v1': 2}}
│ │  
│ │                  # test integrity of deep-copy
│ │ @@ -236,15 +267,15 @@
│ │                                               samplerate=self.samplerate)
│ │              other.trialdefinition = self.trl[dclass]
│ │              complexArr = np.complex64(dummy.trials[0])
│ │              complexNum = 3 + 4j
│ │  
│ │              # Start w/the one operator that does not handle zeros well...
│ │              with pytest.raises(SPYValueError) as spyval:
│ │ -                dummy / 0
│ │ +                _ = dummy / 0
│ │                  assert "expected non-zero scalar for division" in str(spyval.value)
│ │  
│ │              # Go through all supported operators and try to sabotage them
│ │              for operation in arithmetics:
│ │  
│ │                  # Completely wrong operand
│ │                  with pytest.raises(SPYTypeError) as spytyp:
│ │ @@ -332,16 +363,17 @@
│ │                                            samplerate=self.samplerate)
│ │              dummy3.trialdefinition = trl
│ │              assert dummy3 != dummy
│ │  
│ │              # Difference in actual numerical data
│ │              dummy3 = dummy.copy()
│ │              for dsetName in dummy3._hdfFileDatasetProperties:
│ │ -                getattr(dummy3, dsetName)[0] = 2 * np.pi
│ │ -            assert dummy3 != dummy
│ │ +                if dsetName == "data":
│ │ +                    getattr(dummy3, dsetName)[0, 0] = -99
│ │ +            assert dummy3.data != dummy.data
│ │  
│ │              del dummy, dummy3, other
│ │  
│ │          # Same objects but different dimords: `ContinuousData`` children
│ │          for dclass in continuousClasses:
│ │              dummy = getattr(spd, dclass)(self.data[dclass],
│ │                                           samplerate=self.samplerate)
│ │ @@ -358,7 +390,11 @@
│ │                                           trialdefinition=self.trl[dclass],
│ │                                           samplerate=self.samplerate)
│ │              ymmud = getattr(spd, dclass)(self.data[dclass],
│ │                                           dimord=dummy.dimord[::-1],
│ │                                           trialdefinition=self.trl[dclass],
│ │                                           samplerate=self.samplerate)
│ │              assert dummy != ymmud
│ │ +
│ │ +
│ │ +if __name__ == '__main__':
│ │ +    T1 = TestBaseData()
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_cfg.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_cfg.py
│ │┄ Files 4% similar despite different names
│ │ @@ -4,33 +4,28 @@
│ │  #
│ │  
│ │  import pytest
│ │  import numpy as np
│ │  import inspect
│ │  import tempfile
│ │  import os
│ │ +import dask.distributed as dd
│ │  
│ │  # Local imports
│ │  import syncopy as spy
│ │ -from syncopy import __acme__
│ │ -if __acme__:
│ │ -    import dask.distributed as dd
│ │  
│ │  import syncopy.tests.synth_data as synth_data
│ │  from syncopy.shared.tools import StructDict
│ │  
│ │  
│ │ -# Decorator to decide whether or not to run dask-related tests
│ │ -skip_without_acme = pytest.mark.skipif(not __acme__, reason="acme not available")
│ │ -
│ │ -availableFrontend_cfgs = {'freqanalysis': {'method': 'mtmconvol', 't_ftimwin': 0.1},
│ │ +availableFrontend_cfgs = {'freqanalysis': {'method': 'mtmconvol', 't_ftimwin': 0.1, 'foi': np.arange(1,60)},
│ │                            'preprocessing': {'freq': 10, 'filter_class': 'firws', 'filter_type': 'hp'},
│ │                            'resampledata': {'resamplefs': 125, 'lpfreq': 60},
│ │                            'connectivityanalysis': {'method': 'coh', 'tapsmofrq': 5},
│ │ -                          'selectdata': {'trials': [1, 7, 3], 'channel': [2, 0]}
│ │ +                          'selectdata': {'trials': np.array([1, 7, 3]), 'channel': [np.int64(2), 0]}
│ │                            }
│ │  
│ │  
│ │  class TestCfg:
│ │  
│ │      nSamples = 100
│ │      nChannels = 3
│ │ @@ -91,15 +86,15 @@
│ │                  assert np.allclose(res.data[:], res2.data[:])
│ │                  assert res.cfg == res2.cfg
│ │  
│ │                  del res, res2
│ │  
│ │      def test_selection(self):
│ │  
│ │ -        select = {'toilim': self.time_span, 'trials': [1, 2, 3], 'channel': [2, 0]}
│ │ +        select = {'latency': self.time_span, 'trials': [1, 2, 3], 'channel': [2, 0]}
│ │          for frontend in availableFrontend_cfgs.keys():
│ │              # select kw for selectdata makes no direct sense
│ │              if frontend == 'selectdata':
│ │                  continue
│ │              res = getattr(spy, frontend)(self.adata,
│ │                                           cfg=availableFrontend_cfgs[frontend],
│ │                                           select=select)
│ │ @@ -133,30 +128,29 @@
│ │  
│ │      def test_chaining_frontends_with_fooof_types(self):
│ │  
│ │          # only preprocessing makes sense to chain atm
│ │          res_pp = spy.preprocessing(self.adata, cfg=availableFrontend_cfgs['preprocessing'])
│ │  
│ │          frontend = 'freqanalysis'
│ │ -        frontend_cfg = {'method': 'mtmfft', 'output': 'fooof', 'foilim' : [0.5, 100.]}
│ │ +        frontend_cfg = {'method': 'mtmfft', 'output': 'fooof', 'foilim': [0.5, 100.]}
│ │  
│ │          res = getattr(spy, frontend)(res_pp,
│ │                                          cfg=frontend_cfg)
│ │  
│ │          # now replay with cfg from preceding frontend calls
│ │          # note we can use the final results `res.cfg` for both calls!
│ │          res_pp2 = spy.preprocessing(self.adata, res.cfg)
│ │          res2 = getattr(spy, frontend)(res_pp2, res.cfg)
│ │  
│ │          # same results
│ │          assert np.allclose(res.data[:], res2.data[:])
│ │          assert res.cfg == res2.cfg
│ │  
│ │ -    @skip_without_acme
│ │ -    def test_parallel(self, testcluster=None):
│ │ +    def test_parallel(self, testcluster):
│ │  
│ │          client = dd.Client(testcluster)
│ │          all_tests = [attr for attr in self.__dir__()
│ │                       if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
│ │  
│ │          for test_name in all_tests:
│ │              test_method = getattr(self, test_name)
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_computationalroutine.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_computationalroutine.py
│ │┄ Files 5% similar despite different names
│ │ @@ -7,29 +7,24 @@
│ │  import os
│ │  import tempfile
│ │  import time
│ │  import pytest
│ │  import numpy as np
│ │  from glob import glob
│ │  from scipy import signal
│ │ +import dask.distributed as dd
│ │  
│ │  # Local imports
│ │ -from syncopy import __acme__
│ │ -if __acme__:
│ │ -    import dask.distributed as dd
│ │  from syncopy.datatype import AnalogData
│ │ -from syncopy.datatype.base_data import Selector
│ │ +from syncopy.datatype.selector import Selector
│ │  from syncopy.io import load
│ │  from syncopy.shared.computational_routine import ComputationalRoutine
│ │  from syncopy.shared.kwarg_decorators import process_io, unwrap_cfg, unwrap_select
│ │  from syncopy.tests.misc import generate_artificial_data
│ │  
│ │ -# Decorator to decide whether or not to run dask-related tests
│ │ -skip_without_acme = pytest.mark.skipif(not __acme__, reason="acme not available")
│ │ -
│ │  
│ │  @process_io
│ │  def lowpass(arr, b, a=None, noCompute=None, chunkShape=None):
│ │      if noCompute:
│ │          return arr.shape, arr.dtype
│ │      res = signal.filtfilt(b, a, arr.T, padlen=200).T
│ │      return res
│ │ @@ -115,26 +110,25 @@
│ │  
│ │      # Data selections to be tested w/`sigdata`
│ │      sigdataSelections = [None,
│ │                           {"trials": [3, 1, 0],
│ │                            "channel": ["channel" + str(i) for i in range(12, 28)][::-1]},
│ │                           {"trials": [0, 1, 2],
│ │                            "channel": range(0, int(nChannels / 2)),
│ │ -                          "toilim": [-0.25, 0.25]}]
│ │ +                          "latency": [-0.25, 0.25]}]
│ │  
│ │      # Data selections to be tested w/`artdata` generated below (use fixed but arbitrary
│ │ -    # random number seed to randomly select time-points for `toi` (with repetitions)
│ │      seed = np.random.RandomState(13)
│ │      artdataSelections = [None,
│ │                           {"trials": [3, 1, 0],
│ │                            "channel": ["channel" + str(i) for i in range(12, 28)][::-1],
│ │ -                          "toi": None},
│ │ +                          "latency": None},
│ │                           {"trials": [0, 1, 2],
│ │                            "channel": range(0, int(nChannels / 2)),
│ │ -                          "toilim": [-0.5, 0.6]}]
│ │ +                          "latency": [-0.5, 0.6]}]
│ │  
│ │      # Error tolerances and respective quality metrics (depend on data selection!)
│ │      tols = [1e-6, 1e-6, 1e-2]
│ │      metrix = [np.max, np.max, np.mean]
│ │  
│ │  
│ │      def test_sequential_equidistant(self):
│ │ @@ -143,15 +137,15 @@
│ │              out = filter_manager(self.sigdata, self.b, self.a, select=select)
│ │  
│ │              # check correct signal filtering (especially wrt data-selection)
│ │              if select is None:
│ │                  reference = self.orig
│ │              else:
│ │                  ref = []
│ │ -                for tk, trlno in enumerate(sel.trials):
│ │ +                for tk, trlno in enumerate(sel.trial_ids):
│ │                      ref.append(self.origdata.trials[trlno][sel.time[tk], sel.channel])
│ │                      # check for correct time selection
│ │                      assert np.array_equal(out.time[tk], self.sigdata.time[trlno][sel.time[tk]])
│ │                  reference = np.vstack(ref)
│ │              assert self.metrix[sk](np.abs(out.data - reference)) < self.tols[sk]
│ │              assert np.array_equal(out.channel, self.sigdata.channel[sel.channel])
│ │  
│ │ @@ -168,19 +162,19 @@
│ │              out = filter_manager(self.sigdata, self.b, self.a, select=select, keeptrials=False)
│ │  
│ │              # check correct signal filtering (especially wrt data-selection)
│ │              if select is None:
│ │                  reference = self.orig[:self.t.size, :]
│ │              else:
│ │                  ref = np.zeros(out.trials[0].shape)
│ │ -                for tk, trlno in enumerate(sel.trials):
│ │ +                for tk, trlno in enumerate(sel.trial_ids):
│ │                      ref += self.origdata.trials[trlno][sel.time[tk], sel.channel]
│ │                      # check for correct time selection (accounting for trial-averaging)
│ │                      assert np.array_equal(out.time[0], self.sigdata.time[0][sel.time[0]])
│ │ -                reference = ref / len(sel.trials)
│ │ +                reference = ref / len(sel.trial_ids)
│ │              assert self.metrix[sk](np.abs(out.data - reference)) < self.tols[sk]
│ │              assert np.array_equal(out.channel, self.sigdata.channel[sel.channel])
│ │  
│ │              # ensure pre-selection is equivalent to in-place selection
│ │              if select is None:
│ │                  selected = self.sigdata.selectdata()
│ │              else:
│ │ @@ -194,25 +188,21 @@
│ │          for overlapping in [False, True]:
│ │              nonequidata = generate_artificial_data(nTrials=self.nTrials,
│ │                                                     nChannels=self.nChannels,
│ │                                                     equidistant=False,
│ │                                                     overlapping=overlapping,
│ │                                                     inmemory=False)
│ │  
│ │ -            # unsorted, w/repetitions
│ │ -            toi = self.seed.choice(nonequidata.time[0], int(nonequidata.time[0].size))
│ │ -            self.artdataSelections[1]["toi"] = toi
│ │ -
│ │              for select in self.artdataSelections:
│ │                  sel = Selector(nonequidata, select)
│ │                  out = filter_manager(nonequidata, self.b, self.a, select=select)
│ │  
│ │                  # compare expected w/actual shape of computed data
│ │                  reference = 0
│ │ -                for tk, trlno in enumerate(sel.trials):
│ │ +                for tk, trlno in enumerate(sel.trial_ids):
│ │                      reference += nonequidata.trials[trlno][sel.time[tk]].shape[0]
│ │                      # check for correct time selection
│ │                      # FIXME: remove `if` below as soon as `time` prop for lists is fixed
│ │                      if not isinstance(sel.time[0], list):
│ │                          assert np.array_equal(out.time[tk], nonequidata.time[trlno][sel.time[tk]])
│ │  
│ │                  assert out.data.shape[0] == reference
│ │ @@ -231,15 +221,15 @@
│ │  
│ │      def test_sequential_saveload(self):
│ │          for sk, select in enumerate(self.sigdataSelections):
│ │              sel = Selector(self.sigdata, select)
│ │              out = filter_manager(self.sigdata, self.b, self.a, select=select,
│ │                                   log_dict={"a": "this is a", "b": "this is b"})
│ │  
│ │ -            assert len(out.trials) == len(sel.trials)
│ │ +            assert len(out.trials) == len(sel.trial_ids)
│ │              # ensure our `log_dict` specification was respected
│ │              assert "lowpass" in out._log
│ │              assert "a = this is a" in out._log
│ │              assert "b = this is b" in out._log
│ │  
│ │              # ensure pre-selection is equivalent to in-place selection
│ │              if select is None:
│ │ @@ -251,23 +241,27 @@
│ │              assert len(out.trials) == len(out_sel.trials)
│ │              assert "lowpass" in out._log
│ │              assert "a = this is a" in out._log
│ │              assert "b = this is b" in out._log
│ │  
│ │              # save and re-load result, ensure nothing funky happens
│ │              with tempfile.TemporaryDirectory() as tdir:
│ │ +
│ │                  fname = os.path.join(tdir, "dummy")
│ │ +                print(f"test_computationalroutine: saving to {fname}")
│ │                  out.save(fname)
│ │ +                print(f"test_computationalroutine: loading...")
│ │                  dummy = load(fname)
│ │ -                assert out.filename == dummy.filename
│ │ +                print(f"test_computationalroutine: loading done")
│ │ +                assert out.filename == dummy.filename, f"save: expected out.filename '{out.filename}' == dummy.filename '{dummy.filename}'."
│ │                  if select is None:
│ │                      reference = self.orig
│ │                  else:
│ │                      ref = []
│ │ -                    for tk, trlno in enumerate(sel.trials):
│ │ +                    for tk, trlno in enumerate(sel.trial_ids):
│ │                          ref.append(self.origdata.trials[trlno][sel.time[tk], sel.channel])
│ │                          assert np.array_equal(dummy.time[tk], self.sigdata.time[trlno][sel.time[tk]])
│ │                      reference = np.vstack(ref)
│ │                  assert self.metrix[sk](np.abs(dummy.data - reference)) < self.tols[sk]
│ │                  assert np.array_equal(dummy.channel, self.sigdata.channel[sel.channel])
│ │                  time.sleep(0.01)
│ │                  del out
│ │ @@ -277,15 +271,14 @@
│ │                  out_sel.save(fname2)
│ │                  dummy2 = load(fname2)
│ │                  assert np.array_equal(dummy.data, dummy2.data)
│ │                  assert np.array_equal(dummy.channel, dummy2.channel)
│ │                  assert np.array_equal(dummy.time, dummy2.time)
│ │                  del dummy, dummy2, out_sel
│ │  
│ │ -    @skip_without_acme
│ │      def test_parallel_equidistant(self, testcluster):
│ │          client = dd.Client(testcluster)
│ │          for parallel_store in [True, False]:
│ │              for chan_per_worker in [None, self.chanPerWrkr]:
│ │                  for sk, select in enumerate(self.sigdataSelections):
│ │                      # FIXME: remove as soon as channel-parallelization works w/channel selectors
│ │                      if chan_per_worker is not None:
│ │ @@ -298,29 +291,29 @@
│ │                      assert out.data.is_virtual == parallel_store
│ │  
│ │                      # check correct signal filtering (especially wrt data-selection)
│ │                      if select is None:
│ │                          reference = self.orig
│ │                      else:
│ │                          ref = []
│ │ -                        for tk, trlno in enumerate(sel.trials):
│ │ +                        for tk, trlno in enumerate(sel.trial_ids):
│ │                              ref.append(self.origdata.trials[trlno][sel.time[tk], sel.channel])
│ │                              # check for correct time selection
│ │                              assert np.array_equal(out.time[tk], self.sigdata.time[trlno][sel.time[tk]])
│ │                          reference = np.vstack(ref)
│ │                      assert self.metrix[sk](np.abs(out.data - reference)) < self.tols[sk]
│ │                      assert np.array_equal(out.channel, self.sigdata.channel[sel.channel])
│ │  
│ │                      # ensure correct no. HDF5 files were generated for virtual data-set
│ │                      if parallel_store:
│ │                          nfiles = len(glob(os.path.join(os.path.splitext(out.filename)[0], "*.h5")))
│ │                          if chan_per_worker is None:
│ │ -                            assert nfiles == len(sel.trials)
│ │ +                            assert nfiles == len(sel.trial_ids)
│ │                          else:
│ │ -                            assert nfiles == len(sel.trials) * (int(out.channel.size /
│ │ +                            assert nfiles == len(sel.trial_ids) * (int(out.channel.size /
│ │                                                                      chan_per_worker) +
│ │                                                                  int(out.channel.size % chan_per_worker > 0))
│ │  
│ │                      # ensure pre-selection is equivalent to in-place selection
│ │                      if select is None:
│ │                          selected = self.sigdata.selectdata()
│ │                      else:
│ │ @@ -337,76 +330,71 @@
│ │                                           keeptrials=False)
│ │  
│ │                      # check correct signal filtering (especially wrt data-selection)
│ │                      if select is None:
│ │                          reference = self.orig[:self.t.size, :]
│ │                      else:
│ │                          ref = np.zeros(out.trials[0].shape)
│ │ -                        for tk, trlno in enumerate(sel.trials):
│ │ +                        for tk, trlno in enumerate(sel.trial_ids):
│ │                              ref += self.origdata.trials[trlno][sel.time[tk], sel.channel]
│ │                              # check for correct time selection (accounting for trial-averaging)
│ │                              assert np.array_equal(out.time[0], self.sigdata.time[0][sel.time[0]])
│ │ -                        reference = ref / len(sel.trials)
│ │ +                        reference = ref / len(sel.trial_ids)
│ │                      assert self.metrix[sk](np.abs(out.data - reference)) < self.tols[sk]
│ │                      assert np.array_equal(out.channel, self.sigdata.channel[sel.channel])
│ │                      assert out.data.is_virtual == False
│ │  
│ │                      # ensure pre-selection is equivalent to in-place selection
│ │                      out_sel = filter_manager(selected, self.b, self.a,
│ │                                               parallel=True, parallel_store=parallel_store,
│ │                                               keeptrials=False)
│ │                      assert np.allclose(out.data, out_sel.data)
│ │                      assert np.array_equal(out.channel, out_sel.channel)
│ │                      assert np.array_equal(out.time, out_sel.time)
│ │  
│ │          client.close()
│ │  
│ │ -    @skip_without_acme
│ │      def test_parallel_nonequidistant(self, testcluster):
│ │          client = dd.Client(testcluster)
│ │          for overlapping in [False, True]:
│ │              nonequidata = generate_artificial_data(nTrials=self.nTrials,
│ │                                                      nChannels=self.nChannels,
│ │                                                      equidistant=False,
│ │                                                      overlapping=overlapping,
│ │                                                      inmemory=False)
│ │  
│ │ -            # unsorted, w/repetitions
│ │ -            toi = self.seed.choice(nonequidata.time[0], int(nonequidata.time[0].size))
│ │ -            self.artdataSelections[1]["toi"] = toi
│ │ -
│ │              for parallel_store in [True, False]:
│ │                  for chan_per_worker in [None, self.chanPerWrkr]:
│ │                      for select in self.artdataSelections:
│ │                          # FIXME: remove as soon as channel-parallelization works w/channel selectors
│ │                          if chan_per_worker is not None:
│ │                              select = None
│ │                          sel = Selector(nonequidata, select)
│ │                          out = filter_manager(nonequidata, self.b, self.a, select=select,
│ │                                               chan_per_worker=chan_per_worker, parallel=True,
│ │                                               parallel_store=parallel_store)
│ │  
│ │                          # compare expected w/actual shape of computed data
│ │                          reference = 0
│ │ -                        for tk, trlno in enumerate(sel.trials):
│ │ +                        for tk, trlno in enumerate(sel.trial_ids):
│ │                              reference += nonequidata.trials[trlno][sel.time[tk]].shape[0]
│ │                              # check for correct time selection
│ │                              # FIXME: remove `if` below as soon as `time` prop for lists is fixed
│ │                              if not isinstance(sel.time[0], list):
│ │                                  assert np.array_equal(out.time[tk], nonequidata.time[trlno][sel.time[tk]])
│ │                          assert out.data.shape[0] == reference
│ │                          assert np.array_equal(out.channel, nonequidata.channel[sel.channel])
│ │                          assert out.data.is_virtual == parallel_store
│ │  
│ │                          if parallel_store:
│ │                              nfiles = len(glob(os.path.join(os.path.splitext(out.filename)[0], "*.h5")))
│ │                              if chan_per_worker is None:
│ │ -                                assert nfiles == len(sel.trials)
│ │ +                                assert nfiles == len(sel.trial_ids)
│ │                              else:
│ │ -                                assert nfiles == len(sel.trials) * (int(out.channel.size /
│ │ +                                assert nfiles == len(sel.trial_ids) * (int(out.channel.size /
│ │                                                                          chan_per_worker) +
│ │                                                                      int(out.channel.size % chan_per_worker > 0))
│ │  
│ │                          # ensure pre-selection is equivalent to in-place selection
│ │                          if select is None:
│ │                              selected = nonequidata.selectdata()
│ │                          else:
│ │ @@ -417,40 +405,39 @@
│ │                          assert np.allclose(out.data, out_sel.data)
│ │                          assert np.array_equal(out.channel, out_sel.channel)
│ │                          for tk in range(len(out.trials)):
│ │                              assert np.array_equal(out.time[tk], out_sel.time[tk])
│ │  
│ │          client.close()
│ │  
│ │ -    @skip_without_acme
│ │      def test_parallel_saveload(self, testcluster):
│ │          client = dd.Client(testcluster)
│ │          for parallel_store in [True, False]:
│ │              for sk, select in enumerate(self.sigdataSelections):
│ │                  sel = Selector(self.sigdata, select)
│ │                  out = filter_manager(self.sigdata, self.b, self.a, select=select,
│ │                                       log_dict={"a": "this is a", "b": "this is b"},
│ │                                       parallel=True, parallel_store=parallel_store)
│ │  
│ │ -                assert len(out.trials) == len(sel.trials)
│ │ +                assert len(out.trials) == len(sel.trial_ids)
│ │                  # ensure our `log_dict` specification was respected
│ │                  assert "lowpass" in out._log
│ │                  assert "a = this is a" in out._log
│ │                  assert "b = this is b" in out._log
│ │  
│ │                  # ensure pre-selection is equivalent to in-place selection
│ │                  if select is None:
│ │                      selected = self.sigdata.selectdata()
│ │                  else:
│ │                      selected = self.sigdata.selectdata(**select)
│ │                  out_sel = filter_manager(selected, self.b, self.a,
│ │                                           log_dict={"a": "this is a", "b": "this is b"},
│ │                                           parallel=True, parallel_store=parallel_store)
│ │                  # only keyword args (`a` in this case here) are stored in `cfg`
│ │ -                assert len(out.trials) == len(sel.trials)
│ │ +                assert len(out.trials) == len(sel.trial_ids)
│ │                  # ensure our `log_dict` specification was respected
│ │                  assert "lowpass" in out._log
│ │                  assert "a = this is a" in out._log
│ │                  assert "b = this is b" in out._log
│ │  
│ │                  # save and re-load result, ensure nothing funky happens
│ │                  with tempfile.TemporaryDirectory() as tdir:
│ │ @@ -459,15 +446,15 @@
│ │                      dummy = load(fname)
│ │                      assert out.filename == dummy.filename
│ │                      assert not out.data.is_virtual
│ │                      if select is None:
│ │                          reference = self.orig
│ │                      else:
│ │                          ref = []
│ │ -                        for tk, trlno in enumerate(sel.trials):
│ │ +                        for tk, trlno in enumerate(sel.trial_ids):
│ │                              ref.append(self.origdata.trials[trlno][sel.time[tk], sel.channel])
│ │                              assert np.array_equal(dummy.time[tk], self.sigdata.time[trlno][sel.time[tk]])
│ │                          reference = np.vstack(ref)
│ │                      assert self.metrix[sk](np.abs(dummy.data - reference)) < self.tols[sk]
│ │                      assert np.array_equal(dummy.channel, self.sigdata.channel[sel.channel])
│ │                      # del dummy, out
│ │  
│ │ @@ -478,7 +465,11 @@
│ │                      assert np.array_equal(dummy.data, dummy2.data)
│ │                      assert np.array_equal(dummy.channel, dummy2.channel)
│ │                      assert np.array_equal(dummy.time, dummy2.time)
│ │                      assert not dummy2.data.is_virtual
│ │                      del dummy, dummy2, out, out_sel
│ │  
│ │          client.close()
│ │ +
│ │ +if __name__ == "__main__":
│ │ +    T1 = TestComputationalRoutine()
│ │ +
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_connectivity.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_spike_psth.py
│ │┄ Files 26% similar despite different names
│ │ @@ -1,493 +1,390 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │ -# Test connectivity measures
│ │ +# Test Peri-Stimulus Time Histogram
│ │  #
│ │  
│ │ -# 3rd party imports
│ │ -import psutil
│ │ -import pytest
│ │ -import inspect
│ │ -import numpy as np
│ │  import matplotlib.pyplot as ppl
│ │ +import numpy as np
│ │ +import pytest
│ │ +import dask.distributed as dd
│ │  
│ │ -# Local imports
│ │ -
│ │ -from syncopy import __acme__
│ │ -if __acme__:
│ │ -    import dask.distributed as dd
│ │ -
│ │ -from syncopy import AnalogData
│ │ -import syncopy.nwanalysis.connectivity_analysis as ca
│ │ -from syncopy import connectivityanalysis as cafunc
│ │ -import syncopy.tests.synth_data as synth_data
│ │ -import syncopy.tests.helpers as helpers
│ │ +# syncopy imports
│ │ +import syncopy as spy
│ │  from syncopy.shared.errors import SPYValueError
│ │ -from syncopy.shared.tools import get_defaults
│ │ -
│ │ -# Decorator to decide whether or not to run dask-related tests
│ │ -skip_without_acme = pytest.mark.skipif(not __acme__, reason="acme not available")
│ │ -# Decorator to decide whether or not to run memory-intensive tests
│ │ -availMem = psutil.virtual_memory().total
│ │ -minRAM = 5
│ │ -skip_low_mem = pytest.mark.skipif(availMem < minRAM * 1024**3, reason=f"less than {minRAM}GB RAM available")
│ │ -
│ │ -
│ │ -class TestGranger:
│ │ -
│ │ -    nTrials = 100
│ │ -    nChannels = 5
│ │ -    nSamples = 1000
│ │ -    fs = 200
│ │ -
│ │ -    # -- Create a somewhat intricate
│ │ -    # -- network of AR(2) processes
│ │ -
│ │ -    # the adjacency matrix
│ │ -    # encodes coupling strength directly
│ │ -    AdjMat = np.zeros((nChannels, nChannels))
│ │ -    AdjMat[0, 4] = 0.15
│ │ -    AdjMat[3, 4] = 0.15
│ │ -    AdjMat[3, 2] = 0.25
│ │ -    AdjMat[1, 0] = 0.25
│ │ -
│ │ -    # channel indices of coupling
│ │ -    # a number other than 0 at AdjMat(i,j)
│ │ -    # means coupling from i->j
│ │ -    cpl_idx = np.where(AdjMat)
│ │ -    nocpl_idx = np.where(AdjMat == 0)
│ │ -
│ │ -    data = synth_data.AR2_network(nTrials,
│ │ -                                  AdjMat=AdjMat,
│ │ -                                  nSamples=nSamples,
│ │ -                                  samplerate=fs)
│ │ -    time_span = [-1, nSamples / fs - 1]   # -1s offset
│ │ -    foi = np.arange(5, 75)   # in Hz
│ │ -
│ │ -    def test_gr_solution(self, **kwargs):
│ │ -
│ │ -        Gcaus = cafunc(self.data, method='granger',
│ │ -                       tapsmofrq=3, foi=None, **kwargs)
│ │ -
│ │ -        # check all channel combinations with coupling
│ │ -        for i, j in zip(*self.cpl_idx):
│ │ -            peak = Gcaus.data[0, :, i, j].max()
│ │ -            peak_frq = Gcaus.freq[Gcaus.data[0, :, i, j].argmax()]
│ │ -            cval = self.AdjMat[i, j]
│ │ -
│ │ -            dbg_str = f"{peak:.2f}\t{self.AdjMat[i,j]:.2f}\t {peak_frq:.2f}\t"
│ │ -            print(dbg_str, f'\t {i}', f' {j}')
│ │ -
│ │ -            # test for directional coupling
│ │ -            # at the right frequency range
│ │ -            assert peak >= cval
│ │ -            assert 35 < peak_frq < 45
│ │ -
│ │ -            # only plot with defaults
│ │ -            if len(kwargs) == 0:
│ │ -                plot_Granger(Gcaus, i, j)
│ │ -                ppl.legend()
│ │ -
│ │ -    def test_gr_selections(self):
│ │ -
│ │ -        # trial, channel and toi selections
│ │ -        selections = helpers.mk_selection_dicts(self.nTrials,
│ │ -                                                self.nChannels,
│ │ -                                                *self.time_span)
│ │ -
│ │ -        for sel_dct in selections:
│ │ -            Gcaus = cafunc(self.data, method='granger', select=sel_dct)
│ │ -
│ │ -            # check here just for finiteness and positivity
│ │ -            assert np.all(np.isfinite(Gcaus.data))
│ │ -            assert np.all(Gcaus.data[0, ...] >= -1e-10)
│ │ -
│ │ -    def test_gr_foi(self):
│ │ -
│ │ -        try:
│ │ -            cafunc(self.data,
│ │ -                   method='granger',
│ │ -                   foi=np.arange(0, 70)
│ │ -                   )
│ │ -        except SPYValueError as err:
│ │ -            assert 'no foi specification' in str(err)
│ │ -
│ │ -        try:
│ │ -            cafunc(self.data,
│ │ -                   method='granger',
│ │ -                   foilim=[0, 70]
│ │ -                   )
│ │ -        except SPYValueError as err:
│ │ -            assert 'no foi specification' in str(err)
│ │ -
│ │ -    def test_gr_cfg(self):
│ │ -
│ │ -        call = lambda cfg: cafunc(self.data, cfg)
│ │ -        run_cfg_test(call, method='granger',
│ │ -                     cfg=get_defaults(cafunc))
│ │ -
│ │ -    @skip_without_acme
│ │ -    @skip_low_mem
│ │ -    def test_gr_parallel(self, testcluster=None):
│ │ -
│ │ -        ppl.ioff()
│ │ -        client = dd.Client(testcluster)
│ │ -        all_tests = [attr for attr in self.__dir__()
│ │ -                     if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
│ │ -
│ │ -        for test in all_tests:
│ │ -            test_method = getattr(self, test)
│ │ -            test_method()
│ │ -        client.close()
│ │ -        ppl.ion()
│ │ -
│ │ -    def test_gr_padding(self):
│ │ -
│ │ -        pad_length = 6   # seconds
│ │ -        call = lambda pad: self.test_gr_solution(pad=pad)
│ │ -        helpers.run_padding_test(call, pad_length)
│ │ -
│ │ -    def test_gr_polyremoval(self):
│ │ -
│ │ -        # add a constant to the signals
│ │ -        self.data = self.data + 10
│ │ -
│ │ -        call = lambda polyremoval: self.test_gr_solution(polyremoval=polyremoval)
│ │ -        helpers.run_polyremoval_test(call)
│ │ -
│ │ -        # remove the constant again
│ │ -        self.data = self.data - 10
│ │ -
│ │ -
│ │ -class TestCoherence:
│ │ -
│ │ -    nSamples = 1500
│ │ -    nChannels = 6
│ │ -    nTrials = 100
│ │ -    fs = 1000
│ │ -
│ │ -    # -- two harmonics with individual phase diffusion --
│ │ -
│ │ -    f1, f2 = 20, 40
│ │ -    # a lot of phase diffusion (1% per step) in the 20Hz band
│ │ -    s1 = synth_data.phase_diffusion(nTrials, freq=f1,
│ │ -                                    eps=.01,
│ │ -                                    nChannels=nChannels,
│ │ -                                    nSamples=nSamples)
│ │ -    # little diffusion in the 40Hz band
│ │ -    s2 = synth_data.phase_diffusion(nTrials, freq=f2,
│ │ -                                    eps=.001,
│ │ -                                    nChannels=nChannels,
│ │ -                                    nSamples=nSamples)
│ │ -    wn = synth_data.white_noise(nTrials, nChannels=nChannels, nSamples=nSamples)
│ │ -
│ │ -    # superposition
│ │ -    data = s1 + s2 + wn
│ │ -    data.samplerate = fs
│ │ -    time_span = [-1, nSamples / fs - 1]   # -1s offset
│ │ -
│ │ -    def test_coh_solution(self, **kwargs):
│ │ -
│ │ -        res = cafunc(data=self.data,
│ │ -                     method='coh',
│ │ -                     foilim=[5, 60],
│ │ -                     tapsmofrq=1.5,
│ │ -                     **kwargs)
│ │ -
│ │ -        # coherence at the harmonic frequencies
│ │ -        idx_f1 = np.argmin(res.freq < self.f1)
│ │ -        peak_f1 = res.data[0, idx_f1, 0, 1]
│ │ -        idx_f2 = np.argmin(res.freq < self.f2)
│ │ -        peak_f2 = res.data[0, idx_f2, 0, 1]
│ │ -
│ │ -        # check low phase diffusion has high coherence
│ │ -        assert peak_f2 > 0.5
│ │ -        # check that with higher phase diffusion the
│ │ -        # coherence is lower
│ │ -        assert peak_f1 < peak_f2
│ │ -
│ │ -        # check that 5Hz away from the harmonics there
│ │ -        # is low coherence
│ │ -        null_idx = (res.freq < self.f1 - 5) | (res.freq > self.f1 + 5)
│ │ -        null_idx *= (res.freq < self.f2 - 5) | (res.freq > self.f2 + 5)
│ │ -        assert np.all(res.data[0, null_idx, 0, 1] < 0.2)
│ │ -
│ │ -        if kwargs is None:
│ │ -            res.singlepanelplot(channel_i=0, channel_j=1)
│ │ -
│ │ -    def test_coh_selections(self):
│ │ -
│ │ -        selections = helpers.mk_selection_dicts(self.nTrials,
│ │ -                                                self.nChannels,
│ │ -                                                *self.time_span)
│ │ +from syncopy.tests import synth_data as sd
│ │ +from syncopy.statistics.spike_psth import available_outputs
│ │  
│ │ -        for sel_dct in selections:
│ │  
│ │ -            result = cafunc(self.data, method='coh', select=sel_dct)
│ │ +def get_spike_data(nTrials = 10, seed=None):
│ │ +    return sd.poisson_noise(nTrials,
│ │ +                            nUnits=3,
│ │ +                            nChannels=2,
│ │ +                            nSpikes=10_000,
│ │ +                            samplerate=10_000,
│ │ +                            seed=seed)
│ │ +
│ │ +
│ │ +def get_spike_cfg():
│ │ +    cfg = spy.StructDict()
│ │ +    cfg.binsize = 0.3
│ │ +    cfg.latency = [-.5, 1.5]
│ │ +    return cfg
│ │ +
│ │ +
│ │ +class TestPSTH:
│ │ +
│ │ +    # synthetic spike data
│ │ +    spd = get_spike_data(seed=42)
│ │ +
│ │ +    def test_psth_binsize(self):
│ │ +
│ │ +        cfg = spy.StructDict()
│ │ +        cfg.latency = 'maxperiod'  # default
│ │ +
│ │ +        # directly in seconds
│ │ +        cfg.binsize = 0.2
│ │ +        counts = spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        assert isinstance(counts, spy.TimeLockData)
│ │ +        # check that all trials have the same 'time locked' time axis
│ │ +        # as enfored by TimeLockData.trialdefinition setter
│ │ +        assert len(set([t.size for t in counts.time])) == 1
│ │ +
│ │ +        # check that time steps correspond to binsize
│ │ +        assert np.allclose(np.diff(counts.time[0]), cfg.binsize)
│ │ +
│ │ +        # automatic binsize selection
│ │ +        cfg.binsize = 'rice'
│ │ +        counts = spy.spike_psth(self.spd,
│ │ +                                cfg,
│ │ +                                keeptrials=True)
│ │ +        # number of bins is length of time axis
│ │ +        nBins_rice = counts.time[0].size
│ │ +        assert len(set([t.size for t in counts.time])) == 1
│ │ +
│ │ +        cfg.binsize = 'sqrt'
│ │ +        counts = spy.spike_psth(self.spd,
│ │ +                                cfg,
│ │ +                                keeptrials=True)
│ │ +        # number of bins is length of time axis
│ │ +        nBins_sqrt = counts.time[0].size
│ │ +        assert len(set([t.size for t in counts.time])) == 1
│ │ +
│ │ +        # sqrt rule gives more bins than Rice rule
│ │ +        assert nBins_sqrt > nBins_rice
│ │ +
│ │ +    def test_psth_latency(self):
│ │ +
│ │ +        """Test all available `latency` (time window interval) settings"""
│ │ +
│ │ +        cfg = spy.StructDict()
│ │ +        # directly in seconds
│ │ +        cfg.binsize = 0.1
│ │ +
│ │ +        trl_starts = self.spd.trialintervals[:, 0]
│ │ +        trl_ends = self.spd.trialintervals[:, 1]
│ │ +
│ │ +        # -- bins stretch over the largest common time window --
│ │ +        cfg.latency = 'maxperiod'  # frontend default
│ │ +        counts = spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # sampling interval for histogram output
│ │ +        delta_t = 1 / counts.samplerate
│ │ +
│ │ +        # check that histogram time points are less than 1
│ │ +        # delta_t away from the maximal interval boundaries
│ │ +        assert np.abs(trl_starts.min() - counts.time[0][0]) < delta_t
│ │ +        assert np.abs(trl_ends.max() - counts.time[0][-1]) < delta_t
│ │ +
│ │ +        # check that there are NaNs as not all trials have data
│ │ +        # in this maximal interval (due to start/end randomization)
│ │ +        assert np.any(np.isnan(counts.data[:]))
│ │ +
│ │ +        # -- bins stretch over the minimal interval present in all trials --
│ │ +        cfg.latency = 'minperiod'
│ │ +        counts = spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # check that histogram time points are less than 1
│ │ +        # delta_t away from the minimal interval boundaries
│ │ +        assert np.abs(trl_starts.max() - counts.time[0][0]) < delta_t
│ │ +        assert np.abs(trl_ends.min() - counts.time[0][-1]) < delta_t
│ │ +
│ │ +        # check that there are NO NaNs as all trials have data
│ │ +        # in this minimal interval
│ │ +        assert not np.any(np.isnan(counts.data[:]))
│ │ +
│ │ +        # -- prestim --> only events with t < 0
│ │ +        cfg.latency = 'prestim'
│ │ +        counts = spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        assert np.all(counts.time[0] <= 0)
│ │ +
│ │ +        # -- poststim --> only events with t > 0
│ │ +        cfg.latency = 'poststim'
│ │ +        counts = spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        assert np.all(counts.time[0] >= 0)
│ │ +
│ │ +        # -- finally the manual latency interval --
│ │ +        # this is way to big, so we have many NaNs (empty bins)
│ │ +        cfg.latency = [-.5, 1.5]   # in seconds
│ │ +        assert cfg.latency[0] < trl_starts.min()
│ │ +        assert cfg.latency[1] > trl_ends.max()
│ │ +
│ │ +        counts = spy.spike_psth(self.spd, cfg)
│ │ +        # check that histogram time points are less than 1
│ │ +        # delta_t away from the manual set interval boundaries
│ │ +        assert np.abs(cfg.latency[0] - counts.time[0][0]) <= delta_t
│ │ +        # the midpoint gets rounded down, so the last time point is close to
│ │ +        # 1 delta_t off actually..
│ │ +        assert np.allclose(np.abs(cfg.latency[1] - counts.time[0][-1]), delta_t)
│ │ +
│ │ +        # check that there are NaNs as the interval is way too large
│ │ +        assert np.any(np.isnan(counts.data[:]))
│ │ +
│ │ +    def test_psth_vartriallen(self):
│ │ +
│ │ +        """
│ │ +        Test setting vartriallen to False excludes trials which
│ │ +        don't cover the latency time window
│ │ +        """
│ │ +
│ │ +        # everything else default
│ │ +        cfg = spy.StructDict()
│ │ +        cfg.vartriallen = False
│ │ +
│ │ +        starts, ends = self.spd.trialintervals[:, 0], self.spd.trialintervals[:, -1]
│ │ +
│ │ +        # choose latency which excludes 3 trials because
│ │ +        # of starting time (condition is <= latency)
│ │ +        cfg.latency = [np.sort(starts)[-4], ends.min()]
│ │ +        counts = spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # check that 3 trials were excluded
│ │ +        assert len(self.spd.trials) - len(counts.trials) == 3
│ │ +
│ │ +        # choose latency which excludes 3 trials because
│ │ +        # of end time (condition is >= latency)
│ │ +        cfg.latency = [starts.max(), np.sort(ends)[3]]
│ │ +        counts = spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # check that 3 trials were excluded
│ │ +        assert len(self.spd.trials) - len(counts.trials) == 3
│ │ +
│ │ +        # setting latency to 'maxperiod' with vartriallen=False
│ │ +        # excludes all trials which raises an error
│ │ +        cfg.latency = 'maxperiod'
│ │ +        with pytest.raises(SPYValueError,
│ │ +                           match='no trial that completely covers the latency window'):
│ │ +            counts = spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # setting latency to 'minperiod' with vartriallen=False
│ │ +        # excludes no trials by definition of 'minperiod'
│ │ +        cfg.latency = 'minperiod'
│ │ +        counts = spy.spike_psth(self.spd, cfg)
│ │ +        # check that 0 trials were excluded
│ │ +        assert len(self.spd.trials) - len(counts.trials) == 0
│ │ +
│ │ +    def test_psth_outputs(self):
│ │ +
│ │ +        cfg = spy.StructDict()
│ │ +        cfg.latency = 'minperiod'  # to avoid NaNs
│ │ +        cfg.output = 'spikecount'
│ │ +        cfg.binsize = 0.1  # in seconds
│ │ +
│ │ +        counts = spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # -- plot single trial statistics --
│ │ +        # single trials have high variance, see below
│ │ +
│ │ +        last_data = np.zeros(counts.time[0].size)
│ │ +        for chan in counts.channel:
│ │ +            bars = counts.show(trials=5, channel=chan)
│ │ +            ppl.bar(counts.time[0], bars, alpha=0.7, bottom=last_data,
│ │ +                    width=0.9 / counts.samplerate, label=chan)
│ │ +            # for stacking
│ │ +            last_data += bars
│ │ +        ppl.legend()
│ │ +        ppl.xlabel('time (s)')
│ │ +        ppl.ylabel('spike counts')
│ │ +
│ │ +        # -- plot mean and variance --
│ │ +
│ │ +        # shows that each channel-unit combination
│ │ +        # has a flat distribution as expected for poisson noise
│ │ +        # however the absolute intensity differs: we have more and less
│ │ +        # active channels/units by synthetic data costruction
│ │ +
│ │ +        ppl.figure()
│ │ +        ppl.title("Trial statistics")
│ │ +        last_data = np.zeros(len(counts.time[0]))
│ │ +        for chan in range(len(counts.channel)):
│ │ +            bars = counts.avg[:, chan]
│ │ +            yerr = counts.var[:, chan]
│ │ +            ppl.bar(counts.time[0], bars, alpha=0.7, bottom=last_data,
│ │ +                    width=0.9 / counts.samplerate, label=chan, yerr=yerr, capsize=2)
│ │ +            # for stacking
│ │ +            last_data += bars
│ │ +        ppl.legend()
│ │ +        ppl.xlabel('time (s)')
│ │ +        ppl.ylabel('spike counts')
│ │ +
│ │ +        cfg.output = 'rate'  # the default
│ │ +        rates = spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # check that the rates are just the counts times samplerate
│ │ +        assert counts * counts.samplerate == rates
│ │ +
│ │ +        # this gives the spike histogram as normalized density
│ │ +        cfg.output = 'proportion'
│ │ +        cfg.latency = 'maxperiod'  # to provoke NaNs
│ │ +        spike_densities = spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # check that there are NaNs as not all trials have data
│ │ +        # in this maximal interval (due to start/end randomization)
│ │ +        assert np.any(np.isnan(spike_densities.data[:]))
│ │ +
│ │ +        # check for one arbitrary trial should be enough
│ │ +        for chan in spike_densities.channel:
│ │ +            integral = np.nansum(spike_densities.show(trials=2, channel=chan))
│ │ +            assert np.allclose(integral, 1, atol=1e-3)
│ │ +
│ │ +    def test_psth_exceptions(self):
│ │ +
│ │ +        cfg = spy.StructDict()
│ │ +
│ │ +        # -- output validation --
│ │ +
│ │ +        # invalid string
│ │ +        cfg.output = 'counts'
│ │ +        with pytest.raises(SPYValueError,
│ │ +                           match="expected one of"):
│ │ +            spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # invalid type
│ │ +        cfg.output = 12
│ │ +        with pytest.raises(SPYValueError,
│ │ +                           match="expected one of"):
│ │ +            spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # -- binsize validation --
│ │ +
│ │ +        cfg.output = 'rate'
│ │ +        # no negative binsizes
│ │ +        cfg.binsize = -0.2
│ │ +        with pytest.raises(SPYValueError,
│ │ +                           match="expected value to be greater"):
│ │ +            spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        cfg.latency = [0, 0.2]
│ │ +        # binsize larger than time interval
│ │ +        cfg.binsize = 0.3
│ │ +        with pytest.raises(SPYValueError,
│ │ +                           match="less or equals 0.2"):
│ │ +            spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # not available rule
│ │ +        cfg.binsize = 'sth'
│ │ +        with pytest.raises(SPYValueError,
│ │ +                           match="expected one of"):
│ │ +            spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # -- latency validation --
│ │ +
│ │ +        cfg.binsize = 0.1
│ │ +        # not available latency
│ │ +        cfg.latency = 'sth'
│ │ +        with pytest.raises(SPYValueError,
│ │ +                           match="expected one of"):
│ │ +            spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # latency not ordered
│ │ +        cfg.latency = [0.1, 0]
│ │ +        with pytest.raises(SPYValueError,
│ │ +                           match="expected start < end"):
│ │ +            spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +        # latency completely outside of data
│ │ +        cfg.latency = [-999, -99]
│ │ +        with pytest.raises(SPYValueError,
│ │ +                           match="expected end of latency window"):
│ │ +            spy.spike_psth(self.spd, cfg)
│ │ +        cfg.latency = [99, 999]
│ │ +        with pytest.raises(SPYValueError,
│ │ +                           match="expected start of latency window"):
│ │ +            spy.spike_psth(self.spd, cfg)
│ │ +
│ │ +    def test_psth_chan_unit_mapping(self):
│ │ +        """
│ │ +        Test that non-existent channel/unit combinations
│ │ +        are accounted for correctly
│ │ +        """
│ │ +
│ │ +        # check that unit 1 really is there
│ │ +        assert np.any(self.spd.data[:, 2] == 1)
│ │ +        counts = spy.spike_psth(self.spd, output='spikecount')
│ │ +        assert 'channel0_unit1' in counts.channel
│ │ +        assert 'channel1_unit1' in counts.channel
│ │ +
│ │ +        # get rid of unit 1
│ │ +        pruned_spd = self.spd.selectdata(unit=[0, 2])
│ │ +        # check that unit 1 really is gone
│ │ +        assert np.all(pruned_spd.data[:, 2] != 1)
│ │ +
│ │ +        pruned_counts = spy.spike_psth(pruned_spd, output='spikecount')
│ │ +        # check that unit 1 really is gone
│ │ +        assert len(pruned_counts.channel) < len(counts.channel)
│ │ +        assert 'channel0_unit1' not in pruned_counts.channel
│ │ +        assert 'channel1_unit1' not in pruned_counts.channel
│ │ +
│ │ +        # check that counts for remaining channel/units are unchanged
│ │ +        for chan in pruned_counts.channel:
│ │ +            assert np.array_equal(counts.show(trials=4, channel=chan),
│ │ +                                  pruned_counts.show(trials=4, channel=chan),
│ │ +                                  equal_nan=True)
│ │ +
│ │ +        # now the same with an active in-place selection
│ │ +        # Already fixed: #332
│ │ +        # get rid of unit 1
│ │ +        # self.spd.selectdata(unit=[0, 2], inplace=True)
│ │ +
│ │ +        pruned_counts2 = spy.spike_psth(self.spd, output='spikecount', select={'unit': [0, 2]})
│ │ +
│ │ +        # check that unit 1 really is gone
│ │ +        assert len(pruned_counts2.channel) < len(counts.channel)
│ │ +        assert 'channel0_unit1' not in pruned_counts2.channel
│ │ +        assert 'channel1_unit1' not in pruned_counts2.channel
│ │ +        # check that counts for remaining channel/units are unchanged
│ │ +        for chan in pruned_counts2.channel:
│ │ +            assert np.array_equal(counts.show(trials=4, channel=chan),
│ │ +                                  pruned_counts2.show(trials=4, channel=chan),
│ │ +                                  equal_nan=True)
│ │ +
│ │ +    def test_parallel_selection(self, testcluster):
│ │ +
│ │ +        cfg = spy.StructDict()
│ │ +        cfg.latency = 'minperiod'
│ │ +        cfg.parallel = True
│ │  
│ │ -            # check here just for finiteness and positivity
│ │ -            assert np.all(np.isfinite(result.data))
│ │ -            assert np.all(result.data[0, ...] >= -1e-10)
│ │ -
│ │ -    def test_coh_foi(self):
│ │ -
│ │ -        call = lambda foi, foilim: cafunc(self.data,
│ │ -                                          method='coh',
│ │ -                                          foi=foi,
│ │ -                                          foilim=foilim)
│ │ -
│ │ -        helpers.run_foi_test(call, foilim=[0, 70])
│ │ -
│ │ -    def test_coh_cfg(self):
│ │ -
│ │ -        call = lambda cfg: cafunc(self.data, cfg)
│ │ -        run_cfg_test(call, method='coh',
│ │ -                     cfg=get_defaults(cafunc))
│ │ -
│ │ -    @skip_without_acme
│ │ -    @skip_low_mem
│ │ -    def test_coh_parallel(self, testcluster=None):
│ │ -
│ │ -        ppl.ioff()
│ │          client = dd.Client(testcluster)
│ │ -        all_tests = [attr for attr in self.__dir__()
│ │ -                     if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
│ │  
│ │ -        for test in all_tests:
│ │ -            test_method = getattr(self, test)
│ │ -            test_method()
│ │ -        client.close()
│ │ -        ppl.ion()
│ │ +        # test standard run
│ │ +        counts = spy.spike_psth(self.spd, cfg)
│ │ +        # check that there are NO NaNs as all trials
│ │ +        # have data in `minperiod` by definition
│ │ +        assert not np.any(np.isnan(counts.data[:]))
│ │ +
│ │ +        # test channel selection
│ │ +        cfg.select = {'channel': 0}
│ │ +        counts = spy.spike_psth(self.spd, cfg)
│ │ +        assert all(['channel1' not in chan for chan in counts.channel])
│ │  
│ │ -    def test_coh_padding(self):
│ │ -
│ │ -        pad_length = 2   # seconds
│ │ -        call = lambda pad: self.test_coh_solution(pad=pad)
│ │ -        helpers.run_padding_test(call, pad_length)
│ │ -
│ │ -    def test_coh_polyremoval(self):
│ │ -
│ │ -        call = lambda polyremoval: self.test_coh_solution(polyremoval=polyremoval)
│ │ -        helpers.run_polyremoval_test(call)
│ │ -
│ │ -    def test_coh_outputs(self):
│ │ -
│ │ -        for output in ca.coh_outputs:
│ │ -            coh = cafunc(self.data,
│ │ -                         method='coh',
│ │ -                         output=output)
│ │ -
│ │ -            if output in ['complex', 'fourier']:
│ │ -                # we have imaginary parts
│ │ -                assert not np.all(np.imag(coh.trials[0]) == 0)
│ │ -            elif output == 'angle':
│ │ -                # all values in [-pi, pi]
│ │ -                assert np.all((coh.trials[0] < np.pi) | (coh.trials[0] > -np.pi))
│ │ -            else:
│ │ -                # strictly real outputs
│ │ -                assert np.all(np.imag(coh.trials[0]) == 0)
│ │ -
│ │ -
│ │ -class TestCorrelation:
│ │ -
│ │ -    nChannels = 5
│ │ -    nTrials = 10
│ │ -    fs = 1000
│ │ -    nSamples = 2000   # 2s long signals
│ │ -
│ │ -    # -- a single harmonic with phase shifts between channels
│ │ -
│ │ -    f1 = 10   # period is 0.1s
│ │ -    trls = []
│ │ -    for _ in range(nTrials):
│ │ -
│ │ -        # no phase diffusion
│ │ -        p1 = synth_data.phase_diffusion(freq=f1,
│ │ -                                        eps=0,
│ │ -                                        nChannels=nChannels,
│ │ -                                        nSamples=nSamples,
│ │ -                                        return_phase=True)
│ │ -        # same frequency but more diffusion
│ │ -        p2 = synth_data.phase_diffusion(freq=f1,
│ │ -                                        eps=0.1,
│ │ -                                        nChannels=1,
│ │ -                                        nSamples=nSamples,
│ │ -                                        return_phase=True)
│ │ -
│ │ -        # set 2nd channel to higher phase diffusion
│ │ -        p1[:, 1] = p2[:, 0]
│ │ -        # add a pi/2 phase shift for the even channels
│ │ -        p1[:, 2::2] += np.pi / 2
│ │ -
│ │ -        trls.append(np.cos(p1))
│ │ -
│ │ -    data = AnalogData(trls, samplerate=fs)
│ │ -    time_span = [-1, nSamples / fs - 1]  # -1s offset
│ │ -
│ │ -    def test_corr_solution(self, **kwargs):
│ │ -
│ │ -        corr = cafunc(data=self.data, method='corr', **kwargs)
│ │ -
│ │ -        # test 0-lag autocorr is 1 for all channels
│ │ -        assert np.all(corr.data[0, 0].diagonal() > .99)
│ │ -
│ │ -        # test that at exactly the period-lag
│ │ -        # correlations remain high w/o phase diffusion
│ │ -        period_idx = int(1 / self.f1 * self.fs)
│ │ -        # 100 samples is one period
│ │ -        assert np.allclose(100, period_idx)
│ │ -        auto_00 = corr.data[:, 0, 0, 0]
│ │ -        assert np.all(auto_00[::period_idx] > .99)
│ │ -
│ │ -        # test for auto-corr minima at half the period
│ │ -        assert auto_00[period_idx // 2] < -.99
│ │ -        assert auto_00[period_idx // 2 + period_idx] < -.99
│ │ -
│ │ -        # test signal with phase diffusion (2nd channel) has
│ │ -        # decaying correlations (diffusion may lead to later
│ │ -        # increases of auto-correlation again, hence we check
│ │ -        # only the first 5 periods)
│ │ -        auto_11 = corr.data[:, 0, 1, 1]
│ │ -        assert np.all(np.diff(auto_11[::period_idx])[:5] < 0)
│ │ -
│ │ -        # test that a pi/2 phase shift moves the 1st
│ │ -        # crosscorr maximum to 1/4 of the period
│ │ -        cross_02 = corr.data[:, 0, 0, 2]
│ │ -        lag_idx = int(1 / self.f1 * self.fs * 0.25)
│ │ -        # 25 samples is 1/4th period
│ │ -        assert np.allclose(25, lag_idx)
│ │ -        assert cross_02[lag_idx] > 0.99
│ │ -        # same for a period multiple
│ │ -        assert cross_02[lag_idx + period_idx] > .99
│ │ -        # plus half the period a minimum occurs
│ │ -        assert cross_02[lag_idx + period_idx // 2] < -.99
│ │ -
│ │ -        # test for (anti-)symmetry
│ │ -        cross_20 = corr.data[:, 0, 2, 0]
│ │ -        assert cross_20[-lag_idx] > 0.99
│ │ -        assert cross_20[-lag_idx - period_idx] > 0.99
│ │ -
│ │ -        # only plot for simple solution test
│ │ -        if len(kwargs) == 0:
│ │ -            plot_corr(corr, 0, 0, label='corr 0-0')
│ │ -            plot_corr(corr, 1, 1, label='corr 1-1')
│ │ -            plot_corr(corr, 0, 2, label='corr 0-2')
│ │ -            ppl.xlim((-.01, 0.5))
│ │ -            ppl.ylim((-1.1, 1.3))
│ │ -            ppl.legend(ncol=3)
│ │ -
│ │ -    def test_corr_padding(self):
│ │ -
│ │ -        self.test_corr_solution(pad='maxperlen')
│ │ -        # no padding is allowed for
│ │ -        # this method
│ │ -        try:
│ │ -            self.test_corr_solution(pad=1000)
│ │ -        except SPYValueError as err:
│ │ -            assert 'pad' in str(err)
│ │ -            assert 'no padding needed/allowed' in str(err)
│ │ -
│ │ -        try:
│ │ -            self.test_corr_solution(pad='nextpow2')
│ │ -        except SPYValueError as err:
│ │ -            assert 'pad' in str(err)
│ │ -            assert 'no padding needed/allowed' in str(err)
│ │ -
│ │ -        try:
│ │ -            self.test_corr_solution(pad='IamNoPad')
│ │ -        except SPYValueError as err:
│ │ -            assert 'Invalid value of `pad`' in str(err)
│ │ -            assert 'no padding needed/allowed' in str(err)
│ │ -
│ │ -    def test_corr_selections(self):
│ │ -
│ │ -        selections = helpers.mk_selection_dicts(self.nTrials,
│ │ -                                                self.nChannels,
│ │ -                                                *self.time_span)
│ │ -
│ │ -        for sel_dct in selections:
│ │ -
│ │ -            result = cafunc(self.data, method='corr', select=sel_dct)
│ │ -
│ │ -            # check here just for finiteness and positivity
│ │ -            assert np.all(np.isfinite(result.data))
│ │ -
│ │ -    def test_corr_cfg(self):
│ │ -
│ │ -        call = lambda cfg: cafunc(self.data, cfg)
│ │ -        run_cfg_test(call, method='corr',
│ │ -                     positivity=False,
│ │ -                     cfg=get_defaults(cafunc))
│ │ -
│ │ -    @skip_without_acme
│ │ -    @skip_low_mem
│ │ -    def test_corr_parallel(self, testcluster=None):
│ │ -
│ │ -        ppl.ioff()
│ │ -        client = dd.Client(testcluster)
│ │ -        all_tests = [attr for attr in self.__dir__()
│ │ -                     if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
│ │ -
│ │ -        for test in all_tests:
│ │ -            test_method = getattr(self, test)
│ │ -            test_method()
│ │          client.close()
│ │ -        ppl.ion()
│ │ -
│ │ -    def test_corr_polyremoval(self):
│ │ -
│ │ -        call = lambda polyremoval: self.test_corr_solution(polyremoval=polyremoval)
│ │ -        helpers.run_polyremoval_test(call)
│ │ -
│ │ -
│ │ -def run_cfg_test(method_call, method, cfg, positivity=True):
│ │ -
│ │ -    cfg.method = method
│ │ -    if method != 'granger':
│ │ -        cfg.foilim = [0, 70]
│ │ -    # test general tapers with
│ │ -    # additional parameters
│ │ -    cfg.taper = 'kaiser'
│ │ -    cfg.taper_opt = {'beta': 2}
│ │ -
│ │ -    cfg.output = 'abs'
│ │ -
│ │ -    result = method_call(cfg)
│ │ -
│ │ -    # check here just for finiteness and positivity
│ │ -    assert np.all(np.isfinite(result.data))
│ │ -    if positivity:
│ │ -        assert np.all(result.data[0, ...] >= -1e-10)
│ │ -
│ │ -
│ │ -def plot_Granger(G, i, j):
│ │ -
│ │ -    ax = ppl.gca()
│ │ -    ax.set_xlabel('frequency (Hz)')
│ │ -    ax.set_ylabel(r'Granger causality(f)')
│ │ -    ax.plot(G.freq, G.data[0, :, i, j], label=f'Granger {i}-{j}',
│ │ -            alpha=0.7, lw=1.3)
│ │ -    ax.set_ylim((-.1, 1.3))
│ │ -
│ │ -
│ │ -def plot_coh(res, i, j, label=''):
│ │ -
│ │ -    ax = ppl.gca()
│ │ -    ax.set_xlabel('frequency (Hz)')
│ │ -    ax.set_ylabel('coherence $|CSD|^2$')
│ │ -    ax.plot(res.freq, res.data[0, :, i, j], label=label)
│ │ -    ax.legend()
│ │ -
│ │ -
│ │ -def plot_corr(res, i, j, label=''):
│ │ -
│ │ -    ax = ppl.gca()
│ │ -    ax.set_xlabel('lag (s)')
│ │ -    ax.set_ylabel('Correlation')
│ │ -    ax.plot(res.time[0], res.data[:, 0, i, j], label=label)
│ │ -    ax.legend()
│ │  
│ │  
│ │  if __name__ == '__main__':
│ │ -    T1 = TestGranger()
│ │ -    T2 = TestCoherence()
│ │ -    T3 = TestCorrelation()
│ │ +    T1 = TestPSTH()
│ │ +    spd = T1.spd
│ │ +    trl0 = spd.trials[0]
│ │ +    spd.selectdata(unit=[0,2], inplace=True)
│ │ +    arr1 = spd.selection._get_trial(1)
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_decorators.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_decorators.py
│ │┄ Files 18% similar despite different names
│ │ @@ -1,36 +1,38 @@
│ │  # -*- coding: utf-8 -*-
│ │  #
│ │  # Test proper functionality of Syncopy's decorator mechanics
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │ +import numpy as np
│ │  import string
│ │  import pytest
│ │ +
│ │  from syncopy.shared.kwarg_decorators import unwrap_cfg, unwrap_select
│ │  from syncopy.tests.misc import generate_artificial_data
│ │  from syncopy.shared.tools import StructDict
│ │  from syncopy.shared.errors import SPYValueError, SPYTypeError, SPYError
│ │  
│ │  
│ │  @unwrap_cfg
│ │  @unwrap_select
│ │ -def group_objects(*data, groupbychan=None, select=None):
│ │ +def group_objects(data, groupbychan=None, select=None):
│ │      """
│ │      Dummy function that collects the `filename` property of all
│ │      input objects that contain a specific channel given by
│ │      `groupbychan`
│ │      """
│ │ -    group = []
│ │ +
│ │ +    group = None
│ │      if groupbychan:
│ │ -        for obj in data:
│ │ -            if groupbychan in obj.channel:
│ │ -                group.append(obj.filename)
│ │ +        if groupbychan in data.channel:
│ │ +            group = data.filename
│ │      else:
│ │ -        group = [obj.filename for obj in data]
│ │ +        group = data.filename
│ │      return group
│ │  
│ │  
│ │  class TestSpyCalls():
│ │  
│ │      nChan = 13
│ │      nObjs = nChan
│ │ @@ -48,55 +50,55 @@
│ │          obj.channel = list(string.ascii_uppercase[n : nChan + n])
│ │          dataObjs.append(obj)
│ │      data = dataObjs[0]
│ │  
│ │      def test_validcallstyles(self):
│ │  
│ │          # data positional
│ │ -        fname, = group_objects(self.data)
│ │ +        fname = group_objects(self.data)
│ │          assert fname == self.data.filename
│ │  
│ │          # data as keyword
│ │ -        fname, = group_objects(data=self.data)
│ │ +        fname = group_objects(data=self.data)
│ │          assert fname == self.data.filename
│ │  
│ │          # data in cfg
│ │          cfg = StructDict()
│ │          cfg.data = self.data
│ │ -        fname, = group_objects(cfg)
│ │ +        fname = group_objects(cfg)
│ │          assert fname == self.data.filename
│ │  
│ │          # 1. data positional, 2. cfg positional
│ │          cfg = StructDict()
│ │          cfg.groupbychan = None
│ │ -        fname, = group_objects(self.data, cfg)
│ │ +        fname = group_objects(self.data, cfg)
│ │          assert fname == self.data.filename
│ │  
│ │          # 1. cfg positional, 2. data positional
│ │ -        fname, = group_objects(cfg, self.data)
│ │ +        fname = group_objects(cfg, self.data)
│ │          assert fname == self.data.filename
│ │  
│ │          # data positional, cfg as keyword
│ │ -        fname, = group_objects(self.data, cfg=cfg)
│ │ +        fname = group_objects(self.data, cfg=cfg)
│ │          assert fname == self.data.filename
│ │  
│ │          # cfg positional, data as keyword
│ │ -        fname, = group_objects(cfg, data=self.data)
│ │ +        fname = group_objects(cfg, data=self.data)
│ │          assert fname == self.data.filename
│ │  
│ │          # both keywords
│ │ -        fname, = group_objects(cfg=cfg, data=self.data)
│ │ +        fname = group_objects(cfg=cfg, data=self.data)
│ │          assert fname == self.data.filename
│ │  
│ │      def test_invalidcallstyles(self):
│ │  
│ │          # expected error messages
│ │ -        errmsg1 = "expected Syncopy data object(s) provided either via " +\
│ │ +        errmsg1 = "expected Syncopy data object provided either via " +\
│ │                   "`cfg`/keyword or positional arguments, not both"
│ │ -        errmsg2 = "expected Syncopy data object(s) provided either via `cfg` " +\
│ │ +        errmsg2 = "expected Syncopy data object provided either via `cfg` " +\
│ │              "or as keyword argument, not both"
│ │          errmsg3 = "expected either 'data' or 'dataset' in `cfg`/keywords, not both"
│ │  
│ │          # ensure things break reliably for 'data' as well as 'dataset'
│ │          for key in ["data", "dataset"]:
│ │  
│ │              # data + cfg w/data
│ │ @@ -152,86 +154,97 @@
│ │          with pytest.raises(SPYValueError)as exc:
│ │              group_objects(data=self.data, dataset=self.data)
│ │          assert errmsg3 in str(exc.value)
│ │  
│ │          # data/dataset do not contain Syncopy object
│ │          with pytest.raises(SPYError)as exc:
│ │              group_objects(data="invalid")
│ │ -        assert "`data` must be Syncopy data object(s)!" in str(exc.value)
│ │ +        assert "`data` must be Syncopy data object!" in str(exc.value)
│ │  
│ │          # cfg is not dict/StructDict
│ │          with pytest.raises(SPYTypeError)as exc:
│ │              group_objects(cfg="invalid")
│ │          assert "Wrong type of `cfg`: expected dictionary-like" in str(exc.value)
│ │  
│ │ +        # input is just a numpy array
│ │ +        data = np.arange(2)
│ │ +        with pytest.raises(SPYError) as exc:
│ │ +            group_objects(data)
│ │ +        assert "Found no Syncopy data object as input" in str(exc.value)
│ │ +
│ │ +
│ │      def test_varargin(self):
│ │ +        """
│ │ +        This was originally meant to test multiple Syncopy objects
│ │ +        as 'data' input at once, this functionality was deprecated and got
│ │ +        removed. What remains are the `groupbychan` cfg and select tests.
│ │ +        """
│ │  
│ │          # data positional
│ │ -        allFnames = group_objects(*self.dataObjs)
│ │ +        allFnames = [group_objects(data) for data in self.dataObjs]
│ │          assert allFnames == [obj.filename for obj in self.dataObjs]
│ │  
│ │          # data in cfg
│ │          cfg = StructDict()
│ │ -        cfg.data = self.dataObjs
│ │ +        cfg.data = self.dataObjs[0]
│ │          fnameList = group_objects(cfg)
│ │ -        assert allFnames == fnameList
│ │ +        assert allFnames[0] == fnameList
│ │  
│ │          # group objects by single-letter "channels" in various ways
│ │          for letter in ["L", "E", "I", "A"]:
│ │              letterIdx = string.ascii_uppercase.index(letter)
│ │              nOccurences = letterIdx + 1
│ │  
│ │              # data positional + keyword to get "reference"
│ │ -            groupList = group_objects(*self.dataObjs, groupbychan=letter)
│ │ +            groupList = [group_objects(data, groupbychan=letter) for data in self.dataObjs]
│ │ +            groupList = [el for el in groupList if el is not None]
│ │              assert len(groupList) == nOccurences
│ │  
│ │              # 1. data positional, 2. cfg positional
│ │              cfg = StructDict()
│ │              cfg.groupbychan = letter
│ │ -            fnameList = group_objects(*self.dataObjs, cfg)
│ │ +            fnameList = [group_objects(data, cfg) for data in self.dataObjs]
│ │ +            fnameList = [el for el in fnameList if el is not None]
│ │ +
│ │              assert groupList == fnameList
│ │  
│ │              # 1. cfg positional, 2. data positional
│ │ -            fnameList = group_objects(cfg, *self.dataObjs)
│ │ +            fnameList = [group_objects(cfg, data) for data in self.dataObjs]
│ │ +            fnameList = [el for el in fnameList if el is not None]
│ │              assert groupList == fnameList
│ │  
│ │              # data positional, cfg as keyword
│ │ -            fnameList = group_objects(*self.dataObjs, cfg=cfg)
│ │ +            fnameList = [group_objects(data, cfg=cfg) for data in self.dataObjs]
│ │ +            fnameList = [el for el in fnameList if el is not None]
│ │              assert groupList == fnameList
│ │  
│ │              # cfg w/data + keyword
│ │              cfg = StructDict()
│ │ -            cfg.dataset = self.dataObjs
│ │ +            cfg.dataset = self.dataObjs[0]
│ │              cfg.groupbychan = letter
│ │              fnameList = group_objects(cfg)
│ │ -            assert groupList == fnameList
│ │ +            assert self.dataObjs[0].filename == fnameList
│ │  
│ │              # data positional + select keyword
│ │ -            fnameList = group_objects(*self.dataObjs[:letterIdx + 1],
│ │ -                                       select={"channel": [letter]})
│ │ +            fnameList = [group_objects(data, select={"channel": [letter]}) for data in self.dataObjs[:letterIdx + 1]]
│ │ +            fnameList = [el for el in fnameList if el is not None]
│ │              assert groupList == fnameList
│ │  
│ │              # data positional + cfg w/select
│ │              cfg = StructDict()
│ │              cfg.select = {"channel": [letter]}
│ │ -            fnameList = group_objects(*self.dataObjs[:letterIdx + 1], cfg)
│ │ -            assert groupList == fnameList
│ │ +            fnameList = [group_objects(data, cfg) for data in self.dataObjs[:letterIdx + 1]]
│ │ +            fnameList = [el for el in fnameList if el is not None]
│ │  
│ │ -            # cfg w/data + select
│ │ -            cfg = StructDict()
│ │ -            cfg.data = self.dataObjs[:letterIdx + 1]
│ │ -            cfg.select = {"channel": [letter]}
│ │ -            fnameList = group_objects(cfg)
│ │              assert groupList == fnameList
│ │  
│ │          # invalid selection
│ │          with pytest.raises(SPYValueError) as exc:
│ │ -            group_objects(*self.dataObjs, select={"channel": ["Z"]})
│ │ +            group_objects(self.dataObjs[0], select={"channel": ["Z"]})
│ │          assert "expected list/array of channel existing names or indices" in str(exc.value)
│ │  
│ │          # data does not only contain Syncopy objects
│ │          cfg = StructDict()
│ │ -        cfg.data = self.dataObjs + ["invalid"]
│ │ +        cfg.data = "invalid"
│ │          with pytest.raises(SPYError)as exc:
│ │              group_objects(cfg)
│ │ -        assert "`data` must be Syncopy data object(s)!" in str(exc.value)
│ │ -
│ │ +        assert "`data` must be Syncopy data object!" in str(exc.value)
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_discretedata.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_discretedata.py
│ │┄ Files 14% similar despite different names
│ │ @@ -3,70 +3,102 @@
│ │  # Test proper functionality of Syncopy DiscreteData-type classes
│ │  #
│ │  
│ │  # Builtin/3rd party package imports
│ │  import os
│ │  import tempfile
│ │  import time
│ │ -import random
│ │ +import h5py
│ │  import pytest
│ │  import numpy as np
│ │  
│ │ +
│ │  # Local imports
│ │ +import syncopy as spy
│ │  from syncopy.datatype import AnalogData, SpikeData, EventData
│ │ -from syncopy.datatype.base_data import Selector
│ │ -from syncopy.shared.tools import StructDict
│ │ -from syncopy.datatype.methods.selectdata import selectdata
│ │  from syncopy.io import save, load
│ │  from syncopy.shared.errors import SPYValueError, SPYTypeError
│ │ -from syncopy.tests.misc import construct_spy_filename, flush_local_cluster
│ │ -from syncopy import __acme__
│ │ -if __acme__:
│ │ -    import dask.distributed as dd
│ │ -
│ │ -# Decorator to decide whether or not to run dask-related tests
│ │ -skip_without_acme = pytest.mark.skipif(
│ │ -    not __acme__, reason="acme not available")
│ │ +from syncopy.tests.misc import construct_spy_filename
│ │ +from syncopy.tests.test_selectdata import getSpikeData
│ │  
│ │  
│ │  class TestSpikeData():
│ │  
│ │      # Allocate test-dataset
│ │      nc = 10
│ │      ns = 30
│ │      nd = 50
│ │      seed = np.random.RandomState(13)
│ │      data = np.vstack([seed.choice(ns, size=nd),
│ │                        seed.choice(nc, size=nd),
│ │                        seed.choice(int(nc / 2), size=nd)]).T
│ │ +    data = data[data[:,0].argsort()]
│ │      data2 = data.copy()
│ │      data2[:, -1] = data[:, 0]
│ │      data2[:, 0] = data[:, -1]
│ │      trl = np.vstack([np.arange(0, ns, 5),
│ │                       np.arange(5, ns + 5, 5),
│ │                       np.ones((int(ns / 5), )),
│ │                       np.ones((int(ns / 5), )) * np.pi]).T
│ │      num_smp = np.unique(data[:, 0]).size
│ │      num_chn = data[:, 1].max() + 1
│ │      num_unt = data[:, 2].max() + 1
│ │  
│ │ +    def test_init(self):
│ │ +
│ │ +        # data and no labels triggers default labels
│ │ +        dummy = SpikeData(data=4  * np.ones((2, 3), dtype=int))
│ │ +        # labels are 0-based
│ │ +        assert dummy.channel == 'channel05'
│ │ +        assert dummy.unit == 'unit05'
│ │ +
│ │ +        # data and fitting labels is fine
│ │ +        assert isinstance(SpikeData(data=np.ones((2, 3), dtype=int), channel=['only_channel']),
│ │ +                          SpikeData)
│ │ +
│ │ +        # --- invalid inits ---
│ │ +
│ │ +        # non-integer types
│ │ +        with pytest.raises(SPYTypeError, match='expected integer like'):
│ │ +            _ = SpikeData(data=np.ones((2, 3)), unit=['unit1', 'unit2'])
│ │ +
│ │ +        with pytest.raises(SPYTypeError, match='expected integer like'):
│ │ +            data = np.array([np.nan, 2, np.nan])[:, np.newaxis]
│ │ +            _ = SpikeData(data=data, unit=['unit1', 'unit2'])
│ │ +
│ │ +        # data and too many labels
│ │ +        with pytest.raises(SPYValueError, match='expected exactly 1 unit'):
│ │ +            _ = SpikeData(data=np.ones((2, 3), dtype=int), unit=['unit1', 'unit2'])
│ │ +
│ │ +        # no data but labels
│ │ +        with pytest.raises(SPYValueError, match='cannot assign `channel` without data'):
│ │ +            _ = SpikeData(channel=['a', 'b', 'c'])
│ │ +
│ │ +    def test_register_dset(self):
│ │ +        sdata = SpikeData(self.data, samplerate=10)
│ │ +        assert not sdata._is_empty()
│ │ +        sdata._register_dataset("blah", np.zeros((3,3), dtype=float))
│ │ +
│ │ +
│ │      def test_empty(self):
│ │          dummy = SpikeData()
│ │          assert len(dummy.cfg) == 0
│ │          assert dummy.dimord is None
│ │          for attr in ["channel", "data", "sampleinfo", "samplerate",
│ │                       "trialid", "trialinfo", "unit"]:
│ │              assert getattr(dummy, attr) is None
│ │          with pytest.raises(SPYTypeError):
│ │              SpikeData({})
│ │  
│ │      def test_issue_257_fixed_no_error_for_empty_data(self):
│ │ -        """This tests that the data object is created without throwing an error, see #257."""
│ │ -        data = SpikeData(np.column_stack(([],[],[])), dimord = ['sample', 'channel', 'unit'], samplerate = 30000)
│ │ -        assert data.dimord == ["sample", "channel", "unit"]
│ │ +        """This tests that empty datasets are not allowed"""
│ │ +        with pytest.raises(SPYValueError, match='non empty'):
│ │ +            data = SpikeData(np.column_stack(([],[],[])).astype(int),
│ │ +                             dimord=['sample', 'channel', 'unit'],
│ │ +                             samplerate=30000)
│ │  
│ │      def test_nparray(self):
│ │          dummy = SpikeData(self.data)
│ │          assert dummy.dimord == ["sample", "channel", "unit"]
│ │          assert dummy.channel.size == self.num_chn
│ │          # NOTE: SpikeData.sample is currently empty
│ │          # assert dummy.sample.size == self.num_smp
│ │ @@ -149,118 +181,22 @@
│ │              assert dummy2.unit.size == self.num_smp  # swapped
│ │              assert dummy2.data.shape == dummy.data.shape
│ │  
│ │              # Delete all open references to file objects b4 closing tmp dir
│ │              del dummy, dummy2
│ │              time.sleep(0.1)
│ │  
│ │ -    # test data-selection via class method
│ │ -    def test_dataselection(self, fulltests):
│ │ -
│ │ -        # Create testing objects (regular and swapped dimords)
│ │ -        dummy = SpikeData(data=self.data,
│ │ -                          trialdefinition=self.trl,
│ │ -                          samplerate=2.0)
│ │ -        ymmud = SpikeData(data=self.data[:, ::-1],
│ │ -                          trialdefinition=self.trl,
│ │ -                          samplerate=2.0,
│ │ -                          dimord=dummy.dimord[::-1])
│ │ -
│ │ -        # selections are chosen so that result is not empty
│ │ -        trialSelections = [
│ │ -            "all",  # enforce below selections in all trials of `dummy`
│ │ -            [3, 1]  # minimally unordered
│ │ -        ]
│ │ -        chanSelections = [
│ │ -            ["channel03", "channel01", "channel01", "channel02"],  # string selection w/repetition + unordered
│ │ -            [4, 2, 2, 5, 5],   # repetition + unorderd
│ │ -            range(5, 8),  # narrow range
│ │ -            slice(-5, None)  # negative-start slice
│ │ -        ]
│ │ -        toiSelections = [
│ │ -            "all",  # non-type-conform string
│ │ -            [-0.2, 0.6, 0.9, 1.1, 1.3, 1.6, 1.8, 2.2, 2.45, 3.]  # unordered, inexact, repetions
│ │ -        ]
│ │ -        toilimSelections = [
│ │ -            [0.5, 3.5],  # regular range
│ │ -            [1.0, np.inf]  # unbounded from above
│ │ -        ]
│ │ -        unitSelections = [
│ │ -            ["unit1", "unit1", "unit2", "unit3"],  # preserve repetition
│ │ -            [0, 0, 2, 3],  # preserve repetition, don't convert to slice
│ │ -            range(1, 4),  # narrow range
│ │ -            slice(-2, None)  # negative-start slice
│ │ -        ]
│ │ -        timeSelections = list(zip(["toi"] * len(toiSelections), toiSelections)) \
│ │ -            + list(zip(["toilim"] * len(toilimSelections), toilimSelections))
│ │ -
│ │ -        # Randomly pick one selection unless tests are run with `--full`
│ │ -        if fulltests:
│ │ -            trialSels = trialSelections
│ │ -            chanSels = chanSelections
│ │ -            unitSels = unitSelections
│ │ -            timeSels = timeSelections
│ │ -        else:
│ │ -            trialSels = [random.choice(trialSelections)]
│ │ -            chanSels = [random.choice(chanSelections)]
│ │ -            unitSels = [random.choice(unitSelections)]
│ │ -            timeSels = [random.choice(timeSelections)]
│ │ -
│ │ -        for obj in [dummy, ymmud]:
│ │ -            chanIdx = obj.dimord.index("channel")
│ │ -            unitIdx = obj.dimord.index("unit")
│ │ -            chanArr = np.arange(obj.channel.size)
│ │ -            for trialSel in trialSels:
│ │ -                for chanSel in chanSels:
│ │ -                    for unitSel in unitSels:
│ │ -                        for timeSel in timeSels:
│ │ -                            kwdict = {}
│ │ -                            kwdict["trials"] = trialSel
│ │ -                            kwdict["channel"] = chanSel
│ │ -                            kwdict["unit"] = unitSel
│ │ -                            kwdict[timeSel[0]] = timeSel[1]
│ │ -                            cfg = StructDict(kwdict)
│ │ -                            # data selection via class-method + `Selector` instance for indexing
│ │ -                            selected = obj.selectdata(**kwdict)
│ │ -                            selector = Selector(obj, kwdict)
│ │ -                            tk = 0
│ │ -                            for trialno in selector.trials:
│ │ -                                if selector.time[tk]:
│ │ -                                    assert np.array_equal(obj.trials[trialno][selector.time[tk], :],
│ │ -                                                          selected.trials[tk])
│ │ -                                    tk += 1
│ │ -                            assert set(selected.data[:, chanIdx]).issubset(chanArr[selector.channel])
│ │ -                            assert set(selected.channel) == set(obj.channel[selector.channel])
│ │ -                            assert np.array_equal(selected.unit,
│ │ -                                                  obj.unit[np.unique(selected.data[:, unitIdx])])
│ │ -                            cfg.data = obj
│ │ -                            # data selection via package function and `cfg`: ensure equality
│ │ -                            out = selectdata(cfg)
│ │ -                            assert np.array_equal(out.channel, selected.channel)
│ │ -                            assert np.array_equal(out.unit, selected.unit)
│ │ -                            assert np.array_equal(out.data, selected.data)
│ │ -
│ │ -    @skip_without_acme
│ │ -    def test_parallel(self, testcluster, fulltests):
│ │ -        # repeat selected test w/parallel processing engine
│ │ -        client = dd.Client(testcluster)
│ │ -        par_tests = ["test_dataselection"]
│ │ -        for test in par_tests:
│ │ -            getattr(self, test)(fulltests)
│ │ -            flush_local_cluster(testcluster)
│ │ -        client.close()
│ │ -
│ │  
│ │  class TestEventData():
│ │  
│ │      # Allocate test-datasets
│ │      nc = 10
│ │      ns = 30
│ │      data = np.vstack([np.arange(0, ns, 5),
│ │ -                      np.zeros((int(ns / 5), ))]).T
│ │ +                      np.zeros((int(ns / 5), ))]).T.astype(int)
│ │      data[1::2, 1] = 1
│ │      data2 = data.copy()
│ │      data2[:, -1] = data[:, 0]
│ │      data2[:, 0] = data[:, -1]
│ │      data3 = np.hstack([data2, data2])
│ │      trl = np.vstack([np.arange(0, ns, 5),
│ │                       np.arange(5, ns + 5, 5),
│ │ @@ -291,14 +227,19 @@
│ │          assert dummy.trialinfo.shape == (1, 0)
│ │          assert np.array_equal(dummy.data, self.data)
│ │  
│ │          # wrong shape for data-type
│ │          with pytest.raises(SPYValueError):
│ │              EventData(np.ones((3,)))
│ │  
│ │ +    def test_register_dset(self):
│ │ +        edata = EventData(self.data, samplerate=10)
│ │ +        assert not edata._is_empty()
│ │ +        edata._register_dataset("blah", np.zeros((3,3), dtype=float))
│ │ +
│ │      def test_ed_trialretrieval(self):
│ │          # test ``_get_trial`` with NumPy array: regular order
│ │          dummy = EventData(self.data, trialdefinition=self.trl)
│ │          smp = self.data[:, 0]
│ │          for trlno, start in enumerate(range(0, self.ns, 5)):
│ │              idx = np.intersect1d(np.where(smp >= start)[0],
│ │                                   np.where(smp < start + 5)[0])
│ │ @@ -463,15 +404,15 @@
│ │          evt_dummy = EventData(data=data3, dimord=self.customDimord, samplerate=sr_e)
│ │          ang_dummy = AnalogData(self.adata, samplerate=sr_a)
│ │          ang_dummy.definetrial(evt_dummy, pre=pre, post=post, trigger=1)
│ │          assert np.array_equal(ang_dummy.sampleinfo, sinfo_a)
│ │  
│ │          # Extend data and provoke an exception due to out of bounds error
│ │          smp = np.vstack([np.arange(self.ns, int(2.5 * self.ns), 5),
│ │ -                         np.zeros((int((1.5 * self.ns) / 5),))]).T
│ │ +                         np.zeros((int((1.5 * self.ns) / 5),))]).T.astype(int)
│ │          smp[1::2, 1] = 1
│ │          smp = np.hstack([smp, smp])
│ │          data4 = np.vstack([data3, smp])
│ │          evt_dummy = EventData(data=data4, dimord=self.customDimord, samplerate=sr_e)
│ │          evt_dummy.definetrial(pre=pre, post=post, trigger=1)
│ │          # with pytest.raises(SPYValueError):
│ │          # ang_dummy.definetrial(evt_dummy)
│ │ @@ -518,87 +459,129 @@
│ │          evt_dummy = EventData(samplerate=sr_e)
│ │          with pytest.raises(SPYValueError):
│ │              evt_dummy.definetrial(pre=pre, post=post, trigger=1)
│ │          ang_dummy = AnalogData(self.adata, samplerate=sr_a)
│ │          with pytest.raises(SPYValueError):
│ │              ang_dummy.definetrial(evt_dummy, pre=pre, post=post, trigger=1)
│ │  
│ │ -    # test data-selection via class method
│ │ -    def test_ed_dataselection(self, fulltests):
│ │ +class TestWaveform():
│ │  
│ │ -        # Create testing objects (regular and swapped dimords)
│ │ -        dummy = EventData(data=np.hstack([self.data, self.data]),
│ │ -                          dimord=self.customDimord,
│ │ -                          trialdefinition=self.trl,
│ │ -                          samplerate=2.0)
│ │ -        ymmud = EventData(data=np.hstack([self.data[:, ::-1], self.data[:, ::-1]]),
│ │ -                          trialdefinition=self.trl,
│ │ -                          samplerate=2.0,
│ │ -                          dimord=dummy.dimord[::-1])
│ │ -
│ │ -        # selections are chosen so that result is not empty
│ │ -        trialSelections = [
│ │ -            "all",  # enforce below selections in all trials of `dummy`
│ │ -            [3, 1]  # minimally unordered
│ │ -        ]
│ │ -        eventidSelections = [
│ │ -            [0, 0, 1],  # preserve repetition, don't convert to slice
│ │ -            range(0, 2),  # narrow range
│ │ -            slice(-2, None)  # negative-start slice
│ │ -        ]
│ │ -        toiSelections = [
│ │ -            "all",  # non-type-conform string
│ │ -            [-0.2, 0.6, 0.9, 1.1, 1.3, 1.6, 1.8, 2.2, 2.45, 3.]  # unordered, inexact, repetions
│ │ -        ]
│ │ -        toilimSelections = [
│ │ -            [0.5, 3.5],  # regular range
│ │ -            [0.0, np.inf]  # unbounded from above
│ │ -        ]
│ │ -        timeSelections = list(zip(["toi"] * len(toiSelections), toiSelections)) \
│ │ -            + list(zip(["toilim"] * len(toilimSelections), toilimSelections))
│ │ -
│ │ -        # Randomly pick one selection unless tests are run with `--full`
│ │ -        if fulltests:
│ │ -            trialSels = trialSelections
│ │ -            eventidSels = eventidSelections
│ │ -            timeSels = timeSelections
│ │ -        else:
│ │ -            trialSels = [random.choice(trialSelections)]
│ │ -            eventidSels = [random.choice(eventidSelections)]
│ │ -            timeSels = [random.choice(timeSelections)]
│ │ -
│ │ -        for obj in [dummy, ymmud]:
│ │ -            eventidIdx = obj.dimord.index("eventid")
│ │ -            for trialSel in trialSels:
│ │ -                for eventidSel in eventidSels:
│ │ -                    for timeSel in timeSels:
│ │ -                        kwdict = {}
│ │ -                        kwdict["trials"] = trialSel
│ │ -                        kwdict["eventid"] = eventidSel
│ │ -                        kwdict[timeSel[0]] = timeSel[1]
│ │ -                        cfg = StructDict(kwdict)
│ │ -                        # data selection via class-method + `Selector` instance for indexing
│ │ -                        selected = obj.selectdata(**kwdict)
│ │ -                        selector = Selector(obj, kwdict)
│ │ -                        tk = 0
│ │ -                        for trialno in selector.trials:
│ │ -                            if selector.time[tk]:
│ │ -                                assert np.array_equal(obj.trials[trialno][selector.time[tk], :],
│ │ -                                                      selected.trials[tk])
│ │ -                                tk += 1
│ │ -                        assert np.array_equal(selected.eventid,
│ │ -                                              obj.eventid[np.unique(selected.data[:, eventidIdx]).astype(np.intp)])
│ │ -                        cfg.data = obj
│ │ -                        # data selection via package function and `cfg`: ensure equality
│ │ -                        out = selectdata(cfg)
│ │ -                        assert np.array_equal(out.eventid, selected.eventid)
│ │ -                        assert np.array_equal(out.data, selected.data)
│ │ -
│ │ -    @skip_without_acme
│ │ -    def test_ed_parallel(self, testcluster, fulltests):
│ │ -        # repeat selected test w/parallel processing engine
│ │ -        client = dd.Client(testcluster)
│ │ -        par_tests = ["test_ed_dataselection"]
│ │ -        for test in par_tests:
│ │ -            getattr(self, test)(fulltests)
│ │ -            flush_local_cluster(testcluster)
│ │ -        client.close()
│ │ +    def test_waveform_invalid_set(self):
│ │ +        """Sets invalid waveform for data: dimension mismatch"""
│ │ +        spiked = SpikeData(data=np.ones((2, 3), dtype=int), samplerate=10)
│ │ +        assert spiked.data.shape == (2, 3,)
│ │ +        with pytest.raises(SPYValueError, match="wrong size waveform"):
│ │ +            spiked.waveform = np.ones((3, 3), dtype=int)
│ │ +
│ │ +    def test_waveform_invalid_set_emptydata(self):
│ │ +        """Tries to set waveform without any data."""
│ │ +        spiked = SpikeData()
│ │ +        with pytest.raises(SPYValueError, match="Please assign data first"):
│ │ +            spiked.waveform = np.ones((3, 3), dtype=int)
│ │ +
│ │ +    def test_waveform_invalid_set_1dim(self):
│ │ +        """Tries to set waveform with data that has ndim=1."""
│ │ +        spiked = SpikeData(data=np.ones((2, 3), dtype=int), samplerate=10)
│ │ +        with pytest.raises(SPYValueError, match="waveform data with at least 2 dimensions"):
│ │ +            spiked.waveform = np.ones((3), dtype=int)
│ │ +
│ │ +    def test_waveform_valid_set(self):
│ │ +        """Sets waveform in a correct way."""
│ │ +        spiked = SpikeData(data=np.ones((2, 3), dtype=int), samplerate=10)
│ │ +
│ │ +        assert not spiked._is_empty()
│ │ +        assert spiked.data.shape == (2, 3,)
│ │ +        assert type(spiked.data) == h5py.Dataset
│ │ +        assert spiked._get_backing_hdf5_file_handle() is not None
│ │ +        spiked.waveform = np.ones((2, 3), dtype=int)
│ │ +        assert "waveform" in spiked._hdfFileDatasetProperties
│ │ +        assert spiked.waveform.shape == (2, 3,)
│ │ +
│ │ +    def test_waveform_valid_set_with_None(self):
│ │ +        """Sets waveform to None, which is valid."""
│ │ +        spiked = SpikeData(data=np.ones((2, 3), dtype=int), samplerate=10)
│ │ +        assert not spiked._is_empty()
│ │ +        assert spiked.data.shape == (2, 3,)
│ │ +        spiked.waveform = np.ones((2, 3), dtype=int)
│ │ +        assert spiked.waveform.shape == (2, 3,)
│ │ +        spiked.waveform = None
│ │ +        assert spiked.waveform is None
│ │ +        # try to set again
│ │ +        spiked.waveform = np.ones((2, 3), dtype=int)
│ │ +        assert spiked.waveform.shape == (2, 3,)
│ │ +
│ │ +
│ │ +    def test_waveform_selection_trial(self):
│ │ +        numSpikes, waveform_dimsize = 20, 50
│ │ +        spiked = getSpikeData(nSpikes = numSpikes)
│ │ +        assert sum([s.shape[0] for s in spiked.trials]) == numSpikes
│ │ +        assert spiked.waveform is None
│ │ +        spiked.waveform = np.ones((numSpikes, 3, waveform_dimsize), dtype=int)
│ │ +        for spikeidx in range(numSpikes):
│ │ +            spiked.waveform[spikeidx, :, :] = np.ones((3, waveform_dimsize), dtype=int) * spikeidx
│ │ +
│ │ +        trial0_nspikes = spiked.trials[0].shape[0]
│ │ +        trial2_nspikes = spiked.trials[2].shape[0]
│ │ +
│ │ +        # Select 2 trials and verify that the number of spikes is correct.
│ │ +        selection = { 'trials': [0, 2] }
│ │ +        res = spiked.selectdata(selection)
│ │ +        assert len(res.trials) == 2
│ │ +        assert res.trials[0].shape[0] == trial0_nspikes
│ │ +        assert res.trials[1].shape[0] == trial2_nspikes
│ │ +
│ │ +        # Verify that the waveform selection is also correct.
│ │ +        assert res.waveform is not None
│ │ +        assert res.waveform.shape[0] == trial0_nspikes + trial2_nspikes  # Verify selection on waveform
│ │ +
│ │ +        # Verify on data level.
│ │ +        expected_data_indices = np.where((spiked.trialid == 0) | (spiked.trialid == 2))[0]
│ │ +        for spike_idx in range(res.waveform.shape[0]):
│ │ +            assert np.all(res.waveform[spike_idx, :, :] == spiked.waveform[expected_data_indices][spike_idx, :, :])
│ │ +
│ │ +    def test_save_load_with_waveform(self):
│ │ +        """Test saving file with waveform data."""
│ │ +        numSpikes, waveform_dimsize = 20, 50
│ │ +        spiked = getSpikeData(nSpikes = numSpikes)
│ │ +        spiked.waveform = np.ones((numSpikes, 3, waveform_dimsize), dtype=int)
│ │ +
│ │ +        tfile1 = tempfile.NamedTemporaryFile(suffix=".spike", delete=True)
│ │ +        tfile1.close()
│ │ +        tmp_spy_filename = tfile1.name
│ │ +        save(spiked, filename=tmp_spy_filename)
│ │ +        assert "waveform" in h5py.File(tmp_spy_filename, mode="r").keys()
│ │ +        spkd2 = load(filename=tmp_spy_filename)
│ │ +        assert isinstance(spkd2.waveform, h5py.Dataset), f"Expected h5py.Dataset, got {type(spkd2.waveform)}"
│ │ +        assert np.array_equal(spiked.waveform[()], spkd2.waveform[()])
│ │ +
│ │ +        # Test delete/unregister when setting to None.
│ │ +        spkd2.waveform = None
│ │ +        assert spkd2.waveform is None
│ │ +        assert "waveform" not in h5py.File(tmp_spy_filename, mode="r").keys()
│ │ +
│ │ +        # Test that we can set waveform again after deleting it.
│ │ +        spkd2.waveform = np.ones((numSpikes, 3, waveform_dimsize), dtype=int)
│ │ +
│ │ +        tfile1.close()
│ │ +
│ │ +    def test_psth_with_waveform(self):
│ │ +        """Test that the waveform does not break frontend functions, like PSTH.
│ │ +           The waveform should just be ignored, and the resulting TimeLockData
│ │ +           will of course NOT have a waveform.
│ │ +        """
│ │ +        numSpikes, waveform_dimsize = 20, 50
│ │ +        spiked = getSpikeData(nSpikes = numSpikes)
│ │ +        spiked.waveform = np.ones((numSpikes, 3, waveform_dimsize), dtype=int)
│ │ +
│ │ +        cfg = spy.StructDict()
│ │ +        cfg.binsize = 0.1
│ │ +        cfg.latency = 'maxperiod'  # frontend default
│ │ +        res = spy.spike_psth(spiked, cfg)
│ │ +        assert type(res) == spy.TimeLockData
│ │ +        assert not hasattr(res, "waveform")
│ │ +
│ │ +
│ │ +if __name__ == '__main__':
│ │ +
│ │ +    T1 = TestSpikeData()
│ │ +    T2 = TestEventData()
│ │ +    T3 = TestWaveform()
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_info.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_info.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_packagesetup.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_packagesetup.py
│ │┄ Files 10% similar despite different names
│ │ @@ -39,56 +39,74 @@
│ │      assert syncopy.__storage__ == tmpDir
│ │      shutil.rmtree(tmpDir, ignore_errors=True)
│ │      del os.environ["SPYTMPDIR"]
│ │      time.sleep(1)
│ │  
│ │  
│ │  # check if `cleanup` does what it's supposed to do
│ │ -@skip_in_ghactions
│ │ +# @skip_in_ghactions
│ │  def test_cleanup():
│ │      # spawn new Python instance, which creates and saves an `AnalogData` object
│ │      # in custom $SPYTMPDIR; force-kill the process after a few seconds preventing
│ │      # Syncopy from cleaning up its temp storage folder
│ │ -    tmpDir = os.path.join(tempfile.gettempdir(), "spy_zombie")
│ │ +
│ │ +    # this function assumes to be in the root directory of the repository
│ │ +    # if that is not the case we have to move there
│ │ +
│ │ +    cdir = os.getcwd()
│ │ +    while 'syncopy' in cdir:
│ │ +        head, tail = os.path.split(cdir)
│ │ +        if not 'syncopy' in head:
│ │ +            root_dir = os.path.join(head, tail)
│ │ +            os.chdir(root_dir)
│ │ +            break
│ │ +        cdir = head
│ │ +    # check that we are not entirely somewhere else
│ │ +    else:
│ │ +        assert False
│ │ +
│ │ +    tmpDir = tempfile.mkdtemp()
│ │ +
│ │      os.environ["SPYTMPDIR"] = tmpDir
│ │      commandStr = \
│ │          "import os; " +\
│ │          "import time; " +\
│ │ +        "import numpy as np; " +\
│ │          "import syncopy as spy; " +\
│ │ -        "from syncopy.tests.misc import generate_artificial_data; " +\
│ │ -        "dummy = generate_artificial_data(inmemory=False); " +\
│ │ -        "dummy.save(os.path.join(spy.__storage__, 'spy_dummy')); " +\
│ │ +        "dummy = spy.AnalogData(data=np.ones((10,10)), samplerate=1); " +\
│ │          "time.sleep(100)"
│ │      process = subprocess.Popen([sys.executable, "-c", commandStr])
│ │ -    time.sleep(5.)
│ │ +    time.sleep(12)
│ │      process.kill()
│ │  
│ │      # get inventory of external Syncopy instance's temp storage
│ │ -    spyGarbage = glob(os.path.join(tmpDir, "*"))
│ │ -    assert len(spyGarbage)
│ │ +    num_garbage_before = len(glob(os.path.join(tmpDir, "*.analog")))
│ │ +    assert num_garbage_before >= 0
│ │  
│ │      # launch 2nd external instance with same $SPYTMPDIR, create 2nd `AnalogData`
│ │      # object, run `cleanup` and keep instance alive in background (for max. 100s)
│ │      commandStr = \
│ │          "import time; " +\
│ │          "import syncopy as spy; " +\
│ │ -        "from syncopy.tests.misc import generate_artificial_data; " +\
│ │ -        "dummy = generate_artificial_data(inmemory=False); " +\
│ │ -        "spy.cleanup(older_than=0, interactive=False); " +\
│ │ +        "import numpy as np; " +\
│ │ +        "dummy = spy.AnalogData(data=np.ones((10,10)), samplerate=1); " +\
│ │ +        "time.sleep(5)" +\
│ │ +        "spy.cleanup(older_than=0, interactive=False, only_current_session=True); " +\
│ │          "time.sleep(100)"
│ │      process2 = subprocess.Popen([sys.executable, "-c", commandStr],
│ │ -                                 stdout=subprocess.PIPE, stderr=subprocess.PIPE,
│ │ -                                 text=True)
│ │ -    time.sleep(5)
│ │ +                                stdout=subprocess.PIPE, stderr=subprocess.PIPE,
│ │ +                                text=True)
│ │ +    time.sleep(12)
│ │ +
│ │ +    num_garbage_after = len(glob(os.path.join(tmpDir, "*.analog")))
│ │  
│ │      # ensure `cleanup` call removed first instance's garbage but 2nd `AnalogData`
│ │      # belonging to 2nd instance launched above is unharmed
│ │ -    for garbage in spyGarbage:
│ │ -        assert not os.path.exists(garbage)
│ │ -    assert glob(os.path.join(tmpDir, "*.analog"))
│ │ +    assert num_garbage_after == num_garbage_before
│ │  
│ │      # now kill 2nd instance and wipe `tmpDir`
│ │      process2.kill()
│ │      time.sleep(1)
│ │ -    shutil.rmtree(tmpDir)
│ │ +
│ │      del os.environ["SPYTMPDIR"]
│ │      time.sleep(1)
│ │ +    shutil.rmtree(tmpDir)
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_parsers.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_parsers.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_plotting.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_plotting.py
│ │┄ Files 9% similar despite different names
│ │ @@ -19,21 +19,23 @@
│ │  class TestAnalogPlotting():
│ │  
│ │      nTrials = 10
│ │      nChannels = 9
│ │      nSamples = 300
│ │      adata = synth_data.AR2_network(nTrials=nTrials,
│ │                                     AdjMat=np.zeros(nChannels),
│ │ -                                   nSamples=nSamples)
│ │ +                                   nSamples=nSamples,
│ │ +                                   seed=helpers.test_seed)
│ │  
│ │      adata += 0.3 * synth_data.linear_trend(nTrials=nTrials,
│ │                                             y_max=nSamples / 20,
│ │                                             nSamples=nSamples,
│ │                                             nChannels=nChannels)
│ │  
│ │ +
│ │      # add an offset
│ │      adata = adata + 5
│ │  
│ │      # all trials are equal
│ │      toi_min, toi_max = adata.time[0][0], adata.time[0][-1]
│ │  
│ │      def test_ad_plotting(self, **kwargs):
│ │ @@ -44,20 +46,19 @@
│ │          # check if we run the default test
│ │          def_test = not len(kwargs)
│ │  
│ │          if def_test:
│ │              # interactive plotting
│ │              ppl.ion()
│ │              def_kwargs = {'trials': 1,
│ │ -                          'toilim': [self.toi_min, 0.75 * self.toi_max]}
│ │ -            kwargs = def_kwargs
│ │ +                          'latency': [self.toi_min, 1.2 * self.toi_max]}
│ │  
│ │ -            fig1, ax1 = self.adata.singlepanelplot(**kwargs)
│ │ -            fig2, ax2 = self.adata.singlepanelplot(**kwargs, shifted=False)
│ │ -            fig3, axs = self.adata.multipanelplot(**kwargs)
│ │ +            fig1, ax1 = self.adata.singlepanelplot(**def_kwargs)
│ │ +            fig2, ax2 = self.adata.singlepanelplot(**def_kwargs, shifted=False)
│ │ +            fig3, axs = self.adata.multipanelplot(**def_kwargs)
│ │  
│ │              # check axes/figure references work
│ │              ax1.set_title('Shifted signals')
│ │              fig1.tight_layout()
│ │              ax2.set_title('Overlayed signals')
│ │              fig2.tight_layout()
│ │              fig3.suptitle("Multipanel plot")
│ │ @@ -88,27 +89,51 @@
│ │              self.test_ad_plotting(**sel_dict)
│ │  
│ │      def test_ad_exceptions(self):
│ │  
│ │          # empty arrays get returned for empty time selection
│ │          with pytest.raises(SPYValueError) as err:
│ │              self.test_ad_plotting(trials=0,
│ │ -                                  toilim=[self.toi_max + 1, self.toi_max + 2])
│ │ +                                  latency=[self.toi_max + 1, self.toi_max + 2])
│ │              assert "zero size" in str(err)
│ │  
│ │          # invalid channel selection
│ │          with pytest.raises(SPYValueError) as err:
│ │              self.test_ad_plotting(trials=0, channel=self.nChannels + 1)
│ │              assert "channel existing names" in str(err)
│ │  
│ │          # invalid trial selection
│ │          with pytest.raises(SPYValueError) as err:
│ │              self.test_ad_plotting(trials=self.nTrials + 1)
│ │              assert "select: trials" in str(err)
│ │  
│ │ +    def test_ad_dimord(self):
│ │ +        # create new mockup data
│ │ +        rng = np.random.default_rng(helpers.test_seed)
│ │ +        nSamples = 100
│ │ +        nChannels = 4
│ │ +        # single trial with ('channel', 'time') dimord
│ │ +        ad = spy.AnalogData([rng.standard_normal((nChannels, nSamples))], dimord=['channel', 'time'], samplerate=200)
│ │ +
│ │ +        for chan in ad.channel:
│ │ +            fig, ax = ad.singlepanelplot(channel=chan)
│ │ +        # check that the right axis is the time axis
│ │ +        xleft, xright = ax.get_xlim()
│ │ +        assert xright - xleft >= nSamples / ad.samplerate
│ │ +
│ │ +        # test multipanelplot
│ │ +        fig, axs = ad.multipanelplot()
│ │ +        # check that we have indeed nChannels axes
│ │ +        assert axs.size == nChannels
│ │ +
│ │ +        xleft, xright = axs[0,0].get_xlim()
│ │ +        # check that the right axis is the time axis
│ │ +        xleft, xright = ax.get_xlim()
│ │ +        assert xright - xleft >= nSamples / ad.samplerate
│ │ +
│ │  
│ │  class TestSpectralPlotting():
│ │  
│ │      nTrials = 10
│ │      nChannels = 4
│ │      nSamples = 300
│ │      AdjMat = np.zeros((nChannels, nChannels))
│ │ @@ -119,70 +144,86 @@
│ │      # add AR(1) 'background'
│ │      adata = adata + 1.2 * synth_data.AR2_network(nTrials=nTrials,
│ │                                                   AdjMat=AdjMat,
│ │                                                   nSamples=nSamples,
│ │                                                   alphas=[0.8, 0])
│ │  
│ │      # some interesting range
│ │ -    foilim = [1, 400]
│ │ +    frequency = [1, 400]
│ │  
│ │      # all trials are equal
│ │      toi_min, toi_max = adata.time[0][0], adata.time[0][-1]
│ │  
│ │      spec_fft = spy.freqanalysis(adata, tapsmofrq=1)
│ │ +    spec_fft_imag = spy.freqanalysis(adata, output='imag')
│ │ +    spec_fft_mtm = spy.freqanalysis(adata, tapsmofrq=1, keeptapers=True)
│ │ +    spec_fft_complex = spy.freqanalysis(adata, output='fourier')
│ │ +
│ │      spec_wlet = spy.freqanalysis(adata, method='wavelet',
│ │                                   foi=np.arange(0, 400, step=4))
│ │  
│ │      def test_spectral_plotting(self, **kwargs):
│ │  
│ │          # no interactive plotting
│ │          ppl.ioff()
│ │  
│ │          # check if we run the default test
│ │          def_test = not len(kwargs)
│ │  
│ │          if def_test:
│ │              ppl.ion()
│ │ -            kwargs = {'trials': self.nTrials - 1, 'foilim': [5, 300]}
│ │ +            kwargs = {'trials': self.nTrials - 1, 'frequency': [5, 300]}
│ │              # to visually compare
│ │              self.adata.singlepanelplot(trials=self.nTrials - 1, channel=0)
│ │  
│ │              # this simulates the interactive plotting
│ │              fig1, ax1 = self.spec_fft.singlepanelplot(**kwargs)
│ │              fig2, axs = self.spec_fft.multipanelplot(**kwargs)
│ │  
│ │ +            # multi taper
│ │ +            assert self.spec_fft_mtm.taper.size > 1
│ │ +            _, _ = self.spec_fft_mtm.singlepanelplot(channel=3, **kwargs)
│ │ +            _, _ = self.spec_fft_mtm.singlepanelplot(taper=1, **kwargs)
│ │ +            _, _ = self.spec_fft_mtm.multipanelplot(**kwargs)
│ │ +
│ │ +            _, _ = self.spec_fft_imag.singlepanelplot(**kwargs)
│ │ +            _, _ = self.spec_fft_imag.multipanelplot(**kwargs)
│ │ +
│ │ +            res, res2 = self.spec_fft_complex.singlepanelplot(**kwargs)
│ │ +            # no plot of complex valued spectra
│ │ +            assert res is None and res2 is None
│ │ +            res = self.spec_fft_complex.multipanelplot(**kwargs)
│ │ +            assert res is None
│ │ +
│ │              fig3, ax2 = self.spec_wlet.singlepanelplot(channel=0, **kwargs)
│ │              fig4, axs = self.spec_wlet.multipanelplot(**kwargs)
│ │  
│ │              ax1.set_title('AR(1) + AR(2)')
│ │              fig2.suptitle('AR(1) + AR(2)')
│ │          else:
│ │ -            print(kwargs)
│ │ +            self.spec_wlet.multipanelplot(**kwargs)
│ │ +            # latency makes no sense for line plots
│ │ +            kwargs.pop('latency')
│ │              self.spec_fft.singlepanelplot(**kwargs)
│ │              self.spec_fft.multipanelplot(**kwargs)
│ │  
│ │ -            self.spec_wlet.multipanelplot(**kwargs)
│ │              # take the 1st random channel for 2d spectra
│ │              if 'channel' in kwargs:
│ │                  chan = kwargs.pop('channel')[0]
│ │ -            self.spec_wlet.singlepanelplot(channel=chan, **kwargs)
│ │ +                self.spec_wlet.singlepanelplot(channel=chan, **kwargs)
│ │              ppl.close('all')
│ │  
│ │      def test_spectral_selections(self):
│ │  
│ │          # trial, channel and toi selections
│ │          selections = helpers.mk_selection_dicts(self.nTrials,
│ │                                                  self.nChannels - 1,
│ │                                                  toi_min=self.toi_min,
│ │                                                  toi_max=self.toi_max)
│ │  
│ │ -        # add a foi selection
│ │ -        # FIXME: show() and foi selections   #291
│ │ -        # selections[0]['foi'] = np.arange(5, 300, step=2)
│ │ -
│ │          # test all combinations
│ │          for sel_dict in selections:
│ │  
│ │              # only single trial plotting
│ │              # is supported until averaging is availbale
│ │              # take random 1st trial
│ │              sel_dict['trials'] = sel_dict['trials'][0]
│ │ @@ -191,38 +232,31 @@
│ │              self.test_spectral_plotting(**sel_dict)
│ │  
│ │      def test_spectral_exceptions(self):
│ │  
│ │          # empty arrays get returned for empty time selection
│ │          with pytest.raises(SPYValueError) as err:
│ │              self.test_spectral_plotting(trials=0,
│ │ -                                        toilim=[self.toi_max + 1, self.toi_max + 2])
│ │ +                                        latency=[self.toi_max + 1, self.toi_max + 2])
│ │              assert "zero size" in str(err)
│ │  
│ │          # invalid channel selection
│ │          with pytest.raises(SPYValueError) as err:
│ │              self.test_spectral_plotting(trials=0, channel=self.nChannels + 1)
│ │              assert "channel existing names" in str(err)
│ │  
│ │          # invalid trial selection
│ │          with pytest.raises(SPYValueError) as err:
│ │              self.test_spectral_plotting(trials=self.nTrials + 1)
│ │              assert "select: trials" in str(err)
│ │  
│ │          # invalid foi selection
│ │          with pytest.raises(SPYValueError) as err:
│ │ -            self.test_spectral_plotting(trials=0, foilim=[-1, 0])
│ │ -            assert "foi/foilim" in str(err)
│ │ -            assert "bounded by" in str(err)
│ │ -
│ │ -        # invalid foi selection
│ │ -        with pytest.raises(SPYValueError) as err:
│ │ -            self.test_spectral_plotting(trials=0, foi=[-1, 0])
│ │ -            assert "foi/foilim" in str(err)
│ │ -            assert "bounded by" in str(err)
│ │ +            self.test_spectral_plotting(trials=0, frequency=[-1, 0])
│ │ +            assert "frequency" in str(err)
│ │  
│ │  
│ │  class TestCrossSpectralPlotting():
│ │  
│ │      nTrials = 40
│ │      nChannels = 4
│ │      nSamples = 400
│ │ @@ -237,47 +271,51 @@
│ │      adata = adata + .6 * synth_data.AR2_network(nTrials=nTrials,
│ │                                                  AdjMat=np.zeros((nChannels,
│ │                                                                   nChannels)),
│ │                                                  nSamples=nSamples,
│ │                                                  alphas=[0.8, 0])
│ │  
│ │      # some interesting range
│ │ -    foilim = [1, 400]
│ │ +    frequency = [1, 400]
│ │  
│ │      # all trials are equal
│ │      toi_min, toi_max = adata.time[0][0], adata.time[0][-1]
│ │  
│ │      coh = spy.connectivityanalysis(adata, method='coh', tapsmofrq=1)
│ │ +    coh_imag = spy.connectivityanalysis(adata, method='coh', tapsmofrq=1, output='imag')
│ │ +
│ │      corr = spy.connectivityanalysis(adata, method='corr')
│ │      granger = spy.connectivityanalysis(adata, method='granger', tapsmofrq=1)
│ │  
│ │      def test_cs_plotting(self, **kwargs):
│ │  
│ │          # no interactive plotting
│ │          ppl.ioff()
│ │  
│ │          # check if we run the default test
│ │          def_test = not len(kwargs)
│ │  
│ │          if def_test:
│ │              ppl.ion()
│ │  
│ │ -            self.coh.singlepanelplot(channel_i=0, channel_j=1, foilim=[50, 320])
│ │ -            self.coh.singlepanelplot(channel_i=1, channel_j=2, foilim=[50, 320])
│ │ -            self.coh.singlepanelplot(channel_i=2, channel_j=3, foilim=[50, 320])
│ │ -
│ │ -            self.corr.singlepanelplot(channel_i=0, channel_j=1, toilim=[0, .1])
│ │ -            self.corr.singlepanelplot(channel_i=1, channel_j=0, toilim=[0, .1])
│ │ -            self.corr.singlepanelplot(channel_i=2, channel_j=3, toilim=[0, .1])
│ │ -
│ │ -            self.granger.singlepanelplot(channel_i=0, channel_j=1, foilim=[50, 320])
│ │ -            self.granger.singlepanelplot(channel_i=3, channel_j=2, foilim=[50, 320])
│ │ -            self.granger.singlepanelplot(channel_i=2, channel_j=3, foilim=[50, 320])
│ │ +            self.coh.singlepanelplot(channel_i=0, channel_j=1, frequency=[50, 320])
│ │ +            self.coh.singlepanelplot(channel_i=1, channel_j=2, frequency=[50, 320])
│ │ +            self.coh.singlepanelplot(channel_i=2, channel_j=3, frequency=[50, 320])
│ │ +
│ │ +            self.coh_imag.singlepanelplot(channel_i=1, channel_j=2, frequency=[50, 320])
│ │ +
│ │ +            self.corr.singlepanelplot(channel_i=0, channel_j=1, latency=[0, .1])
│ │ +            self.corr.singlepanelplot(channel_i=1, channel_j=0, latency=[0, .1])
│ │ +            self.corr.singlepanelplot(channel_i=2, channel_j=3, latency=[0, .1])
│ │  
│ │ -        elif 'toilim' in kwargs:
│ │ +            self.granger.singlepanelplot(channel_i=0, channel_j=1, frequency=[50, 320])
│ │ +            self.granger.singlepanelplot(channel_i=3, channel_j=2, frequency=[50, 320])
│ │ +            self.granger.singlepanelplot(channel_i=2, channel_j=3, frequency=[50, 320])
│ │ +
│ │ +        elif 'latency' in kwargs:
│ │  
│ │              self.corr.singlepanelplot(**kwargs)
│ │              self.corr.singlepanelplot(**kwargs)
│ │              self.corr.singlepanelplot(**kwargs)
│ │              ppl.close('all')
│ │  
│ │          else:
│ │ @@ -292,45 +330,45 @@
│ │  
│ │      def test_cs_selections(self):
│ │  
│ │          # channel combinations
│ │          chans = itertools.product(self.coh.channel_i[:self.nTrials - 1],
│ │                                    self.coh.channel_j[1:])
│ │  
│ │ -        # out of range toi selections are allowed..
│ │ -        toilim = ([0, .1], [-1, 1], 'all')
│ │ -        toilim_comb = itertools.product(chans, toilim)
│ │ +        # out of range toi selections are no longer allowed..
│ │ +        latency = ([0, .1], 'all')
│ │ +        toilim_comb = itertools.product(chans, latency)
│ │  
│ │          # out of range foi selections are NOT allowed..
│ │ -        foilim = ([10., 82.31], 'all')
│ │ -        foilim_comb = itertools.product(chans, foilim)
│ │ +        frequency = ([10., 82.31], 'all')
│ │ +        foilim_comb = itertools.product(chans, frequency)
│ │  
│ │          for comb in toilim_comb:
│ │              sel_dct = {}
│ │              c1, c2 = comb[0]
│ │              sel_dct['channel_i'] = c1
│ │              sel_dct['channel_j'] = c2
│ │ -            sel_dct['toilim'] = comb[1]
│ │ +            sel_dct['latency'] = comb[1]
│ │              self.test_cs_plotting(**sel_dct)
│ │  
│ │          for comb in foilim_comb:
│ │              sel_dct = {}
│ │              c1, c2 = comb[0]
│ │              sel_dct['channel_i'] = c1
│ │              sel_dct['channel_j'] = c2
│ │ -            sel_dct['foilim'] = comb[1]
│ │ +            sel_dct['frequency'] = comb[1]
│ │              self.test_cs_plotting(**sel_dct)
│ │  
│ │      def test_cs_exceptions(self):
│ │  
│ │          chan_sel = {'channel_i': 0, 'channel_j': 2}
│ │          # empty arrays get returned for empty time selection
│ │          with pytest.raises(SPYValueError) as err:
│ │              self.test_cs_plotting(trials=0,
│ │ -                                  toilim=[self.toi_max + 1, self.toi_max + 2],
│ │ +                                  latency=[self.toi_max + 1, self.toi_max + 2],
│ │                                    **chan_sel)
│ │              assert "zero size" in str(err)
│ │  
│ │          # invalid channel selections
│ │          with pytest.raises(SPYValueError) as err:
│ │              self.test_cs_plotting(trials=0, channel_i=self.nChannels + 1, channel_j=0)
│ │              assert "channel existing names" in str(err)
│ │ @@ -338,22 +376,15 @@
│ │          # invalid trial selection
│ │          with pytest.raises(SPYValueError) as err:
│ │              self.test_cs_plotting(trials=self.nTrials + 1, **chan_sel)
│ │              assert "select: trials" in str(err)
│ │  
│ │          # invalid foi selection
│ │          with pytest.raises(SPYValueError) as err:
│ │ -            self.test_cs_plotting(trials=0, foilim=[-1, 0], **chan_sel)
│ │ -            assert "foi/foilim" in str(err)
│ │ -            assert "bounded by" in str(err)
│ │ -
│ │ -        # invalid foi selection
│ │ -        with pytest.raises(SPYValueError) as err:
│ │ -            self.test_cs_plotting(trials=0, foi=[-1, 0], **chan_sel)
│ │ -            assert "foi/foilim" in str(err)
│ │ -            assert "bounded by" in str(err)
│ │ +            self.test_cs_plotting(trials=0, frequency=[-1, -0.2], **chan_sel)
│ │ +            assert "frequency" in str(err)
│ │  
│ │  
│ │  if __name__ == '__main__':
│ │      T1 = TestAnalogPlotting()
│ │      T2 = TestSpectralPlotting()
│ │      T3 = TestCrossSpectralPlotting()
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_preproc.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_preproc.py
│ │┄ Files 3% similar despite different names
│ │ @@ -7,29 +7,25 @@
│ │  import psutil
│ │  import pytest
│ │  import inspect
│ │  import numpy as np
│ │  import matplotlib.pyplot as ppl
│ │  
│ │  # Local imports
│ │ -from syncopy import __acme__
│ │ -if __acme__:
│ │ -    import dask.distributed as dd
│ │ +import dask.distributed as dd
│ │  
│ │  from syncopy import preprocessing as ppfunc
│ │  from syncopy import AnalogData, freqanalysis
│ │  import syncopy.preproc as preproc  # submodule
│ │  import syncopy.tests.helpers as helpers
│ │  from syncopy.tests import synth_data as sd
│ │  
│ │  from syncopy.shared.errors import SPYValueError
│ │  from syncopy.shared.tools import get_defaults, best_match
│ │  
│ │ -# Decorator to decide whether or not to run dask-related tests
│ │ -skip_without_acme = pytest.mark.skipif(not __acme__, reason="acme not available")
│ │  # Decorator to decide whether or not to run memory-intensive tests
│ │  availMem = psutil.virtual_memory().total
│ │  minRAM = 5
│ │  skip_low_mem = pytest.mark.skipif(availMem < minRAM * 1024**3, reason=f"less than {minRAM}GB RAM available")
│ │  
│ │  # availableFilterTypes = ('lp', 'hp', 'bp', 'bs')
│ │  # availableDirections = ('twopass', 'onepass', 'onepass-minphase')
│ │ @@ -101,15 +97,15 @@
│ │                  # toilim selections can screw up the
│ │                  # frequency axis of freqanalysis/np.fft.rfftfreq :/
│ │                  foilim = [self.freq_kw[ftype], spec_f.freq[-1]]
│ │              else:
│ │                  foilim = self.freq_kw[ftype]
│ │  
│ │              # remaining power after filtering
│ │ -            pow_fil = spec_f.show(channel=0, foilim=foilim).sum()
│ │ +            pow_fil = spec_f.show(channel=0, frequency=foilim).sum()
│ │              _, idx = best_match(spec_f.freq, foilim, span=True)
│ │              # ratio of pass-band to total freqency band
│ │              ratio = len(idx) / nFreq
│ │  
│ │              # at least 80% of the ideal filter power
│ │              # should be still around
│ │              if ftype in ('lp', 'hp'):
│ │ @@ -187,19 +183,19 @@
│ │          cfg.filter_type = 'hp'
│ │  
│ │          result = ppfunc(self.data, cfg)
│ │  
│ │          # check here just for finiteness
│ │          assert np.all(np.isfinite(result.data))
│ │  
│ │ -    @skip_without_acme
│ │ -    def test_but_parallel(self, testcluster=None):
│ │ +    def test_but_parallel(self, testcluster):
│ │  
│ │          ppl.ioff()
│ │          client = dd.Client(testcluster)
│ │ +        print(client)
│ │          all_tests = [attr for attr in self.__dir__()
│ │                       if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
│ │  
│ │          for test_name in all_tests:
│ │              test_method = getattr(self, test_name)
│ │              # don't test parallel selections
│ │              if 'selection' in test_name:
│ │ @@ -313,15 +309,15 @@
│ │                  # toilim selections can screw up the
│ │                  # frequency axis of freqanalysis/np.fft.rfftfreq :/
│ │                  foilim = [self.freq_kw[ftype], spec_f.freq[-1]]
│ │              else:
│ │                  foilim = self.freq_kw[ftype]
│ │  
│ │              # remaining power after filtering
│ │ -            pow_fil = spec_f.show(channel=0, foilim=foilim).sum()
│ │ +            pow_fil = spec_f.show(channel=0, frequency=foilim).sum()
│ │              _, idx = best_match(spec_f.freq, foilim, span=True)
│ │              # ratio of pass-band to total freqency band
│ │              ratio = len(idx) / nFreq
│ │  
│ │              # at least 80% of the ideal filter power
│ │              # should be still around
│ │              if ftype in ('lp', 'hp'):
│ │ @@ -394,16 +390,15 @@
│ │          cfg.filter_type = 'hp'
│ │  
│ │          result = ppfunc(self.data, cfg)
│ │  
│ │          # check here just for finiteness
│ │          assert np.all(np.isfinite(result.data))
│ │  
│ │ -    @skip_without_acme
│ │ -    def test_firws_parallel(self, testcluster=None):
│ │ +    def test_firws_parallel(self, testcluster):
│ │  
│ │          ppl.ioff()
│ │          client = dd.Client(testcluster)
│ │          all_tests = [attr for attr in self.__dir__()
│ │                       if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
│ │  
│ │          for test_name in all_tests:
│ │ @@ -484,28 +479,27 @@
│ │          orig_c0 = self.AData.show(trials=1, channel=0)
│ │          res_c0 = res.show(trials=1, channel=0)
│ │  
│ │          # detrended also means demeaned
│ │          assert np.allclose(np.mean(res.show(trials=1, channel=0)), 0, atol=1e-5)
│ │  
│ │          # check that the linear trend is gone
│ │ -        assert (orig_c0.max() - orig_c0.min()) > 1.8 * (res_c0.max() - res_c0.min())
│ │ +        assert (orig_c0.max() - orig_c0.min()) > 1.5 * (res_c0.max() - res_c0.min())
│ │  
│ │      def test_exceptions(self):
│ │          with pytest.raises(SPYValueError, match='neither filtering, detrending or zscore'):
│ │              ppfunc(self.AData, filter_class=None, polyremoval=None)
│ │  
│ │          with pytest.raises(SPYValueError, match='expected value to be greater'):
│ │              ppfunc(self.AData, filter_class=None, polyremoval=-1)
│ │  
│ │          with pytest.raises(SPYValueError, match='expected value to be greater'):
│ │              ppfunc(self.AData, filter_class=None, polyremoval=2)
│ │  
│ │ -    @skip_without_acme
│ │ -    def test_detr_parallel(self, testcluster=None):
│ │ +    def test_detr_parallel(self, testcluster):
│ │  
│ │          client = dd.Client(testcluster)
│ │          all_tests = [attr for attr in self.__dir__()
│ │                       if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
│ │  
│ │          for test_name in all_tests:
│ │              test_method = getattr(self, test_name)
│ │ @@ -564,16 +558,15 @@
│ │  
│ │          with pytest.raises(SPYValueError, match='neither filtering, detrending or zscore'):
│ │              ppfunc(self.AData, filter_class=None, polyremoval=None, zscore=False)
│ │  
│ │          with pytest.raises(SPYValueError, match='expected either `True` or `False`'):
│ │              ppfunc(self.AData, filter_class=None, zscore=2)
│ │  
│ │ -    @skip_without_acme
│ │ -    def test_zscore_parallel(self, testcluster=None):
│ │ +    def test_zscore_parallel(self, testcluster):
│ │  
│ │          client = dd.Client(testcluster)
│ │          all_tests = [attr for attr in self.__dir__()
│ │                       if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
│ │  
│ │          for test_name in all_tests:
│ │              test_method = getattr(self, test_name)
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_resampledata.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_resampledata.py
│ │┄ Files 4% similar despite different names
│ │ @@ -6,43 +6,39 @@
│ │  # 3rd party imports
│ │  import pytest
│ │  import inspect
│ │  import numpy as np
│ │  import matplotlib.pyplot as ppl
│ │  
│ │  # Local imports
│ │ -from syncopy import __acme__
│ │ -if __acme__:
│ │ -    import dask.distributed as dd
│ │ +import dask.distributed as dd
│ │  
│ │  from syncopy import resampledata, freqanalysis
│ │  import syncopy.tests.synth_data as synth_data
│ │  import syncopy.tests.helpers as helpers
│ │  from syncopy.shared.errors import SPYValueError
│ │  from syncopy.shared.tools import get_defaults
│ │  
│ │ -# Decorator to decide whether or not to run dask-related tests
│ │ -skip_without_acme = pytest.mark.skipif(not __acme__, reason="acme not available")
│ │ -
│ │  # availableFilterTypes = ('lp', 'hp', 'bp', 'bs')
│ │  
│ │  
│ │  class TestDownsampling:
│ │  
│ │ -    nSamples = 1000
│ │ +    nSamples = 991
│ │      nChannels = 4
│ │      nTrials = 100
│ │      fs = 200
│ │      fNy = fs / 2
│ │  
│ │      # -- use flat white noise as test data --
│ │      adata = synth_data.white_noise(nTrials,
│ │                                     nChannels=nChannels,
│ │                                     nSamples=nSamples,
│ │ -                                   samplerate=fs)
│ │ +                                   samplerate=fs,
│ │ +                                   seed=42)
│ │  
│ │      # original spectrum
│ │      spec = freqanalysis(adata, tapsmofrq=1, keeptrials=False)
│ │      # mean of the flat spectrum
│ │      pow_orig = spec.show(channel=0)[5:].mean()
│ │  
│ │      # for toi tests, -1s offset
│ │ @@ -57,14 +53,18 @@
│ │          # check if we run the default test
│ │          def_test = not len(kwargs)
│ │  
│ │          # write default parameters dict
│ │          if def_test:
│ │              kwargs = {'resamplefs': self.fs // 2}
│ │          ds = resampledata(self.adata, method='downsample', **kwargs)
│ │ +        lenTrials = np.diff(ds.sampleinfo).squeeze()
│ │ +        # check for equal trials
│ │ +        assert np.unique(lenTrials).size == 1
│ │ +
│ │          spec_ds = freqanalysis(ds, tapsmofrq=1, keeptrials=False)
│ │  
│ │          # all channels are equal, trim off 0-frequency dip
│ │          pow_ds = spec_ds.show(channel=0)[5:].mean()
│ │  
│ │          if def_test:
│ │  
│ │ @@ -143,16 +143,15 @@
│ │  
│ │          # all channels are equal
│ │          pow_ds = spec_ds.show(channel=0).mean()
│ │  
│ │          # with aa filter power does not change
│ │          assert np.allclose(self.pow_orig, pow_ds, rtol=.5e-1)
│ │  
│ │ -    @skip_without_acme
│ │ -    def test_ds_parallel(self, testcluster=None):
│ │ +    def test_ds_parallel(self, testcluster):
│ │  
│ │          ppl.ioff()
│ │          client = dd.Client(testcluster)
│ │          all_tests = [attr for attr in self.__dir__()
│ │                       if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
│ │  
│ │          for test_name in all_tests:
│ │ @@ -170,15 +169,16 @@
│ │      fs = 200
│ │      fNy = fs / 2
│ │  
│ │      # -- use flat white noise as test data --
│ │      adata = synth_data.white_noise(nTrials,
│ │                                     nChannels=nChannels,
│ │                                     nSamples=nSamples,
│ │ -                                   samplerate=fs)
│ │ +                                   samplerate=fs,
│ │ +                                   seed=42)
│ │  
│ │      # original spectrum
│ │      spec = freqanalysis(adata, tapsmofrq=1, keeptrials=False)
│ │      # mean of the flat spectrum
│ │      pow_orig = spec.show(channel=0).mean()
│ │  
│ │      # for toi tests, -1s offset
│ │ @@ -195,20 +195,24 @@
│ │  
│ │          # write default parameters dict
│ │          if def_test:
│ │              # polyphase method: firws acts on the upsampled data!
│ │              kwargs = {'resamplefs': self.fs * 0.43, 'order': 5000}
│ │  
│ │          rs = resampledata(self.adata, method='resample', **kwargs)
│ │ +        lenTrials = np.diff(rs.sampleinfo).squeeze()
│ │ +        # check for equal trials
│ │ +        assert np.unique(lenTrials).size == 1
│ │ +
│ │          spec_rs = freqanalysis(rs, tapsmofrq=1, keeptrials=False)
│ │  
│ │          # all channels are equal,
│ │          # avoid the nose with 3Hz away from the cut-off
│ │          pow_rs = spec_rs.show(channel=0,
│ │ -                              foilim=[0, kwargs['resamplefs'] / 2 - 3]).mean()
│ │ +                              frequency=[0, kwargs['resamplefs'] / 2 - 3]).mean()
│ │  
│ │          if def_test:
│ │              # here we have aa filtering built in,
│ │              # so the power should be unchanged after resampling
│ │              assert np.allclose(self.pow_orig, pow_rs, rtol=.5e-1)
│ │  
│ │              f, ax = mk_spec_ax()
│ │ @@ -224,31 +228,31 @@
│ │      def test_rs_exceptions(self):
│ │  
│ │          # test wrong method
│ │          with pytest.raises(SPYValueError, match='Invalid value of `method`'):
│ │              resampledata(self.adata, method='nothing-real', resamplefs=self.fs // 2)
│ │  
│ │      def test_rs_selections(self):
│ │ -
│ │ +        np.random.seed(42)
│ │          sel_dicts = helpers.mk_selection_dicts(nTrials=20,
│ │                                                 nChannels=2,
│ │                                                 toi_min=self.time_span[0],
│ │                                                 toi_max=self.time_span[1],
│ │                                                 min_len=3.5)
│ │          for sd in sel_dicts:
│ │ +            print(sd)
│ │              spec_rs = self.test_resampling(select=sd, resamplefs=self.fs / 2.1)
│ │              # remove 3Hz window around the filter cut
│ │              pow_rs = spec_rs.show(channel=0)[:-3].mean()
│ │  
│ │              # test for finitenes and make sure we did not loose power
│ │              assert np.all(np.isfinite(spec_rs.data))
│ │              assert pow_rs >= 0.9 * self.pow_orig
│ │  
│ │ -    @skip_without_acme
│ │ -    def test_rs_parallel(self, testcluster=None):
│ │ +    def test_rs_parallel(self, testcluster):
│ │  
│ │          ppl.ioff()
│ │          client = dd.Client(testcluster)
│ │          all_tests = [attr for attr in self.__dir__()
│ │                       if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
│ │  
│ │          for test_name in all_tests:
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_specest.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_specest.py
│ │┄ Files 5% similar despite different names
│ │ @@ -6,29 +6,24 @@
│ │  # Builtin/3rd party package imports
│ │  import inspect
│ │  import random
│ │  import psutil
│ │  import pytest
│ │  import numpy as np
│ │  import scipy.signal as scisig
│ │ -from syncopy import __acme__
│ │ -if __acme__:
│ │ -    import dask.distributed as dd
│ │ +import dask.distributed as dd
│ │  
│ │  # Local imports
│ │  from syncopy.tests.misc import generate_artificial_data, flush_local_cluster
│ │ -from syncopy import freqanalysis
│ │ -from syncopy.shared.errors import SPYValueError
│ │ -from syncopy.datatype.base_data import Selector
│ │ +from syncopy import freqanalysis, selectdata
│ │ +from syncopy.shared.errors import SPYValueError, SPYError
│ │ +from syncopy.datatype.selector import Selector
│ │  from syncopy.datatype import AnalogData, SpectralData
│ │  from syncopy.shared.tools import StructDict, get_defaults
│ │  
│ │ -# Decorator to decide whether or not to run dask-related tests
│ │ -skip_without_acme = pytest.mark.skipif(not __acme__, reason="acme not available")
│ │ -
│ │  # Decorator to decide whether or not to run memory-intensive tests
│ │  availMem = psutil.virtual_memory().total
│ │  skip_low_mem = pytest.mark.skipif(availMem < 10 * 1024**3, reason="less than 10GB RAM available")
│ │  
│ │  
│ │  # Local helper for constructing TF testing signals
│ │  def _make_tf_signal(nChannels, nTrials, seed, fadeIn=None, fadeOut=None, short=False):
│ │ @@ -94,24 +89,25 @@
│ │      tfData = AnalogData(data=sig, samplerate=fs, trialdefinition=trialdefinition)
│ │  
│ │      return tfData, modulators, even, odd, fader
│ │  
│ │  
│ │  class TestMTMFFT():
│ │  
│ │ +
│ │      # Construct simple trigonometric signal to check FFT consistency: each
│ │      # channel is a sine wave of frequency `freqs[nchan]` with single unique
│ │      # amplitude `amp` and sampling frequency `fs`
│ │      nChannels = 32
│ │      nTrials = 8
│ │      fs = 1024
│ │      fband = np.linspace(1, fs/2, int(np.floor(fs/2)))
│ │      freqs = [88.,  35., 278., 104., 405., 314., 271., 441., 343., 374., 428.,
│ │               367., 75., 118., 289., 310., 510., 102., 123., 417., 273., 449.,
│ │ -             416.,  32., 438., 111., 140., 304., 327., 494.,  23., 493.]
│ │ +             416., 32., 438., 111., 140., 304., 327., 494., 23., 493.]
│ │      freqs = freqs[:nChannels]
│ │      # freqs = np.random.choice(fband[:-2], size=nChannels, replace=False)
│ │      amp = np.pi
│ │      phases = np.random.permutation(np.linspace(0, 2 * np.pi, nChannels))
│ │      t = np.linspace(0, nTrials, nTrials * fs)
│ │      sig = np.zeros((t.size, nChannels), dtype="float32")
│ │      for nchan in range(nChannels):
│ │ @@ -128,38 +124,41 @@
│ │  
│ │      # Data selections to be tested w/data generated based on `sig`
│ │      sigdataSelections = [None,
│ │                           {"trials": [3, 1, 0],
│ │                            "channel": ["channel" + str(i) for i in range(12, 28)][::-1]},
│ │                           {"trials": [0, 1, 2],
│ │                            "channel": range(0, int(nChannels / 2)),
│ │ -                          "toilim": [0.25, 0.75]}]
│ │ +                          "latency": [0.25, 0.75]}]
│ │  
│ │      # Data selections to be tested w/`artdata` generated below (use fixed but arbitrary
│ │      # random number seed to randomly select time-points for `toi` (with repetitions)
│ │      seed = np.random.RandomState(13)
│ │      artdataSelections = [None,
│ │                           {"trials": [3, 1, 0],
│ │ -                          "channel": ["channel" + str(i) for i in range(10, 15)][::-1],
│ │ -                          "toi": None},
│ │ +                          "channel": ["channel" + str(i) for i in range(10, 15)][::-1]},
│ │                           {"trials": [0, 1, 2],
│ │                            "channel": range(0, 8),
│ │ -                          "toilim": [-0.5, 0.6]}]
│ │ +                          "latency": [-0.5, 0.6]}]
│ │  
│ │      # Error tolerances for target amplitudes (depend on data selection!)
│ │      tols = [1, 1, 1.5]
│ │  
│ │      # Error tolerance for frequency-matching
│ │      ftol = 0.25
│ │  
│ │      # Helper function that reduces dataselections (keep `None` selection no matter what)
│ │ -    def test_cut_selections(self, fulltests):
│ │ -        if not fulltests:
│ │ -            self.sigdataSelections.pop(random.choice([-1, 1]))
│ │ -            self.artdataSelections.pop(random.choice([-1, 1]))
│ │ +    def test_cut_selections(self):
│ │ +        self.sigdataSelections.pop(random.choice([-1, 1]))
│ │ +        self.artdataSelections.pop(random.choice([-1, 1]))
│ │ +
│ │ +    @staticmethod
│ │ +    def get_adata():
│ │ +        return AnalogData(data=TestMTMFFT.sig, samplerate=TestMTMFFT.fs,
│ │ +                       trialdefinition=TestMTMFFT.trialdefinition)
│ │  
│ │      def test_output(self):
│ │          # ensure that output type specification is respected
│ │          for select in self.sigdataSelections:
│ │              spec = freqanalysis(self.adata, method="mtmfft", taper="hann",
│ │                                  output="fourier", select=select)
│ │              assert "complex" in spec.data.dtype.name
│ │ @@ -174,125 +173,203 @@
│ │          # ensure channel-specific frequencies are identified correctly
│ │          for sk, select in enumerate(self.sigdataSelections):
│ │              sel = Selector(self.adata, select)
│ │              spec = freqanalysis(self.adata, method="mtmfft", taper="hann",
│ │                                  pad="nextpow2", output="pow", select=select)
│ │  
│ │              chanList = np.arange(self.nChannels)[sel.channel]
│ │ -            amps = np.empty((len(sel.trials) * len(chanList),))
│ │ +            amps = np.empty((len(sel.trial_ids) * len(chanList),))
│ │              k = 0
│ │              for nchan, chan in enumerate(chanList):
│ │                  for ntrial in range(len(spec.trials)):
│ │                      amps[k] = spec.data[ntrial, :, :, nchan].max() / \
│ │                          self.t.size
│ │                      assert np.argmax(
│ │                              spec.data[ntrial, :, :, nchan]) == self.freqs[chan]
│ │                      k += 1
│ │  
│ │              # ensure amplitude is consistent across all channels/trials
│ │              assert np.all(np.diff(amps) < self.tols[sk])
│ │  
│ │ +    def test_normalization(self):
│ │ +
│ │ +        nSamples = 1000
│ │ +        fsample = 500  # 2s long signal
│ │ +        Ampl = 4  # amplitude
│ │ +        # 50Hz harmonic, spectral power is given by: Ampl^2 / 2 = 8
│ │ +        signal = Ampl * np.cos(2 * np.pi * 50 * np.arange(nSamples) * 1 / fsample)
│ │ +
│ │ +        # single signal/channel is enough
│ │ +        ad = AnalogData([signal[:, None]], samplerate=fsample)
│ │ +
│ │ +        cfg = StructDict()
│ │ +        cfg.foilim = [40, 60]
│ │ +        cfg.output = 'pow'
│ │ +        cfg.taper = None
│ │ +
│ │ +        # -- syncopy's default, padding does NOT change power --
│ │ +
│ │ +        cfg.ft_compat = False
│ │ +        cfg.pad = 'maxperlen'  # that's the default -> no padding
│ │ +        spec = freqanalysis(ad, cfg)
│ │ +        peak_power = spec.show().max()
│ │ +        df_no_pad = np.diff(spec.freq)  # freq. resolution
│ │ +        assert np.allclose(peak_power, Ampl**2 / 2, atol=1e-5)
│ │ +
│ │ +        cfg.pad = 4  # in seconds, double the size
│ │ +        spec = freqanalysis(ad, cfg)
│ │ +        df_with_pad = np.diff(spec.freq)
│ │ +        # we double the spectral resolution
│ │ +        assert np.allclose(df_no_pad[0], 2 * df_with_pad[0])
│ │ +        # yet power stays the same
│ │ +        peak_power = spec.show().max()
│ │ +        assert np.allclose(peak_power, Ampl**2 / 2, atol=1e-5)
│ │ +
│ │ +        # -- FT compat mode, padding does dilute the power --
│ │ +
│ │ +        cfg.ft_compat = True
│ │ +        cfg.pad = 'maxperlen'  # that's the default
│ │ +        spec = freqanalysis(ad, cfg)
│ │ +        peak_power = spec.show().max()
│ │ +        df_no_pad = np.diff(spec.freq)
│ │ +        # default padding is no padding if all trials are equally sized,
│ │ +        # so here the results are the same
│ │ +        assert np.allclose(peak_power, Ampl**2 / 2, atol=1e-5)
│ │ +
│ │ +        cfg.pad = 4  # in seconds, double the size
│ │ +        spec = freqanalysis(ad, cfg)
│ │ +        df_with_pad = np.diff(spec.freq)
│ │ +        # we double the spectral resolution
│ │ +        assert np.allclose(df_no_pad[0], df_with_pad[0] * 2)
│ │ +        # here half the power is now lost!
│ │ +        peak_power = spec.show().max()
│ │ +        assert np.allclose(peak_power, Ampl**2 / 4, atol=1e-5)
│ │ +
│ │ +        # -- works the same with tapering --
│ │ +
│ │ +        cfg.ft_compat = False
│ │ +        cfg.pad = 'maxperlen'  # that's the default
│ │ +        cfg.taper = 'kaiser'
│ │ +        cfg.taper_opt = {'beta': 10}
│ │ +        spec = freqanalysis(ad, cfg)
│ │ +        peak_power_no_pad = spec.show().max()
│ │ +
│ │ +        cfg.pad = 4
│ │ +        spec = freqanalysis(ad, cfg)
│ │ +        peak_power_with_pad = spec.show().max()
│ │ +        assert np.allclose(peak_power_no_pad, peak_power_with_pad, atol=1e-5)
│ │ +
│ │ +        cfg.ft_compat = True
│ │ +        cfg.pad = 'maxperlen'  # that's the default
│ │ +        cfg.taper = 'kaiser'
│ │ +        cfg.taper_opt = {'beta': 10}
│ │ +        spec = freqanalysis(ad, cfg)
│ │ +        peak_power_no_pad = spec.show().max()
│ │ +
│ │ +        cfg.pad = 4
│ │ +        spec = freqanalysis(ad, cfg)
│ │ +        peak_power_with_pad = spec.show().max()
│ │ +        # again half the power is lost with FT compat
│ │ +        assert np.allclose(peak_power_no_pad, 2 * peak_power_with_pad, atol=1e-5)
│ │ +
│ │      def test_foi(self):
│ │          for select in self.sigdataSelections:
│ │  
│ │              # `foi` lims outside valid bounds
│ │              with pytest.raises(SPYValueError):
│ │ -                freqanalysis(self.adata, method="mtmfft", taper="hann",
│ │ +                freqanalysis(TestMTMFFT.get_adata(), method="mtmfft", taper="hann",
│ │                               foi=[-0.5, self.fs / 3], select=select)
│ │              with pytest.raises(SPYValueError):
│ │ -                freqanalysis(self.adata, method="mtmfft", taper="hann",
│ │ +                freqanalysis(TestMTMFFT.get_adata(), method="mtmfft", taper="hann",
│ │                               foi=[1, self.fs], select=select)
│ │  
│ │              foi = self.fband[1:int(self.fband.size / 3)]
│ │  
│ │              # offset `foi` by 0.1 Hz - resulting freqs must be unaffected
│ │              ftmp = foi + 0.1
│ │ -            spec = freqanalysis(self.adata, method="mtmfft", taper="hann",
│ │ +            spec = freqanalysis(TestMTMFFT.get_adata(), method="mtmfft", taper="hann",
│ │                                  pad="nextpow2", foi=ftmp, select=select)
│ │              assert np.all(spec.freq == foi)
│ │  
│ │              # unsorted, duplicate entries in `foi` - result must stay the same
│ │              ftmp = np.hstack([foi, np.full(20, foi[0])])
│ │ -            spec = freqanalysis(self.adata, method="mtmfft", taper="hann",
│ │ +            spec = freqanalysis(TestMTMFFT.get_adata(), method="mtmfft", taper="hann",
│ │                                  pad="nextpow2", foi=ftmp, select=select)
│ │              assert np.all(spec.freq == foi)
│ │  
│ │      def test_dpss(self):
│ │  
│ │          for select in self.sigdataSelections:
│ │ -            sel = Selector(self.adata, select)
│ │ +
│ │ +            self.adata.selectdata(select, inplace=True)
│ │ +            sel = self.adata.selection
│ │              chanList = np.arange(self.nChannels)[sel.channel]
│ │ +            self.adata.selection = None
│ │  
│ │              # ensure default setting results in single taper
│ │              spec = freqanalysis(self.adata, method="mtmfft",
│ │                                  tapsmofrq=3, output="pow", select=select)
│ │              assert spec.taper.size == 1
│ │              assert spec.channel.size == len(chanList)
│ │  
│ │              # specify tapers
│ │              spec = freqanalysis(self.adata, method="mtmfft",
│ │                                  tapsmofrq=7, keeptapers=True, select=select)
│ │              assert spec.channel.size == len(chanList)
│ │  
│ │ +            # trigger capture of too large tapsmofrq (edge case)
│ │ +            spec = freqanalysis(self.adata, method="mtmfft",
│ │ +                                tapsmofrq=2, output="pow", select=select)
│ │ +
│ │          # non-equidistant data w/multiple tapers
│ │          artdata = generate_artificial_data(nTrials=5, nChannels=16,
│ │                                             equidistant=False, inmemory=False)
│ │          timeAxis = artdata.dimord.index("time")
│ │          cfg = StructDict()
│ │          cfg.method = "mtmfft"
│ │ -        cfg.tapsmofrq = 9.3
│ │ +        cfg.tapsmofrq = 3.3
│ │          cfg.output = "pow"
│ │  
│ │          for select in self.artdataSelections:
│ │  
│ │ -            # unsorted, w/repetitions, do not pad
│ │ -            if select is not None and "toi" in select.keys():
│ │ -                select["toi"] = self.seed.choice(artdata.time[0], int(artdata.time[0].size))
│ │ -            sel = Selector(artdata, select)
│ │ +            sel = selectdata(artdata, select)
│ │              cfg.select = select
│ │              spec = freqanalysis(cfg, artdata)
│ │  
│ │              # ensure correctness of padding (respecting min. trial length + time-selection)
│ │              if select is None:
│ │                  maxtrlno = np.diff(artdata.sampleinfo).argmax()
│ │                  nSamples = artdata.trials[maxtrlno].shape[timeAxis]
│ │ -            elif "toi" in select:
│ │ -                nSamples = len(select["toi"])
│ │              else:
│ │ -                nSamples = artdata.time[sel.trials[0]][sel.time[0]].size
│ │ +                nSamples = max([trl.shape[0] for trl in sel.trials])
│ │              freqs = np.fft.rfftfreq(nSamples, 1 / artdata.samplerate)
│ │              assert spec.freq.size == freqs.size
│ │              assert np.max(spec.freq - freqs) < self.ftol
│ │  
│ │          # same + reversed dimensional order in input object
│ │          cfg.data = generate_artificial_data(nTrials=5, nChannels=16,
│ │                                              equidistant=False, inmemory=False,
│ │                                              dimord=AnalogData._defaultDimord[::-1])
│ │          timeAxis = cfg.data.dimord.index("time")
│ │          cfg.output = "abs"
│ │          cfg.keeptapers = True
│ │  
│ │          for select in self.artdataSelections:
│ │ -
│ │ -            # unsorted, w/repetitions, do not pad
│ │ -            if select is not None and "toi" in select.keys():
│ │ -                select["toi"] = self.seed.choice(cfg.data.time[0], int(cfg.data.time[0].size))
│ │ -            sel = Selector(cfg.data, select)
│ │ +            sel = selectdata(cfg.data, select)
│ │              cfg.select = select
│ │  
│ │              spec = freqanalysis(cfg)
│ │  
│ │              # ensure correctness of padding (respecting min. trial length + time-selection)
│ │              if select is None:
│ │                  maxtrlno = np.diff(cfg.data.sampleinfo).argmax()
│ │                  nSamples = cfg.data.trials[maxtrlno].shape[timeAxis]
│ │ -            elif "toi" in select:
│ │ -                nSamples = len(select["toi"])
│ │              else:
│ │ -                nSamples = cfg.data.time[sel.trials[0]][sel.time[0]].size
│ │ +                nSamples = max([trl.shape[1] for trl in sel.trials])
│ │ +
│ │              freqs = np.fft.rfftfreq(nSamples, 1 / cfg.data.samplerate)
│ │              assert spec.freq.size == freqs.size
│ │              assert np.max(spec.freq - freqs) < self.ftol
│ │              assert spec.taper.size > 1
│ │  
│ │          # same + overlapping trials
│ │          cfg.data = generate_artificial_data(nTrials=5, nChannels=16,
│ │ @@ -301,37 +378,31 @@
│ │                                              overlapping=True)
│ │          timeAxis = cfg.data.dimord.index("time")
│ │          cfg.keeptapers = False
│ │          cfg.output = "pow"
│ │  
│ │          for select in self.artdataSelections:
│ │  
│ │ -            # unsorted, w/repetitions, do not pad
│ │ -            # cfg.pop("pad", None)
│ │ -            if select is not None and "toi" in select.keys():
│ │ -                select["toi"] = self.seed.choice(cfg.data.time[0], int(cfg.data.time[0].size))
│ │ -            sel = Selector(cfg.data, select)
│ │ +            sel = selectdata(cfg.data, select)
│ │              cfg.select = select
│ │  
│ │              spec = freqanalysis(cfg)
│ │  
│ │              # ensure correctness of padding (respecting min. trial length + time-selection)
│ │              if select is None:
│ │                  maxtrlno = np.diff(cfg.data.sampleinfo).argmax()
│ │                  nSamples = cfg.data.trials[maxtrlno].shape[timeAxis]
│ │ -            elif "toi" in select:
│ │ -                nSamples = len(select["toi"])
│ │              else:
│ │ -                nSamples = cfg.data.time[sel.trials[0]][sel.time[0]].size
│ │ +                nSamples = max([trl.shape[timeAxis] for trl in sel.trials])
│ │ +
│ │              freqs = np.fft.rfftfreq(nSamples, 1 / cfg.data.samplerate)
│ │              assert spec.freq.size == freqs.size
│ │              assert np.max(spec.freq - freqs) < self.ftol
│ │              assert spec.taper.size == 1
│ │  
│ │ -    @skip_without_acme
│ │      @skip_low_mem
│ │      def test_parallel(self, testcluster):
│ │          # collect all tests of current class and repeat them using dask
│ │          client = dd.Client(testcluster)
│ │          all_tests = [attr for attr in self.__dir__()
│ │                       if (inspect.ismethod(getattr(self, attr)) and attr not in ["test_parallel", "test_cut_selections"])]
│ │          for test in all_tests:
│ │ @@ -415,55 +486,58 @@
│ │      nTrials = 3
│ │      seed = 151120
│ │      fadeIn = None
│ │      fadeOut = None
│ │      tfData, modulators, even, odd, fader = _make_tf_signal(nChannels, nTrials, seed,
│ │                                                             fadeIn=fadeIn, fadeOut=fadeOut)
│ │  
│ │ +    @staticmethod
│ │ +    def get_tfdata_mtmconvol():
│ │ +        """
│ │ +        High-frequency signal modulated by slow oscillating cosine and time-decaying noise.
│ │ +        """
│ │ +        return _make_tf_signal(TestMTMConvol.nChannels, TestMTMConvol.nTrials, TestMTMConvol.seed,
│ │ +                                                           fadeIn=TestMTMConvol.fadeIn, fadeOut=TestMTMConvol.fadeOut)[0]
│ │ +
│ │ +
│ │      # Data selection dict for the above object
│ │      dataSelections = [None,
│ │                        {"trials": [1, 2, 0],
│ │                         "channel": ["channel" + str(i) for i in range(2, 6)][::-1]},
│ │                        {"trials": [0, 2],
│ │                         "channel": range(0, nChan2),
│ │ -                       "toilim": [-2, 6.8]}]
│ │ -                    #    "toilim": [-20, 60.8]}] FIXME
│ │ +                       "latency": [-2, 6.8]}]
│ │ +                    #    "latency": [-20, 60.8]}] FIXME
│ │  
│ │      # Helper function that reduces dataselections (keep `None` selection no matter what)
│ │ -    def test_tf_cut_selections(self, fulltests):
│ │ -        if not fulltests:
│ │ -            self.dataSelections.pop(random.choice([-1, 1]))
│ │ +    def test_tf_cut_selections(self):
│ │ +        self.dataSelections.pop(random.choice([-1, 1]))
│ │  
│ │ -    def test_tf_output(self, fulltests):
│ │ +    def test_tf_output(self):
│ │          # Set up basic TF analysis parameters to not slow down things too much
│ │          cfg = get_defaults(freqanalysis)
│ │          cfg.method = "mtmconvol"
│ │          cfg.taper = "hann"
│ │          cfg.toi = np.linspace(-2, 6, 10)
│ │          cfg.t_ftimwin = 1.0
│ │          outputDict = {"fourier": "complex", "abs": "float", "pow": "float"}
│ │  
│ │          for select in self.dataSelections:
│ │ -            if fulltests:
│ │ -                cfg.select = select
│ │ -                if select is not None and "toilim" in cfg.select.keys():
│ │ -                    with pytest.raises(SPYValueError) as err:
│ │ -                        freqanalysis(cfg, self.tfData)
│ │ -                        assert "expected no `toi` specification due to active in-place time-selection" in str(err)
│ │ -                    continue
│ │ -                for key, value in outputDict.items():
│ │ -                    cfg.output = key
│ │ -                    tfSpec = freqanalysis(cfg, self.tfData)
│ │ -                    assert value in tfSpec.data.dtype.name
│ │ -            else:  # randomly pick from 'fourier', 'abs' and 'pow' and work w/smaller signal
│ │ -                cfg.select = {"trials" : 0, "channel" : 1}
│ │ -                cfg.output = random.choice(list(outputDict.keys()))
│ │ -                cfg.toi = np.linspace(-2, 6, 5)
│ │ -                tfSpec = freqanalysis(cfg, _make_tf_signal(2, 2, self.seed, fadeIn=self.fadeIn, fadeOut=self.fadeOut)[0])
│ │ -                assert outputDict[cfg.output] in tfSpec.data.dtype.name
│ │ +            cfg.select = select
│ │ +            if select is not None and "latency" in cfg.select.keys():
│ │ +                with pytest.raises(SPYValueError):
│ │ +                    freqanalysis(cfg, self.tfData)
│ │ +
│ │ +                self.tfData.selection = None
│ │ +                continue
│ │ +
│ │ +            for key, value in outputDict.items():
│ │ +                cfg.output = key
│ │ +                tfSpec = freqanalysis(cfg, self.tfData)
│ │ +                assert value in tfSpec.data.dtype.name
│ │  
│ │      def test_tf_solution(self):
│ │          # Compute "full" non-overlapping TF spectrum, i.e., center analysis windows
│ │          # on all time-points with window-boundaries touching but not intersecting
│ │          cfg = get_defaults(freqanalysis)
│ │          cfg.method = "mtmconvol"
│ │          cfg.taper = "hann"
│ │ @@ -480,53 +554,54 @@
│ │          allFreqs = np.arange(self.tfData.samplerate / 2 + 1)
│ │          foilimFreqs = np.arange(maxFreqs.min(), maxFreqs.max() + 1)
│ │  
│ │          for select in self.dataSelections:
│ │  
│ │              # Compute TF objects w\w/o`foi`/`foilim`
│ │              cfg.select = select
│ │ -            tfSpec = freqanalysis(cfg, self.tfData)
│ │ +            tfSpec = freqanalysis(cfg, TestMTMConvol.get_tfdata_mtmconvol())
│ │              cfg.foi = maxFreqs
│ │ -            tfSpecFoi = freqanalysis(cfg, self.tfData)
│ │ +            tfSpecFoi = freqanalysis(cfg, TestMTMConvol.get_tfdata_mtmconvol())
│ │              cfg.foi = None
│ │              cfg.foilim = [maxFreqs.min(), maxFreqs.max()]
│ │ -            tfSpecFoiLim = freqanalysis(cfg, self.tfData)
│ │ +            tfSpecFoiLim = freqanalysis(cfg, TestMTMConvol.get_tfdata_mtmconvol())
│ │              cfg.foilim = None
│ │  
│ │              # Ensure TF objects contain expected/requested frequencies
│ │              assert np.array_equal(tfSpec.freq, allFreqs)
│ │              assert np.array_equal(tfSpecFoi.freq, maxFreqs)
│ │              assert np.array_equal(tfSpecFoiLim.freq, foilimFreqs)
│ │  
│ │              for tk, trlArr in enumerate(tfSpec.trials):
│ │ +                tfData = TestMTMConvol.get_tfdata_mtmconvol()
│ │  
│ │ -                # Compute expected timing array depending on `toilim`
│ │ +                # Compute expected timing array depending on `latency`
│ │                  trlNo = tk
│ │ -                timeArr = np.arange(self.tfData.time[trlNo][0], self.tfData.time[trlNo][-1])
│ │ +                timeArr = np.arange(tfData.time[trlNo][0], tfData.time[trlNo][-1])
│ │                  timeSelection = slice(None)
│ │                  if select:
│ │                      trlNo = select["trials"][tk]
│ │ -                    if "toilim" in select.keys():
│ │ -                        timeArr = np.arange(*select["toilim"])
│ │ -                        timeStart = int(select['toilim'][0] * self.tfData.samplerate - self.tfData._t0[trlNo])
│ │ -                        timeStop = int(select['toilim'][1] * self.tfData.samplerate - self.tfData._t0[trlNo])
│ │ +                    if "latency" in select.keys():
│ │ +                        timeArr = np.arange(*select["latency"])
│ │ +                        timeStart = int(select['latency'][0] * tfData.samplerate - tfData._t0[trlNo])
│ │ +                        timeStop = int(select['latency'][1] * tfData.samplerate - tfData._t0[trlNo])
│ │                          timeSelection = slice(timeStart, timeStop)
│ │  
│ │                  # Ensure timing array was computed correctly and independent of `foi`/`foilim`
│ │                  assert np.array_equal(timeArr, tfSpec.time[tk])
│ │                  assert np.array_equal(tfSpec.time[tk], tfSpecFoi.time[tk])
│ │                  assert np.array_equal(tfSpecFoi.time[tk], tfSpecFoiLim.time[tk])
│ │  
│ │                  for chan in range(tfSpec.channel.size):
│ │  
│ │                      # Get reference channel in input object to determine underlying modulator
│ │                      chanNo = chan
│ │                      if select:
│ │ -                        if "toilim" not in select.keys():
│ │ -                            chanNo = np.where(self.tfData.channel == select["channel"][chan])[0][0]
│ │ +                        if "latency" not in select.keys():
│ │ +                            chanNo = np.where(tfData.channel == select["channel"][chan])[0][0]
│ │                      if chanNo % 2:
│ │                          modIdx = self.odd[(-1)**trlNo]
│ │                      else:
│ │                          modIdx = self.even[(-1)**trlNo]
│ │                      tfIdx[chanIdx] = chan
│ │                      Zxx = trlArr[tuple(tfIdx)].squeeze()
│ │  
│ │ @@ -572,97 +647,93 @@
│ │  
│ │          # Combine `toi`-testing w/in-place data-pre-selection
│ │          for select in self.dataSelections:
│ │              cfg.select = select
│ │              tStart = self.tfData.time[0][0]
│ │              tStop = self.tfData.time[0][-1]
│ │              if select:
│ │ -                if "toilim" in select.keys():
│ │ -                    tStart = select["toilim"][0]
│ │ -                    tStop = select["toilim"][1]
│ │ +                if "latency" in select.keys():
│ │ +                    tStart = select["latency"][0]
│ │ +                    tStop = select["latency"][1]
│ │  
│ │              # Test TF calculation w/different window-size/-centroids: ensure
│ │              # resulting timing arrays are correct
│ │ +            dt = TestMTMConvol.get_tfdata_mtmconvol()
│ │              for winsize in winSizes:
│ │                  cfg.t_ftimwin = winsize
│ │                  for toi in toiVals:
│ │                      cfg.toi = toi
│ │ -                    tfSpec = freqanalysis(cfg, self.tfData)
│ │ +                    tfSpec = freqanalysis(cfg, dt)
│ │                      tStep = winsize - toi * winsize
│ │                      timeArr = np.arange(tStart, tStop, tStep)
│ │                      assert np.allclose(timeArr, tfSpec.time[0])
│ │  
│ │              # Test window-centroids specified as time-point arrays
│ │ -            if select is not None and "toilim" not in select.keys():
│ │ +            dt = TestMTMConvol.get_tfdata_mtmconvol()
│ │ +            if select is not None and "latency" not in select.keys():
│ │                  cfg.t_ftimwin = 0.05
│ │                  for toi in toiArrs:
│ │                      cfg.toi = toi
│ │ -                    tfSpec = freqanalysis(cfg, self.tfData)
│ │ +                    tfSpec = freqanalysis(cfg, dt)
│ │                      assert np.allclose(cfg.toi, tfSpec.time[0])
│ │                      assert tfSpec.samplerate == 1/(toi[1] - toi[0])
│ │  
│ │                  # Unevenly sampled array: timing currently in lala-land, but sizes must match
│ │                  cfg.toi = [-1, 2, 6]
│ │ -                tfSpec = freqanalysis(cfg, self.tfData)
│ │ +                tfSpec = freqanalysis(cfg, TestMTMConvol.get_tfdata_mtmconvol())
│ │                  assert tfSpec.time[0].size == len(cfg.toi)
│ │  
│ │          # Test correct time-array assembly for ``toi = "all"`` (cut down data signifcantly
│ │          # to not overflow memory here); same for ``toi = 1.0```
│ │          cfg.tapsmofrq = 10
│ │          cfg.keeptapers = True
│ │ -        cfg.select = {"trials": [0], "channel": [0], "toilim": [-0.5, 0.5]}
│ │ +        cfg.select = {"trials": [0], "channel": [0], "latency": [-0.5, 0.5]}
│ │          cfg.toi = "all"
│ │          cfg.t_ftimwin = 0.05
│ │ -        tfSpec = freqanalysis(cfg, self.tfData)
│ │ +        tfSpec = freqanalysis(cfg, TestMTMConvol.get_tfdata_mtmconvol())
│ │          assert tfSpec.taper.size >= 1
│ │          dt = 1 / self.tfData.samplerate
│ │ -        timeArr = np.arange(cfg.select["toilim"][0], cfg.select["toilim"][1] + dt, dt)
│ │ +        timeArr = np.arange(cfg.select["latency"][0], cfg.select["latency"][1] + dt, dt)
│ │          assert np.allclose(tfSpec.time[0], timeArr)
│ │          cfg.toi = 1.0
│ │ -        tfSpec = freqanalysis(cfg, self.tfData)
│ │ +        tfSpec = freqanalysis(cfg, TestMTMConvol.get_tfdata_mtmconvol())
│ │          assert np.allclose(tfSpec.time[0], timeArr)
│ │  
│ │          # Use a window-size larger than the pre-selected interval defined above
│ │          cfg.t_ftimwin = 5.0
│ │          with pytest.raises(SPYValueError) as spyval:
│ │ -            freqanalysis(cfg, self.tfData)
│ │ +            freqanalysis(cfg, TestMTMConvol.get_tfdata_mtmconvol())
│ │              assert "Invalid value of `t_ftimwin`" in str(spyval.value)
│ │          cfg.t_ftimwin = 0.05
│ │  
│ │          # Use `toi` array outside trial boundaries
│ │          cfg.toi = self.tfData.time[0][:10]
│ │ -        with pytest.raises(SPYValueError) as spyval:
│ │ -            freqanalysis(cfg, self.tfData)
│ │ +        with pytest.raises(SPYError) as spyval:
│ │ +            freqanalysis(cfg, TestSuperlet._get_tf_data_superlet())
│ │              errmsg = "Invalid value of `toi`: expected all array elements to be bounded by {} and {}"
│ │ -            assert errmsg.format(*cfg.select["toilim"]) in str(spyval.value)
│ │ +            assert errmsg.format(*cfg.select["latency"]) in str(spyval.value)
│ │  
│ │          # Unsorted `toi` array
│ │          cfg.toi = [0.3, -0.1, 0.2]
│ │ -        with pytest.raises(SPYValueError) as spyval:
│ │ -            freqanalysis(cfg, self.tfData)
│ │ -            assert "Invalid value of `toi`: 'unsorted list/array'" in str(spyval.value)
│ │ +        with pytest.raises(SPYError) as spyval:
│ │ +            freqanalysis(cfg, TestMTMConvol.get_tfdata_mtmconvol())
│ │  
│ │ -    def test_tf_irregular_trials(self, fulltests):
│ │ +    def test_tf_irregular_trials(self):
│ │          # Settings for computing "full" non-overlapping TF-spectrum with DPSS tapers:
│ │          # ensure non-equidistant/overlapping trials are processed (padded) correctly
│ │          # also make sure ``toi = "all"`` works under any circumstance
│ │          cfg = get_defaults(freqanalysis)
│ │          cfg.method = "mtmconvol"
│ │          cfg.tapsmofrq = 2
│ │          cfg.t_ftimwin = 0.3
│ │          cfg.output = "pow"
│ │          cfg.keeptapers = True
│ │  
│ │ -        # Reduce test-data size for quick test runs
│ │ -        if fulltests:
│ │ -            nTrials = 5
│ │ -            nChannels = 8
│ │ -        else:
│ │ -            nTrials = 2
│ │ -            nChannels = 2
│ │ +        nTrials = 2
│ │ +        nChannels = 2
│ │  
│ │          # start harmless: equidistant trials w/multiple tapers
│ │          cfg.toi = 0.0
│ │          # this guy always creates a data set from [-1, ..., 1.9999] seconds
│ │          # no way to change this..
│ │          artdata_len = 3
│ │          artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
│ │ @@ -672,15 +743,15 @@
│ │          for trl_time in tfSpec.time:
│ │              assert np.allclose(artdata_len / cfg.t_ftimwin, trl_time[0].shape)
│ │  
│ │          cfg.toi = "all"
│ │          artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
│ │                                             equidistant=True, inmemory=False)
│ │          # reduce samples, otherwise the the memory usage explodes (nSamples x win_size x nFreq)
│ │ -        rdat = artdata.selectdata(toilim=[0, 0.5])
│ │ +        rdat = artdata.selectdata(latency=[0, 0.5])
│ │          tfSpec = freqanalysis(rdat, **cfg)
│ │          for tk, origTime in enumerate(rdat.time):
│ │              assert np.array_equal(origTime, tfSpec.time[tk])
│ │  
│ │          # non-equidistant trials w/multiple tapers
│ │          cfg.toi = 0.0
│ │          artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
│ │ @@ -688,15 +759,15 @@
│ │          tfSpec = freqanalysis(artdata, **cfg)
│ │          assert tfSpec.taper.size >= 1
│ │          for tk, trl_time in enumerate(tfSpec.time):
│ │              assert np.allclose(np.ceil(artdata.time[tk].size / artdata.samplerate / cfg.t_ftimwin), trl_time.size)
│ │  
│ │          cfg.toi = "all"
│ │          # reduce samples, otherwise the the memory usage explodes (nSamples x win_size x nFreq)
│ │ -        rdat = artdata.selectdata(toilim=[0, 0.5])
│ │ +        rdat = artdata.selectdata(latency=[0, 0.5])
│ │          tfSpec = freqanalysis(rdat, **cfg)
│ │          for tk, origTime in enumerate(rdat.time):
│ │              assert np.array_equal(origTime, tfSpec.time[tk])
│ │  
│ │          # same + reversed dimensional order in input object
│ │          cfg.toi = 0.0
│ │          artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
│ │ @@ -705,60 +776,54 @@
│ │          tfSpec = freqanalysis(artdata, cfg)
│ │          assert tfSpec.taper.size >= 1
│ │          for tk, trl_time in enumerate(tfSpec.time):
│ │              assert np.allclose(np.ceil(artdata.time[tk].size / artdata.samplerate / cfg.t_ftimwin), trl_time.size)
│ │  
│ │          cfg.toi = "all"
│ │          # reduce samples, otherwise the the memory usage explodes (nSamples x win_size x nFreq)
│ │ -        rdat = artdata.selectdata(toilim=[0, 0.5])
│ │ +        rdat = artdata.selectdata(latency=[0, 0.5])
│ │          tfSpec = freqanalysis(rdat, cfg)
│ │  
│ │          # same + overlapping trials
│ │          cfg.toi = 0.0
│ │          artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
│ │                                             equidistant=False, inmemory=False,
│ │                                             dimord=AnalogData._defaultDimord[::-1],
│ │                                             overlapping=True)
│ │          tfSpec = freqanalysis(artdata, cfg)
│ │          assert tfSpec.taper.size >= 1
│ │          for tk, trl_time in enumerate(tfSpec.time):
│ │              assert np.allclose(np.ceil(artdata.time[tk].size / artdata.samplerate / cfg.t_ftimwin), trl_time.size)
│ │  
│ │ -    @skip_without_acme
│ │      @skip_low_mem
│ │ -    def test_tf_parallel(self, testcluster, fulltests):
│ │ +    def test_tf_parallel(self, testcluster):
│ │          # collect all tests of current class and repeat them running concurrently
│ │          client = dd.Client(testcluster)
│ │          quick_tests = [attr for attr in self.__dir__()
│ │                         if (inspect.ismethod(getattr(self, attr)) and attr not in ["test_tf_parallel", "test_tf_cut_selections"])]
│ │          slow_tests = []
│ │          slow_tests.append(quick_tests.pop(quick_tests.index("test_tf_output")))
│ │          slow_tests.append(quick_tests.pop(quick_tests.index("test_tf_irregular_trials")))
│ │          for test in quick_tests:
│ │              getattr(self, test)()
│ │              flush_local_cluster(testcluster)
│ │          for test in slow_tests:
│ │ -            getattr(self, test)(fulltests)
│ │ +            getattr(self, test)()
│ │              flush_local_cluster(testcluster)
│ │  
│ │          # now create uniform `cfg` for remaining SLURM tests
│ │          cfg = StructDict()
│ │          cfg.method = "mtmconvol"
│ │          cfg.taper = "hann"
│ │          cfg.t_ftimwin = 1.0
│ │          cfg.toi = 0
│ │          cfg.output = "pow"
│ │  
│ │ -        # reduce test dataset size unless we're in `--full` mode
│ │ -        if fulltests:
│ │ -            nChannels = self.nChannels
│ │ -            nTrials = self.nTrials
│ │ -        else:
│ │ -            nChannels = 3
│ │ -            nTrials = 2
│ │ +        nChannels = 3
│ │ +        nTrials = 2
│ │  
│ │          # no. of HDF5 files that will make up virtual data-set in case of channel-chunking
│ │          chanPerWrkr = 2
│ │          nFiles = nTrials * (int(nChannels/chanPerWrkr)
│ │                              + int(nChannels % chanPerWrkr > 0))
│ │  
│ │          # simplest case: equidistant trial spacing, all in memory
│ │ @@ -806,36 +871,40 @@
│ │          assert tfSpec.data.shape == (tfSpec.time[0].size, 1, expectedFreqs.size, nChannels)
│ │  
│ │          client.close()
│ │  
│ │  
│ │  class TestWavelet():
│ │  
│ │ -    # Prepare testing signal: ensure `fadeIn` and `fadeOut` are compatible w/`toilim`
│ │ +    # Prepare testing signal: ensure `fadeIn` and `fadeOut` are compatible w/`latency`
│ │      # selection below
│ │      nChannels = 4
│ │      nTrials = 3
│ │      seed = 151120
│ │      fadeIn = -1.5
│ │      fadeOut = 5.5
│ │      tfData, modulators, even, odd, fader = _make_tf_signal(nChannels, nTrials, seed,
│ │                                                             fadeIn=fadeIn, fadeOut=fadeOut)
│ │  
│ │ +    @staticmethod
│ │ +    def get_tfdata_wavelet():
│ │ +        return(_make_tf_signal(TestWavelet.nChannels, TestWavelet.nTrials, TestWavelet.seed,
│ │ +                                                           fadeIn=TestWavelet.fadeIn, fadeOut=TestWavelet.fadeOut)[0])
│ │ +
│ │      # Set up in-place data-selection dicts for the constructed object
│ │      dataSelections = [None,
│ │                        {"trials": [1, 2, 0],
│ │                         "channel": ["channel" + str(i) for i in range(2, 4)][::-1]},
│ │                        {"trials": [0, 2],
│ │                         "channel": range(0, int(nChannels / 2)),
│ │ -                       "toilim": [-2, 6.8]}]
│ │ +                       "latency": [-2, 6.8]}]
│ │  
│ │      # Helper function that reduces dataselections (keep `None` selection no matter what)
│ │ -    def test_wav_cut_selections(self, fulltests):
│ │ -        if not fulltests:
│ │ -            self.dataSelections.pop(random.choice([-1, 1]))
│ │ +    def test_wav_cut_selections(self):
│ │ +        self.dataSelections.pop(random.choice([-1, 1]))
│ │  
│ │      @skip_low_mem
│ │      def test_wav_solution(self):
│ │  
│ │          # Compute TF specturm across entire time-interval (use integer-valued
│ │          # time-points as wavelet centroids)
│ │          cfg = get_defaults(freqanalysis)
│ │ @@ -859,32 +928,32 @@
│ │          for select in self.dataSelections:
│ │  
│ │              # Timing of `tfData` is identical for all trials, so to speed things up,
│ │              # set up `timeArr` here - if `tfData` is modified, these computations have
│ │              # to be moved inside the `enumerate(tfSpec.trials)`-loop!
│ │              timeArr = np.arange(self.tfData.time[0][0], self.tfData.time[0][-1])
│ │              if select:
│ │ -                if "toilim" in select.keys():
│ │ +                if "latency" in select.keys():
│ │                      continue
│ │ -                    timeArr = np.arange(*select["toilim"])
│ │ -                    timeStart = int(select['toilim'][0] * self.tfData.samplerate - self.tfData._t0[0])
│ │ -                    timeStop = int(select['toilim'][1] * self.tfData.samplerate - self.tfData._t0[0])
│ │ +                    timeArr = np.arange(*select["latency"])
│ │ +                    timeStart = int(select['latency'][0] * self.tfData.samplerate - self.tfData._t0[0])
│ │ +                    timeStop = int(select['latency'][1] * self.tfData.samplerate - self.tfData._t0[0])
│ │                      timeSelection = slice(timeStart, timeStop)
│ │              else:
│ │                  timeSelection = np.where(self.fader == 1.0)[0]
│ │              cfg.toi = timeArr
│ │  
│ │              # Compute TF objects w\w/o`foi`/`foilim`
│ │              cfg.select = select
│ │ -            tfSpec = freqanalysis(cfg, self.tfData)
│ │ +            tfSpec = freqanalysis(cfg, TestSuperlet._get_tf_data_superlet())
│ │              cfg.foi = maxFreqs
│ │ -            tfSpecFoi = freqanalysis(cfg, self.tfData)
│ │ +            tfSpecFoi = freqanalysis(cfg, TestWavelet.get_tfdata_wavelet())
│ │              cfg.foi = None
│ │              cfg.foilim = [maxFreqs.min(), maxFreqs.max()]
│ │ -            tfSpecFoiLim = freqanalysis(cfg, self.tfData)
│ │ +            tfSpecFoiLim = freqanalysis(cfg, TestWavelet.get_tfdata_wavelet())
│ │              cfg.foilim = None
│ │  
│ │              # Ensure TF objects contain expected/requested frequencies
│ │              assert 0.2 > tfSpec.freq.min() > 0
│ │              assert tfSpec.freq.max() == (self.tfData.samplerate / 2)
│ │              assert tfSpec.freq.size > 40
│ │              assert np.allclose(tfSpecFoi.freq, maxFreqs)
│ │ @@ -903,15 +972,15 @@
│ │                  assert np.array_equal(tfSpecFoi.time[tk], tfSpecFoiLim.time[tk])
│ │  
│ │                  for chan in range(tfSpec.channel.size):
│ │  
│ │                      # Get reference channel in input object to determine underlying modulator
│ │                      chanNo = chan
│ │                      if select:
│ │ -                        if "toilim" not in select.keys():
│ │ +                        if "latency" not in select.keys():
│ │                              chanNo = np.where(self.tfData.channel == select["channel"][chan])[0][0]
│ │                      if chanNo % 2:
│ │                          modIdx = self.odd[(-1)**trlNo]
│ │                      else:
│ │                          modIdx = self.even[(-1)**trlNo]
│ │                      tfIdx[chanIdx] = chan
│ │                      modulator = self.modulators[timeSelection, modIdx]
│ │ @@ -964,43 +1033,41 @@
│ │          # onset and non-unit spacing
│ │          toiArrs = [np.arange(-2,7),
│ │                     np.arange(-1, 6, 1/self.tfData.samplerate),
│ │                     np.arange(1, 6, 2)]
│ │  
│ │          # Combine `toi`-testing w/in-place data-pre-selection
│ │          for select in self.dataSelections:
│ │ -            if select is not None and "toilim" not in select.keys():
│ │ +            if select is not None and "latency" not in select.keys():
│ │                  cfg.select = select
│ │                  for toi in toiArrs:
│ │                      cfg.toi = toi
│ │                      tfSpec = freqanalysis(cfg, self.tfData)
│ │                      assert np.allclose(cfg.toi, tfSpec.time[0])
│ │                      assert tfSpec.samplerate == 1/(toi[1] - toi[0])
│ │  
│ │          # Test correct time-array assembly for ``toi = "all"`` (cut down data signifcantly
│ │          # to not overflow memory here)
│ │ -        cfg.select = {"trials": [0], "channel": [0], "toilim": [-0.5, 0.5]}
│ │ +        cfg.select = {"trials": [0], "channel": [0], "latency": [-0.5, 0.5]}
│ │          cfg.toi = "all"
│ │ -        tfSpec = freqanalysis(cfg, self.tfData)
│ │ +        tfSpec = freqanalysis(cfg, TestWavelet.get_tfdata_wavelet())
│ │          dt = 1/self.tfData.samplerate
│ │ -        timeArr = np.arange(cfg.select["toilim"][0], cfg.select["toilim"][1] + dt, dt)
│ │ +        timeArr = np.arange(cfg.select["latency"][0], cfg.select["latency"][1] + dt, dt)
│ │          assert np.allclose(tfSpec.time[0], timeArr)
│ │  
│ │          # Use `toi` array outside trial boundaries
│ │          cfg.toi = self.tfData.time[0][:10]
│ │          with pytest.raises(SPYValueError) as spyval:
│ │ -            freqanalysis(cfg, self.tfData)
│ │ -            errmsg = "Invalid value of `toi`: expected all array elements to be bounded by {} and {}"
│ │ -            assert errmsg.format(*cfg.select["toilim"]) in str(spyval.value)
│ │ +            freqanalysis(cfg, TestWavelet.get_tfdata_wavelet())
│ │ +
│ │  
│ │          # Unsorted `toi` array
│ │          cfg.toi = [0.3, -0.1, 0.2]
│ │ -        with pytest.raises(SPYValueError) as spyval:
│ │ -            freqanalysis(cfg, self.tfData)
│ │ -            assert "Invalid value of `toi`: 'unsorted list/array'" in str(spyval.value)
│ │ +        with pytest.raises(SPYValueError):
│ │ +            freqanalysis(cfg, TestSuperlet._get_tf_data_superlet())
│ │  
│ │      def test_wav_irregular_trials(self):
│ │          # Set up wavelet to compute "full" TF spectrum for all time-points
│ │          cfg = get_defaults(freqanalysis)
│ │          cfg.method = "wavelet"
│ │          cfg.wavelet = "Morlet"
│ │          cfg.output = "pow"
│ │ @@ -1033,17 +1100,16 @@
│ │                                             equidistant=False, inmemory=False,
│ │                                             dimord=AnalogData._defaultDimord[::-1],
│ │                                             overlapping=True)
│ │          tfSpec = freqanalysis(cfg)
│ │          for tk, origTime in enumerate(cfg.data.time):
│ │              assert np.array_equal(origTime, tfSpec.time[tk])
│ │  
│ │ -    @skip_without_acme
│ │      @skip_low_mem
│ │ -    def test_wav_parallel(self, testcluster, fulltests):
│ │ +    def test_wav_parallel(self, testcluster):
│ │          # collect all tests of current class and repeat them running concurrently
│ │          client = dd.Client(testcluster)
│ │          all_tests = [attr for attr in self.__dir__()
│ │                       if (inspect.ismethod(getattr(self, attr)) and attr not in ["test_wav_parallel", "test_wav_cut_selections"])]
│ │          for test in all_tests:
│ │              getattr(self, test)()
│ │              flush_local_cluster(testcluster)
│ │ @@ -1051,21 +1117,16 @@
│ │          # now create uniform `cfg` for remaining SLURM tests
│ │          cfg = StructDict()
│ │          cfg.method = "wavelet"
│ │          cfg.wavelet = "Morlet"
│ │          cfg.output = "pow"
│ │          cfg.toi = "all"
│ │  
│ │ -        # reduce test dataset size unless we're in `--full` mode
│ │ -        if fulltests:
│ │ -            nChannels = self.nChannels
│ │ -            nTrials = self.nTrials
│ │ -        else:
│ │ -            nChannels = 3
│ │ -            nTrials = 2
│ │ +        nChannels = 3
│ │ +        nTrials = 2
│ │  
│ │          # no. of HDF5 files that will make up virtual data-set in case of channel-chunking
│ │          chanPerWrkr = 2
│ │          nFiles = nTrials * (int(nChannels/chanPerWrkr) + int(nChannels % chanPerWrkr > 0))
│ │  
│ │          # simplest case: equidistant trial spacing, all in memory
│ │          fileCount = [nTrials, nFiles]
│ │ @@ -1104,18 +1165,23 @@
│ │                                             overlapping=True)
│ │          tfSpec = freqanalysis(artdata, cfg)
│ │          assert np.allclose(tfSpec.freq, expectedFreqs)
│ │          assert tfSpec.data.shape == (tfSpec.time[0].size, 1, expectedFreqs.size, nChannels)
│ │  
│ │          client.close()
│ │  
│ │ -
│ │  class TestSuperlet():
│ │  
│ │ -    # Prepare testing signal: ensure `fadeIn` and `fadeOut` are compatible w/`toilim`
│ │ +    @staticmethod
│ │ +    def _get_tf_data_superlet():
│ │ +        return _make_tf_signal(TestSuperlet.nChannels, TestSuperlet.nTrials, TestSuperlet.seed,
│ │ +                                                           fadeIn=TestSuperlet.fadeIn, fadeOut=TestSuperlet.fadeOut)[0]
│ │ +
│ │ +
│ │ +    # Prepare testing signal: ensure `fadeIn` and `fadeOut` are compatible w/`latency`
│ │      # selection below
│ │      nChannels = 4
│ │      nTrials = 3
│ │      seed = 151120
│ │      fadeIn = -9.5
│ │      fadeOut = 50.5
│ │      tfData, modulators, even, odd, fader = _make_tf_signal(nChannels, nTrials, seed,
│ │ @@ -1123,23 +1189,22 @@
│ │  
│ │      # Set up in-place data-selection dicts for the constructed object
│ │      dataSelections = [None,
│ │                        {"trials": [1, 2, 0],
│ │                         "channel": ["channel" + str(i) for i in range(2, 4)][::-1]},
│ │                        {"trials": [0, 2],
│ │                         "channel": range(0, int(nChannels / 2)),
│ │ -                       "toilim": [-2, 6.8]}]
│ │ +                       "latency": [-2, 6.8]}]
│ │  
│ │      # Helper function that reduces dataselections (keep `None` selection no matter what)
│ │ -    def test_slet_cut_selections(self, fulltests):
│ │ -        if not fulltests:
│ │ -            self.dataSelections.pop(random.choice([-1, 1]))
│ │ +    def test_slet_cut_selections(self):
│ │ +        self.dataSelections.pop(random.choice([-1, 1]))
│ │  
│ │      @skip_low_mem
│ │ -    def test_slet_solution(self, fulltests):
│ │ +    def test_slet_solution(self):
│ │  
│ │          # Compute TF specturm across entire time-interval (use integer-valued
│ │          # time-points as wavelet centroids)
│ │          cfg = get_defaults(freqanalysis)
│ │          cfg.method = "superlet"
│ │          cfg.order_max = 2
│ │          cfg.output = "pow"
│ │ @@ -1156,63 +1221,62 @@
│ │          for select in self.dataSelections:
│ │  
│ │              # Timing of `tfData` is identical for all trials, so to speed things up,
│ │              # set up `timeArr` here - if `tfData` is modified, these computations have
│ │              # to be moved inside the `enumerate(tfSpec.trials)`-loop!
│ │              timeArr = np.arange(self.tfData.time[0][0], self.tfData.time[0][-1])
│ │              if select:
│ │ -                if "toilim" in select.keys():
│ │ -                    timeArr = np.arange(*select["toilim"])
│ │ -                    timeStart = int(select['toilim'][0] * self.tfData.samplerate - self.tfData._t0[0])
│ │ -                    timeStop = int(select['toilim'][1] * self.tfData.samplerate - self.tfData._t0[0])
│ │ +                if "latency" in select.keys():
│ │ +                    timeArr = np.arange(*select["latency"])
│ │ +                    timeStart = int(select['latency'][0] * self.tfData.samplerate - self.tfData._t0[0])
│ │ +                    timeStop = int(select['latency'][1] * self.tfData.samplerate - self.tfData._t0[0])
│ │                      timeSelection = slice(timeStart, timeStop)
│ │              else:
│ │                  timeSelection = np.where(self.fader == 1.0)[0]
│ │              cfg.toi = timeArr
│ │  
│ │              # Skip below tests if `toi` and an in-place time-selection clash
│ │ -            if select is not None and "toilim" in select.keys():
│ │ +            if select is not None and "latency" in select.keys():
│ │                  continue
│ │  
│ │              # Compute TF objects w\w/o`foi`/`foilim`
│ │              cfg.select = select
│ │              cfg.foi = maxFreqs
│ │              tfSpecFoi = freqanalysis(cfg, self.tfData)
│ │              cfg.foi = None
│ │              assert np.allclose(tfSpecFoi.freq, maxFreqs)
│ │              cfg.foilim = [maxFreqs.min(), maxFreqs.max()]
│ │              tfSpecFoiLim = freqanalysis(cfg, self.tfData)
│ │              cfg.foilim = None
│ │              assert np.allclose(tfSpecFoiLim.freq, foilimFreqs)
│ │ -            if fulltests:
│ │ -                tfSpec = freqanalysis(cfg, self.tfData)
│ │ -                assert 0.02 > tfSpec.freq.min() > 0
│ │ -                assert tfSpec.freq.max() == (self.tfData.samplerate / 2)
│ │ -                assert tfSpec.freq.size > 50
│ │ +
│ │ +            tfSpec = freqanalysis(cfg, self.tfData)
│ │ +            assert 0.02 > tfSpec.freq.min() > 0
│ │ +            assert tfSpec.freq.max() == (self.tfData.samplerate / 2)
│ │ +            assert tfSpec.freq.size > 50
│ │  
│ │              for tk, _ in enumerate(tfSpecFoi.trials):
│ │  
│ │                  # Get reference trial-number in input object
│ │                  trlNo = tk
│ │                  if select:
│ │                      trlNo = select["trials"][tk]
│ │  
│ │                  # Ensure timing array was computed correctly and independent of `foi`/`foilim`
│ │                  assert np.array_equal(timeArr, tfSpecFoi.time[tk])
│ │                  assert np.array_equal(tfSpecFoi.time[tk], tfSpecFoiLim.time[tk])
│ │ -                if fulltests:
│ │ -                    assert np.array_equal(timeArr, tfSpec.time[tk])
│ │ -                    assert np.array_equal(tfSpec.time[tk], tfSpecFoi.time[tk])
│ │ +                assert np.array_equal(timeArr, tfSpec.time[tk])
│ │ +                assert np.array_equal(tfSpec.time[tk], tfSpecFoi.time[tk])
│ │  
│ │                  for chan in range(tfSpecFoi.channel.size):
│ │  
│ │                      # Get reference channel in input object to determine underlying modulator
│ │                      chanNo = chan
│ │                      if select:
│ │ -                        if "toilim" not in select.keys():
│ │ +                        if "latency" not in select.keys():
│ │                              chanNo = np.where(self.tfData.channel == select["channel"][chan])[0][0]
│ │                      if chanNo % 2:
│ │                          modIdx = self.odd[(-1)**trlNo]
│ │                      else:
│ │                          modIdx = self.even[(-1)**trlNo]
│ │                      tfIdx[chanIdx] = chan
│ │                      modulator = self.modulators[timeSelection, modIdx]
│ │ @@ -1236,79 +1300,72 @@
│ │                                  height = 0.9 * ZxxThresh
│ │                                  peaks, _ = scisig.find_peaks(peakProfile, prominence=0.75*height, height=height, distance=5)
│ │                              assert np.abs(peaks.size - modCounts[fk]) <= 2
│ │  
│ │                      # Be more lenient w/`tfSpec`: don't scan for min/max freq, but all peaks at once
│ │                      # (auto-scale resolution potentially too coarse to differentiate b/w min/max);
│ │  
│ │ -    def test_slet_toi(self, fulltests):
│ │ +    def test_slet_toi(self):
│ │          # Don't keep trials to speed things up a bit
│ │          cfg = get_defaults(freqanalysis)
│ │          cfg.method = "superlet"
│ │          cfg.order_max = 2
│ │          cfg.output = "pow"
│ │          cfg.keeptrials = False
│ │  
│ │          # Test time-point arrays comprising onset, purely pre-onset, purely after
│ │          # onset and non-unit spacing
│ │          toiArrs = [np.arange(-2,7),
│ │                     np.arange(-1, 6, 1/self.tfData.samplerate),
│ │                     np.arange(1, 6, 2)]
│ │  
│ │ -        # Just pick one `toi` at random for quickly running tests
│ │ -        if not fulltests:
│ │ -            toiArrs = [random.choice(toiArrs)]
│ │ +        toiArrs = [random.choice(toiArrs)]
│ │  
│ │          # Combine `toi`-testing w/in-place data-pre-selection
│ │          for select in self.dataSelections:
│ │ -            if select is not None and "toilim" not in select.keys():
│ │ +            if select is not None and "latency" not in select.keys():
│ │                  cfg.select = select
│ │                  for toi in toiArrs:
│ │                      cfg.toi = toi
│ │                      tfSpec = freqanalysis(cfg, self.tfData)
│ │                      assert np.allclose(cfg.toi, tfSpec.time[0])
│ │                      assert tfSpec.samplerate == 1/(toi[1] - toi[0])
│ │  
│ │          # Test correct time-array assembly for ``toi = "all"`` (cut down data signifcantly
│ │          # to not overflow memory here)
│ │ -        cfg.select = {"trials": [0], "channel": [0], "toilim": [-0.5, 0.5]}
│ │ +        cfg.select = {"trials": [0], "channel": [0], "latency": [-0.5, 0.5]}
│ │          cfg.toi = "all"
│ │          tfSpec = freqanalysis(cfg, self.tfData)
│ │          dt = 1/self.tfData.samplerate
│ │ -        timeArr = np.arange(cfg.select["toilim"][0], cfg.select["toilim"][1] + dt, dt)
│ │ +        timeArr = np.arange(cfg.select["latency"][0], cfg.select["latency"][1] + dt, dt)
│ │          assert np.allclose(tfSpec.time[0], timeArr)
│ │  
│ │          # Use `toi` array outside trial boundaries
│ │          cfg.toi = self.tfData.time[0][:10]
│ │          with pytest.raises(SPYValueError) as spyval:
│ │ -            freqanalysis(cfg, self.tfData)
│ │ +            freqanalysis(cfg, TestSuperlet._get_tf_data_superlet())
│ │              errmsg = "Invalid value of `toi`: expected all array elements to be bounded by {} and {}"
│ │ -            assert errmsg.format(*cfg.select["toilim"]) in str(spyval.value)
│ │ +            assert errmsg.format(*cfg.select["latency"]) in str(spyval.value)
│ │  
│ │          # Unsorted `toi` array
│ │          cfg.toi = [0.3, -0.1, 0.2]
│ │          with pytest.raises(SPYValueError) as spyval:
│ │ -            freqanalysis(cfg, self.tfData)
│ │ +            freqanalysis(cfg, TestSuperlet._get_tf_data_superlet())
│ │              assert "Invalid value of `toi`: 'unsorted list/array'" in str(spyval.value)
│ │  
│ │ -    def test_slet_irregular_trials(self, fulltests):
│ │ +    def test_slet_irregular_trials(self):
│ │          # Set up wavelet to compute "full" TF spectrum for all time-points
│ │          cfg = get_defaults(freqanalysis)
│ │          cfg.method = "superlet"
│ │          cfg.order_max = 2
│ │          cfg.output = "pow"
│ │          cfg.toi = "all"
│ │  
│ │ -        # Reduce test-data size for quick test runs
│ │ -        if fulltests:
│ │ -            nTrials = 5
│ │ -            nChannels = 16
│ │ -        else:
│ │ -            nTrials = 2
│ │ -            nChannels = 2
│ │ +        nTrials = 2
│ │ +        nChannels = 2
│ │  
│ │          # start harmless: equidistant trials w/multiple tapers
│ │          artdata = generate_artificial_data(nTrials=nTrials, nChannels=nChannels,
│ │                                             equidistant=True, inmemory=False)
│ │          tfSpec = freqanalysis(artdata, **cfg)
│ │          for tk, origTime in enumerate(artdata.time):
│ │              assert np.array_equal(origTime, tfSpec.time[tk])
│ │ @@ -1333,23 +1390,22 @@
│ │                                              equidistant=False, inmemory=False,
│ │                                              dimord=AnalogData._defaultDimord[::-1],
│ │                                              overlapping=True)
│ │          tfSpec = freqanalysis(cfg)
│ │          for tk, origTime in enumerate(cfg.data.time):
│ │              assert np.array_equal(origTime, tfSpec.time[tk])
│ │  
│ │ -    @skip_without_acme
│ │      @skip_low_mem
│ │ -    def test_slet_parallel(self, testcluster, fulltests):
│ │ +    def test_slet_parallel(self, testcluster):
│ │          # collect all tests of current class and repeat them running concurrently
│ │          client = dd.Client(testcluster)
│ │          all_tests = [attr for attr in self.__dir__()
│ │                       if (inspect.ismethod(getattr(self, attr)) and attr not in ["test_slet_parallel", "test_cut_slet_selections"])]
│ │          for test in all_tests:
│ │ -            getattr(self, test)(fulltests)
│ │ +            getattr(self, test)()
│ │              flush_local_cluster(testcluster)
│ │  
│ │          # now create uniform `cfg` for remaining SLURM tests
│ │          cfg = StructDict()
│ │          cfg.method = "superlet"
│ │          cfg.order_max = 2
│ │          cfg.output = "pow"
│ │ @@ -1400,7 +1456,8 @@
│ │          assert tfSpec.data.shape == (tfSpec.time[0].size, 1, expectedFreqs.size, self.nChannels)
│ │  
│ │          client.close()
│ │  
│ │  
│ │  if __name__ == '__main__':
│ │      T1 = TestMTMConvol()
│ │ +    T2 = TestMTMFFT()
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_specest_fooof.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_specest_fooof.py
│ │┄ Files 22% similar despite different names
│ │ @@ -2,29 +2,23 @@
│ │  #
│ │  # Test FOOOF integration from user/frontend perspective.
│ │  
│ │  
│ │  import pytest
│ │  import numpy as np
│ │  import inspect
│ │ -import os
│ │  import matplotlib.pyplot as plt
│ │ +import dask.distributed as dd
│ │  
│ │  # Local imports
│ │  from syncopy import freqanalysis
│ │  from syncopy.shared.tools import get_defaults
│ │  from syncopy.shared.errors import SPYValueError
│ │ -from syncopy.tests.synth_data import AR2_network, phase_diffusion
│ │ +from syncopy.tests.test_metadata import _get_fooof_signal
│ │  import syncopy as spy
│ │ -from syncopy import __acme__
│ │ -if __acme__:
│ │ -    import dask.distributed as dd
│ │ -
│ │ -# Decorator to decide whether or not to run dask-related tests
│ │ -skip_without_acme = pytest.mark.skipif(not __acme__, reason="acme not available")
│ │  
│ │  
│ │  def _plot_powerspec_linear(freqs, powers, title="Power spectrum"):
│ │      """Simple, internal plotting function to plot x versus y. Uses linear scale.
│ │  
│ │      Parameters
│ │      ----------
│ │ @@ -77,41 +71,23 @@
│ │          raise ValueError("Parameter 'dt' must be a syncopy.datatype instance.")
│ │      fig, ax = dt.singlepanelplot()
│ │      if title is not None:
│ │          ax.set_title(title)
│ │      return fig, ax
│ │  
│ │  
│ │ -def _get_fooof_signal(nTrials=100):
│ │ -    """
│ │ -    Produce suitable test signal for fooof, with peaks at 30 and 50 Hz.
│ │ -
│ │ -    Note: One must perform trial averaging during the FFT to get realistic
│ │ -    data out of it (and reduce noise). Then work with the averaged data.
│ │ -
│ │ -    Returns AnalogData instance.
│ │ -    """
│ │ -    nSamples = 1000
│ │ -    nChannels = 1
│ │ -    samplerate = 1000
│ │ -    ar1_part = AR2_network(AdjMat=np.zeros(1), nSamples=nSamples, alphas=[0.9, 0], nTrials=nTrials)
│ │ -    pd1 = phase_diffusion(freq=30., eps=.1, fs=samplerate, nChannels=nChannels, nSamples=nSamples, nTrials=nTrials)
│ │ -    pd2 = phase_diffusion(freq=50., eps=.1, fs=samplerate, nChannels=nChannels, nSamples=nSamples, nTrials=nTrials)
│ │ -    signal = ar1_part + .8 * pd1 + 0.6 * pd2
│ │ -    return signal
│ │ -
│ │ -
│ │  class TestFooofSpy():
│ │      """
│ │      Test the frontend (user API) for running FOOOF. FOOOF is a post-processing of an FFT, and
│ │      to request the post-processing, the user sets the method to "mtmfft", and the output to
│ │      one of the available FOOOF output types.
│ │      """
│ │  
│ │ -    tfData = _get_fooof_signal()
│ │ +    seed = 42
│ │ +    tfData = _get_fooof_signal(seed=seed)
│ │  
│ │      @staticmethod
│ │      def get_fooof_cfg():
│ │          cfg = get_defaults(freqanalysis)
│ │          cfg.method = "mtmfft"
│ │          cfg.taper = "hann"
│ │          cfg.select = {"channel": 0}
│ │ @@ -127,28 +103,28 @@
│ │              consider it an error to pass an input frequency axis that contains the zero, and throw an
│ │              error in the frontend to stop before any expensive computations happen. This test checks for
│ │              that error.
│ │          """
│ │          cfg = TestFooofSpy.get_fooof_cfg()
│ │          cfg['foilim'] = [0., 100.]    # Include the zero in tfData.
│ │          with pytest.raises(SPYValueError) as err:
│ │ -            _ = freqanalysis(cfg, self.tfData)  # tfData contains zero.
│ │ +            _ = freqanalysis(cfg, _get_fooof_signal(seed=self.seed))  # tfData contains zero.
│ │          assert "a frequency range that does not include zero" in str(err.value)
│ │  
│ │ -    def test_output_fooof_works_with_freq_zero_in_data_after_setting_foilim(self):
│ │ +    def test_output_fooof_works_with_freq_zero_in_data_after_setting_frequency(self):
│ │          """
│ │          This tests the intended operation with output type 'fooof': with an input that does not
│ │ -        include zero, ensured by using the 'foilim' argument/setting when calling freqanalysis.
│ │ +        include zero, ensured by using the 'frequency' argument/setting when calling freqanalysis.
│ │  
│ │          This returns the full, fooofed spectrum.
│ │          """
│ │          cfg = TestFooofSpy.get_fooof_cfg()
│ │          cfg.pop('fooof_opt', None)
│ │ -        fooof_opt = {'peak_width_limits': (1.0, 12.0)}  # Increase lower limit to avoid foooof warning.
│ │ -        spec_dt = freqanalysis(cfg, self.tfData, fooof_opt=fooof_opt)
│ │ +        fooof_opt = {'peak_width_limits': (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
│ │ +        spec_dt = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
│ │  
│ │          # check frequency axis
│ │          assert spec_dt.freq.size == 100
│ │          assert spec_dt.freq[0] == 1
│ │          assert spec_dt.freq[99] == 100.
│ │  
│ │          # check the log
│ │ @@ -166,25 +142,22 @@
│ │          assert spec_dt.cfg['freqanalysis']['output'] == 'fooof'
│ │  
│ │          # Plot it.
│ │          # _plot_powerspec_linear(freqs=spec_dt.freq, powers=spec_dt.data[0, 0, :, 0], title="fooof full model, for ar1 data (linear scale)")
│ │          # spp(spec_dt, "FOOOF full model")
│ │          # plt.savefig("spp.png")
│ │  
│ │ -    def test_output_fooof_aperiodic(self, show_data=False):
│ │ +    def test_output_fooof_aperiodic(self):
│ │          """Test fooof with output type 'fooof_aperiodic'. A spectrum containing only the aperiodic part is returned."""
│ │  
│ │ -        if show_data:
│ │ -            _show_spec_log(self.tfData)
│ │ -
│ │          cfg = TestFooofSpy.get_fooof_cfg()
│ │          cfg.output = "fooof_aperiodic"
│ │          cfg.pop('fooof_opt', None)
│ │ -        fooof_opt = {'peak_width_limits': (1.0, 12.0)}  # Increase lower limit to avoid foooof warning.
│ │ -        spec_dt = freqanalysis(cfg, self.tfData, fooof_opt=fooof_opt)
│ │ +        fooof_opt = {'peak_width_limits': (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
│ │ +        spec_dt = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
│ │  
│ │          # log
│ │          assert "fooof" in spec_dt._log  # from the method
│ │          assert "fooof_method = fooof_aperiodic" in spec_dt._log
│ │          assert "fooof_peaks" not in spec_dt._log
│ │  
│ │          # check the data
│ │ @@ -193,71 +166,61 @@
│ │          assert not np.isnan(spec_dt.data).any()
│ │  
│ │      def test_output_fooof_peaks(self):
│ │          """Test fooof with output type 'fooof_peaks'. A spectrum containing only the peaks (actually, the Gaussians fit to the peaks) is returned."""
│ │          cfg = TestFooofSpy.get_fooof_cfg()
│ │          cfg.output = "fooof_peaks"
│ │          cfg.pop('fooof_opt', None)
│ │ -        fooof_opt = {'peak_width_limits': (1.0, 12.0)}  # Increase lower limit to avoid foooof warning.
│ │ -        spec_dt = freqanalysis(cfg, self.tfData, fooof_opt=fooof_opt)
│ │ +        fooof_opt = {'peak_width_limits': (1.0, 12.0)}  # Increase lower limit to avoid fooof warning.
│ │ +        spec_dt = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
│ │          assert spec_dt.data.ndim == 4
│ │          assert "fooof" in spec_dt._log
│ │          assert "fooof_method = fooof_peaks" in spec_dt._log
│ │          assert "fooof_aperiodic" not in spec_dt._log
│ │ -        # _plot_powerspec_linear(freqs=spec_dt.freq, powers=np.ravel(spec_dt.data), title="fooof peaks, for ar1 data (linear scale)")
│ │ -        # spp(spec_dt, "FOOOF peaks")
│ │  
│ │      def test_different_fooof_outputs_are_consistent(self):
│ │          """Test fooof with all output types plotted into a single plot and ensure consistent output."""
│ │          cfg = TestFooofSpy.get_fooof_cfg()
│ │          cfg['output'] = "pow"
│ │          cfg['foilim'] = [10, 70]
│ │          cfg.pop('fooof_opt', None)
│ │          fooof_opt = {'peak_width_limits': (6.0, 12.0),
│ │ -                     'min_peak_height': 0.2}  # Increase lower limit to avoid foooof warning.
│ │ +                     'min_peak_height': 0.2}  # Increase lower limit to avoid fooof warning.
│ │  
│ │ -        out_fft = freqanalysis(cfg, self.tfData)
│ │ +        out_fft = freqanalysis(cfg, _get_fooof_signal(seed=self.seed))
│ │          cfg['output'] = "fooof"
│ │ -        out_fooof = freqanalysis(cfg, self.tfData, fooof_opt=fooof_opt)
│ │ +        out_fooof = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
│ │          cfg['output'] = "fooof_aperiodic"
│ │ -        out_fooof_aperiodic = freqanalysis(cfg, self.tfData, fooof_opt=fooof_opt)
│ │ +        out_fooof_aperiodic = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
│ │          cfg['output'] = "fooof_peaks"
│ │ -        out_fooof_peaks = freqanalysis(cfg, self.tfData, fooof_opt=fooof_opt)
│ │ +        out_fooof_peaks = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
│ │  
│ │          assert (out_fooof.freq == out_fooof_aperiodic.freq).all()
│ │          assert (out_fooof.freq == out_fooof_peaks.freq).all()
│ │  
│ │ -        freqs = out_fooof.freq
│ │ -
│ │          assert out_fooof.data.shape == out_fooof_aperiodic.data.shape
│ │          assert out_fooof.data.shape == out_fooof_peaks.data.shape
│ │  
│ │          # biggest peak is at 30Hz
│ │          f1_ind = out_fooof_peaks.show(channel=0).argmax()
│ │          assert 27 < out_fooof_peaks.freq[f1_ind] < 33
│ │  
│ │          plot_data = {"Raw input data": np.ravel(out_fft.data), "Fooofed spectrum": np.ravel(out_fooof.data), "Fooof aperiodic fit": np.ravel(out_fooof_aperiodic.data), "Fooof peaks fit": np.ravel(out_fooof_peaks.data)}
│ │ -        _plot_powerspec_linear(freqs, powers=plot_data, title="Outputs from different fooof methods for ar1 data (linear scale)")
│ │ +        #_plot_powerspec_linear(out_fooof.freq, powers=plot_data, title="Outputs from different fooof methods for ar1 data (linear scale)")
│ │  
│ │      def test_frontend_settings_are_merged_with_defaults_used_in_backend(self):
│ │          cfg = TestFooofSpy.get_fooof_cfg()
│ │          cfg.output = "fooof_peaks"
│ │          cfg.pop('fooof_opt', None)
│ │          fooof_opt = {'max_n_peaks': 8, 'peak_width_limits': (1.0, 12.0)}
│ │ -        spec_dt = freqanalysis(cfg, self.tfData, fooof_opt=fooof_opt)
│ │ +        spec_dt = freqanalysis(cfg, _get_fooof_signal(seed=self.seed), fooof_opt=fooof_opt)
│ │  
│ │          assert spec_dt.data.ndim == 4
│ │  
│ │ -        # TODO later: test whether the settings returned as 2nd return value include
│ │ -        #  our custom value for fooof_opt['max_n_peaks']. Not possible yet on
│ │ -        #  this level as we have no way to get the 'details' return value.
│ │ -        #  This is verified in backend tests though.
│ │ -
│ │ -    @skip_without_acme
│ │ -    def test_parallel(self, testcluster=None):
│ │ +    def test_parallel(self, testcluster):
│ │  
│ │          plt.ioff()
│ │          client = dd.Client(testcluster)
│ │          all_tests = [attr for attr in self.__dir__()
│ │                       if (inspect.ismethod(getattr(self, attr)) and 'parallel' not in attr)]
│ │  
│ │          for test in all_tests:
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_spyio.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_spyio.py
│ │┄ Files 6% similar despite different names
│ │ @@ -8,32 +8,36 @@
│ │  import tempfile
│ │  import shutil
│ │  import h5py
│ │  import time
│ │  import pytest
│ │  import numpy as np
│ │  from glob import glob
│ │ +import matplotlib.pyplot as ppl
│ │  
│ │  # Local imports
│ │ +import syncopy as spy
│ │  from syncopy.datatype import AnalogData
│ │ -from syncopy.io import save, load, load_ft_raw, load_tdt
│ │ +from syncopy.io import save, load, load_ft_raw, load_tdt, load_nwb
│ │  from syncopy.shared.filetypes import FILE_EXT
│ │  from syncopy.shared.errors import (
│ │      SPYValueError,
│ │      SPYIOError,
│ │      SPYError,
│ │      SPYTypeError
│ │  )
│ │  import syncopy.datatype as swd
│ │  from syncopy.tests.misc import generate_artificial_data
│ │  
│ │  
│ │ +
│ │  # Decorator to detect if test data dir is available
│ │  on_esi = os.path.isdir('/cs/slurm/syncopy')
│ │  skip_no_esi = pytest.mark.skipif(not on_esi, reason="ESI fs not available")
│ │ +skip_no_nwb = pytest.mark.skipif(not spy.__nwb__, reason="pynwb not installed")
│ │  
│ │  
│ │  class TestSpyIO():
│ │  
│ │      # Allocate test-datasets for AnalogData, SpectralData, SpikeData and EventData objects
│ │      nc = 10
│ │      ns = 30
│ │ @@ -58,20 +62,20 @@
│ │      data["CrossSpectralData"] = np.arange(1, nc * nc * ns * nf + 1).reshape(ns, nf, nc, nc)
│ │      trl["CrossSpectralData"] = trl["AnalogData"]
│ │  
│ │      # Use a fixed random number generator seed to simulate a 2D SpikeData array
│ │      seed = np.random.RandomState(13)
│ │      data["SpikeData"] = np.vstack([seed.choice(ns, size=nd),
│ │                                     seed.choice(nc, size=nd),
│ │ -                                   seed.choice(int(nc / 2), size=nd)]).T
│ │ +                                   seed.choice(int(nc / 2), size=nd)]).T.astype(int)
│ │      trl["SpikeData"] = trl["AnalogData"]
│ │  
│ │      # Generate bogus trigger timings
│ │      data["EventData"] = np.vstack([np.arange(0, ns, 5),
│ │ -                                   np.zeros((int(ns / 5), ))]).T
│ │ +                                   np.zeros((int(ns / 5), ))]).T.astype(int)
│ │      data["EventData"][1::2, 1] = 1
│ │      trl["EventData"] = trl["AnalogData"]
│ │  
│ │      # Define data classes to be used in tests below
│ │      classes = ["AnalogData", "SpectralData", "CrossSpectralData", "SpikeData", "EventData"]
│ │  
│ │      # Test correct handling of object log
│ │ @@ -561,10 +565,56 @@
│ │          with pytest.raises(SPYValueError, match='Invalid value of `start_code`'):
│ │              load_tdt(self.tdt_dir, start_code=999999, end_code=self.end_code)
│ │  
│ │          with pytest.raises(SPYValueError, match='Invalid value of `end_code`'):
│ │              load_tdt(self.tdt_dir, start_code=self.start_code, end_code=999999)
│ │  
│ │  
│ │ +@skip_no_esi
│ │ +@skip_no_nwb
│ │ +class TestNWBImporter:
│ │ +
│ │ +    nwb_filename = '/cs/slurm/syncopy/NWBdata/test.nwb'
│ │ +
│ │ +    def test_load_nwb(self):
│ │ +
│ │ +        spy_filename = self.nwb_filename.split('/')[-1][:-4] + '.spy'
│ │ +        out = load_nwb(self.nwb_filename, memuse=2000)
│ │ +        edata, adata1, adata2 = list(out.values())
│ │ +
│ │ +        assert isinstance(adata2, spy.AnalogData)
│ │ +        assert isinstance(edata, spy.EventData)
│ │ +        assert np.any(~np.isnan(adata2.data))
│ │ +        assert np.any(adata2.data != 0)
│ │ +
│ │ +        snippet = adata2.selectdata(latency=[30, 32])
│ │ +
│ │ +        snippet.singlepanelplot(latency=[30, 30.3], channel=3)
│ │ +        ppl.gcf().suptitle('raw data')
│ │ +
│ │ +        # Bandpass filter
│ │ +        lfp = spy.preprocessing(snippet, filter_class='but', freq=[10, 100],
│ │ +                                filter_type='bp', order=8)
│ │ +
│ │ +        # Downsample
│ │ +        lfp = spy.resampledata(lfp, resamplefs=2000, method='downsample')
│ │ +        lfp.info = adata2.info
│ │ +        lfp.singlepanelplot(channel=3)
│ │ +        ppl.gcf().suptitle('bp-filtered 10-100Hz and resampled')
│ │ +
│ │ +        spec = spy.freqanalysis(lfp, foilim=[5, 150])
│ │ +        spec.singlepanelplot(channel=[1, 3])
│ │ +        ppl.gcf().suptitle('bp-filtered 10-100Hz and resampled')
│ │ +
│ │ +        # test save and load
│ │ +        with tempfile.TemporaryDirectory() as tdir:
│ │ +            lfp.save(os.path.join(tdir, spy_filename))
│ │ +            lfp2 = spy.load(os.path.join(tdir, spy_filename))
│ │ +
│ │ +            assert np.allclose(lfp.data, lfp2.data)
│ │ +
│ │ +
│ │  if __name__ == '__main__':
│ │ +    T0 = TestSpyIO()
│ │      T1 = TestFTImporter()
│ │      T2 = TestTDTImporter()
│ │ +    T3 = TestNWBImporter()
│ │   --- esi-syncopy-2022.8/syncopy/tests/test_spytools.py
│ ├── +++ esi_syncopy-2023.3/syncopy/tests/test_spytools.py
│ │┄ Files identical despite different names
│ │   --- esi-syncopy-2022.8/setup.py
│ ├── +++ esi_syncopy-2023.3/PKG-INFO
│ │┄ Files 26% similar despite different names
│ │ @@ -1,54 +1,112 @@
│ │ -# -*- coding: utf-8 -*-
│ │ -from setuptools import setup
│ │ +Metadata-Version: 2.1
│ │ +Name: esi-syncopy
│ │ +Version: 2023.3
│ │ +Summary: A toolkit for user-friendly large-scale electrophysiology data analysis. Syncopy is compatible with the Matlab toolbox FieldTrip.
│ │ +Home-page: https://syncopy.org
│ │ +License: BSD-3-Clause
│ │ +Author: Stefan Fürtinger
│ │ +Author-email: sfuerti@esi-frankfurt.de
│ │ +Requires-Python: >=3.8,<4.0
│ │ +Classifier: Environment :: Console
│ │ +Classifier: Framework :: Jupyter
│ │ +Classifier: License :: OSI Approved :: BSD License
│ │ +Classifier: Operating System :: OS Independent
│ │ +Classifier: Programming Language :: Python :: 3
│ │ +Classifier: Programming Language :: Python :: 3.8
│ │ +Classifier: Programming Language :: Python :: 3.9
│ │ +Classifier: Programming Language :: Python :: 3.10
│ │ +Classifier: Programming Language :: Python :: 3.11
│ │ +Classifier: Topic :: Scientific/Engineering
│ │ +Requires-Dist: dask-jobqueue (>=0.8)
│ │ +Requires-Dist: dask[distributed] (>=2022.6)
│ │ +Requires-Dist: fooof (>=1.0)
│ │ +Requires-Dist: h5py (>=2.9)
│ │ +Requires-Dist: matplotlib (>=3.5)
│ │ +Requires-Dist: natsort (>=8.1.0,<9.0.0)
│ │ +Requires-Dist: numpy (>=1.10)
│ │ +Requires-Dist: psutil (>=5.9)
│ │ +Requires-Dist: scipy (>=1.5)
│ │ +Requires-Dist: tqdm (>=4.31)
│ │ +Project-URL: Repository, https://github.com/esi-neuroscience/syncopy
│ │ +Description-Content-Type: text/x-rst
│ │ +
│ │ +.. image:: https://raw.githubusercontent.com/esi-neuroscience/syncopy/master/doc/source/_static/syncopy_logo_small.png
│ │ +	   :alt: Syncopy-Logo
│ │ +
│ │ +Systems Neuroscience Computing in Python
│ │ +========================================
│ │ +
│ │ +
│ │ +|Conda Version| |PyPi Version| |License|
│ │ +
│ │ +.. |Conda Version| image:: https://img.shields.io/conda/vn/conda-forge/esi-syncopy.svg
│ │ +   :target: https://anaconda.org/conda-forge/esi-syncopy
│ │ +.. |PyPI version| image:: https://badge.fury.io/py/esi-syncopy.svg
│ │ +   :target: https://badge.fury.io/py/esi-syncopy
│ │ +.. |License| image:: https://img.shields.io/github/license/esi-neuroscience/syncopy
│ │ +
│ │ +|Master Tests| |Master Coverage|
│ │ +
│ │ +.. |Master Tests| image:: https://github.com/esi-neuroscience/syncopy/actions/workflows/cov_test_workflow.yml/badge.svg?branch=master
│ │ +   :target: https://github.com/esi-neuroscience/syncopy/actions/workflows/cov_test_workflow.yml
│ │ +.. |Master Coverage| image:: https://codecov.io/gh/esi-neuroscience/syncopy/branch/master/graph/badge.svg?token=JEI3QQGNBQ
│ │ +   :target: https://codecov.io/gh/esi-neuroscience/syncopy
│ │ +
│ │ +Syncopy aims to be a user-friendly toolkit for *large-scale*
│ │ +electrophysiology data-analysis in Python. We strive to achieve the following goals:
│ │ +
│ │ +1. Syncopy is a *fully open source Python* environment for electrophysiology
│ │ +   data analysis.
│ │ +2. Syncopy is *scalable* and built for *very large datasets*. It automatically
│ │ +   makes use of available computing resources and is developed with built-in
│ │ +   parallelism in mind.
│ │ +3. Syncopy is *compatible with FieldTrip*. Data and results can be loaded into
│ │ +   MATLAB and Python, and parameter names and function call syntax are as similar as possible.
│ │ +
│ │ +Syncopy is developed at the
│ │ +`Ernst Strüngmann Institute (ESI) gGmbH for Neuroscience in Cooperation with Max Planck Society <https://www.esi-frankfurt.de/>`_
│ │ +and released free of charge under the
│ │ +`BSD 3-Clause "New" or "Revised" License <https://en.wikipedia.org/wiki/BSD_licenses#3-clause_license_(%22BSD_License_2.0%22,_%22Revised_BSD_License%22,_%22New_BSD_License%22,_or_%22Modified_BSD_License%22)>`_.
│ │ +
│ │ +Contact
│ │ +-------
│ │ +To report bugs or ask questions please use our `GitHub issue tracker <https://github.com/esi-neuroscience/syncopy/issues>`_.
│ │ +For general inquiries please contact syncopy (at) esi-frankfurt.de.
│ │ +
│ │ +Installation
│ │ +============
│ │ +
│ │ +We recommend to install SynCoPy into a new conda environment:
│ │ +
│ │ +#. Install the `Anaconda Distribution for your Operating System <https://www.anaconda.com/products/distribution>`_ if you do not yet have it.
│ │ +#. Start a new terminal.
│ │ +
│ │ +   * You can do this by starting ```Anaconda navigator```, selecting ```Environments``` in the left tab, selecting the ```base (root)``` environment, and clicking the green play button and then ```Open Terminal```.
│ │ +   * Alternatively, under Linux, you can just type ```bash``` in your active terminal to start a new session.
│ │ +
│ │ +You should see a terminal with a command prompt that starts with ```(base)```, indicating that you are
│ │ +in the conda ```base``` environment.
│ │ +
│ │ +Now we create a new environment named ```syncopy``` and install syncopy into this environment:
│ │ +
│ │ +.. code-block:: bash
│ │ +
│ │ +   conda create -y --name syncopy
│ │ +   conda activate syncopy
│ │ +   conda install -y -c conda-forge esi-syncopy
│ │ +
│ │ +Getting Started
│ │ +===============
│ │ +Please visit our `online documentation <http://syncopy.org>`_.
│ │ +
│ │ +Developer Installation
│ │ +-----------------------
│ │ +
│ │ +To get the latest development version, please clone our GitHub repository:
│ │ +
│ │ +.. code-block:: bash
│ │ +
│ │ +   git clone https://github.com/esi-neuroscience/syncopy.git
│ │ +   cd syncopy/
│ │ +   pip install -e .
│ │  
│ │ -packages = \
│ │ -['syncopy',
│ │ - 'syncopy.datatype',
│ │ - 'syncopy.datatype.methods',
│ │ - 'syncopy.io',
│ │ - 'syncopy.nwanalysis',
│ │ - 'syncopy.plotting',
│ │ - 'syncopy.preproc',
│ │ - 'syncopy.shared',
│ │ - 'syncopy.specest',
│ │ - 'syncopy.specest.wavelets',
│ │ - 'syncopy.spikes',
│ │ - 'syncopy.statistics',
│ │ - 'syncopy.tests',
│ │ - 'syncopy.tests.backend']
│ │ -
│ │ -package_data = \
│ │ -{'': ['*']}
│ │ -
│ │ -install_requires = \
│ │ -['esi-acme==2022.7',
│ │ - 'fooof>=1.0',
│ │ - 'h5py>=2.9',
│ │ - 'ipdb>=0.13.9,<0.14.0',
│ │ - 'matplotlib>=3.5',
│ │ - 'memory-profiler>=0.60.0,<0.61.0',
│ │ - 'natsort>=8.1.0,<9.0.0',
│ │ - 'numpy>=1.10',
│ │ - 'numpydoc>=1.4.0,<2.0.0',
│ │ - 'psutil',
│ │ - 'scipy>=1.5',
│ │ - 'tqdm>=4.31']
│ │ -
│ │ -setup_kwargs = {
│ │ -    'name': 'esi-syncopy',
│ │ -    'version': '2022.8',
│ │ -    'description': 'A toolkit for user-friendly large-scale electrophysiology data analysis. Syncopy is compatible with the Matlab toolbox FieldTrip.',
│ │ -    'long_description': '.. image:: https://raw.githubusercontent.com/esi-neuroscience/syncopy/master/doc/source/_static/syncopy_logo.png\n   :alt: Syncopy-Logo\n\nSystems Neuroscience Computing in Python\n========================================\n\n\n|Conda Version| |PyPi Version| |License|\n\n.. |Conda Version| image:: https://img.shields.io/conda/vn/conda-forge/esi-syncopy.svg\n   :target: https://anaconda.org/conda-forge/esi-syncopy\n.. |PyPI version| image:: https://badge.fury.io/py/esi-syncopy.svg\n   :target: https://badge.fury.io/py/esi-syncopy\n.. |License| image:: https://img.shields.io/github/license/esi-neuroscience/syncopy\n\nmaster branch status: |Master Tests| |Master Coverage|\n\n.. |Master Tests| image:: https://github.com/esi-neuroscience/syncopy/actions/workflows/cov_test_workflow.yml/badge.svg?branch=master\n   :target: https://github.com/esi-neuroscience/syncopy/actions/workflows/cov_test_workflow.yml\n.. |Master Coverage| image:: https://codecov.io/gh/esi-neuroscience/syncopy/branch/master/graph/badge.svg?token=JEI3QQGNBQ\n   :target: https://codecov.io/gh/esi-neuroscience/syncopy\n\ndev branch status: |Dev Tests| |Dev Coverage|\n\n.. |Dev Tests| image:: https://github.com/esi-neuroscience/syncopy/actions/workflows/cov_test_workflow.yml/badge.svg?branch=dev\n   :target: https://github.com/esi-neuroscience/syncopy/actions/workflows/cov_test_workflow.yml\n.. |Dev Coverage| image:: https://codecov.io/gh/esi-neuroscience/syncopy/branch/dev/graph/badge.svg?token=JEI3QQGNBQ\n   :target: https://codecov.io/gh/esi-neuroscience/syncopy\n\nSyncopy aims to be a user-friendly toolkit for *large-scale*\nelectrophysiology data-analysis in Python. We strive to achieve the following goals:\n\n1. Syncopy is a *fully open source Python* environment for electrophysiology\n   data analysis.\n2. Syncopy is *scalable* and built for *very large datasets*. It automatically\n   makes use of available computing resources and is developed with built-in\n   parallelism in mind.\n3. Syncopy is *compatible with FieldTrip*. Data and results can be loaded into\n   MATLAB and Python, parameter names and function call syntax are as similar as possible\n\nSyncopy is developed at the\n`Ernst Strüngmann Institute (ESI) gGmbH for Neuroscience in Cooperation with Max Planck Society <https://www.esi-frankfurt.de/>`_\nand released free of charge under the\n`BSD 3-Clause "New" or "Revised" License <https://en.wikipedia.org/wiki/BSD_licenses#3-clause_license_(%22BSD_License_2.0%22,_%22Revised_BSD_License%22,_%22New_BSD_License%22,_or_%22Modified_BSD_License%22)>`_.\n\nContact\n-------\nTo report bugs or ask questions please use our `GitHub issue tracker <https://github.com/esi-neuroscience/syncopy/issues>`_.\nFor general inquiries please contact syncopy (at) esi-frankfurt.de.\n\nInstallation\n============\nSyncopy is available on pip\n\n.. code-block:: bash\n\n   pip install esi-syncopy\n\nFor using SynCoPy\'s parallel processing capabilities, `ACME <https://github.com/esi-neuroscience/acme>`_ is required\n\n.. code-block:: bash\n\n   conda install -c conda-forge esi-acme\n\nTo get the latest development version, please clone our GitHub repository:\n\n.. code-block:: bash\n\n   git clone https://github.com/esi-neuroscience/syncopy.git\n   cd syncopy/\n   pip install -e .\n\nGetting Started\n===============\nPlease visit our `online documentation <http://syncopy.org>`_.\n',
│ │ -    'author': 'Stefan Fürtinger',
│ │ -    'author_email': 'sfuerti@esi-frankfurt.de',
│ │ -    'maintainer': None,
│ │ -    'maintainer_email': None,
│ │ -    'url': 'https://syncopy.org',
│ │ -    'packages': packages,
│ │ -    'package_data': package_data,
│ │ -    'install_requires': install_requires,
│ │ -    'python_requires': '>=3.8,<3.9',
│ │ -}
│ │ -
│ │ -
│ │ -setup(**setup_kwargs)
