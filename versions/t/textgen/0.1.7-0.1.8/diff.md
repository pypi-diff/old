# Comparing `tmp/textgen-0.1.7.tar.gz` & `tmp/textgen-0.1.8.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "textgen-0.1.7.tar", last modified: Mon Nov 28 04:00:54 2022, max compression
+gzip compressed data, was "textgen-0.1.8.tar", last modified: Fri Apr  7 09:20:39 2023, max compression
```

## Comparing `textgen-0.1.7.tar` & `textgen-0.1.8.tar`

### file list

```diff
@@ -1,64 +1,64 @@
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.222367 textgen-0.1.7/
--rw-r--r--   0 xuming     (501) staff       (20)    11357 2021-08-05 02:59:06.000000 textgen-0.1.7/LICENSE
--rw-r--r--   0 xuming     (501) staff       (20)    26535 2022-11-28 04:00:54.222825 textgen-0.1.7/PKG-INFO
--rw-r--r--   0 xuming     (501) staff       (20)    22182 2022-11-27 06:14:50.000000 textgen-0.1.7/README.md
--rw-r--r--   0 xuming     (501) staff       (20)      309 2022-11-28 04:00:54.223645 textgen-0.1.7/setup.cfg
--rw-r--r--   0 xuming     (501) staff       (20)     1356 2022-11-28 03:49:38.000000 textgen-0.1.7/setup.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.126855 textgen-0.1.7/textgen/
--rw-r--r--   0 xuming     (501) staff       (20)     1291 2022-11-28 03:51:10.000000 textgen-0.1.7/textgen/__init__.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.145789 textgen-0.1.7/textgen/augment/
--rw-r--r--   0 xuming     (501) staff       (20)      133 2022-06-30 07:59:18.000000 textgen-0.1.7/textgen/augment/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)     1493 2022-05-09 11:34:10.000000 textgen-0.1.7/textgen/augment/sentence_level_augment.py
--rw-r--r--   0 xuming     (501) staff       (20)     3384 2022-09-26 07:08:57.000000 textgen-0.1.7/textgen/augment/text_augment.py
--rw-r--r--   0 xuming     (501) staff       (20)     2256 2022-07-04 08:00:34.000000 textgen-0.1.7/textgen/augment/tokenizer.py
--rw-r--r--   0 xuming     (501) staff       (20)     1971 2021-08-05 02:59:06.000000 textgen-0.1.7/textgen/augment/translate_api.py
--rw-r--r--   0 xuming     (501) staff       (20)    13027 2022-09-26 07:39:41.000000 textgen-0.1.7/textgen/augment/word_level_augment.py
--rw-r--r--   0 xuming     (501) staff       (20)     1240 2022-05-09 11:34:10.000000 textgen-0.1.7/textgen/augment/word_vocab.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.150234 textgen-0.1.7/textgen/config/
--rw-r--r--   0 xuming     (501) staff       (20)      140 2021-08-05 02:59:06.000000 textgen-0.1.7/textgen/config/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)     1845 2022-06-30 03:18:51.000000 textgen-0.1.7/textgen/config/global_args.py
--rw-r--r--   0 xuming     (501) staff       (20)    12871 2022-09-13 02:20:38.000000 textgen-0.1.7/textgen/config/model_args.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.152904 textgen-0.1.7/textgen/custom_models/
--rwxr-xr-x   0 xuming     (501) staff       (20)        0 2021-08-05 02:59:06.000000 textgen-0.1.7/textgen/custom_models/__init__.py
--rwxr-xr-x   0 xuming     (501) staff       (20)    32645 2022-08-24 03:27:39.000000 textgen-0.1.7/textgen/custom_models/models.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.157554 textgen-0.1.7/textgen/data/
--rwxr-xr-x   0 xuming     (501) staff       (20)    48098 2019-08-25 14:57:21.000000 textgen-0.1.7/textgen/data/HowNetPOSWord.txt
--rw-r--r--   0 xuming     (501) staff       (20)    17438 2021-10-25 11:27:11.000000 textgen-0.1.7/textgen/data/stopwords.txt
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.164772 textgen-0.1.7/textgen/language_generation/
--rwxr-xr-x   0 xuming     (501) staff       (20)      292 2021-08-05 02:59:06.000000 textgen-0.1.7/textgen/language_generation/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)    10056 2022-07-05 07:48:33.000000 textgen-0.1.7/textgen/language_generation/language_generation_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     2829 2022-06-14 12:08:00.000000 textgen-0.1.7/textgen/language_generation/language_generation_utils.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.177204 textgen-0.1.7/textgen/language_modeling/
--rwxr-xr-x   0 xuming     (501) staff       (20)      407 2022-09-10 06:38:25.000000 textgen-0.1.7/textgen/language_modeling/__init__.py
--rwxr-xr-x   0 xuming     (501) staff       (20)    56833 2022-09-09 11:41:34.000000 textgen-0.1.7/textgen/language_modeling/language_modeling_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     8938 2022-06-14 12:08:00.000000 textgen-0.1.7/textgen/language_modeling/language_modeling_utils.py
--rw-r--r--   0 xuming     (501) staff       (20)    65595 2022-11-27 05:36:55.000000 textgen-0.1.7/textgen/language_modeling/songnet_model.py
--rw-r--r--   0 xuming     (501) staff       (20)    17103 2022-11-28 03:49:38.000000 textgen-0.1.7/textgen/language_modeling/songnet_utils.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.184424 textgen-0.1.7/textgen/question_answering/
--rwxr-xr-x   0 xuming     (501) staff       (20)      287 2021-08-05 02:59:06.000000 textgen-0.1.7/textgen/question_answering/__init__.py
--rwxr-xr-x   0 xuming     (501) staff       (20)    61756 2022-09-09 11:41:34.000000 textgen-0.1.7/textgen/question_answering/question_answering_model.py
--rwxr-xr-x   0 xuming     (501) staff       (20)    77131 2022-06-14 12:36:04.000000 textgen-0.1.7/textgen/question_answering/question_answering_utils.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.202822 textgen-0.1.7/textgen/seq2seq/
--rwxr-xr-x   0 xuming     (501) staff       (20)      313 2022-08-07 03:00:11.000000 textgen-0.1.7/textgen/seq2seq/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)    73298 2022-09-09 11:41:34.000000 textgen-0.1.7/textgen/seq2seq/bart_seq2seq_model.py
--rw-r--r--   0 xuming     (501) staff       (20)    18559 2022-06-19 09:29:30.000000 textgen-0.1.7/textgen/seq2seq/bart_seq2seq_utils.py
--rw-r--r--   0 xuming     (501) staff       (20)    23456 2022-08-25 03:06:56.000000 textgen-0.1.7/textgen/seq2seq/conv_seq2seq_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     4345 2022-05-10 10:48:02.000000 textgen-0.1.7/textgen/seq2seq/data_reader.py
--rw-r--r--   0 xuming     (501) staff       (20)    19319 2022-08-25 03:06:56.000000 textgen-0.1.7/textgen/seq2seq/seq2seq_model.py
--rw-r--r--   0 xuming     (501) staff       (20)    10741 2022-06-15 12:49:21.000000 textgen-0.1.7/textgen/seq2seq/seq2seq_trainer.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.216496 textgen-0.1.7/textgen/t5/
--rwxr-xr-x   0 xuming     (501) staff       (20)      272 2022-08-23 06:18:39.000000 textgen-0.1.7/textgen/t5/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)    50493 2022-11-16 12:03:51.000000 textgen-0.1.7/textgen/t5/copyt5_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     6917 2022-11-16 13:02:34.000000 textgen-0.1.7/textgen/t5/copyt5_utils.py
--rw-r--r--   0 xuming     (501) staff       (20)    50541 2022-11-16 12:03:51.000000 textgen-0.1.7/textgen/t5/t5_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     7146 2022-11-17 02:54:45.000000 textgen-0.1.7/textgen/t5/t5_utils.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.220869 textgen-0.1.7/textgen/unsup_generation/
--rw-r--r--   0 xuming     (501) staff       (20)      246 2022-09-06 02:24:58.000000 textgen-0.1.7/textgen/unsup_generation/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)     3456 2022-09-26 07:01:36.000000 textgen-0.1.7/textgen/unsup_generation/tgls_model.py
--rw-r--r--   0 xuming     (501) staff       (20)    27678 2022-09-06 02:29:56.000000 textgen-0.1.7/textgen/unsup_generation/tgls_util.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.133352 textgen-0.1.7/textgen.egg-info/
--rw-r--r--   0 xuming     (501) staff       (20)    26535 2022-11-28 04:00:53.000000 textgen-0.1.7/textgen.egg-info/PKG-INFO
--rw-r--r--   0 xuming     (501) staff       (20)     1663 2022-11-28 04:00:53.000000 textgen-0.1.7/textgen.egg-info/SOURCES.txt
--rw-r--r--   0 xuming     (501) staff       (20)        1 2022-11-28 04:00:53.000000 textgen-0.1.7/textgen.egg-info/dependency_links.txt
--rw-r--r--   0 xuming     (501) staff       (20)      128 2022-11-28 04:00:53.000000 textgen-0.1.7/textgen.egg-info/requires.txt
--rw-r--r--   0 xuming     (501) staff       (20)        8 2022-11-28 04:00:53.000000 textgen-0.1.7/textgen.egg-info/top_level.txt
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.889828 textgen-0.1.8/
+-rw-r--r--   0 xuming     (501) staff       (20)    11357 2021-08-05 02:59:06.000000 textgen-0.1.8/LICENSE
+-rw-r--r--   0 xuming     (501) staff       (20)    26585 2023-04-07 09:20:39.890673 textgen-0.1.8/PKG-INFO
+-rw-r--r--   0 xuming     (501) staff       (20)    22208 2023-03-26 02:30:43.000000 textgen-0.1.8/README.md
+-rw-r--r--   0 xuming     (501) staff       (20)      309 2023-04-07 09:20:39.893100 textgen-0.1.8/setup.cfg
+-rw-r--r--   0 xuming     (501) staff       (20)     1395 2023-04-07 02:35:13.000000 textgen-0.1.8/setup.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.829813 textgen-0.1.8/textgen/
+-rw-r--r--   0 xuming     (501) staff       (20)     1288 2023-04-07 02:35:13.000000 textgen-0.1.8/textgen/__init__.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.839844 textgen-0.1.8/textgen/augment/
+-rw-r--r--   0 xuming     (501) staff       (20)      133 2022-06-30 07:59:18.000000 textgen-0.1.8/textgen/augment/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1493 2022-05-09 11:34:10.000000 textgen-0.1.8/textgen/augment/sentence_level_augment.py
+-rw-r--r--   0 xuming     (501) staff       (20)     3384 2022-09-26 07:08:57.000000 textgen-0.1.8/textgen/augment/text_augment.py
+-rw-r--r--   0 xuming     (501) staff       (20)     2256 2022-07-04 08:00:34.000000 textgen-0.1.8/textgen/augment/tokenizer.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1971 2021-08-05 02:59:06.000000 textgen-0.1.8/textgen/augment/translate_api.py
+-rw-r--r--   0 xuming     (501) staff       (20)    13027 2022-09-26 07:39:41.000000 textgen-0.1.8/textgen/augment/word_level_augment.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1240 2022-05-09 11:34:10.000000 textgen-0.1.8/textgen/augment/word_vocab.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.843073 textgen-0.1.8/textgen/chatglm/
+-rw-r--r--   0 xuming     (501) staff       (20)       80 2023-03-26 02:30:43.000000 textgen-0.1.8/textgen/chatglm/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)    26069 2023-04-07 08:08:24.000000 textgen-0.1.8/textgen/chatglm/chatglm_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     5884 2023-04-07 06:57:13.000000 textgen-0.1.8/textgen/chatglm/chatglm_utils.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.845533 textgen-0.1.8/textgen/config/
+-rw-r--r--   0 xuming     (501) staff       (20)      140 2021-08-05 02:59:06.000000 textgen-0.1.8/textgen/config/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1845 2022-06-30 03:18:51.000000 textgen-0.1.8/textgen/config/global_args.py
+-rw-r--r--   0 xuming     (501) staff       (20)    12292 2023-04-07 08:08:24.000000 textgen-0.1.8/textgen/config/model_args.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.847246 textgen-0.1.8/textgen/custom_models/
+-rwxr-xr-x   0 xuming     (501) staff       (20)        0 2021-08-05 02:59:06.000000 textgen-0.1.8/textgen/custom_models/__init__.py
+-rwxr-xr-x   0 xuming     (501) staff       (20)    32645 2022-08-24 03:27:39.000000 textgen-0.1.8/textgen/custom_models/models.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.850267 textgen-0.1.8/textgen/data/
+-rwxr-xr-x   0 xuming     (501) staff       (20)    48098 2019-08-25 14:57:21.000000 textgen-0.1.8/textgen/data/HowNetPOSWord.txt
+-rw-r--r--   0 xuming     (501) staff       (20)    17438 2021-10-25 11:27:11.000000 textgen-0.1.8/textgen/data/stopwords.txt
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.854199 textgen-0.1.8/textgen/language_generation/
+-rwxr-xr-x   0 xuming     (501) staff       (20)      292 2021-08-05 02:59:06.000000 textgen-0.1.8/textgen/language_generation/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)    10056 2022-07-05 07:48:33.000000 textgen-0.1.8/textgen/language_generation/language_generation_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     2829 2022-06-14 12:08:00.000000 textgen-0.1.8/textgen/language_generation/language_generation_utils.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.862985 textgen-0.1.8/textgen/language_modeling/
+-rwxr-xr-x   0 xuming     (501) staff       (20)      407 2022-09-10 06:38:25.000000 textgen-0.1.8/textgen/language_modeling/__init__.py
+-rwxr-xr-x   0 xuming     (501) staff       (20)    56833 2022-09-09 11:41:34.000000 textgen-0.1.8/textgen/language_modeling/language_modeling_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     8938 2022-06-14 12:08:00.000000 textgen-0.1.8/textgen/language_modeling/language_modeling_utils.py
+-rw-r--r--   0 xuming     (501) staff       (20)    65595 2022-11-27 05:36:55.000000 textgen-0.1.8/textgen/language_modeling/songnet_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)    17103 2022-12-05 07:28:16.000000 textgen-0.1.8/textgen/language_modeling/songnet_utils.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.873459 textgen-0.1.8/textgen/seq2seq/
+-rwxr-xr-x   0 xuming     (501) staff       (20)      313 2022-08-07 03:00:11.000000 textgen-0.1.8/textgen/seq2seq/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)    73298 2022-09-09 11:41:34.000000 textgen-0.1.8/textgen/seq2seq/bart_seq2seq_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)    18559 2022-06-19 09:29:30.000000 textgen-0.1.8/textgen/seq2seq/bart_seq2seq_utils.py
+-rw-r--r--   0 xuming     (501) staff       (20)    23456 2022-08-25 03:06:56.000000 textgen-0.1.8/textgen/seq2seq/conv_seq2seq_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     4345 2022-05-10 10:48:02.000000 textgen-0.1.8/textgen/seq2seq/data_reader.py
+-rw-r--r--   0 xuming     (501) staff       (20)    19319 2022-08-25 03:06:56.000000 textgen-0.1.8/textgen/seq2seq/seq2seq_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)    10741 2022-06-15 12:49:21.000000 textgen-0.1.8/textgen/seq2seq/seq2seq_trainer.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.881625 textgen-0.1.8/textgen/t5/
+-rwxr-xr-x   0 xuming     (501) staff       (20)      272 2022-08-23 06:18:39.000000 textgen-0.1.8/textgen/t5/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)    50493 2022-11-16 12:03:51.000000 textgen-0.1.8/textgen/t5/copyt5_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     6917 2022-11-16 13:02:34.000000 textgen-0.1.8/textgen/t5/copyt5_utils.py
+-rw-r--r--   0 xuming     (501) staff       (20)    50541 2023-02-16 08:10:32.000000 textgen-0.1.8/textgen/t5/t5_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     7146 2022-11-17 02:54:45.000000 textgen-0.1.8/textgen/t5/t5_utils.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.888068 textgen-0.1.8/textgen/unsup_generation/
+-rw-r--r--   0 xuming     (501) staff       (20)      246 2022-09-06 02:24:58.000000 textgen-0.1.8/textgen/unsup_generation/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)     3456 2022-09-26 07:01:36.000000 textgen-0.1.8/textgen/unsup_generation/tgls_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)    27678 2022-09-06 02:29:56.000000 textgen-0.1.8/textgen/unsup_generation/tgls_util.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.833383 textgen-0.1.8/textgen.egg-info/
+-rw-r--r--   0 xuming     (501) staff       (20)    26585 2023-04-07 09:20:39.000000 textgen-0.1.8/textgen.egg-info/PKG-INFO
+-rw-r--r--   0 xuming     (501) staff       (20)     1608 2023-04-07 09:20:39.000000 textgen-0.1.8/textgen.egg-info/SOURCES.txt
+-rw-r--r--   0 xuming     (501) staff       (20)        1 2023-04-07 09:20:39.000000 textgen-0.1.8/textgen.egg-info/dependency_links.txt
+-rw-r--r--   0 xuming     (501) staff       (20)      145 2023-04-07 09:20:39.000000 textgen-0.1.8/textgen.egg-info/requires.txt
+-rw-r--r--   0 xuming     (501) staff       (20)        8 2023-04-07 09:20:39.000000 textgen-0.1.8/textgen.egg-info/top_level.txt
```

### Comparing `textgen-0.1.7/LICENSE` & `textgen-0.1.8/LICENSE`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/PKG-INFO` & `textgen-0.1.8/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: textgen
-Version: 0.1.7
+Version: 0.1.8
 Summary: Text Generation Model
 Home-page: https://github.com/shibing624/textgen
 Author: XuMing
 Author-email: xuming624@qq.com
 License: Apache 2.0
 Description: [![PyPI version](https://badge.fury.io/py/textgen.svg)](https://badge.fury.io/py/textgen)
         [![Downloads](https://pepy.tech/badge/textgen)](https://pepy.tech/project/textgen)
@@ -19,56 +19,62 @@
         
         ğŸŒˆ Implementation of Text Generation models.
         
         **textgen**å®ç°äº†å¤šç§æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼šUDAã€GPT2ã€Seq2Seqã€BARTã€T5ã€SongNetç­‰æ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ã€‚
         
         **Guide**
         
-        - [Question](#Question)
-        - [Solution](#Solution)
         - [Feature](#Feature)
         - [Install](#install)
         - [Usage](#usage)
         - [Contact](#Contact)
         - [Reference](#reference)
         
-        # Question
-        
-        æ–‡æœ¬ç”Ÿæˆï¼Œæ–‡æœ¬æ•°æ®å¢å¼ºæ€ä¹ˆåšï¼Ÿ
-        
-        # Solution
-        ## æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
+        # Feature
+        ## æ–‡æœ¬ç”Ÿæˆ
         
-        1. Seq2Seqã€ConvSeq2Seqã€BART
-        2. GPT2ã€SongNet
-        3. T5ã€CopyT5
+        1. seq2seq: Seq2Seqã€ConvSeq2Seqã€BART
+        2. language_modeling: GPT2ã€SongNet
+        3. t5: T5ã€CopyT5
+        4. question_answering: BERTã€XLNet
+        5. chatglm: ChatGLM
         
         ## æ–‡æœ¬æ‰©å¢
         ### è¯ç²’åº¦æ‰©å¢
         1. UDAï¼Œéæ ¸å¿ƒè¯æ›¿æ¢
         2. EDAï¼Œç®€å•æ•°æ®å¢å¼ºæŠ€æœ¯ï¼šç›¸ä¼¼è¯ã€åŒä¹‰è¯æ›¿æ¢ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢
         
         ### å¥ç²’åº¦æ‰©å¢
         1. å›è¯‘ï¼ˆBT, Back Translateï¼‰ï¼šä¸­æ–‡-è‹±æ–‡-ä¸­æ–‡
         2. GPT2æ¨¡å‹ç»­å†™ï¼šçŸ­æ–‡æœ¬->é•¿æ–‡æœ¬
         3. BARTæ‘˜è¦æ¨¡å‹ï¼šé•¿æ–‡æœ¬->çŸ­æ–‡æœ¬
         4. TGLSï¼šæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
         
-        
-        # Feature
-        
+        ## åŠŸèƒ½åˆ—è¡¨
         - [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)ç®—æ³•å’ŒEDAç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
         - [BT(å›è¯‘)](textgen/augment/sentence_level_augment.py)ï¼šæœ¬é¡¹ç›®åŸºäºç™¾åº¦ç¿»è¯‘APIå®ç°äº†å›è¯‘åŠŸèƒ½ï¼Œå…ˆæŠŠä¸­æ–‡å¥å­ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œå†æŠŠè‹±æ–‡ç¿»è¯‘ä¸ºæ–°çš„ä¸­æ–‡
         - [Seq2Seq](textgen/seq2seq)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†Seq2Seqã€ConvSeq2Seqã€BARTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [T5](textgen/t5)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†T5å’ŒCopyT5æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€å¯¹è”ç”Ÿæˆã€æ–‡æ¡ˆæ’°å†™ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [GPT2](textgen/language_modeling)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†GTP2æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡ç« ç”Ÿæˆã€å¯¹è”ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [SongNet](textgen/language_modeling/songnet_model.py)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†SongNetæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºè§„èŒƒæ ¼å¼çš„è¯—è¯ã€æ­Œè¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [TGLS](textgen/unsup_generation)ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬
         
         
+        ## Release Models
+        releaseåŸºäº`textgen`è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å·²ç»releaseåˆ°HuggingFace modelsï¼ŒæŒ‡å®šæ¨¡å‹åç§°`textgen`ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
+        
+        |Model|Arch|Intro|Training|Inference|
+        |:-- |:--- |:--- |:--- |:--- |
+        |[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
+        |[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
+        |[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
+        |[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
+        |[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
+        
+        
         # Demo
         
         HuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate
         
         ![](docs/hf.png)
         
         run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:
@@ -260,22 +266,14 @@
         output:
         ```shell
         inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
         outputs: ['äººå·¥æ™ºèƒ½æœ‰ä¸¤ä¸ªå¹¿ä¹‰çš„å®šä¹‰,ä»»ä½•æ‹Ÿäººçš„æœºæ¢°,å¦‚åœ¨å¡é›·å°”capeks', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨Python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Šå·¥ä½œ!', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦']
         ```
         
         
-        ### T5 æ¨¡å‹åº”ç”¨
-        
-        releaseåŸºäºT5çš„fine-tunedåçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å…¨éƒ¨releaseåˆ°HuggingFace modelsï¼Œ`textgen`å¯è‡ªåŠ¨ä¸‹è½½ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
-        
-        |Model|Arch|Intro|Training|Inference|
-        |:-- |:--- |:--- |:--- |:--- |
-        |[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
-        |[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
         
         
         ## GPT2 æ¨¡å‹
         
         ### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ
         
         ä½¿ç”¨ä¸­æ–‡æ•°æ®é›†ï¼ˆæ®µè½æ ¼å¼ï¼Œ`\n`é—´éš”ï¼‰ï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè¯—æ­Œç”Ÿæˆã€æ–‡ç« ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
@@ -300,23 +298,14 @@
         ## SongNet æ¨¡å‹
         
         æ ¼å¼æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œpaperè§[SongNet: Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022)ï¼Œ
         é€‚ç”¨äºå¼ºéŸµå¾‹æ ¼å¼è¦æ±‚çš„è¯—æ­Œã€å¯¹è”ã€æ­Œè¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
         
         example: [examples/language_generation/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)
         
-        ### SongNet æ¨¡å‹åº”ç”¨
-        
-        releaseåŸºäºSongNetçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å…¨éƒ¨releaseåˆ°HuggingFace modelsï¼Œ`textgen`å¯è‡ªåŠ¨ä¸‹è½½ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
-        
-        |Model|Arch|Intro|Training|Inference|
-        |:-- |:--- |:--- |:--- |:--- |
-        |[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
-        |[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
-        |[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
         
         
         ## Keyword Text Augmentation(EDA/UDA)
         
         example: [examples/text_augmentation_demo.py](examples/text_augmentation_demo.py)
         
         ```python
@@ -434,14 +423,28 @@
         - Issue(å»ºè®®)
           ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
         - é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
         - å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚
         
         <img src="docs/wechat.jpeg" width="200" />
         
+        
+        # Citation
+        
+        å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†textgenï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š
+        
+        ```latex
+        @misc{textgen,
+          title={textgen: Text Generation Tool},
+          author={Xu Ming},
+          year={2021},
+          howpublished={\url{https://github.com/shibing624/textgen}},
+        }
+        ```
+        
         # License
         
         æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ textgençš„é“¾æ¥å’Œæˆæƒåè®®ã€‚
         
         # Contribute
         
         é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š
```

### Comparing `textgen-0.1.7/README.md` & `textgen-0.1.8/README.md`

 * *Files 3% similar despite different names*

```diff
@@ -11,56 +11,62 @@
 
 ğŸŒˆ Implementation of Text Generation models.
 
 **textgen**å®ç°äº†å¤šç§æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼šUDAã€GPT2ã€Seq2Seqã€BARTã€T5ã€SongNetç­‰æ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ã€‚
 
 **Guide**
 
-- [Question](#Question)
-- [Solution](#Solution)
 - [Feature](#Feature)
 - [Install](#install)
 - [Usage](#usage)
 - [Contact](#Contact)
 - [Reference](#reference)
 
-# Question
-
-æ–‡æœ¬ç”Ÿæˆï¼Œæ–‡æœ¬æ•°æ®å¢å¼ºæ€ä¹ˆåšï¼Ÿ
-
-# Solution
-## æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
+# Feature
+## æ–‡æœ¬ç”Ÿæˆ
 
-1. Seq2Seqã€ConvSeq2Seqã€BART
-2. GPT2ã€SongNet
-3. T5ã€CopyT5
+1. seq2seq: Seq2Seqã€ConvSeq2Seqã€BART
+2. language_modeling: GPT2ã€SongNet
+3. t5: T5ã€CopyT5
+4. question_answering: BERTã€XLNet
+5. chatglm: ChatGLM
 
 ## æ–‡æœ¬æ‰©å¢
 ### è¯ç²’åº¦æ‰©å¢
 1. UDAï¼Œéæ ¸å¿ƒè¯æ›¿æ¢
 2. EDAï¼Œç®€å•æ•°æ®å¢å¼ºæŠ€æœ¯ï¼šç›¸ä¼¼è¯ã€åŒä¹‰è¯æ›¿æ¢ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢
 
 ### å¥ç²’åº¦æ‰©å¢
 1. å›è¯‘ï¼ˆBT, Back Translateï¼‰ï¼šä¸­æ–‡-è‹±æ–‡-ä¸­æ–‡
 2. GPT2æ¨¡å‹ç»­å†™ï¼šçŸ­æ–‡æœ¬->é•¿æ–‡æœ¬
 3. BARTæ‘˜è¦æ¨¡å‹ï¼šé•¿æ–‡æœ¬->çŸ­æ–‡æœ¬
 4. TGLSï¼šæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
 
-
-# Feature
-
+## åŠŸèƒ½åˆ—è¡¨
 - [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)ç®—æ³•å’ŒEDAç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
 - [BT(å›è¯‘)](textgen/augment/sentence_level_augment.py)ï¼šæœ¬é¡¹ç›®åŸºäºç™¾åº¦ç¿»è¯‘APIå®ç°äº†å›è¯‘åŠŸèƒ½ï¼Œå…ˆæŠŠä¸­æ–‡å¥å­ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œå†æŠŠè‹±æ–‡ç¿»è¯‘ä¸ºæ–°çš„ä¸­æ–‡
 - [Seq2Seq](textgen/seq2seq)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†Seq2Seqã€ConvSeq2Seqã€BARTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
 - [T5](textgen/t5)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†T5å’ŒCopyT5æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€å¯¹è”ç”Ÿæˆã€æ–‡æ¡ˆæ’°å†™ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
 - [GPT2](textgen/language_modeling)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†GTP2æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡ç« ç”Ÿæˆã€å¯¹è”ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
 - [SongNet](textgen/language_modeling/songnet_model.py)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†SongNetæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºè§„èŒƒæ ¼å¼çš„è¯—è¯ã€æ­Œè¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
 - [TGLS](textgen/unsup_generation)ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬
 
 
+## Release Models
+releaseåŸºäº`textgen`è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å·²ç»releaseåˆ°HuggingFace modelsï¼ŒæŒ‡å®šæ¨¡å‹åç§°`textgen`ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
+
+|Model|Arch|Intro|Training|Inference|
+|:-- |:--- |:--- |:--- |:--- |
+|[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
+|[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
+|[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
+|[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
+|[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
+
+
 # Demo
 
 HuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate
 
 ![](docs/hf.png)
 
 run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:
@@ -252,22 +258,14 @@
 output:
 ```shell
 inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
 outputs: ['äººå·¥æ™ºèƒ½æœ‰ä¸¤ä¸ªå¹¿ä¹‰çš„å®šä¹‰,ä»»ä½•æ‹Ÿäººçš„æœºæ¢°,å¦‚åœ¨å¡é›·å°”capeks', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨Python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Šå·¥ä½œ!', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦']
 ```
 
 
-### T5 æ¨¡å‹åº”ç”¨
-
-releaseåŸºäºT5çš„fine-tunedåçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å…¨éƒ¨releaseåˆ°HuggingFace modelsï¼Œ`textgen`å¯è‡ªåŠ¨ä¸‹è½½ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
-
-|Model|Arch|Intro|Training|Inference|
-|:-- |:--- |:--- |:--- |:--- |
-|[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
-|[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
 
 
 ## GPT2 æ¨¡å‹
 
 ### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ
 
 ä½¿ç”¨ä¸­æ–‡æ•°æ®é›†ï¼ˆæ®µè½æ ¼å¼ï¼Œ`\n`é—´éš”ï¼‰ï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè¯—æ­Œç”Ÿæˆã€æ–‡ç« ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
@@ -292,23 +290,14 @@
 ## SongNet æ¨¡å‹
 
 æ ¼å¼æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œpaperè§[SongNet: Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022)ï¼Œ
 é€‚ç”¨äºå¼ºéŸµå¾‹æ ¼å¼è¦æ±‚çš„è¯—æ­Œã€å¯¹è”ã€æ­Œè¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
 
 example: [examples/language_generation/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)
 
-### SongNet æ¨¡å‹åº”ç”¨
-
-releaseåŸºäºSongNetçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å…¨éƒ¨releaseåˆ°HuggingFace modelsï¼Œ`textgen`å¯è‡ªåŠ¨ä¸‹è½½ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
-
-|Model|Arch|Intro|Training|Inference|
-|:-- |:--- |:--- |:--- |:--- |
-|[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
-|[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
-|[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
 
 
 ## Keyword Text Augmentation(EDA/UDA)
 
 example: [examples/text_augmentation_demo.py](examples/text_augmentation_demo.py)
 
 ```python
@@ -426,14 +415,28 @@
 - Issue(å»ºè®®)
   ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
 - é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
 - å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚
 
 <img src="docs/wechat.jpeg" width="200" />
 
+
+# Citation
+
+å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†textgenï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š
+
+```latex
+@misc{textgen,
+  title={textgen: Text Generation Tool},
+  author={Xu Ming},
+  year={2021},
+  howpublished={\url{https://github.com/shibing624/textgen}},
+}
+```
+
 # License
 
 æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ textgençš„é“¾æ¥å’Œæˆæƒåè®®ã€‚
 
 # Contribute
 
 é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š
```

### Comparing `textgen-0.1.7/setup.py` & `textgen-0.1.8/setup.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 from setuptools import setup, find_packages
 
 with open('README.md', 'r', encoding='utf-8') as f:
     readme = f.read()
 
 setup(
     name='textgen',
-    version='0.1.7',
+    version='0.1.8',
     description='Text Generation Model',
     long_description=readme,
     long_description_content_type='text/markdown',
     author='XuMing',
     author_email='xuming624@qq.com',
     url='https://github.com/shibing624/textgen',
     license="Apache 2.0",
@@ -34,14 +34,16 @@
         'text2vec',
         'tensorboard',
         'tqdm>=4.47.0',
         'pandas',
         'wandb>=0.10.32',
         'sacremoses',
         'Rouge',
+        'cpm_kernels',
+        'peft',
     ],
     packages=find_packages(exclude=['tests']),
     package_dir={'textgen': 'textgen'},
     package_data={
         'textgen': ['*.*', 'data/*'],
     }
 )
```

### Comparing `textgen-0.1.7/textgen/__init__.py` & `textgen-0.1.8/textgen/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,33 +1,36 @@
 # -*- coding: utf-8 -*-
 """
 @author:XuMing(xuming624@qq.com)
 @description: 
 """
 
-__version__ = '0.1.7'
+__version__ = '0.1.8'
 
 from textgen.augment.text_augment import TextAugment
 
 from textgen.config.model_args import LanguageGenerationArgs
 from textgen.language_generation.language_generation_model import LanguageGenerationModel
 
 from textgen.config.model_args import LanguageModelingArgs
 from textgen.language_modeling.language_modeling_model import LanguageModelingModel
+
 from textgen.config.model_args import SongNetArgs
 from textgen.language_modeling.songnet_model import SongNetModel
 from textgen.language_modeling.songnet_utils import SongNetTokenizer, snapshot_download
 
-from textgen.config.model_args import QuestionAnsweringArgs
-from textgen.question_answering.question_answering_model import QuestionAnsweringModel
-
 from textgen.config.model_args import Seq2SeqArgs
-from textgen.seq2seq.bart_seq2seq_model import BartSeq2SeqModel
 from textgen.seq2seq.seq2seq_model import Seq2SeqModel
 from textgen.seq2seq.conv_seq2seq_model import ConvSeq2SeqModel
+from textgen.seq2seq.bart_seq2seq_model import BartSeq2SeqModel
 
-from textgen.config.model_args import T5Args, CopyT5Args
+from textgen.config.model_args import T5Args
 from textgen.t5.t5_model import T5Model
+
+from textgen.config.model_args import CopyT5Args
 from textgen.t5.copyt5_model import CopyT5Model
 from textgen.t5.copyt5_utils import ZHTokenizer
 
 from textgen.unsup_generation.tgls_model import TglsModel
+
+from textgen.chatglm.chatglm_model import ChatGlmModel
+from textgen.config.model_args import ChatGlmArgs
```

### Comparing `textgen-0.1.7/textgen/augment/sentence_level_augment.py` & `textgen-0.1.8/textgen/augment/sentence_level_augment.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/augment/text_augment.py` & `textgen-0.1.8/textgen/augment/text_augment.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/augment/tokenizer.py` & `textgen-0.1.8/textgen/augment/tokenizer.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/augment/translate_api.py` & `textgen-0.1.8/textgen/augment/translate_api.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/augment/word_level_augment.py` & `textgen-0.1.8/textgen/augment/word_level_augment.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/augment/word_vocab.py` & `textgen-0.1.8/textgen/augment/word_vocab.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/config/global_args.py` & `textgen-0.1.8/textgen/config/global_args.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/config/model_args.py` & `textgen-0.1.8/textgen/config/model_args.py`

 * *Files 11% similar despite different names*

```diff
@@ -2,18 +2,18 @@
 """
 @author:XuMing(xuming624@qq.com)
 @description: refer https://github.com/ThilinaRajapakse/simpletransformers
 """
 import json
 import os
 import sys
-import warnings
+from loguru import logger
 from dataclasses import asdict, dataclass, field
 from multiprocessing import cpu_count
-
+from typing import Optional
 from torch.utils.data import Dataset
 
 
 def get_default_process_count():
     process_count = cpu_count() - 2 if cpu_count() > 2 else 1
     if sys.platform == "win32":
         process_count = min(process_count, 61)
@@ -114,49 +114,38 @@
 
     def get_args_for_saving(self):
         args_for_saving = {key: value for key, value in asdict(self).items() if key not in self.not_saved_args}
         return args_for_saving
 
     def save(self, output_dir):
         os.makedirs(output_dir, exist_ok=True)
-        with open(os.path.join(output_dir, "model_args.json"), "w") as f:
+        with open(os.path.join(output_dir, "model_args.json"), "w", encoding='utf-8') as f:
             args_dict = self.get_args_for_saving()
+            if args_dict['dataset_class'] is not None and not isinstance(args_dict["dataset_class"], str):
+                args_dict['dataset_class'] = type(args_dict['dataset_class']).__name__
             if args_dict["tokenizer_type"] is not None and not isinstance(args_dict["tokenizer_type"], str):
                 args_dict["tokenizer_type"] = type(args_dict["tokenizer_type"]).__name__
             json.dump(args_dict, f)
 
     def load(self, input_dir):
         if input_dir:
             model_args_file = os.path.join(input_dir, "model_args.json")
             if os.path.isfile(model_args_file):
-                with open(model_args_file, "r") as f:
+                with open(model_args_file, "r", encoding='utf-8') as f:
                     model_args = json.load(f)
-
+                if model_args["dataset_class"]:
+                    logger.warning(
+                        "This model was trained using a custom dataset_class."
+                        "This cannot be loaded automatically and must be specified in the model args"
+                        "when loading the model."
+                    )
                 self.update_from_dict(model_args)
 
 
 @dataclass
-class QuestionAnsweringArgs(ModelArgs):
-    """
-    Model args for a QuestionAnsweringModel
-    """
-
-    model_class: str = "QuestionAnsweringModel"
-    doc_stride: int = 384
-    early_stopping_metric: str = "correct"
-    early_stopping_metric_minimize: bool = False
-    lazy_loading: bool = False
-    max_answer_length: int = 100
-    max_query_length: int = 64
-    n_best_size: int = 20
-    null_score_diff_threshold: float = 0.0
-    special_tokens_list: list = field(default_factory=list)
-
-
-@dataclass
 class T5Args(ModelArgs):
     """
     Model args for a T5Model
     """
 
     model_class: str = "T5Model"
     dataset_class: Dataset = None
@@ -238,37 +227,14 @@
     vocab_size: int = None
     clean_text: bool = True
     handle_chinese_chars: bool = True
     special_tokens_list: list = field(default_factory=list)
     strip_accents: bool = True
     local_rank: int = -1
 
-    def save(self, output_dir):
-        os.makedirs(output_dir, exist_ok=True)
-        with open(os.path.join(output_dir, "model_args.json"), "w") as f:
-            args_dict = self.get_args_for_saving()
-            if args_dict["dataset_class"] is not None:
-                args_dict["dataset_class"] = type(args_dict["dataset_class"]).__name__
-            # json.dump(self.get_args_for_saving(), f)
-            json.dump(args_dict, f)
-
-    def load(self, input_dir):
-        if input_dir:
-            model_args_file = os.path.join(input_dir, "model_args.json")
-            if os.path.isfile(model_args_file):
-                with open(model_args_file, "r") as f:
-                    model_args = json.load(f)
-                if model_args["dataset_class"]:
-                    warnings.warn(
-                        "This model was trained using a custom dataset_class."
-                        "This cannot be loaded automatically and must be specified in the model args"
-                        "when loading the model."
-                    )
-                self.update_from_dict(model_args)
-
 
 @dataclass
 class Seq2SeqArgs(ModelArgs):
     """
     Model args for a Seq2SeqModel
     """
 
@@ -293,36 +259,14 @@
     save_knowledge_dataset: bool = True
     save_knowledge_dataset_with_checkpoints: bool = False
     split_text_character: str = " "
     split_text_n: int = 100
     src_lang: str = "en_XX"
     tgt_lang: str = "ro_RO"
 
-    def save(self, output_dir):
-        os.makedirs(output_dir, exist_ok=True)
-        with open(os.path.join(output_dir, "model_args.json"), "w") as f:
-            args_dict = self.get_args_for_saving()
-            if args_dict["dataset_class"] is not None:
-                args_dict["dataset_class"] = type(args_dict["dataset_class"]).__name__
-            json.dump(args_dict, f)
-
-    def load(self, input_dir):
-        if input_dir:
-            model_args_file = os.path.join(input_dir, "model_args.json")
-            if os.path.isfile(model_args_file):
-                with open(model_args_file, "r") as f:
-                    model_args = json.load(f)
-                if model_args["dataset_class"]:
-                    warnings.warn(
-                        "This model was trained using a custom dataset_class."
-                        "This cannot be loaded automatically and must be specified in the model args"
-                        "when loading the model."
-                    )
-                self.update_from_dict(model_args)
-
 
 @dataclass
 class LanguageGenerationArgs(ModelArgs):
     """
     Model args for a LanguageGenerationModel
     """
 
@@ -381,7 +325,53 @@
     ff_embed_dim: int = 3072
     num_heads: int = 12
     num_layers: int = 12
     dropout: float = 0.2
     warmup_ratio: float = 0.05
     weight_decay: float = 0.0
     smoothing_factor: float = 0.1
+
+
+@dataclass
+class ChatGlmArgs(ModelArgs):
+    """
+    Model args for a ChatGLMModel
+    """
+
+    model_class: str = "ChatGlmArgs"
+    dataset_class: Dataset = None
+    fp16: bool = True
+    int8: bool = False
+    quantization_bit:int = None  # if use quantization bit, set 4, else None
+    debug: bool = False
+    max_seq_length: int = 256  # max length of input sequence
+    max_length = 384  # max length of the sequence to be generated
+    do_sample: bool = True
+    early_stopping: bool = True
+    evaluate_generated_text: bool = False
+    length_penalty: float = 2.0
+    num_beams: int = 1
+    num_return_sequences: int = 1
+    repetition_penalty: float = 1.0
+    temperature: float = 0.95
+    special_tokens_list: list = field(default_factory=list)
+    top_k: float = None
+    top_p: float = 0.7
+    model_name_or_path: Optional[str] = field(default="THUDM/chatglm-6b")
+    dataset_name_or_path: Optional[str] = field(default="shibing624/alpaca-zh")
+    use_lora: bool = True
+    lora_name: str = field(default="adapter_model.bin")
+    lora_rank: int = field(default=8)
+    lora_alpha = 32
+    lora_dropout = 0.1
+    lora_target_modules = ["query_key_value"]
+    lora_bias = "none"
+    only_lora_state_dict: bool = False
+    num_train_epochs = 1
+    max_steps = -1
+    per_device_train_batch_size = 2
+    gradient_accumulation_steps = 1
+    save_total_limit = 2
+    remove_unused_columns = False
+    logging_steps = 50
+    resume_from_checkpoint:str = None
+
```

### Comparing `textgen-0.1.7/textgen/custom_models/models.py` & `textgen-0.1.8/textgen/custom_models/models.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/data/HowNetPOSWord.txt` & `textgen-0.1.8/textgen/data/HowNetPOSWord.txt`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/data/stopwords.txt` & `textgen-0.1.8/textgen/data/stopwords.txt`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/language_generation/language_generation_model.py` & `textgen-0.1.8/textgen/language_generation/language_generation_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/language_generation/language_generation_utils.py` & `textgen-0.1.8/textgen/language_generation/language_generation_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/language_modeling/language_modeling_model.py` & `textgen-0.1.8/textgen/language_modeling/language_modeling_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/language_modeling/language_modeling_utils.py` & `textgen-0.1.8/textgen/language_modeling/language_modeling_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/language_modeling/songnet_model.py` & `textgen-0.1.8/textgen/language_modeling/songnet_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/language_modeling/songnet_utils.py` & `textgen-0.1.8/textgen/language_modeling/songnet_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/question_answering/question_answering_model.py` & `textgen-0.1.8/textgen/t5/t5_model.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,407 +1,201 @@
 # -*- coding: utf-8 -*-
 """
 @author:XuMing(xuming624@qq.com)
 @description: refer https://github.com/ThilinaRajapakse/simpletransformers
 """
 
-import json
-import logging
 import math
 import os
 import random
 import warnings
 from dataclasses import asdict
-from multiprocessing import cpu_count
+from multiprocessing import Pool
 
 import numpy as np
 import pandas as pd
 import torch
-from scipy.stats import pearsonr
-from sklearn.metrics import (
-    confusion_matrix,
-    label_ranking_average_precision_score,
-    matthews_corrcoef,
-    mean_squared_error,
-)
+from loguru import logger
+from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
 from torch.utils.tensorboard import SummaryWriter
-from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
-from torch.utils.data.distributed import DistributedSampler
 from tqdm.auto import tqdm, trange
+from transformers import ByT5Tokenizer
+from transformers import MT5Config, MT5ForConditionalGeneration
+from transformers import T5Config, T5ForConditionalGeneration, T5Tokenizer
+from transformers.optimization import AdamW, Adafactor
 from transformers.optimization import (
     get_constant_schedule,
     get_constant_schedule_with_warmup,
     get_linear_schedule_with_warmup,
     get_cosine_schedule_with_warmup,
     get_cosine_with_hard_restarts_schedule_with_warmup,
     get_polynomial_decay_schedule_with_warmup,
 )
-from transformers.optimization import AdamW, Adafactor
-from transformers import (
-    AlbertConfig,
-    AlbertForQuestionAnswering,
-    AlbertTokenizer,
-    AutoConfig,
-    AutoModelForQuestionAnswering,
-    AutoTokenizer,
-    BartConfig,
-    BartForQuestionAnswering,
-    BartTokenizer,
-    BertConfig,
-    BertForQuestionAnswering,
-    BertTokenizer,
-    CamembertConfig,
-    CamembertForQuestionAnswering,
-    CamembertTokenizer,
-    DistilBertConfig,
-    DistilBertForQuestionAnswering,
-    DistilBertTokenizer,
-    ElectraConfig,
-    ElectraTokenizer,
-    LongformerConfig,
-    LongformerForQuestionAnswering,
-    LongformerTokenizer,
-    MPNetConfig,
-    MPNetForQuestionAnswering,
-    MPNetTokenizer,
-    MobileBertConfig,
-    MobileBertForQuestionAnswering,
-    MobileBertTokenizer,
-    RobertaConfig,
-    RobertaForQuestionAnswering,
-    RobertaTokenizer,
-    SqueezeBertConfig,
-    SqueezeBertForQuestionAnswering,
-    SqueezeBertTokenizer,
-    WEIGHTS_NAME,
-    XLMConfig,
-    XLMForQuestionAnswering,
-    XLMRobertaConfig,
-    XLMRobertaTokenizer,
-    XLMTokenizer,
-    XLNetConfig,
-    XLNetForQuestionAnswering,
-    XLNetTokenizer,
-)
 
-from textgen.config.model_args import QuestionAnsweringArgs
-from textgen.custom_models.models import ElectraForQuestionAnswering, XLMRobertaForQuestionAnswering
-from textgen.question_answering.question_answering_utils import (
-    LazyQuestionAnsweringDataset,
-    RawResult,
-    RawResultExtended,
-    build_examples,
-    get_best_predictions,
-    get_best_predictions_extended,
-    get_examples,
-    load_hf_dataset,
-    squad_convert_examples_to_features,
-    to_list,
-    write_predictions,
-    write_predictions_extended,
-)
-from loguru import logger
+from textgen.config.model_args import T5Args
+from textgen.t5.t5_utils import T5Dataset, load_hf_dataset
 
 try:
     import wandb
 
     wandb_available = True
 except ImportError:
     wandb_available = False
 
 has_cuda = torch.cuda.is_available()
 os.environ["TOKENIZERS_PARALLELISM"] = "FALSE"
 os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
 
 
-class QuestionAnsweringModel:
+def chunks(lst, n):
+    """Yield successive n-sized chunks from lst."""
+    for i in range(0, len(lst), n):
+        yield lst[i: i + n]
+
+
+MODEL_CLASSES = {
+    "t5": (T5Config, T5ForConditionalGeneration),
+    "mt5": (MT5Config, MT5ForConditionalGeneration),
+    "byt5": (T5Config, T5ForConditionalGeneration),
+}
+
+
+class T5Model:
     def __init__(
-            self, model_type, model_name, args=None, use_cuda=has_cuda, cuda_device=-1, **kwargs
+            self,
+            model_type,
+            model_name,
+            args=None,
+            tokenizer=None,
+            use_cuda=has_cuda,
+            cuda_device=-1,
+            **kwargs,
     ):
+
         """
-        Initializes a QuestionAnsweringModel model.
+        Initializes a T5Model model.
 
         Args:
-            model_type: The type of model (bert, xlnet, xlm, distilbert)
-            model_name: Default Transformer model name or path to a directory containing Transformer model file (pytorch_nodel.bin).
-            args (optional): Default args will be used if this parameter is not provided. If provided,
-                it should be a dict containing the args that should be changed in the default args'
+            model_type: The type of model (t5, mt5, byt5)
+            model_name: The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.
+            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.
             use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.
             cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.
+            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.
         """  # noqa: ignore flake8"
 
-        MODEL_CLASSES = {
-            "albert": (AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer),
-            "auto": (AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer),
-            "bart": (BartConfig, BartForQuestionAnswering, BartTokenizer),
-            "bert": (BertConfig, BertForQuestionAnswering, BertTokenizer),
-            "camembert": (
-                CamembertConfig,
-                CamembertForQuestionAnswering,
-                CamembertTokenizer,
-            ),
-            "distilbert": (
-                DistilBertConfig,
-                DistilBertForQuestionAnswering,
-                DistilBertTokenizer,
-            ),
-            "electra": (ElectraConfig, ElectraForQuestionAnswering, ElectraTokenizer),
-            "longformer": (
-                LongformerConfig,
-                LongformerForQuestionAnswering,
-                LongformerTokenizer,
-            ),
-            "mobilebert": (
-                MobileBertConfig,
-                MobileBertForQuestionAnswering,
-                MobileBertTokenizer,
-            ),
-            "mpnet": (MPNetConfig, MPNetForQuestionAnswering, MPNetTokenizer),
-            "roberta": (RobertaConfig, RobertaForQuestionAnswering, RobertaTokenizer),
-            "squeezebert": (
-                SqueezeBertConfig,
-                SqueezeBertForQuestionAnswering,
-                SqueezeBertTokenizer,
-            ),
-            "xlm": (XLMConfig, XLMForQuestionAnswering, XLMTokenizer),
-            "xlmroberta": (
-                XLMRobertaConfig,
-                XLMRobertaForQuestionAnswering,
-                XLMRobertaTokenizer,
-            ),
-            "xlnet": (XLNetConfig, XLNetForQuestionAnswering, XLNetTokenizer),
-        }
-
         self.args = self._load_model_args(model_name)
 
         if isinstance(args, dict):
             self.args.update_from_dict(args)
-        elif isinstance(args, QuestionAnsweringArgs):
+        elif isinstance(args, T5Args):
             self.args = args
 
         self.is_sweeping = False
 
         if self.args.manual_seed:
             random.seed(self.args.manual_seed)
             np.random.seed(self.args.manual_seed)
             torch.manual_seed(self.args.manual_seed)
             if self.args.n_gpu > 0:
                 torch.cuda.manual_seed_all(self.args.manual_seed)
 
-        if not use_cuda:
-            self.args.fp16 = False
-
-        config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]
-        self.config = config_class.from_pretrained(model_name, **self.args.config)
-        if not self.args.quantized_model:
-            self.model = model_class.from_pretrained(
-                model_name, config=self.config, **kwargs
-            )
-        else:
-            quantized_weights = torch.load(
-                os.path.join(model_name, "pytorch_model.bin")
-            )
-            self.model = model_class.from_pretrained(
-                None, config=self.config, state_dict=quantized_weights
-            )
-
-        if self.args.dynamic_quantize:
-            self.model = torch.quantization.quantize_dynamic(
-                self.model, {torch.nn.Linear}, dtype=torch.qint8
-            )
-        if self.args.quantized_model:
-            self.model.load_state_dict(quantized_weights)
-        if self.args.dynamic_quantize:
-            self.args.quantized_model = True
-
         if use_cuda:
             if torch.cuda.is_available():
                 if cuda_device == -1:
                     self.device = torch.device("cuda")
                 else:
                     self.device = torch.device(f"cuda:{cuda_device}")
             else:
                 raise ValueError(
                     "'use_cuda' set to True when cuda is unavailable."
-                    " Make sure CUDA is available or set use_cuda=False."
+                    "Make sure CUDA is available or set `use_cuda=False`."
                 )
         else:
             self.device = "cpu"
         logger.debug(f"Device: {self.device}")
 
         self.results = {}
 
-        if self.args.fp16:
-            try:
-                from torch.cuda import amp
-            except AttributeError:
-                raise AttributeError(
-                    "fp16 requires Pytorch >= 1.6. Please update Pytorch or turn off fp16."
-                )
+        config_class, model_class = MODEL_CLASSES[model_type]
+
+        if model_name is None:
+            self.config = self.args.config
+            self.model = model_class(config=self.config)
+        else:
+            self.config = config_class.from_pretrained(model_name, **self.args.config)
+            self.model = model_class.from_pretrained(model_name, config=self.config)
 
-        if model_type == "auto":
-            self.tokenizer = tokenizer_class.from_pretrained(model_name, **kwargs)
+        if isinstance(tokenizer, T5Tokenizer):
+            self.tokenizer = tokenizer
+            self.model.resize_token_embeddings(len(self.tokenizer))
+        elif model_type == "byt5":
+            self.tokenizer = ByT5Tokenizer.from_pretrained(model_name, truncate=True)
         else:
-            self.tokenizer = tokenizer_class.from_pretrained(
-                model_name, do_lower_case=self.args.do_lower_case, **kwargs
+            self.tokenizer = T5Tokenizer.from_pretrained(model_name, truncate=True)
+
+        if self.args.dynamic_quantize:
+            self.model = torch.quantization.quantize_dynamic(
+                self.model, {torch.nn.Linear}, dtype=torch.qint8
             )
 
+        if not use_cuda:
+            self.args.fp16 = False
+
         if self.args.special_tokens_list:
             self.tokenizer.add_tokens(
                 self.args.special_tokens_list, special_tokens=True
             )
             self.model.resize_token_embeddings(len(self.tokenizer))
 
-        self.args.model_name = model_name
         self.args.model_type = model_type
+        if model_name is None:
+            self.args.model_name = "T5_from_scratch"
+        else:
+            self.args.model_name = model_name
 
-        self.wandb_run_id = None
         if self.args.wandb_project and not wandb_available:
             warnings.warn(
                 "wandb_project specified but wandb is not available. Wandb disabled."
             )
             self.args.wandb_project = None
 
-    def load_and_cache_examples(
-            self, examples, evaluate=False, no_cache=False, output_examples=False
-    ):
-        """
-        Converts a list of examples to a TensorDataset containing InputFeatures. Caches the InputFeatures.
-
-        Utility function for train() and eval() methods. Not intended to be used directly.
-        """
-
-        tokenizer = self.tokenizer
-        args = self.args
-
-        if not no_cache:
-            no_cache = args.no_cache
-
-        if not no_cache:
-            os.makedirs(self.args.cache_dir, exist_ok=True)
-
-        examples = get_examples(examples, is_training=not evaluate)
-
-        mode = "dev" if evaluate else "train"
-        cached_features_file = os.path.join(
-            args.cache_dir,
-            "cached_{}_{}_{}_{}".format(
-                mode, args.model_type, args.max_seq_length, len(examples)
-            ),
-        )
-
-        if os.path.exists(cached_features_file) and (
-                (not args.reprocess_input_data and not no_cache)
-                or (mode == "dev" and args.use_cached_eval_features)
-        ):
-            features = torch.load(cached_features_file)
-            logger.info(f" Features loaded from cache at {cached_features_file}")
-
-            # Convert to Tensors and build dataset
-            all_input_ids = torch.tensor(
-                [f.input_ids for f in features], dtype=torch.long
-            )
-            all_attention_masks = torch.tensor(
-                [f.attention_mask for f in features], dtype=torch.long
-            )
-            all_token_type_ids = torch.tensor(
-                [f.token_type_ids for f in features], dtype=torch.long
-            )
-            all_cls_index = torch.tensor(
-                [f.cls_index for f in features], dtype=torch.long
-            )
-            all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)
-            all_is_impossible = torch.tensor(
-                [f.is_impossible for f in features], dtype=torch.float
-            )
-
-            if mode == "dev":
-                all_feature_index = torch.arange(
-                    all_input_ids.size(0), dtype=torch.long
-                )
-                dataset = TensorDataset(
-                    all_input_ids,
-                    all_attention_masks,
-                    all_token_type_ids,
-                    all_feature_index,
-                    all_cls_index,
-                    all_p_mask,
-                )
-            else:
-                all_start_positions = torch.tensor(
-                    [f.start_position for f in features], dtype=torch.long
-                )
-                all_end_positions = torch.tensor(
-                    [f.end_position for f in features], dtype=torch.long
-                )
-                dataset = TensorDataset(
-                    all_input_ids,
-                    all_attention_masks,
-                    all_token_type_ids,
-                    all_start_positions,
-                    all_end_positions,
-                    all_cls_index,
-                    all_p_mask,
-                    all_is_impossible,
-                )
-        else:
-            logger.info(" Converting to features started.")
-
-            features, dataset = squad_convert_examples_to_features(
-                examples=examples,
-                tokenizer=tokenizer,
-                max_seq_length=args.max_seq_length,
-                doc_stride=args.doc_stride,
-                max_query_length=args.max_query_length,
-                is_training=not evaluate,
-                tqdm_enabled=not args.silent,
-                threads=args.process_count,
-                args=args,
-            )
-
-            if not no_cache:
-                torch.save(features, cached_features_file)
-
-        if output_examples:
-            return dataset, examples, features
-        return dataset
-
     def train_model(
             self,
             train_data,
-            output_dir=False,
+            output_dir=None,
             show_running_loss=True,
             args=None,
             eval_data=None,
             verbose=True,
             **kwargs,
     ):
         """
         Trains the model using 'train_data'
 
         Args:
-            train_data: Path to JSON file containing training data OR list of Python dicts in the correct format. The model will be trained on this data.
+            train_data: Pandas DataFrame containing the 3 columns - `prefix`, `input_text`, `target_text`.
+                        - `prefix`: A string indicating the task to perform. (E.g. `"question"`, `"stsb"`)
+                        - `input_text`: The input text sequence. `prefix` is automatically prepended to form the full input. (<prefix>: <input_text>)
+                        - `target_text`: The target sequence
             output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.
             show_running_loss (optional): Set to False to prevent running loss from being printed to console. Defaults to True.
             args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.
-            eval_data (optional): Path to JSON file containing evaluation data against which evaluation will be performed when evaluate_during_training is enabled.
-                Is required if evaluate_during_training is enabled.
+            eval_data (optional): A DataFrame against which evaluation will be performed when evaluate_during_training is enabled. Is required if evaluate_during_training is enabled.
             **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).
-                A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.
+                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs
+                        will be lists of strings. Note that this will slow down training significantly as the predicted sequences need to be generated.
+
         Returns:
             global_step: Number of global steps trained
             training_details: Average training loss if evaluate_during_training is False or full training progress scores if evaluate_during_training is True
         """  # noqa: ignore flake8"
 
         if args:
             self.args.update_from_dict(args)
-
-        if self.args.silent:
-            show_running_loss = False
-
         if self.args.evaluate_during_training and eval_data is None:
             raise ValueError(
                 "evaluate_during_training is enabled but eval_data is not specified."
                 " Pass eval_data to model.train_model() if using evaluate_during_training."
             )
 
         if not output_dir:
@@ -410,58 +204,40 @@
         if (
                 os.path.exists(output_dir)
                 and os.listdir(output_dir)
                 and not self.args.overwrite_output_dir
         ):
             raise ValueError(
                 "Output directory ({}) already exists and is not empty."
-                "Use --overwrite_output_dir to overcome.".format(output_dir)
+                " Set args.overwrite_output_dir = True to overcome.".format(output_dir)
             )
 
         self._move_model_to_device()
 
-        if self.args.use_hf_datasets:
-            train_dataset = load_hf_dataset(
-                train_data, self.tokenizer, self.args, is_training=True
-            )
-        elif self.args.lazy_loading:
-            if isinstance(train_data, str):
-                train_dataset = LazyQuestionAnsweringDataset(
-                    train_data, self.tokenizer, self.args
-                )
-            else:
-                raise ValueError(
-                    "Input must be given as a path to a file when using lazy loading"
-                )
-        else:
-            if isinstance(train_data, str):
-                with open(train_data, "r", encoding=self.args.encoding) as f:
-                    train_examples = json.load(f)
-            else:
-                train_examples = train_data
-
-            train_dataset = self.load_and_cache_examples(train_examples)
+        train_dataset = self.load_and_cache_examples(train_data, verbose=verbose)
 
         os.makedirs(output_dir, exist_ok=True)
 
         global_step, training_details = self.train(
             train_dataset,
             output_dir,
             show_running_loss=show_running_loss,
             eval_data=eval_data,
+            verbose=verbose,
             **kwargs,
         )
 
         self.save_model(model=self.model)
 
-        logger.info(
-            " Training of {} model complete. Saved to {}.".format(
-                self.args.model_type, output_dir
+        if verbose:
+            logger.info(
+                " Training of {} model complete. Saved to {}.".format(
+                    self.args.model_name, output_dir
+                )
             )
-        )
 
         return global_step, training_details
 
     def train(
             self,
             train_dataset,
             output_dir,
@@ -474,29 +250,38 @@
         Trains the model on train_dataset.
 
         Utility function to be used by the train_model() method. Not intended to be used directly.
         """
 
         model = self.model
         args = self.args
+        device = self.device
 
         tb_writer = SummaryWriter(log_dir=args.tensorboard_dir)
         train_sampler = RandomSampler(train_dataset)
         train_dataloader = DataLoader(
             train_dataset,
             sampler=train_sampler,
             batch_size=args.train_batch_size,
             num_workers=self.args.dataloader_num_workers,
         )
 
-        t_total = (
-                len(train_dataloader)
-                // args.gradient_accumulation_steps
-                * args.num_train_epochs
-        )
+        if args.max_steps > 0:
+            t_total = args.max_steps
+            args.num_train_epochs = (
+                    args.max_steps
+                    // (len(train_dataloader) // args.gradient_accumulation_steps)
+                    + 1
+            )
+        else:
+            t_total = (
+                    len(train_dataloader)
+                    // args.gradient_accumulation_steps
+                    * args.num_train_epochs
+            )
 
         no_decay = ["bias", "LayerNorm.weight"]
 
         optimizer_grouped_parameters = []
         custom_parameter_names = set()
         for group in self.args.custom_parameter_groups:
             params = group.pop("params")
@@ -623,17 +408,32 @@
                 lr_end=args.polynomial_decay_schedule_lr_end,
                 power=args.polynomial_decay_schedule_power,
             )
 
         else:
             raise ValueError("{} is not a valid scheduler.".format(args.scheduler))
 
+        if (
+                args.model_name
+                and os.path.isfile(os.path.join(args.model_name, "optimizer.pt"))
+                and os.path.isfile(os.path.join(args.model_name, "scheduler.pt"))
+        ):
+            # Load in optimizer and scheduler states
+            optimizer.load_state_dict(
+                torch.load(os.path.join(args.model_name, "optimizer.pt"))
+            )
+            scheduler.load_state_dict(
+                torch.load(os.path.join(args.model_name, "scheduler.pt"))
+            )
+
         if args.n_gpu > 1:
             model = torch.nn.DataParallel(model)
 
+        logger.info(" Training started")
+
         global_step = 0
         training_progress_scores = None
         tr_loss, logging_loss = 0.0, 0.0
         model.zero_grad()
         train_iterator = trange(
             int(args.num_train_epochs), desc="Epoch", disable=args.silent, mininterval=0
         )
@@ -641,15 +441,15 @@
         best_eval_metric = None
         early_stopping_counter = 0
         steps_trained_in_current_epoch = 0
         epochs_trained = 0
 
         if args.model_name and os.path.exists(args.model_name):
             try:
-                # set global_step to global_step of last saved checkpoint from model path
+                # set global_step to gobal_step of last saved checkpoint from model path
                 checkpoint_suffix = args.model_name.split("/")[-1].split("-")
                 if len(checkpoint_suffix) > 2:
                     checkpoint_suffix = checkpoint_suffix[1]
                 else:
                     checkpoint_suffix = checkpoint_suffix[-1]
                 global_step = int(checkpoint_suffix)
                 epochs_trained = global_step // (
@@ -685,15 +485,15 @@
             self.wandb_run_id = wandb.run.id
 
         if args.fp16:
             from torch.cuda import amp
 
             scaler = amp.GradScaler()
 
-        for _ in train_iterator:
+        for current_epoch in train_iterator:
             model.train()
             if epochs_trained > 0:
                 epochs_trained -= 1
                 continue
             train_iterator.set_description(
                 f"Epoch {epoch_number + 1} of {args.num_train_epochs}"
             )
@@ -788,15 +588,20 @@
                         )
 
                     if args.evaluate_during_training and (
                             args.evaluate_during_training_steps > 0
                             and global_step % args.evaluate_during_training_steps == 0
                     ):
                         # Only evaluate when single GPU otherwise metrics may not average well
-                        results, _ = self.eval_model(eval_data, verbose=False, **kwargs)
+                        results = self.eval_model(
+                            eval_data,
+                            verbose=verbose and args.evaluate_during_training_verbose,
+                            silent=args.evaluate_during_training_silent,
+                            **kwargs,
+                        )
                         for key, value in results.items():
                             try:
                                 tb_writer.add_scalar(
                                     "eval_{}".format(key), value, global_step
                                 )
                             except (NotImplementedError, AssertionError):
                                 pass
@@ -933,20 +738,26 @@
                 output_dir, "checkpoint-{}-epoch-{}".format(global_step, epoch_number)
             )
 
             if args.save_model_every_epoch:
                 self.save_model(output_dir_current, optimizer, scheduler, model=model)
 
             if args.evaluate_during_training and args.evaluate_each_epoch:
-                results, _ = self.eval_model(eval_data, verbose=False, **kwargs)
-
-                self.save_model(
-                    output_dir_current, optimizer, scheduler, results=results
+                results = self.eval_model(
+                    eval_data,
+                    verbose=verbose and args.evaluate_during_training_verbose,
+                    silent=args.evaluate_during_training_silent,
+                    **kwargs,
                 )
 
+                if args.save_eval_checkpoints:
+                    self.save_model(
+                        output_dir_current, optimizer, scheduler, results=results
+                    )
+
                 training_progress_scores["global_step"].append(global_step)
                 training_progress_scores["train_loss"].append(current_loss)
                 for key in results:
                     training_progress_scores[key].append(results[key])
                 report = pd.DataFrame(training_progress_scores)
                 report.to_csv(
                     os.path.join(args.output_dir, "training_progress_scores.csv"),
@@ -1058,476 +869,323 @@
             global_step,
             tr_loss / global_step
             if not self.args.evaluate_during_training
             else training_progress_scores,
         )
 
     def eval_model(
-            self, eval_data, output_dir=None, verbose=False, verbose_logging=False, **kwargs
+            self, eval_data, output_dir=None, verbose=True, silent=False, **kwargs
     ):
         """
         Evaluates the model on eval_data. Saves results to output_dir.
 
         Args:
-            eval_data: Path to JSON file containing evaluation data OR list of Python dicts in the correct format. The model will be evaluated on this data.
+            eval_data: Pandas DataFrame containing the 3 columns - `prefix`, `input_text`, `target_text`.
+                        - `prefix`: A string indicating the task to perform. (E.g. `"question"`, `"stsb"`)
+                        - `input_text`: The input text sequence. `prefix` is automatically prepended to form the full input. (<prefix>: <input_text>)
+                        - `target_text`: The target sequence
             output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.
             verbose: If verbose, results will be printed to the console on completion of evaluation.
-            verbose_logging: Log info related to feature conversion and writing predictions.
+            silent: If silent, tqdm progress bars will be hidden.
             **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).
-                A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.
-
+                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs
+                        will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.
         Returns:
-            result: Dictionary containing evaluation results. (correct, similar, incorrect)
-            text: A dictionary containing the 3 dictionaries correct_text, similar_text (the predicted answer is a substring of the correct answer or vise versa), incorrect_text.
+            results: Dictionary containing evaluation results.
         """  # noqa: ignore flake8"
 
         if not output_dir:
             output_dir = self.args.output_dir
 
         self._move_model_to_device()
 
-        all_predictions, all_nbest_json, scores_diff_json, eval_loss = self.evaluate(
-            eval_data, output_dir, verbose_logging=verbose
+        eval_dataset = self.load_and_cache_examples(
+            eval_data, evaluate=True, verbose=verbose, silent=silent
         )
+        os.makedirs(output_dir, exist_ok=True)
 
-        if isinstance(eval_data, str):
-            with open(eval_data, "r", encoding=self.args.encoding) as f:
-                truth = json.load(f)
-        else:
-            truth = eval_data
+        result = self.evaluate(
+            eval_dataset, output_dir, verbose=verbose, silent=silent, **kwargs
+        )
+        self.results.update(result)
 
-        result, texts = self.calculate_results(truth, all_predictions, **kwargs)
-        result["eval_loss"] = eval_loss
+        if self.args.evaluate_generated_text:
+            if self.args.preprocess_inputs:
+                to_predict = [
+                    prefix + ": " + input_text
+                    for prefix, input_text in zip(
+                        eval_data["prefix"], eval_data["input_text"]
+                    )
+                ]
+            else:
+                to_predict = [
+                    prefix + input_text
+                    for prefix, input_text in zip(
+                        eval_data["prefix"], eval_data["input_text"]
+                    )
+                ]
+            preds = self.predict(to_predict)
 
-        self.results.update(result)
+            result = self.compute_metrics(
+                eval_data["target_text"].tolist(), preds, **kwargs
+            )
+            self.results.update(result)
 
         if verbose:
             logger.info(self.results)
 
-        return result, texts
+        return self.results
 
-    def evaluate(self, eval_data, output_dir, verbose_logging=False):
+    def evaluate(self, eval_dataset, output_dir, verbose=True, silent=False, **kwargs):
         """
-        Evaluates the model on eval_data.
+        Evaluates the model on eval_dataset.
 
         Utility function to be used by the eval_model() method. Not intended to be used directly.
         """
-        tokenizer = self.tokenizer
+
         model = self.model
         args = self.args
+        eval_output_dir = output_dir
+        device = self.device
 
-        if isinstance(eval_data, str):
-            with open(eval_data, "r", encoding=self.args.encoding) as f:
-                eval_examples = json.load(f)
-        else:
-            eval_examples = eval_data
-
-        eval_dataset, examples, features = self.load_and_cache_examples(
-            eval_examples, evaluate=True, output_examples=True
-        )
+        results = {}
 
         eval_sampler = SequentialSampler(eval_dataset)
         eval_dataloader = DataLoader(
             eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size
         )
 
+        if args.n_gpu > 1:
+            model = torch.nn.DataParallel(model)
+
         eval_loss = 0.0
         nb_eval_steps = 0
         model.eval()
 
-        if args.n_gpu > 1:
-            model = torch.nn.DataParallel(model)
-
         if self.args.fp16:
             from torch.cuda import amp
 
-        all_results = []
         for batch in tqdm(
-                eval_dataloader, disable=args.silent, desc="Running Evaluation"
+                eval_dataloader, disable=args.silent or silent, desc="Running Evaluation"
         ):
-            batch = tuple(t.to(self.device) for t in batch)
-
+            inputs = self._get_inputs_dict(batch)
             with torch.no_grad():
-                inputs = {
-                    "input_ids": batch[0],
-                    "attention_mask": batch[1],
-                    "token_type_ids": batch[2],
-                }
-
-                if self.args.model_type in [
-                    "xlm",
-                    "roberta",
-                    "distilbert",
-                    "camembert",
-                    "electra",
-                    "xlmroberta",
-                    "bart",
-                ]:
-                    del inputs["token_type_ids"]
-
-                example_indices = batch[3]
-
-                if args.model_type in ["xlnet", "xlm"]:
-                    inputs.update({"cls_index": batch[4], "p_mask": batch[5]})
-
                 if self.args.fp16:
                     with amp.autocast():
                         outputs = model(**inputs)
-                        eval_loss += outputs[0].mean().item()
+                        loss = outputs[0]
                 else:
                     outputs = model(**inputs)
-                    eval_loss += outputs[0].mean().item()
-
-                for i, example_index in enumerate(example_indices):
-                    eval_feature = features[example_index.item()]
-                    unique_id = int(eval_feature.unique_id)
-                    if args.model_type in ["xlnet", "xlm"]:
-                        # XLNet uses a more complex post-processing procedure
-                        result = RawResultExtended(
-                            unique_id=unique_id,
-                            start_top_log_probs=to_list(outputs[0][i]),
-                            start_top_index=to_list(outputs[1][i]),
-                            end_top_log_probs=to_list(outputs[2][i]),
-                            end_top_index=to_list(outputs[3][i]),
-                            cls_logits=to_list(outputs[4][i]),
-                        )
-                    else:
-                        result = RawResult(
-                            unique_id=unique_id,
-                            start_logits=to_list(outputs[0][i]),
-                            end_logits=to_list(outputs[1][i]),
-                        )
-                    all_results.append(result)
-
+                    loss = outputs[0]
+                if self.args.n_gpu > 1:
+                    loss = loss.mean()
+                eval_loss += loss.item()
             nb_eval_steps += 1
 
         eval_loss = eval_loss / nb_eval_steps
 
-        prefix = "test"
-        os.makedirs(output_dir, exist_ok=True)
-
-        output_prediction_file = os.path.join(
-            output_dir, "predictions_{}.json".format(prefix)
-        )
-        output_nbest_file = os.path.join(
-            output_dir, "nbest_predictions_{}.json".format(prefix)
-        )
-        output_null_log_odds_file = os.path.join(
-            output_dir, "null_odds_{}.json".format(prefix)
-        )
+        results["eval_loss"] = eval_loss
 
-        if args.model_type in ["xlnet", "xlm"]:
-            # XLNet uses a more complex post-processing procedure
-            (
-                all_predictions,
-                all_nbest_json,
-                scores_diff_json,
-            ) = write_predictions_extended(
-                examples,
-                features,
-                all_results,
-                args.n_best_size,
-                args.max_answer_length,
-                output_prediction_file,
-                output_nbest_file,
-                output_null_log_odds_file,
-                eval_data,
-                model.config.start_n_top,
-                model.config.end_n_top,
-                True,
-                tokenizer,
-                verbose_logging,
-            )
-        else:
-            all_predictions, all_nbest_json, scores_diff_json = write_predictions(
-                examples,
-                features,
-                all_results,
-                args.n_best_size,
-                args.max_answer_length,
-                False,
-                output_prediction_file,
-                output_nbest_file,
-                output_null_log_odds_file,
-                verbose_logging,
-                True,
-                args.null_score_diff_threshold,
-            )
+        output_eval_file = os.path.join(eval_output_dir, "eval_results.txt")
+        with open(output_eval_file, "w") as writer:
+            for key in sorted(results.keys()):
+                writer.write("{} = {}\n".format(key, str(results[key])))
 
-        return all_predictions, all_nbest_json, scores_diff_json, eval_loss
+        return results
 
-    def predict(self, to_predict, n_best_size=None):
+    def predict(self, to_predict, split_on_space=False):
         """
-        Performs predictions on a list of python dicts containing contexts and qas.
+        Performs predictions on a list of text.
 
         Args:
-            to_predict: A python list of python dicts containing contexts and questions to be sent to the model for prediction.
-                        E.g: predict([
-                            {
-                                'context': "Some context as a demo",
-                                'qas': [
-                                    {'id': '0', 'question': 'What is the context here?'},
-                                    {'id': '1', 'question': 'What is this for?'}
-                                ]
-                            }
-                        ])
-            n_best_size (Optional): Number of predictions to return. args.n_best_size will be used if not specified.
+            to_predict: A python list of text (str) to be sent to the model for prediction. Note that the prefix should be prepended to the text.
+            split_on_space (optional): If True, input is english string, if False, input is chinese string.
 
         Returns:
-            list: A python list  of dicts containing the predicted answer/answers, and id for each question in to_predict.
-            list: A python list  of dicts containing the predicted probability/probabilities, and id for each question in to_predict.
+            preds: A python list of the generated sequences.
         """  # noqa: ignore flake8"
-        tokenizer = self.tokenizer
-        device = self.device
-        model = self.model
-        args = self.args
-
-        if not n_best_size:
-            n_best_size = args.n_best_size
 
         self._move_model_to_device()
 
-        eval_examples = build_examples(to_predict)
-        eval_dataset, examples, features = self.load_and_cache_examples(
-            eval_examples, evaluate=True, output_examples=True, no_cache=True
-        )
-
-        eval_sampler = SequentialSampler(eval_dataset)
-        eval_dataloader = DataLoader(
-            eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size
-        )
-
-        model.eval()
-
-        if args.n_gpu > 1:
-            model = torch.nn.DataParallel(model)
-
-        if self.args.fp16:
-            from torch.cuda import amp
-
-        all_results = []
+        all_outputs = []
+        # Batching
         for batch in tqdm(
-                eval_dataloader, disable=args.silent, desc="Running Prediction"
+                [
+                    to_predict[i: i + self.args.eval_batch_size]
+                    for i in range(0, len(to_predict), self.args.eval_batch_size)
+                ],
+                desc="Generating outputs",
+                disable=self.args.silent,
         ):
-            batch = tuple(t.to(self.device) for t in batch)
-
-            with torch.no_grad():
-                inputs = {
-                    "input_ids": batch[0],
-                    "attention_mask": batch[1],
-                    "token_type_ids": batch[2],
-                }
-
-                if self.args.model_type in [
-                    "xlm",
-                    "roberta",
-                    "distilbert",
-                    "camembert",
-                    "electra",
-                    "xlmroberta",
-                    "bart",
-                ]:
-                    del inputs["token_type_ids"]
-
-                example_indices = batch[3]
-
-                if args.model_type in ["xlnet", "xlm"]:
-                    inputs.update({"cls_index": batch[4], "p_mask": batch[5]})
-
-                if self.args.fp16:
-                    with amp.autocast():
-                        outputs = model(**inputs)
+            input_batch = self.tokenizer.prepare_seq2seq_batch(
+                src_texts=batch,
+                max_length=self.args.max_seq_length,
+                padding="max_length",
+                return_tensors="pt",
+                truncation=True,
+            )
+            input_ids = input_batch["input_ids"]
+            attention_mask = input_batch["attention_mask"]
+
+            input_ids = input_ids.to(self.device)
+            attention_mask = attention_mask.to(self.device)
+
+            outputs = self.model.generate(
+                input_ids=input_ids,
+                attention_mask=attention_mask,
+                num_beams=self.args.num_beams,
+                max_length=self.args.max_length,
+                length_penalty=self.args.length_penalty,
+                early_stopping=self.args.early_stopping,
+                repetition_penalty=self.args.repetition_penalty,
+                do_sample=self.args.do_sample,
+                top_k=self.args.top_k,
+                top_p=self.args.top_p,
+                num_return_sequences=self.args.num_return_sequences,
+            )
+            all_outputs.extend(outputs.cpu().numpy())
+
+        if self.args.use_multiprocessed_decoding:
+            self.model.to("cpu")
+            with Pool(self.args.process_count) as p:
+                if self.args.multiprocessing_chunksize == -1:
+                    chunksize = max(
+                        len(all_outputs) // (self.args.process_count * 2), 500
+                    )
                 else:
-                    outputs = model(**inputs)
-
-                for i, example_index in enumerate(example_indices):
-                    eval_feature = features[example_index.item()]
-                    unique_id = int(eval_feature.unique_id)
-                    if args.model_type in ["xlnet", "xlm"]:
-                        # XLNet uses a more complex post-processing procedure
-                        result = RawResultExtended(
-                            unique_id=unique_id,
-                            start_top_log_probs=to_list(outputs[0][i]),
-                            start_top_index=to_list(outputs[1][i]),
-                            end_top_log_probs=to_list(outputs[2][i]),
-                            end_top_index=to_list(outputs[3][i]),
-                            cls_logits=to_list(outputs[4][i]),
-                        )
-                    else:
-                        result = RawResult(
-                            unique_id=unique_id,
-                            start_logits=to_list(outputs[0][i]),
-                            end_logits=to_list(outputs[1][i]),
-                        )
-                    all_results.append(result)
-
-        if args.model_type in ["xlnet", "xlm"]:
-            answers = get_best_predictions_extended(
-                examples,
-                features,
-                all_results,
-                n_best_size,
-                args.max_answer_length,
-                model.config.start_n_top,
-                model.config.end_n_top,
-                True,
-                tokenizer,
-                args.null_score_diff_threshold,
-            )
+                    chunksize = self.args.multiprocessing_chunksize
+                outputs = list(
+                    tqdm(
+                        p.imap(self._decode, all_outputs, chunksize=chunksize),
+                        total=len(all_outputs),
+                        desc="Decoding outputs",
+                        disable=self.args.silent,
+                    )
+                )
+            self._move_model_to_device()
         else:
-            answers = get_best_predictions(
-                examples,
-                features,
-                all_results,
-                n_best_size,
-                args.max_answer_length,
-                False,
-                False,
-                True,
-                False,
-            )
-
-        answer_list = [
-            {"id": answer["id"], "answer": answer["answer"][:-1]} for answer in answers
-        ]
-        probability_list = [
-            {"id": answer["id"], "probability": answer["probability"][:-1]}
-            for answer in answers
-        ]
-
-        return answer_list, probability_list
-
-    def calculate_results(self, truth, predictions, **kwargs):
-        truth_dict = {}
-        questions_dict = {}
-        for item in truth:
-            for answer in item["qas"]:
-                if answer["answers"]:
-                    truth_dict[answer["id"]] = answer["answers"][0]["text"]
-                else:
-                    truth_dict[answer["id"]] = ""
-                questions_dict[answer["id"]] = answer["question"]
+            outputs = [
+                self.tokenizer.decode(
+                    output_id,
+                    skip_special_tokens=self.args.skip_special_tokens,
+                    clean_up_tokenization_spaces=True,
+                )
+                for output_id in all_outputs
+            ]
+        if not split_on_space:
+            outputs = [''.join(gen_text.split(' ')) for gen_text in outputs]
+        if self.args.num_return_sequences > 1:
+            return [
+                outputs[i: i + self.args.num_return_sequences]
+                for i in range(0, len(outputs), self.args.num_return_sequences)
+            ]
+        else:
+            return outputs
 
-        correct = 0
-        incorrect = 0
-        similar = 0
-        correct_text = {}
-        incorrect_text = {}
-        similar_text = {}
-        predicted_answers = []
-        true_answers = []
-
-        for q_id, answer in truth_dict.items():
-            predicted_answers.append(predictions[q_id])
-            true_answers.append(answer)
-            if predictions[q_id].strip() == answer.strip():
-                correct += 1
-                correct_text[q_id] = answer
-            elif (
-                    predictions[q_id].strip() in answer.strip()
-                    or answer.strip() in predictions[q_id].strip()
-            ):
-                similar += 1
-                similar_text[q_id] = {
-                    "truth": answer,
-                    "predicted": predictions[q_id],
-                    "question": questions_dict[q_id],
-                }
-            else:
-                incorrect += 1
-                incorrect_text[q_id] = {
-                    "truth": answer,
-                    "predicted": predictions[q_id],
-                    "question": questions_dict[q_id],
-                }
+    def _decode(self, output_id):
+        return self.tokenizer.decode(
+            output_id,
+            skip_special_tokens=self.args.skip_special_tokens,
+            clean_up_tokenization_spaces=True,
+        )
 
-        extra_metrics = {}
-        for metric, func in kwargs.items():
-            extra_metrics[metric] = func(true_answers, predicted_answers)
+    def compute_metrics(self, labels, preds, **kwargs):
+        """
+        Computes the evaluation metrics for the model predictions.
 
-        result = {
-            "correct": correct,
-            "similar": similar,
-            "incorrect": incorrect,
-            **extra_metrics,
-        }
+        Args:
+            labels: List of target sequences
+            preds: List of model generated outputs
+            **kwargs: Custom metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).
+                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs
+                        will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.
 
-        texts = {
-            "correct_text": correct_text,
-            "similar_text": similar_text,
-            "incorrect_text": incorrect_text,
-        }
+        Returns:
+            result: Dictionary containing evaluation results.
+        """  # noqa: ignore flake8"
+        assert len(labels) == len(preds)
+
+        results = {}
+        for metric, func in kwargs.items():
+            results[metric] = func(labels, preds)
 
-        return result, texts
+        return results
 
     def _move_model_to_device(self):
         self.model.to(self.device)
 
-    def _get_last_metrics(self, metric_values):
-        return {metric: values[-1] for metric, values in metric_values.items()}
-
     def _get_inputs_dict(self, batch):
         if self.args.use_hf_datasets:
-            inputs = {key: value.to(self.device) for key, value in batch.items()}
-
-            if self.args.model_type in [
-                "xlm",
-                "roberta",
-                "distilbert",
-                "camembert",
-                "electra",
-                "xlmroberta",
-                "bart",
-            ]:
-                del inputs["token_type_ids"]
-            if self.args.model_type not in ["xlnet", "xlm"]:
-                del inputs["cls_index"]
-                del inputs["p_mask"]
+            inputs = {**batch, "labels": batch["input_ids"]}
 
-            return inputs
+            return {key: value.to(self.device) for key, value in inputs.items()}
         else:
             batch = tuple(t.to(self.device) for t in batch)
+
+            input_ids = batch[0]
+            attention_mask = batch[1]
+            labels = batch[2]
+            labels[labels == self.tokenizer.pad_token_id] = -100
+
             inputs = {
-                "input_ids": batch[0],
-                "attention_mask": batch[1],
-                "token_type_ids": batch[2],
-                "start_positions": batch[3],
-                "end_positions": batch[4],
+                "input_ids": input_ids,
+                "attention_mask": attention_mask,
+                "labels": labels,
             }
 
-            if self.args.model_type in [
-                "xlm",
-                "roberta",
-                "distilbert",
-                "camembert",
-                "electra",
-                "xlmroberta",
-                "bart",
-            ]:
-                del inputs["token_type_ids"]
+            return inputs
 
-            if self.args.model_type in ["xlnet", "xlm"]:
-                inputs.update({"cls_index": batch[5], "p_mask": batch[6]})
+    def load_and_cache_examples(
+            self, data, evaluate=False, no_cache=False, verbose=True, silent=False
+    ):
+        """
+        Creates a T5Dataset from data.
 
-            return inputs
+        Utility function for train() and eval() methods. Not intended to be used directly.
+        """
+
+        tokenizer = self.tokenizer
+        args = self.args
+
+        if not no_cache:
+            no_cache = args.no_cache
+
+        if not no_cache:
+            os.makedirs(self.args.cache_dir, exist_ok=True)
+
+        mode = "dev" if evaluate else "train"
+
+        if self.args.use_hf_datasets:
+            dataset = load_hf_dataset(data, tokenizer, self.args)
+            return dataset
+        elif args.dataset_class:
+            CustomDataset = args.dataset_class
+            return CustomDataset(tokenizer, args, data, mode)
+        else:
+            return T5Dataset(
+                tokenizer,
+                self.args,
+                data,
+                mode,
+            )
 
     def _create_training_progress_scores(self, **kwargs):
         extra_metrics = {key: [] for key in kwargs}
         training_progress_scores = {
             "global_step": [],
-            "correct": [],
-            "similar": [],
-            "incorrect": [],
-            "train_loss": [],
             "eval_loss": [],
+            "train_loss": [],
             **extra_metrics,
         }
 
         return training_progress_scores
 
+    def _get_last_metrics(self, metric_values):
+        return {metric: values[-1] for metric, values in metric_values.items()}
+
     def save_model(
             self, output_dir=None, optimizer=None, scheduler=None, model=None, results=None
     ):
         if not output_dir:
             output_dir = self.args.output_dir
         os.makedirs(output_dir, exist_ok=True)
 
@@ -1553,13 +1211,13 @@
                     writer.write("{} = {}\n".format(key, str(results[key])))
 
     def save_model_args(self, output_dir):
         os.makedirs(output_dir, exist_ok=True)
         self.args.save(output_dir)
 
     def _load_model_args(self, input_dir):
-        args = QuestionAnsweringArgs()
+        args = T5Args()
         args.load(input_dir)
         return args
 
     def get_named_parameters(self):
         return [n for n, p in self.model.named_parameters()]
```

### Comparing `textgen-0.1.7/textgen/seq2seq/bart_seq2seq_model.py` & `textgen-0.1.8/textgen/seq2seq/bart_seq2seq_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/seq2seq/bart_seq2seq_utils.py` & `textgen-0.1.8/textgen/seq2seq/bart_seq2seq_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/seq2seq/conv_seq2seq_model.py` & `textgen-0.1.8/textgen/seq2seq/conv_seq2seq_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/seq2seq/data_reader.py` & `textgen-0.1.8/textgen/seq2seq/data_reader.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/seq2seq/seq2seq_model.py` & `textgen-0.1.8/textgen/seq2seq/seq2seq_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/seq2seq/seq2seq_trainer.py` & `textgen-0.1.8/textgen/seq2seq/seq2seq_trainer.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/t5/copyt5_model.py` & `textgen-0.1.8/textgen/t5/copyt5_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/t5/copyt5_utils.py` & `textgen-0.1.8/textgen/t5/copyt5_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/t5/t5_utils.py` & `textgen-0.1.8/textgen/t5/t5_utils.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/unsup_generation/tgls_model.py` & `textgen-0.1.8/textgen/unsup_generation/tgls_model.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen/unsup_generation/tgls_util.py` & `textgen-0.1.8/textgen/unsup_generation/tgls_util.py`

 * *Files identical despite different names*

### Comparing `textgen-0.1.7/textgen.egg-info/PKG-INFO` & `textgen-0.1.8/textgen.egg-info/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: textgen
-Version: 0.1.7
+Version: 0.1.8
 Summary: Text Generation Model
 Home-page: https://github.com/shibing624/textgen
 Author: XuMing
 Author-email: xuming624@qq.com
 License: Apache 2.0
 Description: [![PyPI version](https://badge.fury.io/py/textgen.svg)](https://badge.fury.io/py/textgen)
         [![Downloads](https://pepy.tech/badge/textgen)](https://pepy.tech/project/textgen)
@@ -19,56 +19,62 @@
         
         ğŸŒˆ Implementation of Text Generation models.
         
         **textgen**å®ç°äº†å¤šç§æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼šUDAã€GPT2ã€Seq2Seqã€BARTã€T5ã€SongNetç­‰æ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ã€‚
         
         **Guide**
         
-        - [Question](#Question)
-        - [Solution](#Solution)
         - [Feature](#Feature)
         - [Install](#install)
         - [Usage](#usage)
         - [Contact](#Contact)
         - [Reference](#reference)
         
-        # Question
-        
-        æ–‡æœ¬ç”Ÿæˆï¼Œæ–‡æœ¬æ•°æ®å¢å¼ºæ€ä¹ˆåšï¼Ÿ
-        
-        # Solution
-        ## æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
+        # Feature
+        ## æ–‡æœ¬ç”Ÿæˆ
         
-        1. Seq2Seqã€ConvSeq2Seqã€BART
-        2. GPT2ã€SongNet
-        3. T5ã€CopyT5
+        1. seq2seq: Seq2Seqã€ConvSeq2Seqã€BART
+        2. language_modeling: GPT2ã€SongNet
+        3. t5: T5ã€CopyT5
+        4. question_answering: BERTã€XLNet
+        5. chatglm: ChatGLM
         
         ## æ–‡æœ¬æ‰©å¢
         ### è¯ç²’åº¦æ‰©å¢
         1. UDAï¼Œéæ ¸å¿ƒè¯æ›¿æ¢
         2. EDAï¼Œç®€å•æ•°æ®å¢å¼ºæŠ€æœ¯ï¼šç›¸ä¼¼è¯ã€åŒä¹‰è¯æ›¿æ¢ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢
         
         ### å¥ç²’åº¦æ‰©å¢
         1. å›è¯‘ï¼ˆBT, Back Translateï¼‰ï¼šä¸­æ–‡-è‹±æ–‡-ä¸­æ–‡
         2. GPT2æ¨¡å‹ç»­å†™ï¼šçŸ­æ–‡æœ¬->é•¿æ–‡æœ¬
         3. BARTæ‘˜è¦æ¨¡å‹ï¼šé•¿æ–‡æœ¬->çŸ­æ–‡æœ¬
         4. TGLSï¼šæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
         
-        
-        # Feature
-        
+        ## åŠŸèƒ½åˆ—è¡¨
         - [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)ç®—æ³•å’ŒEDAç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
         - [BT(å›è¯‘)](textgen/augment/sentence_level_augment.py)ï¼šæœ¬é¡¹ç›®åŸºäºç™¾åº¦ç¿»è¯‘APIå®ç°äº†å›è¯‘åŠŸèƒ½ï¼Œå…ˆæŠŠä¸­æ–‡å¥å­ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œå†æŠŠè‹±æ–‡ç¿»è¯‘ä¸ºæ–°çš„ä¸­æ–‡
         - [Seq2Seq](textgen/seq2seq)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†Seq2Seqã€ConvSeq2Seqã€BARTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [T5](textgen/t5)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†T5å’ŒCopyT5æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€å¯¹è”ç”Ÿæˆã€æ–‡æ¡ˆæ’°å†™ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [GPT2](textgen/language_modeling)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†GTP2æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡ç« ç”Ÿæˆã€å¯¹è”ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [SongNet](textgen/language_modeling/songnet_model.py)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†SongNetæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºè§„èŒƒæ ¼å¼çš„è¯—è¯ã€æ­Œè¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
         - [TGLS](textgen/unsup_generation)ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬
         
         
+        ## Release Models
+        releaseåŸºäº`textgen`è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å·²ç»releaseåˆ°HuggingFace modelsï¼ŒæŒ‡å®šæ¨¡å‹åç§°`textgen`ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
+        
+        |Model|Arch|Intro|Training|Inference|
+        |:-- |:--- |:--- |:--- |:--- |
+        |[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
+        |[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
+        |[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
+        |[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
+        |[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
+        
+        
         # Demo
         
         HuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate
         
         ![](docs/hf.png)
         
         run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:
@@ -260,22 +266,14 @@
         output:
         ```shell
         inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
         outputs: ['äººå·¥æ™ºèƒ½æœ‰ä¸¤ä¸ªå¹¿ä¹‰çš„å®šä¹‰,ä»»ä½•æ‹Ÿäººçš„æœºæ¢°,å¦‚åœ¨å¡é›·å°”capeks', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨Python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Šå·¥ä½œ!', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦']
         ```
         
         
-        ### T5 æ¨¡å‹åº”ç”¨
-        
-        releaseåŸºäºT5çš„fine-tunedåçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å…¨éƒ¨releaseåˆ°HuggingFace modelsï¼Œ`textgen`å¯è‡ªåŠ¨ä¸‹è½½ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
-        
-        |Model|Arch|Intro|Training|Inference|
-        |:-- |:--- |:--- |:--- |:--- |
-        |[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
-        |[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
         
         
         ## GPT2 æ¨¡å‹
         
         ### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ
         
         ä½¿ç”¨ä¸­æ–‡æ•°æ®é›†ï¼ˆæ®µè½æ ¼å¼ï¼Œ`\n`é—´éš”ï¼‰ï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè¯—æ­Œç”Ÿæˆã€æ–‡ç« ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
@@ -300,23 +298,14 @@
         ## SongNet æ¨¡å‹
         
         æ ¼å¼æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œpaperè§[SongNet: Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022)ï¼Œ
         é€‚ç”¨äºå¼ºéŸµå¾‹æ ¼å¼è¦æ±‚çš„è¯—æ­Œã€å¯¹è”ã€æ­Œè¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
         
         example: [examples/language_generation/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)
         
-        ### SongNet æ¨¡å‹åº”ç”¨
-        
-        releaseåŸºäºSongNetçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å…¨éƒ¨releaseåˆ°HuggingFace modelsï¼Œ`textgen`å¯è‡ªåŠ¨ä¸‹è½½ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
-        
-        |Model|Arch|Intro|Training|Inference|
-        |:-- |:--- |:--- |:--- |:--- |
-        |[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
-        |[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
-        |[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
         
         
         ## Keyword Text Augmentation(EDA/UDA)
         
         example: [examples/text_augmentation_demo.py](examples/text_augmentation_demo.py)
         
         ```python
@@ -434,14 +423,28 @@
         - Issue(å»ºè®®)
           ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
         - é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
         - å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚
         
         <img src="docs/wechat.jpeg" width="200" />
         
+        
+        # Citation
+        
+        å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†textgenï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š
+        
+        ```latex
+        @misc{textgen,
+          title={textgen: Text Generation Tool},
+          author={Xu Ming},
+          year={2021},
+          howpublished={\url{https://github.com/shibing624/textgen}},
+        }
+        ```
+        
         # License
         
         æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ textgençš„é“¾æ¥å’Œæˆæƒåè®®ã€‚
         
         # Contribute
         
         é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š
```

### Comparing `textgen-0.1.7/textgen.egg-info/SOURCES.txt` & `textgen-0.1.8/textgen.egg-info/SOURCES.txt`

 * *Files 8% similar despite different names*

```diff
@@ -11,14 +11,17 @@
 textgen/augment/__init__.py
 textgen/augment/sentence_level_augment.py
 textgen/augment/text_augment.py
 textgen/augment/tokenizer.py
 textgen/augment/translate_api.py
 textgen/augment/word_level_augment.py
 textgen/augment/word_vocab.py
+textgen/chatglm/__init__.py
+textgen/chatglm/chatglm_model.py
+textgen/chatglm/chatglm_utils.py
 textgen/config/__init__.py
 textgen/config/global_args.py
 textgen/config/model_args.py
 textgen/custom_models/__init__.py
 textgen/custom_models/models.py
 textgen/data/HowNetPOSWord.txt
 textgen/data/stopwords.txt
@@ -26,17 +29,14 @@
 textgen/language_generation/language_generation_model.py
 textgen/language_generation/language_generation_utils.py
 textgen/language_modeling/__init__.py
 textgen/language_modeling/language_modeling_model.py
 textgen/language_modeling/language_modeling_utils.py
 textgen/language_modeling/songnet_model.py
 textgen/language_modeling/songnet_utils.py
-textgen/question_answering/__init__.py
-textgen/question_answering/question_answering_model.py
-textgen/question_answering/question_answering_utils.py
 textgen/seq2seq/__init__.py
 textgen/seq2seq/bart_seq2seq_model.py
 textgen/seq2seq/bart_seq2seq_utils.py
 textgen/seq2seq/conv_seq2seq_model.py
 textgen/seq2seq/data_reader.py
 textgen/seq2seq/seq2seq_model.py
 textgen/seq2seq/seq2seq_trainer.py
```

