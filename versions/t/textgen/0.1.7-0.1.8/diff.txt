--- tmp/textgen-0.1.7.tar.gz
+++ tmp/textgen-0.1.8.tar.gz
â”œâ”€â”€ filetype from file(1)
â”‚ @@ -1 +1 @@
â”‚ -gzip compressed data, was "textgen-0.1.7.tar", last modified: Mon Nov 28 04:00:54 2022, max compression
â”‚ +gzip compressed data, was "textgen-0.1.8.tar", last modified: Fri Apr  7 09:20:39 2023, max compression
â”‚   --- textgen-0.1.7.tar
â”œâ”€â”€ +++ textgen-0.1.8.tar
â”‚ â”œâ”€â”€ file list
â”‚ â”‚ @@ -1,64 +1,64 @@
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.222367 textgen-0.1.7/
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    11357 2021-08-05 02:59:06.000000 textgen-0.1.7/LICENSE
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    26535 2022-11-28 04:00:54.222825 textgen-0.1.7/PKG-INFO
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    22182 2022-11-27 06:14:50.000000 textgen-0.1.7/README.md
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)      309 2022-11-28 04:00:54.223645 textgen-0.1.7/setup.cfg
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     1356 2022-11-28 03:49:38.000000 textgen-0.1.7/setup.py
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.126855 textgen-0.1.7/textgen/
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     1291 2022-11-28 03:51:10.000000 textgen-0.1.7/textgen/__init__.py
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.145789 textgen-0.1.7/textgen/augment/
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)      133 2022-06-30 07:59:18.000000 textgen-0.1.7/textgen/augment/__init__.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     1493 2022-05-09 11:34:10.000000 textgen-0.1.7/textgen/augment/sentence_level_augment.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     3384 2022-09-26 07:08:57.000000 textgen-0.1.7/textgen/augment/text_augment.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     2256 2022-07-04 08:00:34.000000 textgen-0.1.7/textgen/augment/tokenizer.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     1971 2021-08-05 02:59:06.000000 textgen-0.1.7/textgen/augment/translate_api.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    13027 2022-09-26 07:39:41.000000 textgen-0.1.7/textgen/augment/word_level_augment.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     1240 2022-05-09 11:34:10.000000 textgen-0.1.7/textgen/augment/word_vocab.py
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.150234 textgen-0.1.7/textgen/config/
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)      140 2021-08-05 02:59:06.000000 textgen-0.1.7/textgen/config/__init__.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     1845 2022-06-30 03:18:51.000000 textgen-0.1.7/textgen/config/global_args.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    12871 2022-09-13 02:20:38.000000 textgen-0.1.7/textgen/config/model_args.py
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.152904 textgen-0.1.7/textgen/custom_models/
â”‚ â”‚ --rwxr-xr-x   0 xuming     (501) staff       (20)        0 2021-08-05 02:59:06.000000 textgen-0.1.7/textgen/custom_models/__init__.py
â”‚ â”‚ --rwxr-xr-x   0 xuming     (501) staff       (20)    32645 2022-08-24 03:27:39.000000 textgen-0.1.7/textgen/custom_models/models.py
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.157554 textgen-0.1.7/textgen/data/
â”‚ â”‚ --rwxr-xr-x   0 xuming     (501) staff       (20)    48098 2019-08-25 14:57:21.000000 textgen-0.1.7/textgen/data/HowNetPOSWord.txt
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    17438 2021-10-25 11:27:11.000000 textgen-0.1.7/textgen/data/stopwords.txt
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.164772 textgen-0.1.7/textgen/language_generation/
â”‚ â”‚ --rwxr-xr-x   0 xuming     (501) staff       (20)      292 2021-08-05 02:59:06.000000 textgen-0.1.7/textgen/language_generation/__init__.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    10056 2022-07-05 07:48:33.000000 textgen-0.1.7/textgen/language_generation/language_generation_model.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     2829 2022-06-14 12:08:00.000000 textgen-0.1.7/textgen/language_generation/language_generation_utils.py
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.177204 textgen-0.1.7/textgen/language_modeling/
â”‚ â”‚ --rwxr-xr-x   0 xuming     (501) staff       (20)      407 2022-09-10 06:38:25.000000 textgen-0.1.7/textgen/language_modeling/__init__.py
â”‚ â”‚ --rwxr-xr-x   0 xuming     (501) staff       (20)    56833 2022-09-09 11:41:34.000000 textgen-0.1.7/textgen/language_modeling/language_modeling_model.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     8938 2022-06-14 12:08:00.000000 textgen-0.1.7/textgen/language_modeling/language_modeling_utils.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    65595 2022-11-27 05:36:55.000000 textgen-0.1.7/textgen/language_modeling/songnet_model.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    17103 2022-11-28 03:49:38.000000 textgen-0.1.7/textgen/language_modeling/songnet_utils.py
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.184424 textgen-0.1.7/textgen/question_answering/
â”‚ â”‚ --rwxr-xr-x   0 xuming     (501) staff       (20)      287 2021-08-05 02:59:06.000000 textgen-0.1.7/textgen/question_answering/__init__.py
â”‚ â”‚ --rwxr-xr-x   0 xuming     (501) staff       (20)    61756 2022-09-09 11:41:34.000000 textgen-0.1.7/textgen/question_answering/question_answering_model.py
â”‚ â”‚ --rwxr-xr-x   0 xuming     (501) staff       (20)    77131 2022-06-14 12:36:04.000000 textgen-0.1.7/textgen/question_answering/question_answering_utils.py
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.202822 textgen-0.1.7/textgen/seq2seq/
â”‚ â”‚ --rwxr-xr-x   0 xuming     (501) staff       (20)      313 2022-08-07 03:00:11.000000 textgen-0.1.7/textgen/seq2seq/__init__.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    73298 2022-09-09 11:41:34.000000 textgen-0.1.7/textgen/seq2seq/bart_seq2seq_model.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    18559 2022-06-19 09:29:30.000000 textgen-0.1.7/textgen/seq2seq/bart_seq2seq_utils.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    23456 2022-08-25 03:06:56.000000 textgen-0.1.7/textgen/seq2seq/conv_seq2seq_model.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     4345 2022-05-10 10:48:02.000000 textgen-0.1.7/textgen/seq2seq/data_reader.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    19319 2022-08-25 03:06:56.000000 textgen-0.1.7/textgen/seq2seq/seq2seq_model.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    10741 2022-06-15 12:49:21.000000 textgen-0.1.7/textgen/seq2seq/seq2seq_trainer.py
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.216496 textgen-0.1.7/textgen/t5/
â”‚ â”‚ --rwxr-xr-x   0 xuming     (501) staff       (20)      272 2022-08-23 06:18:39.000000 textgen-0.1.7/textgen/t5/__init__.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    50493 2022-11-16 12:03:51.000000 textgen-0.1.7/textgen/t5/copyt5_model.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     6917 2022-11-16 13:02:34.000000 textgen-0.1.7/textgen/t5/copyt5_utils.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    50541 2022-11-16 12:03:51.000000 textgen-0.1.7/textgen/t5/t5_model.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     7146 2022-11-17 02:54:45.000000 textgen-0.1.7/textgen/t5/t5_utils.py
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.220869 textgen-0.1.7/textgen/unsup_generation/
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)      246 2022-09-06 02:24:58.000000 textgen-0.1.7/textgen/unsup_generation/__init__.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     3456 2022-09-26 07:01:36.000000 textgen-0.1.7/textgen/unsup_generation/tgls_model.py
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    27678 2022-09-06 02:29:56.000000 textgen-0.1.7/textgen/unsup_generation/tgls_util.py
â”‚ â”‚ -drwxr-xr-x   0 xuming     (501) staff       (20)        0 2022-11-28 04:00:54.133352 textgen-0.1.7/textgen.egg-info/
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)    26535 2022-11-28 04:00:53.000000 textgen-0.1.7/textgen.egg-info/PKG-INFO
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)     1663 2022-11-28 04:00:53.000000 textgen-0.1.7/textgen.egg-info/SOURCES.txt
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)        1 2022-11-28 04:00:53.000000 textgen-0.1.7/textgen.egg-info/dependency_links.txt
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)      128 2022-11-28 04:00:53.000000 textgen-0.1.7/textgen.egg-info/requires.txt
â”‚ â”‚ --rw-r--r--   0 xuming     (501) staff       (20)        8 2022-11-28 04:00:53.000000 textgen-0.1.7/textgen.egg-info/top_level.txt
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.889828 textgen-0.1.8/
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    11357 2021-08-05 02:59:06.000000 textgen-0.1.8/LICENSE
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    26585 2023-04-07 09:20:39.890673 textgen-0.1.8/PKG-INFO
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    22208 2023-03-26 02:30:43.000000 textgen-0.1.8/README.md
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)      309 2023-04-07 09:20:39.893100 textgen-0.1.8/setup.cfg
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     1395 2023-04-07 02:35:13.000000 textgen-0.1.8/setup.py
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.829813 textgen-0.1.8/textgen/
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     1288 2023-04-07 02:35:13.000000 textgen-0.1.8/textgen/__init__.py
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.839844 textgen-0.1.8/textgen/augment/
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)      133 2022-06-30 07:59:18.000000 textgen-0.1.8/textgen/augment/__init__.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     1493 2022-05-09 11:34:10.000000 textgen-0.1.8/textgen/augment/sentence_level_augment.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     3384 2022-09-26 07:08:57.000000 textgen-0.1.8/textgen/augment/text_augment.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     2256 2022-07-04 08:00:34.000000 textgen-0.1.8/textgen/augment/tokenizer.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     1971 2021-08-05 02:59:06.000000 textgen-0.1.8/textgen/augment/translate_api.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    13027 2022-09-26 07:39:41.000000 textgen-0.1.8/textgen/augment/word_level_augment.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     1240 2022-05-09 11:34:10.000000 textgen-0.1.8/textgen/augment/word_vocab.py
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.843073 textgen-0.1.8/textgen/chatglm/
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)       80 2023-03-26 02:30:43.000000 textgen-0.1.8/textgen/chatglm/__init__.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    26069 2023-04-07 08:08:24.000000 textgen-0.1.8/textgen/chatglm/chatglm_model.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     5884 2023-04-07 06:57:13.000000 textgen-0.1.8/textgen/chatglm/chatglm_utils.py
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.845533 textgen-0.1.8/textgen/config/
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)      140 2021-08-05 02:59:06.000000 textgen-0.1.8/textgen/config/__init__.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     1845 2022-06-30 03:18:51.000000 textgen-0.1.8/textgen/config/global_args.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    12292 2023-04-07 08:08:24.000000 textgen-0.1.8/textgen/config/model_args.py
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.847246 textgen-0.1.8/textgen/custom_models/
â”‚ â”‚ +-rwxr-xr-x   0 xuming     (501) staff       (20)        0 2021-08-05 02:59:06.000000 textgen-0.1.8/textgen/custom_models/__init__.py
â”‚ â”‚ +-rwxr-xr-x   0 xuming     (501) staff       (20)    32645 2022-08-24 03:27:39.000000 textgen-0.1.8/textgen/custom_models/models.py
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.850267 textgen-0.1.8/textgen/data/
â”‚ â”‚ +-rwxr-xr-x   0 xuming     (501) staff       (20)    48098 2019-08-25 14:57:21.000000 textgen-0.1.8/textgen/data/HowNetPOSWord.txt
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    17438 2021-10-25 11:27:11.000000 textgen-0.1.8/textgen/data/stopwords.txt
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.854199 textgen-0.1.8/textgen/language_generation/
â”‚ â”‚ +-rwxr-xr-x   0 xuming     (501) staff       (20)      292 2021-08-05 02:59:06.000000 textgen-0.1.8/textgen/language_generation/__init__.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    10056 2022-07-05 07:48:33.000000 textgen-0.1.8/textgen/language_generation/language_generation_model.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     2829 2022-06-14 12:08:00.000000 textgen-0.1.8/textgen/language_generation/language_generation_utils.py
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.862985 textgen-0.1.8/textgen/language_modeling/
â”‚ â”‚ +-rwxr-xr-x   0 xuming     (501) staff       (20)      407 2022-09-10 06:38:25.000000 textgen-0.1.8/textgen/language_modeling/__init__.py
â”‚ â”‚ +-rwxr-xr-x   0 xuming     (501) staff       (20)    56833 2022-09-09 11:41:34.000000 textgen-0.1.8/textgen/language_modeling/language_modeling_model.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     8938 2022-06-14 12:08:00.000000 textgen-0.1.8/textgen/language_modeling/language_modeling_utils.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    65595 2022-11-27 05:36:55.000000 textgen-0.1.8/textgen/language_modeling/songnet_model.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    17103 2022-12-05 07:28:16.000000 textgen-0.1.8/textgen/language_modeling/songnet_utils.py
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.873459 textgen-0.1.8/textgen/seq2seq/
â”‚ â”‚ +-rwxr-xr-x   0 xuming     (501) staff       (20)      313 2022-08-07 03:00:11.000000 textgen-0.1.8/textgen/seq2seq/__init__.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    73298 2022-09-09 11:41:34.000000 textgen-0.1.8/textgen/seq2seq/bart_seq2seq_model.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    18559 2022-06-19 09:29:30.000000 textgen-0.1.8/textgen/seq2seq/bart_seq2seq_utils.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    23456 2022-08-25 03:06:56.000000 textgen-0.1.8/textgen/seq2seq/conv_seq2seq_model.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     4345 2022-05-10 10:48:02.000000 textgen-0.1.8/textgen/seq2seq/data_reader.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    19319 2022-08-25 03:06:56.000000 textgen-0.1.8/textgen/seq2seq/seq2seq_model.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    10741 2022-06-15 12:49:21.000000 textgen-0.1.8/textgen/seq2seq/seq2seq_trainer.py
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.881625 textgen-0.1.8/textgen/t5/
â”‚ â”‚ +-rwxr-xr-x   0 xuming     (501) staff       (20)      272 2022-08-23 06:18:39.000000 textgen-0.1.8/textgen/t5/__init__.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    50493 2022-11-16 12:03:51.000000 textgen-0.1.8/textgen/t5/copyt5_model.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     6917 2022-11-16 13:02:34.000000 textgen-0.1.8/textgen/t5/copyt5_utils.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    50541 2023-02-16 08:10:32.000000 textgen-0.1.8/textgen/t5/t5_model.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     7146 2022-11-17 02:54:45.000000 textgen-0.1.8/textgen/t5/t5_utils.py
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.888068 textgen-0.1.8/textgen/unsup_generation/
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)      246 2022-09-06 02:24:58.000000 textgen-0.1.8/textgen/unsup_generation/__init__.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     3456 2022-09-26 07:01:36.000000 textgen-0.1.8/textgen/unsup_generation/tgls_model.py
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    27678 2022-09-06 02:29:56.000000 textgen-0.1.8/textgen/unsup_generation/tgls_util.py
â”‚ â”‚ +drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-04-07 09:20:39.833383 textgen-0.1.8/textgen.egg-info/
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)    26585 2023-04-07 09:20:39.000000 textgen-0.1.8/textgen.egg-info/PKG-INFO
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)     1608 2023-04-07 09:20:39.000000 textgen-0.1.8/textgen.egg-info/SOURCES.txt
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)        1 2023-04-07 09:20:39.000000 textgen-0.1.8/textgen.egg-info/dependency_links.txt
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)      145 2023-04-07 09:20:39.000000 textgen-0.1.8/textgen.egg-info/requires.txt
â”‚ â”‚ +-rw-r--r--   0 xuming     (501) staff       (20)        8 2023-04-07 09:20:39.000000 textgen-0.1.8/textgen.egg-info/top_level.txt
â”‚ â”‚   --- textgen-0.1.7/LICENSE
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/LICENSE
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/PKG-INFO
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/PKG-INFO
â”‚ â”‚â”„ Files 3% similar despite different names
â”‚ â”‚ @@ -1,10 +1,10 @@
â”‚ â”‚  Metadata-Version: 2.1
â”‚ â”‚  Name: textgen
â”‚ â”‚ -Version: 0.1.7
â”‚ â”‚ +Version: 0.1.8
â”‚ â”‚  Summary: Text Generation Model
â”‚ â”‚  Home-page: https://github.com/shibing624/textgen
â”‚ â”‚  Author: XuMing
â”‚ â”‚  Author-email: xuming624@qq.com
â”‚ â”‚  License: Apache 2.0
â”‚ â”‚  Description: [![PyPI version](https://badge.fury.io/py/textgen.svg)](https://badge.fury.io/py/textgen)
â”‚ â”‚          [![Downloads](https://pepy.tech/badge/textgen)](https://pepy.tech/project/textgen)
â”‚ â”‚ @@ -19,56 +19,62 @@
â”‚ â”‚          
â”‚ â”‚          ğŸŒˆ Implementation of Text Generation models.
â”‚ â”‚          
â”‚ â”‚          **textgen**å®ç°äº†å¤šç§æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼šUDAã€GPT2ã€Seq2Seqã€BARTã€T5ã€SongNetç­‰æ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ã€‚
â”‚ â”‚          
â”‚ â”‚          **Guide**
â”‚ â”‚          
â”‚ â”‚ -        - [Question](#Question)
â”‚ â”‚ -        - [Solution](#Solution)
â”‚ â”‚          - [Feature](#Feature)
â”‚ â”‚          - [Install](#install)
â”‚ â”‚          - [Usage](#usage)
â”‚ â”‚          - [Contact](#Contact)
â”‚ â”‚          - [Reference](#reference)
â”‚ â”‚          
â”‚ â”‚ -        # Question
â”‚ â”‚ -        
â”‚ â”‚ -        æ–‡æœ¬ç”Ÿæˆï¼Œæ–‡æœ¬æ•°æ®å¢å¼ºæ€ä¹ˆåšï¼Ÿ
â”‚ â”‚ -        
â”‚ â”‚ -        # Solution
â”‚ â”‚ -        ## æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
â”‚ â”‚ +        # Feature
â”‚ â”‚ +        ## æ–‡æœ¬ç”Ÿæˆ
â”‚ â”‚          
â”‚ â”‚ -        1. Seq2Seqã€ConvSeq2Seqã€BART
â”‚ â”‚ -        2. GPT2ã€SongNet
â”‚ â”‚ -        3. T5ã€CopyT5
â”‚ â”‚ +        1. seq2seq: Seq2Seqã€ConvSeq2Seqã€BART
â”‚ â”‚ +        2. language_modeling: GPT2ã€SongNet
â”‚ â”‚ +        3. t5: T5ã€CopyT5
â”‚ â”‚ +        4. question_answering: BERTã€XLNet
â”‚ â”‚ +        5. chatglm: ChatGLM
â”‚ â”‚          
â”‚ â”‚          ## æ–‡æœ¬æ‰©å¢
â”‚ â”‚          ### è¯ç²’åº¦æ‰©å¢
â”‚ â”‚          1. UDAï¼Œéæ ¸å¿ƒè¯æ›¿æ¢
â”‚ â”‚          2. EDAï¼Œç®€å•æ•°æ®å¢å¼ºæŠ€æœ¯ï¼šç›¸ä¼¼è¯ã€åŒä¹‰è¯æ›¿æ¢ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢
â”‚ â”‚          
â”‚ â”‚          ### å¥ç²’åº¦æ‰©å¢
â”‚ â”‚          1. å›è¯‘ï¼ˆBT, Back Translateï¼‰ï¼šä¸­æ–‡-è‹±æ–‡-ä¸­æ–‡
â”‚ â”‚          2. GPT2æ¨¡å‹ç»­å†™ï¼šçŸ­æ–‡æœ¬->é•¿æ–‡æœ¬
â”‚ â”‚          3. BARTæ‘˜è¦æ¨¡å‹ï¼šé•¿æ–‡æœ¬->çŸ­æ–‡æœ¬
â”‚ â”‚          4. TGLSï¼šæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
â”‚ â”‚          
â”‚ â”‚ -        
â”‚ â”‚ -        # Feature
â”‚ â”‚ -        
â”‚ â”‚ +        ## åŠŸèƒ½åˆ—è¡¨
â”‚ â”‚          - [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)ç®—æ³•å’ŒEDAç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
â”‚ â”‚          - [BT(å›è¯‘)](textgen/augment/sentence_level_augment.py)ï¼šæœ¬é¡¹ç›®åŸºäºç™¾åº¦ç¿»è¯‘APIå®ç°äº†å›è¯‘åŠŸèƒ½ï¼Œå…ˆæŠŠä¸­æ–‡å¥å­ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œå†æŠŠè‹±æ–‡ç¿»è¯‘ä¸ºæ–°çš„ä¸­æ–‡
â”‚ â”‚          - [Seq2Seq](textgen/seq2seq)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†Seq2Seqã€ConvSeq2Seqã€BARTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
â”‚ â”‚          - [T5](textgen/t5)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†T5å’ŒCopyT5æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€å¯¹è”ç”Ÿæˆã€æ–‡æ¡ˆæ’°å†™ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
â”‚ â”‚          - [GPT2](textgen/language_modeling)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†GTP2æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡ç« ç”Ÿæˆã€å¯¹è”ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
â”‚ â”‚          - [SongNet](textgen/language_modeling/songnet_model.py)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†SongNetæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºè§„èŒƒæ ¼å¼çš„è¯—è¯ã€æ­Œè¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
â”‚ â”‚          - [TGLS](textgen/unsup_generation)ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬
â”‚ â”‚          
â”‚ â”‚          
â”‚ â”‚ +        ## Release Models
â”‚ â”‚ +        releaseåŸºäº`textgen`è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å·²ç»releaseåˆ°HuggingFace modelsï¼ŒæŒ‡å®šæ¨¡å‹åç§°`textgen`ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
â”‚ â”‚ +        
â”‚ â”‚ +        |Model|Arch|Intro|Training|Inference|
â”‚ â”‚ +        |:-- |:--- |:--- |:--- |:--- |
â”‚ â”‚ +        |[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
â”‚ â”‚ +        |[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
â”‚ â”‚ +        |[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
â”‚ â”‚ +        |[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
â”‚ â”‚ +        |[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
â”‚ â”‚ +        
â”‚ â”‚ +        
â”‚ â”‚          # Demo
â”‚ â”‚          
â”‚ â”‚          HuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate
â”‚ â”‚          
â”‚ â”‚          ![](docs/hf.png)
â”‚ â”‚          
â”‚ â”‚          run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:
â”‚ â”‚ @@ -260,22 +266,14 @@
â”‚ â”‚          output:
â”‚ â”‚          ```shell
â”‚ â”‚          inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
â”‚ â”‚          outputs: ['äººå·¥æ™ºèƒ½æœ‰ä¸¤ä¸ªå¹¿ä¹‰çš„å®šä¹‰,ä»»ä½•æ‹Ÿäººçš„æœºæ¢°,å¦‚åœ¨å¡é›·å°”capeks', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨Python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Šå·¥ä½œ!', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦']
â”‚ â”‚          ```
â”‚ â”‚          
â”‚ â”‚          
â”‚ â”‚ -        ### T5 æ¨¡å‹åº”ç”¨
â”‚ â”‚ -        
â”‚ â”‚ -        releaseåŸºäºT5çš„fine-tunedåçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å…¨éƒ¨releaseåˆ°HuggingFace modelsï¼Œ`textgen`å¯è‡ªåŠ¨ä¸‹è½½ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
â”‚ â”‚ -        
â”‚ â”‚ -        |Model|Arch|Intro|Training|Inference|
â”‚ â”‚ -        |:-- |:--- |:--- |:--- |:--- |
â”‚ â”‚ -        |[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
â”‚ â”‚ -        |[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
â”‚ â”‚          
â”‚ â”‚          
â”‚ â”‚          ## GPT2 æ¨¡å‹
â”‚ â”‚          
â”‚ â”‚          ### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ
â”‚ â”‚          
â”‚ â”‚          ä½¿ç”¨ä¸­æ–‡æ•°æ®é›†ï¼ˆæ®µè½æ ¼å¼ï¼Œ`\n`é—´éš”ï¼‰ï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè¯—æ­Œç”Ÿæˆã€æ–‡ç« ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
â”‚ â”‚ @@ -300,23 +298,14 @@
â”‚ â”‚          ## SongNet æ¨¡å‹
â”‚ â”‚          
â”‚ â”‚          æ ¼å¼æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œpaperè§[SongNet: Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022)ï¼Œ
â”‚ â”‚          é€‚ç”¨äºå¼ºéŸµå¾‹æ ¼å¼è¦æ±‚çš„è¯—æ­Œã€å¯¹è”ã€æ­Œè¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
â”‚ â”‚          
â”‚ â”‚          example: [examples/language_generation/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)
â”‚ â”‚          
â”‚ â”‚ -        ### SongNet æ¨¡å‹åº”ç”¨
â”‚ â”‚ -        
â”‚ â”‚ -        releaseåŸºäºSongNetçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å…¨éƒ¨releaseåˆ°HuggingFace modelsï¼Œ`textgen`å¯è‡ªåŠ¨ä¸‹è½½ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
â”‚ â”‚ -        
â”‚ â”‚ -        |Model|Arch|Intro|Training|Inference|
â”‚ â”‚ -        |:-- |:--- |:--- |:--- |:--- |
â”‚ â”‚ -        |[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
â”‚ â”‚ -        |[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
â”‚ â”‚ -        |[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
â”‚ â”‚          
â”‚ â”‚          
â”‚ â”‚          ## Keyword Text Augmentation(EDA/UDA)
â”‚ â”‚          
â”‚ â”‚          example: [examples/text_augmentation_demo.py](examples/text_augmentation_demo.py)
â”‚ â”‚          
â”‚ â”‚          ```python
â”‚ â”‚ @@ -434,14 +423,28 @@
â”‚ â”‚          - Issue(å»ºè®®)
â”‚ â”‚            ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
â”‚ â”‚          - é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
â”‚ â”‚          - å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚
â”‚ â”‚          
â”‚ â”‚          <img src="docs/wechat.jpeg" width="200" />
â”‚ â”‚          
â”‚ â”‚ +        
â”‚ â”‚ +        # Citation
â”‚ â”‚ +        
â”‚ â”‚ +        å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†textgenï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š
â”‚ â”‚ +        
â”‚ â”‚ +        ```latex
â”‚ â”‚ +        @misc{textgen,
â”‚ â”‚ +          title={textgen: Text Generation Tool},
â”‚ â”‚ +          author={Xu Ming},
â”‚ â”‚ +          year={2021},
â”‚ â”‚ +          howpublished={\url{https://github.com/shibing624/textgen}},
â”‚ â”‚ +        }
â”‚ â”‚ +        ```
â”‚ â”‚ +        
â”‚ â”‚          # License
â”‚ â”‚          
â”‚ â”‚          æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ textgençš„é“¾æ¥å’Œæˆæƒåè®®ã€‚
â”‚ â”‚          
â”‚ â”‚          # Contribute
â”‚ â”‚          
â”‚ â”‚          é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š
â”‚ â”‚   --- textgen-0.1.7/README.md
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/README.md
â”‚ â”‚â”„ Files 3% similar despite different names
â”‚ â”‚ @@ -11,56 +11,62 @@
â”‚ â”‚  
â”‚ â”‚  ğŸŒˆ Implementation of Text Generation models.
â”‚ â”‚  
â”‚ â”‚  **textgen**å®ç°äº†å¤šç§æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼šUDAã€GPT2ã€Seq2Seqã€BARTã€T5ã€SongNetç­‰æ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ã€‚
â”‚ â”‚  
â”‚ â”‚  **Guide**
â”‚ â”‚  
â”‚ â”‚ -- [Question](#Question)
â”‚ â”‚ -- [Solution](#Solution)
â”‚ â”‚  - [Feature](#Feature)
â”‚ â”‚  - [Install](#install)
â”‚ â”‚  - [Usage](#usage)
â”‚ â”‚  - [Contact](#Contact)
â”‚ â”‚  - [Reference](#reference)
â”‚ â”‚  
â”‚ â”‚ -# Question
â”‚ â”‚ -
â”‚ â”‚ -æ–‡æœ¬ç”Ÿæˆï¼Œæ–‡æœ¬æ•°æ®å¢å¼ºæ€ä¹ˆåšï¼Ÿ
â”‚ â”‚ -
â”‚ â”‚ -# Solution
â”‚ â”‚ -## æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
â”‚ â”‚ +# Feature
â”‚ â”‚ +## æ–‡æœ¬ç”Ÿæˆ
â”‚ â”‚  
â”‚ â”‚ -1. Seq2Seqã€ConvSeq2Seqã€BART
â”‚ â”‚ -2. GPT2ã€SongNet
â”‚ â”‚ -3. T5ã€CopyT5
â”‚ â”‚ +1. seq2seq: Seq2Seqã€ConvSeq2Seqã€BART
â”‚ â”‚ +2. language_modeling: GPT2ã€SongNet
â”‚ â”‚ +3. t5: T5ã€CopyT5
â”‚ â”‚ +4. question_answering: BERTã€XLNet
â”‚ â”‚ +5. chatglm: ChatGLM
â”‚ â”‚  
â”‚ â”‚  ## æ–‡æœ¬æ‰©å¢
â”‚ â”‚  ### è¯ç²’åº¦æ‰©å¢
â”‚ â”‚  1. UDAï¼Œéæ ¸å¿ƒè¯æ›¿æ¢
â”‚ â”‚  2. EDAï¼Œç®€å•æ•°æ®å¢å¼ºæŠ€æœ¯ï¼šç›¸ä¼¼è¯ã€åŒä¹‰è¯æ›¿æ¢ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢
â”‚ â”‚  
â”‚ â”‚  ### å¥ç²’åº¦æ‰©å¢
â”‚ â”‚  1. å›è¯‘ï¼ˆBT, Back Translateï¼‰ï¼šä¸­æ–‡-è‹±æ–‡-ä¸­æ–‡
â”‚ â”‚  2. GPT2æ¨¡å‹ç»­å†™ï¼šçŸ­æ–‡æœ¬->é•¿æ–‡æœ¬
â”‚ â”‚  3. BARTæ‘˜è¦æ¨¡å‹ï¼šé•¿æ–‡æœ¬->çŸ­æ–‡æœ¬
â”‚ â”‚  4. TGLSï¼šæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
â”‚ â”‚  
â”‚ â”‚ -
â”‚ â”‚ -# Feature
â”‚ â”‚ -
â”‚ â”‚ +## åŠŸèƒ½åˆ—è¡¨
â”‚ â”‚  - [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)ç®—æ³•å’ŒEDAç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
â”‚ â”‚  - [BT(å›è¯‘)](textgen/augment/sentence_level_augment.py)ï¼šæœ¬é¡¹ç›®åŸºäºç™¾åº¦ç¿»è¯‘APIå®ç°äº†å›è¯‘åŠŸèƒ½ï¼Œå…ˆæŠŠä¸­æ–‡å¥å­ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œå†æŠŠè‹±æ–‡ç¿»è¯‘ä¸ºæ–°çš„ä¸­æ–‡
â”‚ â”‚  - [Seq2Seq](textgen/seq2seq)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†Seq2Seqã€ConvSeq2Seqã€BARTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
â”‚ â”‚  - [T5](textgen/t5)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†T5å’ŒCopyT5æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€å¯¹è”ç”Ÿæˆã€æ–‡æ¡ˆæ’°å†™ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
â”‚ â”‚  - [GPT2](textgen/language_modeling)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†GTP2æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡ç« ç”Ÿæˆã€å¯¹è”ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
â”‚ â”‚  - [SongNet](textgen/language_modeling/songnet_model.py)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†SongNetæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºè§„èŒƒæ ¼å¼çš„è¯—è¯ã€æ­Œè¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
â”‚ â”‚  - [TGLS](textgen/unsup_generation)ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚ +## Release Models
â”‚ â”‚ +releaseåŸºäº`textgen`è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å·²ç»releaseåˆ°HuggingFace modelsï¼ŒæŒ‡å®šæ¨¡å‹åç§°`textgen`ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
â”‚ â”‚ +
â”‚ â”‚ +|Model|Arch|Intro|Training|Inference|
â”‚ â”‚ +|:-- |:--- |:--- |:--- |:--- |
â”‚ â”‚ +|[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
â”‚ â”‚ +|[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
â”‚ â”‚ +|[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
â”‚ â”‚ +|[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
â”‚ â”‚ +|[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
â”‚ â”‚ +
â”‚ â”‚ +
â”‚ â”‚  # Demo
â”‚ â”‚  
â”‚ â”‚  HuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate
â”‚ â”‚  
â”‚ â”‚  ![](docs/hf.png)
â”‚ â”‚  
â”‚ â”‚  run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:
â”‚ â”‚ @@ -252,22 +258,14 @@
â”‚ â”‚  output:
â”‚ â”‚  ```shell
â”‚ â”‚  inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
â”‚ â”‚  outputs: ['äººå·¥æ™ºèƒ½æœ‰ä¸¤ä¸ªå¹¿ä¹‰çš„å®šä¹‰,ä»»ä½•æ‹Ÿäººçš„æœºæ¢°,å¦‚åœ¨å¡é›·å°”capeks', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨Python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Šå·¥ä½œ!', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦']
â”‚ â”‚  ```
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚ -### T5 æ¨¡å‹åº”ç”¨
â”‚ â”‚ -
â”‚ â”‚ -releaseåŸºäºT5çš„fine-tunedåçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å…¨éƒ¨releaseåˆ°HuggingFace modelsï¼Œ`textgen`å¯è‡ªåŠ¨ä¸‹è½½ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
â”‚ â”‚ -
â”‚ â”‚ -|Model|Arch|Intro|Training|Inference|
â”‚ â”‚ -|:-- |:--- |:--- |:--- |:--- |
â”‚ â”‚ -|[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
â”‚ â”‚ -|[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚  ## GPT2 æ¨¡å‹
â”‚ â”‚  
â”‚ â”‚  ### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ
â”‚ â”‚  
â”‚ â”‚  ä½¿ç”¨ä¸­æ–‡æ•°æ®é›†ï¼ˆæ®µè½æ ¼å¼ï¼Œ`\n`é—´éš”ï¼‰ï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè¯—æ­Œç”Ÿæˆã€æ–‡ç« ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
â”‚ â”‚ @@ -292,23 +290,14 @@
â”‚ â”‚  ## SongNet æ¨¡å‹
â”‚ â”‚  
â”‚ â”‚  æ ¼å¼æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œpaperè§[SongNet: Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022)ï¼Œ
â”‚ â”‚  é€‚ç”¨äºå¼ºéŸµå¾‹æ ¼å¼è¦æ±‚çš„è¯—æ­Œã€å¯¹è”ã€æ­Œè¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
â”‚ â”‚  
â”‚ â”‚  example: [examples/language_generation/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)
â”‚ â”‚  
â”‚ â”‚ -### SongNet æ¨¡å‹åº”ç”¨
â”‚ â”‚ -
â”‚ â”‚ -releaseåŸºäºSongNetçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å…¨éƒ¨releaseåˆ°HuggingFace modelsï¼Œ`textgen`å¯è‡ªåŠ¨ä¸‹è½½ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
â”‚ â”‚ -
â”‚ â”‚ -|Model|Arch|Intro|Training|Inference|
â”‚ â”‚ -|:-- |:--- |:--- |:--- |:--- |
â”‚ â”‚ -|[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
â”‚ â”‚ -|[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
â”‚ â”‚ -|[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚  ## Keyword Text Augmentation(EDA/UDA)
â”‚ â”‚  
â”‚ â”‚  example: [examples/text_augmentation_demo.py](examples/text_augmentation_demo.py)
â”‚ â”‚  
â”‚ â”‚  ```python
â”‚ â”‚ @@ -426,14 +415,28 @@
â”‚ â”‚  - Issue(å»ºè®®)
â”‚ â”‚    ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
â”‚ â”‚  - é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
â”‚ â”‚  - å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚
â”‚ â”‚  
â”‚ â”‚  <img src="docs/wechat.jpeg" width="200" />
â”‚ â”‚  
â”‚ â”‚ +
â”‚ â”‚ +# Citation
â”‚ â”‚ +
â”‚ â”‚ +å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†textgenï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š
â”‚ â”‚ +
â”‚ â”‚ +```latex
â”‚ â”‚ +@misc{textgen,
â”‚ â”‚ +  title={textgen: Text Generation Tool},
â”‚ â”‚ +  author={Xu Ming},
â”‚ â”‚ +  year={2021},
â”‚ â”‚ +  howpublished={\url{https://github.com/shibing624/textgen}},
â”‚ â”‚ +}
â”‚ â”‚ +```
â”‚ â”‚ +
â”‚ â”‚  # License
â”‚ â”‚  
â”‚ â”‚  æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ textgençš„é“¾æ¥å’Œæˆæƒåè®®ã€‚
â”‚ â”‚  
â”‚ â”‚  # Contribute
â”‚ â”‚  
â”‚ â”‚  é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š
â”‚ â”‚   --- textgen-0.1.7/setup.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/setup.py
â”‚ â”‚â”„ Files 5% similar despite different names
â”‚ â”‚ @@ -3,15 +3,15 @@
â”‚ â”‚  from setuptools import setup, find_packages
â”‚ â”‚  
â”‚ â”‚  with open('README.md', 'r', encoding='utf-8') as f:
â”‚ â”‚      readme = f.read()
â”‚ â”‚  
â”‚ â”‚  setup(
â”‚ â”‚      name='textgen',
â”‚ â”‚ -    version='0.1.7',
â”‚ â”‚ +    version='0.1.8',
â”‚ â”‚      description='Text Generation Model',
â”‚ â”‚      long_description=readme,
â”‚ â”‚      long_description_content_type='text/markdown',
â”‚ â”‚      author='XuMing',
â”‚ â”‚      author_email='xuming624@qq.com',
â”‚ â”‚      url='https://github.com/shibing624/textgen',
â”‚ â”‚      license="Apache 2.0",
â”‚ â”‚ @@ -34,14 +34,16 @@
â”‚ â”‚          'text2vec',
â”‚ â”‚          'tensorboard',
â”‚ â”‚          'tqdm>=4.47.0',
â”‚ â”‚          'pandas',
â”‚ â”‚          'wandb>=0.10.32',
â”‚ â”‚          'sacremoses',
â”‚ â”‚          'Rouge',
â”‚ â”‚ +        'cpm_kernels',
â”‚ â”‚ +        'peft',
â”‚ â”‚      ],
â”‚ â”‚      packages=find_packages(exclude=['tests']),
â”‚ â”‚      package_dir={'textgen': 'textgen'},
â”‚ â”‚      package_data={
â”‚ â”‚          'textgen': ['*.*', 'data/*'],
â”‚ â”‚      }
â”‚ â”‚  )
â”‚ â”‚   --- textgen-0.1.7/textgen/__init__.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/__init__.py
â”‚ â”‚â”„ Files 26% similar despite different names
â”‚ â”‚ @@ -1,33 +1,36 @@
â”‚ â”‚  # -*- coding: utf-8 -*-
â”‚ â”‚  """
â”‚ â”‚  @author:XuMing(xuming624@qq.com)
â”‚ â”‚  @description: 
â”‚ â”‚  """
â”‚ â”‚  
â”‚ â”‚ -__version__ = '0.1.7'
â”‚ â”‚ +__version__ = '0.1.8'
â”‚ â”‚  
â”‚ â”‚  from textgen.augment.text_augment import TextAugment
â”‚ â”‚  
â”‚ â”‚  from textgen.config.model_args import LanguageGenerationArgs
â”‚ â”‚  from textgen.language_generation.language_generation_model import LanguageGenerationModel
â”‚ â”‚  
â”‚ â”‚  from textgen.config.model_args import LanguageModelingArgs
â”‚ â”‚  from textgen.language_modeling.language_modeling_model import LanguageModelingModel
â”‚ â”‚ +
â”‚ â”‚  from textgen.config.model_args import SongNetArgs
â”‚ â”‚  from textgen.language_modeling.songnet_model import SongNetModel
â”‚ â”‚  from textgen.language_modeling.songnet_utils import SongNetTokenizer, snapshot_download
â”‚ â”‚  
â”‚ â”‚ -from textgen.config.model_args import QuestionAnsweringArgs
â”‚ â”‚ -from textgen.question_answering.question_answering_model import QuestionAnsweringModel
â”‚ â”‚ -
â”‚ â”‚  from textgen.config.model_args import Seq2SeqArgs
â”‚ â”‚ -from textgen.seq2seq.bart_seq2seq_model import BartSeq2SeqModel
â”‚ â”‚  from textgen.seq2seq.seq2seq_model import Seq2SeqModel
â”‚ â”‚  from textgen.seq2seq.conv_seq2seq_model import ConvSeq2SeqModel
â”‚ â”‚ +from textgen.seq2seq.bart_seq2seq_model import BartSeq2SeqModel
â”‚ â”‚  
â”‚ â”‚ -from textgen.config.model_args import T5Args, CopyT5Args
â”‚ â”‚ +from textgen.config.model_args import T5Args
â”‚ â”‚  from textgen.t5.t5_model import T5Model
â”‚ â”‚ +
â”‚ â”‚ +from textgen.config.model_args import CopyT5Args
â”‚ â”‚  from textgen.t5.copyt5_model import CopyT5Model
â”‚ â”‚  from textgen.t5.copyt5_utils import ZHTokenizer
â”‚ â”‚  
â”‚ â”‚  from textgen.unsup_generation.tgls_model import TglsModel
â”‚ â”‚ +
â”‚ â”‚ +from textgen.chatglm.chatglm_model import ChatGlmModel
â”‚ â”‚ +from textgen.config.model_args import ChatGlmArgs
â”‚ â”‚   --- textgen-0.1.7/textgen/augment/sentence_level_augment.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/augment/sentence_level_augment.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/augment/text_augment.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/augment/text_augment.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/augment/tokenizer.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/augment/tokenizer.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/augment/translate_api.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/augment/translate_api.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/augment/word_level_augment.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/augment/word_level_augment.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/augment/word_vocab.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/augment/word_vocab.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/config/global_args.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/config/global_args.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/config/model_args.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/config/model_args.py
â”‚ â”‚â”„ Files 11% similar despite different names
â”‚ â”‚ @@ -2,18 +2,18 @@
â”‚ â”‚  """
â”‚ â”‚  @author:XuMing(xuming624@qq.com)
â”‚ â”‚  @description: refer https://github.com/ThilinaRajapakse/simpletransformers
â”‚ â”‚  """
â”‚ â”‚  import json
â”‚ â”‚  import os
â”‚ â”‚  import sys
â”‚ â”‚ -import warnings
â”‚ â”‚ +from loguru import logger
â”‚ â”‚  from dataclasses import asdict, dataclass, field
â”‚ â”‚  from multiprocessing import cpu_count
â”‚ â”‚ -
â”‚ â”‚ +from typing import Optional
â”‚ â”‚  from torch.utils.data import Dataset
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚  def get_default_process_count():
â”‚ â”‚      process_count = cpu_count() - 2 if cpu_count() > 2 else 1
â”‚ â”‚      if sys.platform == "win32":
â”‚ â”‚          process_count = min(process_count, 61)
â”‚ â”‚ @@ -114,49 +114,38 @@
â”‚ â”‚  
â”‚ â”‚      def get_args_for_saving(self):
â”‚ â”‚          args_for_saving = {key: value for key, value in asdict(self).items() if key not in self.not_saved_args}
â”‚ â”‚          return args_for_saving
â”‚ â”‚  
â”‚ â”‚      def save(self, output_dir):
â”‚ â”‚          os.makedirs(output_dir, exist_ok=True)
â”‚ â”‚ -        with open(os.path.join(output_dir, "model_args.json"), "w") as f:
â”‚ â”‚ +        with open(os.path.join(output_dir, "model_args.json"), "w", encoding='utf-8') as f:
â”‚ â”‚              args_dict = self.get_args_for_saving()
â”‚ â”‚ +            if args_dict['dataset_class'] is not None and not isinstance(args_dict["dataset_class"], str):
â”‚ â”‚ +                args_dict['dataset_class'] = type(args_dict['dataset_class']).__name__
â”‚ â”‚              if args_dict["tokenizer_type"] is not None and not isinstance(args_dict["tokenizer_type"], str):
â”‚ â”‚                  args_dict["tokenizer_type"] = type(args_dict["tokenizer_type"]).__name__
â”‚ â”‚              json.dump(args_dict, f)
â”‚ â”‚  
â”‚ â”‚      def load(self, input_dir):
â”‚ â”‚          if input_dir:
â”‚ â”‚              model_args_file = os.path.join(input_dir, "model_args.json")
â”‚ â”‚              if os.path.isfile(model_args_file):
â”‚ â”‚ -                with open(model_args_file, "r") as f:
â”‚ â”‚ +                with open(model_args_file, "r", encoding='utf-8') as f:
â”‚ â”‚                      model_args = json.load(f)
â”‚ â”‚ -
â”‚ â”‚ +                if model_args["dataset_class"]:
â”‚ â”‚ +                    logger.warning(
â”‚ â”‚ +                        "This model was trained using a custom dataset_class."
â”‚ â”‚ +                        "This cannot be loaded automatically and must be specified in the model args"
â”‚ â”‚ +                        "when loading the model."
â”‚ â”‚ +                    )
â”‚ â”‚                  self.update_from_dict(model_args)
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚  @dataclass
â”‚ â”‚ -class QuestionAnsweringArgs(ModelArgs):
â”‚ â”‚ -    """
â”‚ â”‚ -    Model args for a QuestionAnsweringModel
â”‚ â”‚ -    """
â”‚ â”‚ -
â”‚ â”‚ -    model_class: str = "QuestionAnsweringModel"
â”‚ â”‚ -    doc_stride: int = 384
â”‚ â”‚ -    early_stopping_metric: str = "correct"
â”‚ â”‚ -    early_stopping_metric_minimize: bool = False
â”‚ â”‚ -    lazy_loading: bool = False
â”‚ â”‚ -    max_answer_length: int = 100
â”‚ â”‚ -    max_query_length: int = 64
â”‚ â”‚ -    n_best_size: int = 20
â”‚ â”‚ -    null_score_diff_threshold: float = 0.0
â”‚ â”‚ -    special_tokens_list: list = field(default_factory=list)
â”‚ â”‚ -
â”‚ â”‚ -
â”‚ â”‚ -@dataclass
â”‚ â”‚  class T5Args(ModelArgs):
â”‚ â”‚      """
â”‚ â”‚      Model args for a T5Model
â”‚ â”‚      """
â”‚ â”‚  
â”‚ â”‚      model_class: str = "T5Model"
â”‚ â”‚      dataset_class: Dataset = None
â”‚ â”‚ @@ -238,37 +227,14 @@
â”‚ â”‚      vocab_size: int = None
â”‚ â”‚      clean_text: bool = True
â”‚ â”‚      handle_chinese_chars: bool = True
â”‚ â”‚      special_tokens_list: list = field(default_factory=list)
â”‚ â”‚      strip_accents: bool = True
â”‚ â”‚      local_rank: int = -1
â”‚ â”‚  
â”‚ â”‚ -    def save(self, output_dir):
â”‚ â”‚ -        os.makedirs(output_dir, exist_ok=True)
â”‚ â”‚ -        with open(os.path.join(output_dir, "model_args.json"), "w") as f:
â”‚ â”‚ -            args_dict = self.get_args_for_saving()
â”‚ â”‚ -            if args_dict["dataset_class"] is not None:
â”‚ â”‚ -                args_dict["dataset_class"] = type(args_dict["dataset_class"]).__name__
â”‚ â”‚ -            # json.dump(self.get_args_for_saving(), f)
â”‚ â”‚ -            json.dump(args_dict, f)
â”‚ â”‚ -
â”‚ â”‚ -    def load(self, input_dir):
â”‚ â”‚ -        if input_dir:
â”‚ â”‚ -            model_args_file = os.path.join(input_dir, "model_args.json")
â”‚ â”‚ -            if os.path.isfile(model_args_file):
â”‚ â”‚ -                with open(model_args_file, "r") as f:
â”‚ â”‚ -                    model_args = json.load(f)
â”‚ â”‚ -                if model_args["dataset_class"]:
â”‚ â”‚ -                    warnings.warn(
â”‚ â”‚ -                        "This model was trained using a custom dataset_class."
â”‚ â”‚ -                        "This cannot be loaded automatically and must be specified in the model args"
â”‚ â”‚ -                        "when loading the model."
â”‚ â”‚ -                    )
â”‚ â”‚ -                self.update_from_dict(model_args)
â”‚ â”‚ -
â”‚ â”‚  
â”‚ â”‚  @dataclass
â”‚ â”‚  class Seq2SeqArgs(ModelArgs):
â”‚ â”‚      """
â”‚ â”‚      Model args for a Seq2SeqModel
â”‚ â”‚      """
â”‚ â”‚  
â”‚ â”‚ @@ -293,36 +259,14 @@
â”‚ â”‚      save_knowledge_dataset: bool = True
â”‚ â”‚      save_knowledge_dataset_with_checkpoints: bool = False
â”‚ â”‚      split_text_character: str = " "
â”‚ â”‚      split_text_n: int = 100
â”‚ â”‚      src_lang: str = "en_XX"
â”‚ â”‚      tgt_lang: str = "ro_RO"
â”‚ â”‚  
â”‚ â”‚ -    def save(self, output_dir):
â”‚ â”‚ -        os.makedirs(output_dir, exist_ok=True)
â”‚ â”‚ -        with open(os.path.join(output_dir, "model_args.json"), "w") as f:
â”‚ â”‚ -            args_dict = self.get_args_for_saving()
â”‚ â”‚ -            if args_dict["dataset_class"] is not None:
â”‚ â”‚ -                args_dict["dataset_class"] = type(args_dict["dataset_class"]).__name__
â”‚ â”‚ -            json.dump(args_dict, f)
â”‚ â”‚ -
â”‚ â”‚ -    def load(self, input_dir):
â”‚ â”‚ -        if input_dir:
â”‚ â”‚ -            model_args_file = os.path.join(input_dir, "model_args.json")
â”‚ â”‚ -            if os.path.isfile(model_args_file):
â”‚ â”‚ -                with open(model_args_file, "r") as f:
â”‚ â”‚ -                    model_args = json.load(f)
â”‚ â”‚ -                if model_args["dataset_class"]:
â”‚ â”‚ -                    warnings.warn(
â”‚ â”‚ -                        "This model was trained using a custom dataset_class."
â”‚ â”‚ -                        "This cannot be loaded automatically and must be specified in the model args"
â”‚ â”‚ -                        "when loading the model."
â”‚ â”‚ -                    )
â”‚ â”‚ -                self.update_from_dict(model_args)
â”‚ â”‚ -
â”‚ â”‚  
â”‚ â”‚  @dataclass
â”‚ â”‚  class LanguageGenerationArgs(ModelArgs):
â”‚ â”‚      """
â”‚ â”‚      Model args for a LanguageGenerationModel
â”‚ â”‚      """
â”‚ â”‚  
â”‚ â”‚ @@ -381,7 +325,53 @@
â”‚ â”‚      ff_embed_dim: int = 3072
â”‚ â”‚      num_heads: int = 12
â”‚ â”‚      num_layers: int = 12
â”‚ â”‚      dropout: float = 0.2
â”‚ â”‚      warmup_ratio: float = 0.05
â”‚ â”‚      weight_decay: float = 0.0
â”‚ â”‚      smoothing_factor: float = 0.1
â”‚ â”‚ +
â”‚ â”‚ +
â”‚ â”‚ +@dataclass
â”‚ â”‚ +class ChatGlmArgs(ModelArgs):
â”‚ â”‚ +    """
â”‚ â”‚ +    Model args for a ChatGLMModel
â”‚ â”‚ +    """
â”‚ â”‚ +
â”‚ â”‚ +    model_class: str = "ChatGlmArgs"
â”‚ â”‚ +    dataset_class: Dataset = None
â”‚ â”‚ +    fp16: bool = True
â”‚ â”‚ +    int8: bool = False
â”‚ â”‚ +    quantization_bit:int = None  # if use quantization bit, set 4, else None
â”‚ â”‚ +    debug: bool = False
â”‚ â”‚ +    max_seq_length: int = 256  # max length of input sequence
â”‚ â”‚ +    max_length = 384  # max length of the sequence to be generated
â”‚ â”‚ +    do_sample: bool = True
â”‚ â”‚ +    early_stopping: bool = True
â”‚ â”‚ +    evaluate_generated_text: bool = False
â”‚ â”‚ +    length_penalty: float = 2.0
â”‚ â”‚ +    num_beams: int = 1
â”‚ â”‚ +    num_return_sequences: int = 1
â”‚ â”‚ +    repetition_penalty: float = 1.0
â”‚ â”‚ +    temperature: float = 0.95
â”‚ â”‚ +    special_tokens_list: list = field(default_factory=list)
â”‚ â”‚ +    top_k: float = None
â”‚ â”‚ +    top_p: float = 0.7
â”‚ â”‚ +    model_name_or_path: Optional[str] = field(default="THUDM/chatglm-6b")
â”‚ â”‚ +    dataset_name_or_path: Optional[str] = field(default="shibing624/alpaca-zh")
â”‚ â”‚ +    use_lora: bool = True
â”‚ â”‚ +    lora_name: str = field(default="adapter_model.bin")
â”‚ â”‚ +    lora_rank: int = field(default=8)
â”‚ â”‚ +    lora_alpha = 32
â”‚ â”‚ +    lora_dropout = 0.1
â”‚ â”‚ +    lora_target_modules = ["query_key_value"]
â”‚ â”‚ +    lora_bias = "none"
â”‚ â”‚ +    only_lora_state_dict: bool = False
â”‚ â”‚ +    num_train_epochs = 1
â”‚ â”‚ +    max_steps = -1
â”‚ â”‚ +    per_device_train_batch_size = 2
â”‚ â”‚ +    gradient_accumulation_steps = 1
â”‚ â”‚ +    save_total_limit = 2
â”‚ â”‚ +    remove_unused_columns = False
â”‚ â”‚ +    logging_steps = 50
â”‚ â”‚ +    resume_from_checkpoint:str = None
â”‚ â”‚ +
â”‚ â”‚   --- textgen-0.1.7/textgen/custom_models/models.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/custom_models/models.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/data/HowNetPOSWord.txt
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/data/HowNetPOSWord.txt
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/data/stopwords.txt
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/data/stopwords.txt
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/language_generation/language_generation_model.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/language_generation/language_generation_model.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/language_generation/language_generation_utils.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/language_generation/language_generation_utils.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/language_modeling/language_modeling_model.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/language_modeling/language_modeling_model.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/language_modeling/language_modeling_utils.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/language_modeling/language_modeling_utils.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/language_modeling/songnet_model.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/language_modeling/songnet_model.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/language_modeling/songnet_utils.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/language_modeling/songnet_utils.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/question_answering/question_answering_model.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/t5/t5_model.py
â”‚ â”‚â”„ Files 15% similar despite different names
â”‚ â”‚ @@ -1,407 +1,201 @@
â”‚ â”‚  # -*- coding: utf-8 -*-
â”‚ â”‚  """
â”‚ â”‚  @author:XuMing(xuming624@qq.com)
â”‚ â”‚  @description: refer https://github.com/ThilinaRajapakse/simpletransformers
â”‚ â”‚  """
â”‚ â”‚  
â”‚ â”‚ -import json
â”‚ â”‚ -import logging
â”‚ â”‚  import math
â”‚ â”‚  import os
â”‚ â”‚  import random
â”‚ â”‚  import warnings
â”‚ â”‚  from dataclasses import asdict
â”‚ â”‚ -from multiprocessing import cpu_count
â”‚ â”‚ +from multiprocessing import Pool
â”‚ â”‚  
â”‚ â”‚  import numpy as np
â”‚ â”‚  import pandas as pd
â”‚ â”‚  import torch
â”‚ â”‚ -from scipy.stats import pearsonr
â”‚ â”‚ -from sklearn.metrics import (
â”‚ â”‚ -    confusion_matrix,
â”‚ â”‚ -    label_ranking_average_precision_score,
â”‚ â”‚ -    matthews_corrcoef,
â”‚ â”‚ -    mean_squared_error,
â”‚ â”‚ -)
â”‚ â”‚ +from loguru import logger
â”‚ â”‚ +from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
â”‚ â”‚  from torch.utils.tensorboard import SummaryWriter
â”‚ â”‚ -from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
â”‚ â”‚ -from torch.utils.data.distributed import DistributedSampler
â”‚ â”‚  from tqdm.auto import tqdm, trange
â”‚ â”‚ +from transformers import ByT5Tokenizer
â”‚ â”‚ +from transformers import MT5Config, MT5ForConditionalGeneration
â”‚ â”‚ +from transformers import T5Config, T5ForConditionalGeneration, T5Tokenizer
â”‚ â”‚ +from transformers.optimization import AdamW, Adafactor
â”‚ â”‚  from transformers.optimization import (
â”‚ â”‚      get_constant_schedule,
â”‚ â”‚      get_constant_schedule_with_warmup,
â”‚ â”‚      get_linear_schedule_with_warmup,
â”‚ â”‚      get_cosine_schedule_with_warmup,
â”‚ â”‚      get_cosine_with_hard_restarts_schedule_with_warmup,
â”‚ â”‚      get_polynomial_decay_schedule_with_warmup,
â”‚ â”‚  )
â”‚ â”‚ -from transformers.optimization import AdamW, Adafactor
â”‚ â”‚ -from transformers import (
â”‚ â”‚ -    AlbertConfig,
â”‚ â”‚ -    AlbertForQuestionAnswering,
â”‚ â”‚ -    AlbertTokenizer,
â”‚ â”‚ -    AutoConfig,
â”‚ â”‚ -    AutoModelForQuestionAnswering,
â”‚ â”‚ -    AutoTokenizer,
â”‚ â”‚ -    BartConfig,
â”‚ â”‚ -    BartForQuestionAnswering,
â”‚ â”‚ -    BartTokenizer,
â”‚ â”‚ -    BertConfig,
â”‚ â”‚ -    BertForQuestionAnswering,
â”‚ â”‚ -    BertTokenizer,
â”‚ â”‚ -    CamembertConfig,
â”‚ â”‚ -    CamembertForQuestionAnswering,
â”‚ â”‚ -    CamembertTokenizer,
â”‚ â”‚ -    DistilBertConfig,
â”‚ â”‚ -    DistilBertForQuestionAnswering,
â”‚ â”‚ -    DistilBertTokenizer,
â”‚ â”‚ -    ElectraConfig,
â”‚ â”‚ -    ElectraTokenizer,
â”‚ â”‚ -    LongformerConfig,
â”‚ â”‚ -    LongformerForQuestionAnswering,
â”‚ â”‚ -    LongformerTokenizer,
â”‚ â”‚ -    MPNetConfig,
â”‚ â”‚ -    MPNetForQuestionAnswering,
â”‚ â”‚ -    MPNetTokenizer,
â”‚ â”‚ -    MobileBertConfig,
â”‚ â”‚ -    MobileBertForQuestionAnswering,
â”‚ â”‚ -    MobileBertTokenizer,
â”‚ â”‚ -    RobertaConfig,
â”‚ â”‚ -    RobertaForQuestionAnswering,
â”‚ â”‚ -    RobertaTokenizer,
â”‚ â”‚ -    SqueezeBertConfig,
â”‚ â”‚ -    SqueezeBertForQuestionAnswering,
â”‚ â”‚ -    SqueezeBertTokenizer,
â”‚ â”‚ -    WEIGHTS_NAME,
â”‚ â”‚ -    XLMConfig,
â”‚ â”‚ -    XLMForQuestionAnswering,
â”‚ â”‚ -    XLMRobertaConfig,
â”‚ â”‚ -    XLMRobertaTokenizer,
â”‚ â”‚ -    XLMTokenizer,
â”‚ â”‚ -    XLNetConfig,
â”‚ â”‚ -    XLNetForQuestionAnswering,
â”‚ â”‚ -    XLNetTokenizer,
â”‚ â”‚ -)
â”‚ â”‚  
â”‚ â”‚ -from textgen.config.model_args import QuestionAnsweringArgs
â”‚ â”‚ -from textgen.custom_models.models import ElectraForQuestionAnswering, XLMRobertaForQuestionAnswering
â”‚ â”‚ -from textgen.question_answering.question_answering_utils import (
â”‚ â”‚ -    LazyQuestionAnsweringDataset,
â”‚ â”‚ -    RawResult,
â”‚ â”‚ -    RawResultExtended,
â”‚ â”‚ -    build_examples,
â”‚ â”‚ -    get_best_predictions,
â”‚ â”‚ -    get_best_predictions_extended,
â”‚ â”‚ -    get_examples,
â”‚ â”‚ -    load_hf_dataset,
â”‚ â”‚ -    squad_convert_examples_to_features,
â”‚ â”‚ -    to_list,
â”‚ â”‚ -    write_predictions,
â”‚ â”‚ -    write_predictions_extended,
â”‚ â”‚ -)
â”‚ â”‚ -from loguru import logger
â”‚ â”‚ +from textgen.config.model_args import T5Args
â”‚ â”‚ +from textgen.t5.t5_utils import T5Dataset, load_hf_dataset
â”‚ â”‚  
â”‚ â”‚  try:
â”‚ â”‚      import wandb
â”‚ â”‚  
â”‚ â”‚      wandb_available = True
â”‚ â”‚  except ImportError:
â”‚ â”‚      wandb_available = False
â”‚ â”‚  
â”‚ â”‚  has_cuda = torch.cuda.is_available()
â”‚ â”‚  os.environ["TOKENIZERS_PARALLELISM"] = "FALSE"
â”‚ â”‚  os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚ -class QuestionAnsweringModel:
â”‚ â”‚ +def chunks(lst, n):
â”‚ â”‚ +    """Yield successive n-sized chunks from lst."""
â”‚ â”‚ +    for i in range(0, len(lst), n):
â”‚ â”‚ +        yield lst[i: i + n]
â”‚ â”‚ +
â”‚ â”‚ +
â”‚ â”‚ +MODEL_CLASSES = {
â”‚ â”‚ +    "t5": (T5Config, T5ForConditionalGeneration),
â”‚ â”‚ +    "mt5": (MT5Config, MT5ForConditionalGeneration),
â”‚ â”‚ +    "byt5": (T5Config, T5ForConditionalGeneration),
â”‚ â”‚ +}
â”‚ â”‚ +
â”‚ â”‚ +
â”‚ â”‚ +class T5Model:
â”‚ â”‚      def __init__(
â”‚ â”‚ -            self, model_type, model_name, args=None, use_cuda=has_cuda, cuda_device=-1, **kwargs
â”‚ â”‚ +            self,
â”‚ â”‚ +            model_type,
â”‚ â”‚ +            model_name,
â”‚ â”‚ +            args=None,
â”‚ â”‚ +            tokenizer=None,
â”‚ â”‚ +            use_cuda=has_cuda,
â”‚ â”‚ +            cuda_device=-1,
â”‚ â”‚ +            **kwargs,
â”‚ â”‚      ):
â”‚ â”‚ +
â”‚ â”‚          """
â”‚ â”‚ -        Initializes a QuestionAnsweringModel model.
â”‚ â”‚ +        Initializes a T5Model model.
â”‚ â”‚  
â”‚ â”‚          Args:
â”‚ â”‚ -            model_type: The type of model (bert, xlnet, xlm, distilbert)
â”‚ â”‚ -            model_name: Default Transformer model name or path to a directory containing Transformer model file (pytorch_nodel.bin).
â”‚ â”‚ -            args (optional): Default args will be used if this parameter is not provided. If provided,
â”‚ â”‚ -                it should be a dict containing the args that should be changed in the default args'
â”‚ â”‚ +            model_type: The type of model (t5, mt5, byt5)
â”‚ â”‚ +            model_name: The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.
â”‚ â”‚ +            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.
â”‚ â”‚              use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.
â”‚ â”‚              cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.
â”‚ â”‚ +            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.
â”‚ â”‚          """  # noqa: ignore flake8"
â”‚ â”‚  
â”‚ â”‚ -        MODEL_CLASSES = {
â”‚ â”‚ -            "albert": (AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer),
â”‚ â”‚ -            "auto": (AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer),
â”‚ â”‚ -            "bart": (BartConfig, BartForQuestionAnswering, BartTokenizer),
â”‚ â”‚ -            "bert": (BertConfig, BertForQuestionAnswering, BertTokenizer),
â”‚ â”‚ -            "camembert": (
â”‚ â”‚ -                CamembertConfig,
â”‚ â”‚ -                CamembertForQuestionAnswering,
â”‚ â”‚ -                CamembertTokenizer,
â”‚ â”‚ -            ),
â”‚ â”‚ -            "distilbert": (
â”‚ â”‚ -                DistilBertConfig,
â”‚ â”‚ -                DistilBertForQuestionAnswering,
â”‚ â”‚ -                DistilBertTokenizer,
â”‚ â”‚ -            ),
â”‚ â”‚ -            "electra": (ElectraConfig, ElectraForQuestionAnswering, ElectraTokenizer),
â”‚ â”‚ -            "longformer": (
â”‚ â”‚ -                LongformerConfig,
â”‚ â”‚ -                LongformerForQuestionAnswering,
â”‚ â”‚ -                LongformerTokenizer,
â”‚ â”‚ -            ),
â”‚ â”‚ -            "mobilebert": (
â”‚ â”‚ -                MobileBertConfig,
â”‚ â”‚ -                MobileBertForQuestionAnswering,
â”‚ â”‚ -                MobileBertTokenizer,
â”‚ â”‚ -            ),
â”‚ â”‚ -            "mpnet": (MPNetConfig, MPNetForQuestionAnswering, MPNetTokenizer),
â”‚ â”‚ -            "roberta": (RobertaConfig, RobertaForQuestionAnswering, RobertaTokenizer),
â”‚ â”‚ -            "squeezebert": (
â”‚ â”‚ -                SqueezeBertConfig,
â”‚ â”‚ -                SqueezeBertForQuestionAnswering,
â”‚ â”‚ -                SqueezeBertTokenizer,
â”‚ â”‚ -            ),
â”‚ â”‚ -            "xlm": (XLMConfig, XLMForQuestionAnswering, XLMTokenizer),
â”‚ â”‚ -            "xlmroberta": (
â”‚ â”‚ -                XLMRobertaConfig,
â”‚ â”‚ -                XLMRobertaForQuestionAnswering,
â”‚ â”‚ -                XLMRobertaTokenizer,
â”‚ â”‚ -            ),
â”‚ â”‚ -            "xlnet": (XLNetConfig, XLNetForQuestionAnswering, XLNetTokenizer),
â”‚ â”‚ -        }
â”‚ â”‚ -
â”‚ â”‚          self.args = self._load_model_args(model_name)
â”‚ â”‚  
â”‚ â”‚          if isinstance(args, dict):
â”‚ â”‚              self.args.update_from_dict(args)
â”‚ â”‚ -        elif isinstance(args, QuestionAnsweringArgs):
â”‚ â”‚ +        elif isinstance(args, T5Args):
â”‚ â”‚              self.args = args
â”‚ â”‚  
â”‚ â”‚          self.is_sweeping = False
â”‚ â”‚  
â”‚ â”‚          if self.args.manual_seed:
â”‚ â”‚              random.seed(self.args.manual_seed)
â”‚ â”‚              np.random.seed(self.args.manual_seed)
â”‚ â”‚              torch.manual_seed(self.args.manual_seed)
â”‚ â”‚              if self.args.n_gpu > 0:
â”‚ â”‚                  torch.cuda.manual_seed_all(self.args.manual_seed)
â”‚ â”‚  
â”‚ â”‚ -        if not use_cuda:
â”‚ â”‚ -            self.args.fp16 = False
â”‚ â”‚ -
â”‚ â”‚ -        config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]
â”‚ â”‚ -        self.config = config_class.from_pretrained(model_name, **self.args.config)
â”‚ â”‚ -        if not self.args.quantized_model:
â”‚ â”‚ -            self.model = model_class.from_pretrained(
â”‚ â”‚ -                model_name, config=self.config, **kwargs
â”‚ â”‚ -            )
â”‚ â”‚ -        else:
â”‚ â”‚ -            quantized_weights = torch.load(
â”‚ â”‚ -                os.path.join(model_name, "pytorch_model.bin")
â”‚ â”‚ -            )
â”‚ â”‚ -            self.model = model_class.from_pretrained(
â”‚ â”‚ -                None, config=self.config, state_dict=quantized_weights
â”‚ â”‚ -            )
â”‚ â”‚ -
â”‚ â”‚ -        if self.args.dynamic_quantize:
â”‚ â”‚ -            self.model = torch.quantization.quantize_dynamic(
â”‚ â”‚ -                self.model, {torch.nn.Linear}, dtype=torch.qint8
â”‚ â”‚ -            )
â”‚ â”‚ -        if self.args.quantized_model:
â”‚ â”‚ -            self.model.load_state_dict(quantized_weights)
â”‚ â”‚ -        if self.args.dynamic_quantize:
â”‚ â”‚ -            self.args.quantized_model = True
â”‚ â”‚ -
â”‚ â”‚          if use_cuda:
â”‚ â”‚              if torch.cuda.is_available():
â”‚ â”‚                  if cuda_device == -1:
â”‚ â”‚                      self.device = torch.device("cuda")
â”‚ â”‚                  else:
â”‚ â”‚                      self.device = torch.device(f"cuda:{cuda_device}")
â”‚ â”‚              else:
â”‚ â”‚                  raise ValueError(
â”‚ â”‚                      "'use_cuda' set to True when cuda is unavailable."
â”‚ â”‚ -                    " Make sure CUDA is available or set use_cuda=False."
â”‚ â”‚ +                    "Make sure CUDA is available or set `use_cuda=False`."
â”‚ â”‚                  )
â”‚ â”‚          else:
â”‚ â”‚              self.device = "cpu"
â”‚ â”‚          logger.debug(f"Device: {self.device}")
â”‚ â”‚  
â”‚ â”‚          self.results = {}
â”‚ â”‚  
â”‚ â”‚ -        if self.args.fp16:
â”‚ â”‚ -            try:
â”‚ â”‚ -                from torch.cuda import amp
â”‚ â”‚ -            except AttributeError:
â”‚ â”‚ -                raise AttributeError(
â”‚ â”‚ -                    "fp16 requires Pytorch >= 1.6. Please update Pytorch or turn off fp16."
â”‚ â”‚ -                )
â”‚ â”‚ +        config_class, model_class = MODEL_CLASSES[model_type]
â”‚ â”‚ +
â”‚ â”‚ +        if model_name is None:
â”‚ â”‚ +            self.config = self.args.config
â”‚ â”‚ +            self.model = model_class(config=self.config)
â”‚ â”‚ +        else:
â”‚ â”‚ +            self.config = config_class.from_pretrained(model_name, **self.args.config)
â”‚ â”‚ +            self.model = model_class.from_pretrained(model_name, config=self.config)
â”‚ â”‚  
â”‚ â”‚ -        if model_type == "auto":
â”‚ â”‚ -            self.tokenizer = tokenizer_class.from_pretrained(model_name, **kwargs)
â”‚ â”‚ +        if isinstance(tokenizer, T5Tokenizer):
â”‚ â”‚ +            self.tokenizer = tokenizer
â”‚ â”‚ +            self.model.resize_token_embeddings(len(self.tokenizer))
â”‚ â”‚ +        elif model_type == "byt5":
â”‚ â”‚ +            self.tokenizer = ByT5Tokenizer.from_pretrained(model_name, truncate=True)
â”‚ â”‚          else:
â”‚ â”‚ -            self.tokenizer = tokenizer_class.from_pretrained(
â”‚ â”‚ -                model_name, do_lower_case=self.args.do_lower_case, **kwargs
â”‚ â”‚ +            self.tokenizer = T5Tokenizer.from_pretrained(model_name, truncate=True)
â”‚ â”‚ +
â”‚ â”‚ +        if self.args.dynamic_quantize:
â”‚ â”‚ +            self.model = torch.quantization.quantize_dynamic(
â”‚ â”‚ +                self.model, {torch.nn.Linear}, dtype=torch.qint8
â”‚ â”‚              )
â”‚ â”‚  
â”‚ â”‚ +        if not use_cuda:
â”‚ â”‚ +            self.args.fp16 = False
â”‚ â”‚ +
â”‚ â”‚          if self.args.special_tokens_list:
â”‚ â”‚              self.tokenizer.add_tokens(
â”‚ â”‚                  self.args.special_tokens_list, special_tokens=True
â”‚ â”‚              )
â”‚ â”‚              self.model.resize_token_embeddings(len(self.tokenizer))
â”‚ â”‚  
â”‚ â”‚ -        self.args.model_name = model_name
â”‚ â”‚          self.args.model_type = model_type
â”‚ â”‚ +        if model_name is None:
â”‚ â”‚ +            self.args.model_name = "T5_from_scratch"
â”‚ â”‚ +        else:
â”‚ â”‚ +            self.args.model_name = model_name
â”‚ â”‚  
â”‚ â”‚ -        self.wandb_run_id = None
â”‚ â”‚          if self.args.wandb_project and not wandb_available:
â”‚ â”‚              warnings.warn(
â”‚ â”‚                  "wandb_project specified but wandb is not available. Wandb disabled."
â”‚ â”‚              )
â”‚ â”‚              self.args.wandb_project = None
â”‚ â”‚  
â”‚ â”‚ -    def load_and_cache_examples(
â”‚ â”‚ -            self, examples, evaluate=False, no_cache=False, output_examples=False
â”‚ â”‚ -    ):
â”‚ â”‚ -        """
â”‚ â”‚ -        Converts a list of examples to a TensorDataset containing InputFeatures. Caches the InputFeatures.
â”‚ â”‚ -
â”‚ â”‚ -        Utility function for train() and eval() methods. Not intended to be used directly.
â”‚ â”‚ -        """
â”‚ â”‚ -
â”‚ â”‚ -        tokenizer = self.tokenizer
â”‚ â”‚ -        args = self.args
â”‚ â”‚ -
â”‚ â”‚ -        if not no_cache:
â”‚ â”‚ -            no_cache = args.no_cache
â”‚ â”‚ -
â”‚ â”‚ -        if not no_cache:
â”‚ â”‚ -            os.makedirs(self.args.cache_dir, exist_ok=True)
â”‚ â”‚ -
â”‚ â”‚ -        examples = get_examples(examples, is_training=not evaluate)
â”‚ â”‚ -
â”‚ â”‚ -        mode = "dev" if evaluate else "train"
â”‚ â”‚ -        cached_features_file = os.path.join(
â”‚ â”‚ -            args.cache_dir,
â”‚ â”‚ -            "cached_{}_{}_{}_{}".format(
â”‚ â”‚ -                mode, args.model_type, args.max_seq_length, len(examples)
â”‚ â”‚ -            ),
â”‚ â”‚ -        )
â”‚ â”‚ -
â”‚ â”‚ -        if os.path.exists(cached_features_file) and (
â”‚ â”‚ -                (not args.reprocess_input_data and not no_cache)
â”‚ â”‚ -                or (mode == "dev" and args.use_cached_eval_features)
â”‚ â”‚ -        ):
â”‚ â”‚ -            features = torch.load(cached_features_file)
â”‚ â”‚ -            logger.info(f" Features loaded from cache at {cached_features_file}")
â”‚ â”‚ -
â”‚ â”‚ -            # Convert to Tensors and build dataset
â”‚ â”‚ -            all_input_ids = torch.tensor(
â”‚ â”‚ -                [f.input_ids for f in features], dtype=torch.long
â”‚ â”‚ -            )
â”‚ â”‚ -            all_attention_masks = torch.tensor(
â”‚ â”‚ -                [f.attention_mask for f in features], dtype=torch.long
â”‚ â”‚ -            )
â”‚ â”‚ -            all_token_type_ids = torch.tensor(
â”‚ â”‚ -                [f.token_type_ids for f in features], dtype=torch.long
â”‚ â”‚ -            )
â”‚ â”‚ -            all_cls_index = torch.tensor(
â”‚ â”‚ -                [f.cls_index for f in features], dtype=torch.long
â”‚ â”‚ -            )
â”‚ â”‚ -            all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)
â”‚ â”‚ -            all_is_impossible = torch.tensor(
â”‚ â”‚ -                [f.is_impossible for f in features], dtype=torch.float
â”‚ â”‚ -            )
â”‚ â”‚ -
â”‚ â”‚ -            if mode == "dev":
â”‚ â”‚ -                all_feature_index = torch.arange(
â”‚ â”‚ -                    all_input_ids.size(0), dtype=torch.long
â”‚ â”‚ -                )
â”‚ â”‚ -                dataset = TensorDataset(
â”‚ â”‚ -                    all_input_ids,
â”‚ â”‚ -                    all_attention_masks,
â”‚ â”‚ -                    all_token_type_ids,
â”‚ â”‚ -                    all_feature_index,
â”‚ â”‚ -                    all_cls_index,
â”‚ â”‚ -                    all_p_mask,
â”‚ â”‚ -                )
â”‚ â”‚ -            else:
â”‚ â”‚ -                all_start_positions = torch.tensor(
â”‚ â”‚ -                    [f.start_position for f in features], dtype=torch.long
â”‚ â”‚ -                )
â”‚ â”‚ -                all_end_positions = torch.tensor(
â”‚ â”‚ -                    [f.end_position for f in features], dtype=torch.long
â”‚ â”‚ -                )
â”‚ â”‚ -                dataset = TensorDataset(
â”‚ â”‚ -                    all_input_ids,
â”‚ â”‚ -                    all_attention_masks,
â”‚ â”‚ -                    all_token_type_ids,
â”‚ â”‚ -                    all_start_positions,
â”‚ â”‚ -                    all_end_positions,
â”‚ â”‚ -                    all_cls_index,
â”‚ â”‚ -                    all_p_mask,
â”‚ â”‚ -                    all_is_impossible,
â”‚ â”‚ -                )
â”‚ â”‚ -        else:
â”‚ â”‚ -            logger.info(" Converting to features started.")
â”‚ â”‚ -
â”‚ â”‚ -            features, dataset = squad_convert_examples_to_features(
â”‚ â”‚ -                examples=examples,
â”‚ â”‚ -                tokenizer=tokenizer,
â”‚ â”‚ -                max_seq_length=args.max_seq_length,
â”‚ â”‚ -                doc_stride=args.doc_stride,
â”‚ â”‚ -                max_query_length=args.max_query_length,
â”‚ â”‚ -                is_training=not evaluate,
â”‚ â”‚ -                tqdm_enabled=not args.silent,
â”‚ â”‚ -                threads=args.process_count,
â”‚ â”‚ -                args=args,
â”‚ â”‚ -            )
â”‚ â”‚ -
â”‚ â”‚ -            if not no_cache:
â”‚ â”‚ -                torch.save(features, cached_features_file)
â”‚ â”‚ -
â”‚ â”‚ -        if output_examples:
â”‚ â”‚ -            return dataset, examples, features
â”‚ â”‚ -        return dataset
â”‚ â”‚ -
â”‚ â”‚      def train_model(
â”‚ â”‚              self,
â”‚ â”‚              train_data,
â”‚ â”‚ -            output_dir=False,
â”‚ â”‚ +            output_dir=None,
â”‚ â”‚              show_running_loss=True,
â”‚ â”‚              args=None,
â”‚ â”‚              eval_data=None,
â”‚ â”‚              verbose=True,
â”‚ â”‚              **kwargs,
â”‚ â”‚      ):
â”‚ â”‚          """
â”‚ â”‚          Trains the model using 'train_data'
â”‚ â”‚  
â”‚ â”‚          Args:
â”‚ â”‚ -            train_data: Path to JSON file containing training data OR list of Python dicts in the correct format. The model will be trained on this data.
â”‚ â”‚ +            train_data: Pandas DataFrame containing the 3 columns - `prefix`, `input_text`, `target_text`.
â”‚ â”‚ +                        - `prefix`: A string indicating the task to perform. (E.g. `"question"`, `"stsb"`)
â”‚ â”‚ +                        - `input_text`: The input text sequence. `prefix` is automatically prepended to form the full input. (<prefix>: <input_text>)
â”‚ â”‚ +                        - `target_text`: The target sequence
â”‚ â”‚              output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.
â”‚ â”‚              show_running_loss (optional): Set to False to prevent running loss from being printed to console. Defaults to True.
â”‚ â”‚              args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.
â”‚ â”‚ -            eval_data (optional): Path to JSON file containing evaluation data against which evaluation will be performed when evaluate_during_training is enabled.
â”‚ â”‚ -                Is required if evaluate_during_training is enabled.
â”‚ â”‚ +            eval_data (optional): A DataFrame against which evaluation will be performed when evaluate_during_training is enabled. Is required if evaluate_during_training is enabled.
â”‚ â”‚              **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).
â”‚ â”‚ -                A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.
â”‚ â”‚ +                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs
â”‚ â”‚ +                        will be lists of strings. Note that this will slow down training significantly as the predicted sequences need to be generated.
â”‚ â”‚ +
â”‚ â”‚          Returns:
â”‚ â”‚              global_step: Number of global steps trained
â”‚ â”‚              training_details: Average training loss if evaluate_during_training is False or full training progress scores if evaluate_during_training is True
â”‚ â”‚          """  # noqa: ignore flake8"
â”‚ â”‚  
â”‚ â”‚          if args:
â”‚ â”‚              self.args.update_from_dict(args)
â”‚ â”‚ -
â”‚ â”‚ -        if self.args.silent:
â”‚ â”‚ -            show_running_loss = False
â”‚ â”‚ -
â”‚ â”‚          if self.args.evaluate_during_training and eval_data is None:
â”‚ â”‚              raise ValueError(
â”‚ â”‚                  "evaluate_during_training is enabled but eval_data is not specified."
â”‚ â”‚                  " Pass eval_data to model.train_model() if using evaluate_during_training."
â”‚ â”‚              )
â”‚ â”‚  
â”‚ â”‚          if not output_dir:
â”‚ â”‚ @@ -410,58 +204,40 @@
â”‚ â”‚          if (
â”‚ â”‚                  os.path.exists(output_dir)
â”‚ â”‚                  and os.listdir(output_dir)
â”‚ â”‚                  and not self.args.overwrite_output_dir
â”‚ â”‚          ):
â”‚ â”‚              raise ValueError(
â”‚ â”‚                  "Output directory ({}) already exists and is not empty."
â”‚ â”‚ -                "Use --overwrite_output_dir to overcome.".format(output_dir)
â”‚ â”‚ +                " Set args.overwrite_output_dir = True to overcome.".format(output_dir)
â”‚ â”‚              )
â”‚ â”‚  
â”‚ â”‚          self._move_model_to_device()
â”‚ â”‚  
â”‚ â”‚ -        if self.args.use_hf_datasets:
â”‚ â”‚ -            train_dataset = load_hf_dataset(
â”‚ â”‚ -                train_data, self.tokenizer, self.args, is_training=True
â”‚ â”‚ -            )
â”‚ â”‚ -        elif self.args.lazy_loading:
â”‚ â”‚ -            if isinstance(train_data, str):
â”‚ â”‚ -                train_dataset = LazyQuestionAnsweringDataset(
â”‚ â”‚ -                    train_data, self.tokenizer, self.args
â”‚ â”‚ -                )
â”‚ â”‚ -            else:
â”‚ â”‚ -                raise ValueError(
â”‚ â”‚ -                    "Input must be given as a path to a file when using lazy loading"
â”‚ â”‚ -                )
â”‚ â”‚ -        else:
â”‚ â”‚ -            if isinstance(train_data, str):
â”‚ â”‚ -                with open(train_data, "r", encoding=self.args.encoding) as f:
â”‚ â”‚ -                    train_examples = json.load(f)
â”‚ â”‚ -            else:
â”‚ â”‚ -                train_examples = train_data
â”‚ â”‚ -
â”‚ â”‚ -            train_dataset = self.load_and_cache_examples(train_examples)
â”‚ â”‚ +        train_dataset = self.load_and_cache_examples(train_data, verbose=verbose)
â”‚ â”‚  
â”‚ â”‚          os.makedirs(output_dir, exist_ok=True)
â”‚ â”‚  
â”‚ â”‚          global_step, training_details = self.train(
â”‚ â”‚              train_dataset,
â”‚ â”‚              output_dir,
â”‚ â”‚              show_running_loss=show_running_loss,
â”‚ â”‚              eval_data=eval_data,
â”‚ â”‚ +            verbose=verbose,
â”‚ â”‚              **kwargs,
â”‚ â”‚          )
â”‚ â”‚  
â”‚ â”‚          self.save_model(model=self.model)
â”‚ â”‚  
â”‚ â”‚ -        logger.info(
â”‚ â”‚ -            " Training of {} model complete. Saved to {}.".format(
â”‚ â”‚ -                self.args.model_type, output_dir
â”‚ â”‚ +        if verbose:
â”‚ â”‚ +            logger.info(
â”‚ â”‚ +                " Training of {} model complete. Saved to {}.".format(
â”‚ â”‚ +                    self.args.model_name, output_dir
â”‚ â”‚ +                )
â”‚ â”‚              )
â”‚ â”‚ -        )
â”‚ â”‚  
â”‚ â”‚          return global_step, training_details
â”‚ â”‚  
â”‚ â”‚      def train(
â”‚ â”‚              self,
â”‚ â”‚              train_dataset,
â”‚ â”‚              output_dir,
â”‚ â”‚ @@ -474,29 +250,38 @@
â”‚ â”‚          Trains the model on train_dataset.
â”‚ â”‚  
â”‚ â”‚          Utility function to be used by the train_model() method. Not intended to be used directly.
â”‚ â”‚          """
â”‚ â”‚  
â”‚ â”‚          model = self.model
â”‚ â”‚          args = self.args
â”‚ â”‚ +        device = self.device
â”‚ â”‚  
â”‚ â”‚          tb_writer = SummaryWriter(log_dir=args.tensorboard_dir)
â”‚ â”‚          train_sampler = RandomSampler(train_dataset)
â”‚ â”‚          train_dataloader = DataLoader(
â”‚ â”‚              train_dataset,
â”‚ â”‚              sampler=train_sampler,
â”‚ â”‚              batch_size=args.train_batch_size,
â”‚ â”‚              num_workers=self.args.dataloader_num_workers,
â”‚ â”‚          )
â”‚ â”‚  
â”‚ â”‚ -        t_total = (
â”‚ â”‚ -                len(train_dataloader)
â”‚ â”‚ -                // args.gradient_accumulation_steps
â”‚ â”‚ -                * args.num_train_epochs
â”‚ â”‚ -        )
â”‚ â”‚ +        if args.max_steps > 0:
â”‚ â”‚ +            t_total = args.max_steps
â”‚ â”‚ +            args.num_train_epochs = (
â”‚ â”‚ +                    args.max_steps
â”‚ â”‚ +                    // (len(train_dataloader) // args.gradient_accumulation_steps)
â”‚ â”‚ +                    + 1
â”‚ â”‚ +            )
â”‚ â”‚ +        else:
â”‚ â”‚ +            t_total = (
â”‚ â”‚ +                    len(train_dataloader)
â”‚ â”‚ +                    // args.gradient_accumulation_steps
â”‚ â”‚ +                    * args.num_train_epochs
â”‚ â”‚ +            )
â”‚ â”‚  
â”‚ â”‚          no_decay = ["bias", "LayerNorm.weight"]
â”‚ â”‚  
â”‚ â”‚          optimizer_grouped_parameters = []
â”‚ â”‚          custom_parameter_names = set()
â”‚ â”‚          for group in self.args.custom_parameter_groups:
â”‚ â”‚              params = group.pop("params")
â”‚ â”‚ @@ -623,17 +408,32 @@
â”‚ â”‚                  lr_end=args.polynomial_decay_schedule_lr_end,
â”‚ â”‚                  power=args.polynomial_decay_schedule_power,
â”‚ â”‚              )
â”‚ â”‚  
â”‚ â”‚          else:
â”‚ â”‚              raise ValueError("{} is not a valid scheduler.".format(args.scheduler))
â”‚ â”‚  
â”‚ â”‚ +        if (
â”‚ â”‚ +                args.model_name
â”‚ â”‚ +                and os.path.isfile(os.path.join(args.model_name, "optimizer.pt"))
â”‚ â”‚ +                and os.path.isfile(os.path.join(args.model_name, "scheduler.pt"))
â”‚ â”‚ +        ):
â”‚ â”‚ +            # Load in optimizer and scheduler states
â”‚ â”‚ +            optimizer.load_state_dict(
â”‚ â”‚ +                torch.load(os.path.join(args.model_name, "optimizer.pt"))
â”‚ â”‚ +            )
â”‚ â”‚ +            scheduler.load_state_dict(
â”‚ â”‚ +                torch.load(os.path.join(args.model_name, "scheduler.pt"))
â”‚ â”‚ +            )
â”‚ â”‚ +
â”‚ â”‚          if args.n_gpu > 1:
â”‚ â”‚              model = torch.nn.DataParallel(model)
â”‚ â”‚  
â”‚ â”‚ +        logger.info(" Training started")
â”‚ â”‚ +
â”‚ â”‚          global_step = 0
â”‚ â”‚          training_progress_scores = None
â”‚ â”‚          tr_loss, logging_loss = 0.0, 0.0
â”‚ â”‚          model.zero_grad()
â”‚ â”‚          train_iterator = trange(
â”‚ â”‚              int(args.num_train_epochs), desc="Epoch", disable=args.silent, mininterval=0
â”‚ â”‚          )
â”‚ â”‚ @@ -641,15 +441,15 @@
â”‚ â”‚          best_eval_metric = None
â”‚ â”‚          early_stopping_counter = 0
â”‚ â”‚          steps_trained_in_current_epoch = 0
â”‚ â”‚          epochs_trained = 0
â”‚ â”‚  
â”‚ â”‚          if args.model_name and os.path.exists(args.model_name):
â”‚ â”‚              try:
â”‚ â”‚ -                # set global_step to global_step of last saved checkpoint from model path
â”‚ â”‚ +                # set global_step to gobal_step of last saved checkpoint from model path
â”‚ â”‚                  checkpoint_suffix = args.model_name.split("/")[-1].split("-")
â”‚ â”‚                  if len(checkpoint_suffix) > 2:
â”‚ â”‚                      checkpoint_suffix = checkpoint_suffix[1]
â”‚ â”‚                  else:
â”‚ â”‚                      checkpoint_suffix = checkpoint_suffix[-1]
â”‚ â”‚                  global_step = int(checkpoint_suffix)
â”‚ â”‚                  epochs_trained = global_step // (
â”‚ â”‚ @@ -685,15 +485,15 @@
â”‚ â”‚              self.wandb_run_id = wandb.run.id
â”‚ â”‚  
â”‚ â”‚          if args.fp16:
â”‚ â”‚              from torch.cuda import amp
â”‚ â”‚  
â”‚ â”‚              scaler = amp.GradScaler()
â”‚ â”‚  
â”‚ â”‚ -        for _ in train_iterator:
â”‚ â”‚ +        for current_epoch in train_iterator:
â”‚ â”‚              model.train()
â”‚ â”‚              if epochs_trained > 0:
â”‚ â”‚                  epochs_trained -= 1
â”‚ â”‚                  continue
â”‚ â”‚              train_iterator.set_description(
â”‚ â”‚                  f"Epoch {epoch_number + 1} of {args.num_train_epochs}"
â”‚ â”‚              )
â”‚ â”‚ @@ -788,15 +588,20 @@
â”‚ â”‚                          )
â”‚ â”‚  
â”‚ â”‚                      if args.evaluate_during_training and (
â”‚ â”‚                              args.evaluate_during_training_steps > 0
â”‚ â”‚                              and global_step % args.evaluate_during_training_steps == 0
â”‚ â”‚                      ):
â”‚ â”‚                          # Only evaluate when single GPU otherwise metrics may not average well
â”‚ â”‚ -                        results, _ = self.eval_model(eval_data, verbose=False, **kwargs)
â”‚ â”‚ +                        results = self.eval_model(
â”‚ â”‚ +                            eval_data,
â”‚ â”‚ +                            verbose=verbose and args.evaluate_during_training_verbose,
â”‚ â”‚ +                            silent=args.evaluate_during_training_silent,
â”‚ â”‚ +                            **kwargs,
â”‚ â”‚ +                        )
â”‚ â”‚                          for key, value in results.items():
â”‚ â”‚                              try:
â”‚ â”‚                                  tb_writer.add_scalar(
â”‚ â”‚                                      "eval_{}".format(key), value, global_step
â”‚ â”‚                                  )
â”‚ â”‚                              except (NotImplementedError, AssertionError):
â”‚ â”‚                                  pass
â”‚ â”‚ @@ -933,20 +738,26 @@
â”‚ â”‚                  output_dir, "checkpoint-{}-epoch-{}".format(global_step, epoch_number)
â”‚ â”‚              )
â”‚ â”‚  
â”‚ â”‚              if args.save_model_every_epoch:
â”‚ â”‚                  self.save_model(output_dir_current, optimizer, scheduler, model=model)
â”‚ â”‚  
â”‚ â”‚              if args.evaluate_during_training and args.evaluate_each_epoch:
â”‚ â”‚ -                results, _ = self.eval_model(eval_data, verbose=False, **kwargs)
â”‚ â”‚ -
â”‚ â”‚ -                self.save_model(
â”‚ â”‚ -                    output_dir_current, optimizer, scheduler, results=results
â”‚ â”‚ +                results = self.eval_model(
â”‚ â”‚ +                    eval_data,
â”‚ â”‚ +                    verbose=verbose and args.evaluate_during_training_verbose,
â”‚ â”‚ +                    silent=args.evaluate_during_training_silent,
â”‚ â”‚ +                    **kwargs,
â”‚ â”‚                  )
â”‚ â”‚  
â”‚ â”‚ +                if args.save_eval_checkpoints:
â”‚ â”‚ +                    self.save_model(
â”‚ â”‚ +                        output_dir_current, optimizer, scheduler, results=results
â”‚ â”‚ +                    )
â”‚ â”‚ +
â”‚ â”‚                  training_progress_scores["global_step"].append(global_step)
â”‚ â”‚                  training_progress_scores["train_loss"].append(current_loss)
â”‚ â”‚                  for key in results:
â”‚ â”‚                      training_progress_scores[key].append(results[key])
â”‚ â”‚                  report = pd.DataFrame(training_progress_scores)
â”‚ â”‚                  report.to_csv(
â”‚ â”‚                      os.path.join(args.output_dir, "training_progress_scores.csv"),
â”‚ â”‚ @@ -1058,476 +869,323 @@
â”‚ â”‚              global_step,
â”‚ â”‚              tr_loss / global_step
â”‚ â”‚              if not self.args.evaluate_during_training
â”‚ â”‚              else training_progress_scores,
â”‚ â”‚          )
â”‚ â”‚  
â”‚ â”‚      def eval_model(
â”‚ â”‚ -            self, eval_data, output_dir=None, verbose=False, verbose_logging=False, **kwargs
â”‚ â”‚ +            self, eval_data, output_dir=None, verbose=True, silent=False, **kwargs
â”‚ â”‚      ):
â”‚ â”‚          """
â”‚ â”‚          Evaluates the model on eval_data. Saves results to output_dir.
â”‚ â”‚  
â”‚ â”‚          Args:
â”‚ â”‚ -            eval_data: Path to JSON file containing evaluation data OR list of Python dicts in the correct format. The model will be evaluated on this data.
â”‚ â”‚ +            eval_data: Pandas DataFrame containing the 3 columns - `prefix`, `input_text`, `target_text`.
â”‚ â”‚ +                        - `prefix`: A string indicating the task to perform. (E.g. `"question"`, `"stsb"`)
â”‚ â”‚ +                        - `input_text`: The input text sequence. `prefix` is automatically prepended to form the full input. (<prefix>: <input_text>)
â”‚ â”‚ +                        - `target_text`: The target sequence
â”‚ â”‚              output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.
â”‚ â”‚              verbose: If verbose, results will be printed to the console on completion of evaluation.
â”‚ â”‚ -            verbose_logging: Log info related to feature conversion and writing predictions.
â”‚ â”‚ +            silent: If silent, tqdm progress bars will be hidden.
â”‚ â”‚              **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).
â”‚ â”‚ -                A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.
â”‚ â”‚ -
â”‚ â”‚ +                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs
â”‚ â”‚ +                        will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.
â”‚ â”‚          Returns:
â”‚ â”‚ -            result: Dictionary containing evaluation results. (correct, similar, incorrect)
â”‚ â”‚ -            text: A dictionary containing the 3 dictionaries correct_text, similar_text (the predicted answer is a substring of the correct answer or vise versa), incorrect_text.
â”‚ â”‚ +            results: Dictionary containing evaluation results.
â”‚ â”‚          """  # noqa: ignore flake8"
â”‚ â”‚  
â”‚ â”‚          if not output_dir:
â”‚ â”‚              output_dir = self.args.output_dir
â”‚ â”‚  
â”‚ â”‚          self._move_model_to_device()
â”‚ â”‚  
â”‚ â”‚ -        all_predictions, all_nbest_json, scores_diff_json, eval_loss = self.evaluate(
â”‚ â”‚ -            eval_data, output_dir, verbose_logging=verbose
â”‚ â”‚ +        eval_dataset = self.load_and_cache_examples(
â”‚ â”‚ +            eval_data, evaluate=True, verbose=verbose, silent=silent
â”‚ â”‚          )
â”‚ â”‚ +        os.makedirs(output_dir, exist_ok=True)
â”‚ â”‚  
â”‚ â”‚ -        if isinstance(eval_data, str):
â”‚ â”‚ -            with open(eval_data, "r", encoding=self.args.encoding) as f:
â”‚ â”‚ -                truth = json.load(f)
â”‚ â”‚ -        else:
â”‚ â”‚ -            truth = eval_data
â”‚ â”‚ +        result = self.evaluate(
â”‚ â”‚ +            eval_dataset, output_dir, verbose=verbose, silent=silent, **kwargs
â”‚ â”‚ +        )
â”‚ â”‚ +        self.results.update(result)
â”‚ â”‚  
â”‚ â”‚ -        result, texts = self.calculate_results(truth, all_predictions, **kwargs)
â”‚ â”‚ -        result["eval_loss"] = eval_loss
â”‚ â”‚ +        if self.args.evaluate_generated_text:
â”‚ â”‚ +            if self.args.preprocess_inputs:
â”‚ â”‚ +                to_predict = [
â”‚ â”‚ +                    prefix + ": " + input_text
â”‚ â”‚ +                    for prefix, input_text in zip(
â”‚ â”‚ +                        eval_data["prefix"], eval_data["input_text"]
â”‚ â”‚ +                    )
â”‚ â”‚ +                ]
â”‚ â”‚ +            else:
â”‚ â”‚ +                to_predict = [
â”‚ â”‚ +                    prefix + input_text
â”‚ â”‚ +                    for prefix, input_text in zip(
â”‚ â”‚ +                        eval_data["prefix"], eval_data["input_text"]
â”‚ â”‚ +                    )
â”‚ â”‚ +                ]
â”‚ â”‚ +            preds = self.predict(to_predict)
â”‚ â”‚  
â”‚ â”‚ -        self.results.update(result)
â”‚ â”‚ +            result = self.compute_metrics(
â”‚ â”‚ +                eval_data["target_text"].tolist(), preds, **kwargs
â”‚ â”‚ +            )
â”‚ â”‚ +            self.results.update(result)
â”‚ â”‚  
â”‚ â”‚          if verbose:
â”‚ â”‚              logger.info(self.results)
â”‚ â”‚  
â”‚ â”‚ -        return result, texts
â”‚ â”‚ +        return self.results
â”‚ â”‚  
â”‚ â”‚ -    def evaluate(self, eval_data, output_dir, verbose_logging=False):
â”‚ â”‚ +    def evaluate(self, eval_dataset, output_dir, verbose=True, silent=False, **kwargs):
â”‚ â”‚          """
â”‚ â”‚ -        Evaluates the model on eval_data.
â”‚ â”‚ +        Evaluates the model on eval_dataset.
â”‚ â”‚  
â”‚ â”‚          Utility function to be used by the eval_model() method. Not intended to be used directly.
â”‚ â”‚          """
â”‚ â”‚ -        tokenizer = self.tokenizer
â”‚ â”‚ +
â”‚ â”‚          model = self.model
â”‚ â”‚          args = self.args
â”‚ â”‚ +        eval_output_dir = output_dir
â”‚ â”‚ +        device = self.device
â”‚ â”‚  
â”‚ â”‚ -        if isinstance(eval_data, str):
â”‚ â”‚ -            with open(eval_data, "r", encoding=self.args.encoding) as f:
â”‚ â”‚ -                eval_examples = json.load(f)
â”‚ â”‚ -        else:
â”‚ â”‚ -            eval_examples = eval_data
â”‚ â”‚ -
â”‚ â”‚ -        eval_dataset, examples, features = self.load_and_cache_examples(
â”‚ â”‚ -            eval_examples, evaluate=True, output_examples=True
â”‚ â”‚ -        )
â”‚ â”‚ +        results = {}
â”‚ â”‚  
â”‚ â”‚          eval_sampler = SequentialSampler(eval_dataset)
â”‚ â”‚          eval_dataloader = DataLoader(
â”‚ â”‚              eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size
â”‚ â”‚          )
â”‚ â”‚  
â”‚ â”‚ +        if args.n_gpu > 1:
â”‚ â”‚ +            model = torch.nn.DataParallel(model)
â”‚ â”‚ +
â”‚ â”‚          eval_loss = 0.0
â”‚ â”‚          nb_eval_steps = 0
â”‚ â”‚          model.eval()
â”‚ â”‚  
â”‚ â”‚ -        if args.n_gpu > 1:
â”‚ â”‚ -            model = torch.nn.DataParallel(model)
â”‚ â”‚ -
â”‚ â”‚          if self.args.fp16:
â”‚ â”‚              from torch.cuda import amp
â”‚ â”‚  
â”‚ â”‚ -        all_results = []
â”‚ â”‚          for batch in tqdm(
â”‚ â”‚ -                eval_dataloader, disable=args.silent, desc="Running Evaluation"
â”‚ â”‚ +                eval_dataloader, disable=args.silent or silent, desc="Running Evaluation"
â”‚ â”‚          ):
â”‚ â”‚ -            batch = tuple(t.to(self.device) for t in batch)
â”‚ â”‚ -
â”‚ â”‚ +            inputs = self._get_inputs_dict(batch)
â”‚ â”‚              with torch.no_grad():
â”‚ â”‚ -                inputs = {
â”‚ â”‚ -                    "input_ids": batch[0],
â”‚ â”‚ -                    "attention_mask": batch[1],
â”‚ â”‚ -                    "token_type_ids": batch[2],
â”‚ â”‚ -                }
â”‚ â”‚ -
â”‚ â”‚ -                if self.args.model_type in [
â”‚ â”‚ -                    "xlm",
â”‚ â”‚ -                    "roberta",
â”‚ â”‚ -                    "distilbert",
â”‚ â”‚ -                    "camembert",
â”‚ â”‚ -                    "electra",
â”‚ â”‚ -                    "xlmroberta",
â”‚ â”‚ -                    "bart",
â”‚ â”‚ -                ]:
â”‚ â”‚ -                    del inputs["token_type_ids"]
â”‚ â”‚ -
â”‚ â”‚ -                example_indices = batch[3]
â”‚ â”‚ -
â”‚ â”‚ -                if args.model_type in ["xlnet", "xlm"]:
â”‚ â”‚ -                    inputs.update({"cls_index": batch[4], "p_mask": batch[5]})
â”‚ â”‚ -
â”‚ â”‚                  if self.args.fp16:
â”‚ â”‚                      with amp.autocast():
â”‚ â”‚                          outputs = model(**inputs)
â”‚ â”‚ -                        eval_loss += outputs[0].mean().item()
â”‚ â”‚ +                        loss = outputs[0]
â”‚ â”‚                  else:
â”‚ â”‚                      outputs = model(**inputs)
â”‚ â”‚ -                    eval_loss += outputs[0].mean().item()
â”‚ â”‚ -
â”‚ â”‚ -                for i, example_index in enumerate(example_indices):
â”‚ â”‚ -                    eval_feature = features[example_index.item()]
â”‚ â”‚ -                    unique_id = int(eval_feature.unique_id)
â”‚ â”‚ -                    if args.model_type in ["xlnet", "xlm"]:
â”‚ â”‚ -                        # XLNet uses a more complex post-processing procedure
â”‚ â”‚ -                        result = RawResultExtended(
â”‚ â”‚ -                            unique_id=unique_id,
â”‚ â”‚ -                            start_top_log_probs=to_list(outputs[0][i]),
â”‚ â”‚ -                            start_top_index=to_list(outputs[1][i]),
â”‚ â”‚ -                            end_top_log_probs=to_list(outputs[2][i]),
â”‚ â”‚ -                            end_top_index=to_list(outputs[3][i]),
â”‚ â”‚ -                            cls_logits=to_list(outputs[4][i]),
â”‚ â”‚ -                        )
â”‚ â”‚ -                    else:
â”‚ â”‚ -                        result = RawResult(
â”‚ â”‚ -                            unique_id=unique_id,
â”‚ â”‚ -                            start_logits=to_list(outputs[0][i]),
â”‚ â”‚ -                            end_logits=to_list(outputs[1][i]),
â”‚ â”‚ -                        )
â”‚ â”‚ -                    all_results.append(result)
â”‚ â”‚ -
â”‚ â”‚ +                    loss = outputs[0]
â”‚ â”‚ +                if self.args.n_gpu > 1:
â”‚ â”‚ +                    loss = loss.mean()
â”‚ â”‚ +                eval_loss += loss.item()
â”‚ â”‚              nb_eval_steps += 1
â”‚ â”‚  
â”‚ â”‚          eval_loss = eval_loss / nb_eval_steps
â”‚ â”‚  
â”‚ â”‚ -        prefix = "test"
â”‚ â”‚ -        os.makedirs(output_dir, exist_ok=True)
â”‚ â”‚ -
â”‚ â”‚ -        output_prediction_file = os.path.join(
â”‚ â”‚ -            output_dir, "predictions_{}.json".format(prefix)
â”‚ â”‚ -        )
â”‚ â”‚ -        output_nbest_file = os.path.join(
â”‚ â”‚ -            output_dir, "nbest_predictions_{}.json".format(prefix)
â”‚ â”‚ -        )
â”‚ â”‚ -        output_null_log_odds_file = os.path.join(
â”‚ â”‚ -            output_dir, "null_odds_{}.json".format(prefix)
â”‚ â”‚ -        )
â”‚ â”‚ +        results["eval_loss"] = eval_loss
â”‚ â”‚  
â”‚ â”‚ -        if args.model_type in ["xlnet", "xlm"]:
â”‚ â”‚ -            # XLNet uses a more complex post-processing procedure
â”‚ â”‚ -            (
â”‚ â”‚ -                all_predictions,
â”‚ â”‚ -                all_nbest_json,
â”‚ â”‚ -                scores_diff_json,
â”‚ â”‚ -            ) = write_predictions_extended(
â”‚ â”‚ -                examples,
â”‚ â”‚ -                features,
â”‚ â”‚ -                all_results,
â”‚ â”‚ -                args.n_best_size,
â”‚ â”‚ -                args.max_answer_length,
â”‚ â”‚ -                output_prediction_file,
â”‚ â”‚ -                output_nbest_file,
â”‚ â”‚ -                output_null_log_odds_file,
â”‚ â”‚ -                eval_data,
â”‚ â”‚ -                model.config.start_n_top,
â”‚ â”‚ -                model.config.end_n_top,
â”‚ â”‚ -                True,
â”‚ â”‚ -                tokenizer,
â”‚ â”‚ -                verbose_logging,
â”‚ â”‚ -            )
â”‚ â”‚ -        else:
â”‚ â”‚ -            all_predictions, all_nbest_json, scores_diff_json = write_predictions(
â”‚ â”‚ -                examples,
â”‚ â”‚ -                features,
â”‚ â”‚ -                all_results,
â”‚ â”‚ -                args.n_best_size,
â”‚ â”‚ -                args.max_answer_length,
â”‚ â”‚ -                False,
â”‚ â”‚ -                output_prediction_file,
â”‚ â”‚ -                output_nbest_file,
â”‚ â”‚ -                output_null_log_odds_file,
â”‚ â”‚ -                verbose_logging,
â”‚ â”‚ -                True,
â”‚ â”‚ -                args.null_score_diff_threshold,
â”‚ â”‚ -            )
â”‚ â”‚ +        output_eval_file = os.path.join(eval_output_dir, "eval_results.txt")
â”‚ â”‚ +        with open(output_eval_file, "w") as writer:
â”‚ â”‚ +            for key in sorted(results.keys()):
â”‚ â”‚ +                writer.write("{} = {}\n".format(key, str(results[key])))
â”‚ â”‚  
â”‚ â”‚ -        return all_predictions, all_nbest_json, scores_diff_json, eval_loss
â”‚ â”‚ +        return results
â”‚ â”‚  
â”‚ â”‚ -    def predict(self, to_predict, n_best_size=None):
â”‚ â”‚ +    def predict(self, to_predict, split_on_space=False):
â”‚ â”‚          """
â”‚ â”‚ -        Performs predictions on a list of python dicts containing contexts and qas.
â”‚ â”‚ +        Performs predictions on a list of text.
â”‚ â”‚  
â”‚ â”‚          Args:
â”‚ â”‚ -            to_predict: A python list of python dicts containing contexts and questions to be sent to the model for prediction.
â”‚ â”‚ -                        E.g: predict([
â”‚ â”‚ -                            {
â”‚ â”‚ -                                'context': "Some context as a demo",
â”‚ â”‚ -                                'qas': [
â”‚ â”‚ -                                    {'id': '0', 'question': 'What is the context here?'},
â”‚ â”‚ -                                    {'id': '1', 'question': 'What is this for?'}
â”‚ â”‚ -                                ]
â”‚ â”‚ -                            }
â”‚ â”‚ -                        ])
â”‚ â”‚ -            n_best_size (Optional): Number of predictions to return. args.n_best_size will be used if not specified.
â”‚ â”‚ +            to_predict: A python list of text (str) to be sent to the model for prediction. Note that the prefix should be prepended to the text.
â”‚ â”‚ +            split_on_space (optional): If True, input is english string, if False, input is chinese string.
â”‚ â”‚  
â”‚ â”‚          Returns:
â”‚ â”‚ -            list: A python list  of dicts containing the predicted answer/answers, and id for each question in to_predict.
â”‚ â”‚ -            list: A python list  of dicts containing the predicted probability/probabilities, and id for each question in to_predict.
â”‚ â”‚ +            preds: A python list of the generated sequences.
â”‚ â”‚          """  # noqa: ignore flake8"
â”‚ â”‚ -        tokenizer = self.tokenizer
â”‚ â”‚ -        device = self.device
â”‚ â”‚ -        model = self.model
â”‚ â”‚ -        args = self.args
â”‚ â”‚ -
â”‚ â”‚ -        if not n_best_size:
â”‚ â”‚ -            n_best_size = args.n_best_size
â”‚ â”‚  
â”‚ â”‚          self._move_model_to_device()
â”‚ â”‚  
â”‚ â”‚ -        eval_examples = build_examples(to_predict)
â”‚ â”‚ -        eval_dataset, examples, features = self.load_and_cache_examples(
â”‚ â”‚ -            eval_examples, evaluate=True, output_examples=True, no_cache=True
â”‚ â”‚ -        )
â”‚ â”‚ -
â”‚ â”‚ -        eval_sampler = SequentialSampler(eval_dataset)
â”‚ â”‚ -        eval_dataloader = DataLoader(
â”‚ â”‚ -            eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size
â”‚ â”‚ -        )
â”‚ â”‚ -
â”‚ â”‚ -        model.eval()
â”‚ â”‚ -
â”‚ â”‚ -        if args.n_gpu > 1:
â”‚ â”‚ -            model = torch.nn.DataParallel(model)
â”‚ â”‚ -
â”‚ â”‚ -        if self.args.fp16:
â”‚ â”‚ -            from torch.cuda import amp
â”‚ â”‚ -
â”‚ â”‚ -        all_results = []
â”‚ â”‚ +        all_outputs = []
â”‚ â”‚ +        # Batching
â”‚ â”‚          for batch in tqdm(
â”‚ â”‚ -                eval_dataloader, disable=args.silent, desc="Running Prediction"
â”‚ â”‚ +                [
â”‚ â”‚ +                    to_predict[i: i + self.args.eval_batch_size]
â”‚ â”‚ +                    for i in range(0, len(to_predict), self.args.eval_batch_size)
â”‚ â”‚ +                ],
â”‚ â”‚ +                desc="Generating outputs",
â”‚ â”‚ +                disable=self.args.silent,
â”‚ â”‚          ):
â”‚ â”‚ -            batch = tuple(t.to(self.device) for t in batch)
â”‚ â”‚ -
â”‚ â”‚ -            with torch.no_grad():
â”‚ â”‚ -                inputs = {
â”‚ â”‚ -                    "input_ids": batch[0],
â”‚ â”‚ -                    "attention_mask": batch[1],
â”‚ â”‚ -                    "token_type_ids": batch[2],
â”‚ â”‚ -                }
â”‚ â”‚ -
â”‚ â”‚ -                if self.args.model_type in [
â”‚ â”‚ -                    "xlm",
â”‚ â”‚ -                    "roberta",
â”‚ â”‚ -                    "distilbert",
â”‚ â”‚ -                    "camembert",
â”‚ â”‚ -                    "electra",
â”‚ â”‚ -                    "xlmroberta",
â”‚ â”‚ -                    "bart",
â”‚ â”‚ -                ]:
â”‚ â”‚ -                    del inputs["token_type_ids"]
â”‚ â”‚ -
â”‚ â”‚ -                example_indices = batch[3]
â”‚ â”‚ -
â”‚ â”‚ -                if args.model_type in ["xlnet", "xlm"]:
â”‚ â”‚ -                    inputs.update({"cls_index": batch[4], "p_mask": batch[5]})
â”‚ â”‚ -
â”‚ â”‚ -                if self.args.fp16:
â”‚ â”‚ -                    with amp.autocast():
â”‚ â”‚ -                        outputs = model(**inputs)
â”‚ â”‚ +            input_batch = self.tokenizer.prepare_seq2seq_batch(
â”‚ â”‚ +                src_texts=batch,
â”‚ â”‚ +                max_length=self.args.max_seq_length,
â”‚ â”‚ +                padding="max_length",
â”‚ â”‚ +                return_tensors="pt",
â”‚ â”‚ +                truncation=True,
â”‚ â”‚ +            )
â”‚ â”‚ +            input_ids = input_batch["input_ids"]
â”‚ â”‚ +            attention_mask = input_batch["attention_mask"]
â”‚ â”‚ +
â”‚ â”‚ +            input_ids = input_ids.to(self.device)
â”‚ â”‚ +            attention_mask = attention_mask.to(self.device)
â”‚ â”‚ +
â”‚ â”‚ +            outputs = self.model.generate(
â”‚ â”‚ +                input_ids=input_ids,
â”‚ â”‚ +                attention_mask=attention_mask,
â”‚ â”‚ +                num_beams=self.args.num_beams,
â”‚ â”‚ +                max_length=self.args.max_length,
â”‚ â”‚ +                length_penalty=self.args.length_penalty,
â”‚ â”‚ +                early_stopping=self.args.early_stopping,
â”‚ â”‚ +                repetition_penalty=self.args.repetition_penalty,
â”‚ â”‚ +                do_sample=self.args.do_sample,
â”‚ â”‚ +                top_k=self.args.top_k,
â”‚ â”‚ +                top_p=self.args.top_p,
â”‚ â”‚ +                num_return_sequences=self.args.num_return_sequences,
â”‚ â”‚ +            )
â”‚ â”‚ +            all_outputs.extend(outputs.cpu().numpy())
â”‚ â”‚ +
â”‚ â”‚ +        if self.args.use_multiprocessed_decoding:
â”‚ â”‚ +            self.model.to("cpu")
â”‚ â”‚ +            with Pool(self.args.process_count) as p:
â”‚ â”‚ +                if self.args.multiprocessing_chunksize == -1:
â”‚ â”‚ +                    chunksize = max(
â”‚ â”‚ +                        len(all_outputs) // (self.args.process_count * 2), 500
â”‚ â”‚ +                    )
â”‚ â”‚                  else:
â”‚ â”‚ -                    outputs = model(**inputs)
â”‚ â”‚ -
â”‚ â”‚ -                for i, example_index in enumerate(example_indices):
â”‚ â”‚ -                    eval_feature = features[example_index.item()]
â”‚ â”‚ -                    unique_id = int(eval_feature.unique_id)
â”‚ â”‚ -                    if args.model_type in ["xlnet", "xlm"]:
â”‚ â”‚ -                        # XLNet uses a more complex post-processing procedure
â”‚ â”‚ -                        result = RawResultExtended(
â”‚ â”‚ -                            unique_id=unique_id,
â”‚ â”‚ -                            start_top_log_probs=to_list(outputs[0][i]),
â”‚ â”‚ -                            start_top_index=to_list(outputs[1][i]),
â”‚ â”‚ -                            end_top_log_probs=to_list(outputs[2][i]),
â”‚ â”‚ -                            end_top_index=to_list(outputs[3][i]),
â”‚ â”‚ -                            cls_logits=to_list(outputs[4][i]),
â”‚ â”‚ -                        )
â”‚ â”‚ -                    else:
â”‚ â”‚ -                        result = RawResult(
â”‚ â”‚ -                            unique_id=unique_id,
â”‚ â”‚ -                            start_logits=to_list(outputs[0][i]),
â”‚ â”‚ -                            end_logits=to_list(outputs[1][i]),
â”‚ â”‚ -                        )
â”‚ â”‚ -                    all_results.append(result)
â”‚ â”‚ -
â”‚ â”‚ -        if args.model_type in ["xlnet", "xlm"]:
â”‚ â”‚ -            answers = get_best_predictions_extended(
â”‚ â”‚ -                examples,
â”‚ â”‚ -                features,
â”‚ â”‚ -                all_results,
â”‚ â”‚ -                n_best_size,
â”‚ â”‚ -                args.max_answer_length,
â”‚ â”‚ -                model.config.start_n_top,
â”‚ â”‚ -                model.config.end_n_top,
â”‚ â”‚ -                True,
â”‚ â”‚ -                tokenizer,
â”‚ â”‚ -                args.null_score_diff_threshold,
â”‚ â”‚ -            )
â”‚ â”‚ +                    chunksize = self.args.multiprocessing_chunksize
â”‚ â”‚ +                outputs = list(
â”‚ â”‚ +                    tqdm(
â”‚ â”‚ +                        p.imap(self._decode, all_outputs, chunksize=chunksize),
â”‚ â”‚ +                        total=len(all_outputs),
â”‚ â”‚ +                        desc="Decoding outputs",
â”‚ â”‚ +                        disable=self.args.silent,
â”‚ â”‚ +                    )
â”‚ â”‚ +                )
â”‚ â”‚ +            self._move_model_to_device()
â”‚ â”‚          else:
â”‚ â”‚ -            answers = get_best_predictions(
â”‚ â”‚ -                examples,
â”‚ â”‚ -                features,
â”‚ â”‚ -                all_results,
â”‚ â”‚ -                n_best_size,
â”‚ â”‚ -                args.max_answer_length,
â”‚ â”‚ -                False,
â”‚ â”‚ -                False,
â”‚ â”‚ -                True,
â”‚ â”‚ -                False,
â”‚ â”‚ -            )
â”‚ â”‚ -
â”‚ â”‚ -        answer_list = [
â”‚ â”‚ -            {"id": answer["id"], "answer": answer["answer"][:-1]} for answer in answers
â”‚ â”‚ -        ]
â”‚ â”‚ -        probability_list = [
â”‚ â”‚ -            {"id": answer["id"], "probability": answer["probability"][:-1]}
â”‚ â”‚ -            for answer in answers
â”‚ â”‚ -        ]
â”‚ â”‚ -
â”‚ â”‚ -        return answer_list, probability_list
â”‚ â”‚ -
â”‚ â”‚ -    def calculate_results(self, truth, predictions, **kwargs):
â”‚ â”‚ -        truth_dict = {}
â”‚ â”‚ -        questions_dict = {}
â”‚ â”‚ -        for item in truth:
â”‚ â”‚ -            for answer in item["qas"]:
â”‚ â”‚ -                if answer["answers"]:
â”‚ â”‚ -                    truth_dict[answer["id"]] = answer["answers"][0]["text"]
â”‚ â”‚ -                else:
â”‚ â”‚ -                    truth_dict[answer["id"]] = ""
â”‚ â”‚ -                questions_dict[answer["id"]] = answer["question"]
â”‚ â”‚ +            outputs = [
â”‚ â”‚ +                self.tokenizer.decode(
â”‚ â”‚ +                    output_id,
â”‚ â”‚ +                    skip_special_tokens=self.args.skip_special_tokens,
â”‚ â”‚ +                    clean_up_tokenization_spaces=True,
â”‚ â”‚ +                )
â”‚ â”‚ +                for output_id in all_outputs
â”‚ â”‚ +            ]
â”‚ â”‚ +        if not split_on_space:
â”‚ â”‚ +            outputs = [''.join(gen_text.split(' ')) for gen_text in outputs]
â”‚ â”‚ +        if self.args.num_return_sequences > 1:
â”‚ â”‚ +            return [
â”‚ â”‚ +                outputs[i: i + self.args.num_return_sequences]
â”‚ â”‚ +                for i in range(0, len(outputs), self.args.num_return_sequences)
â”‚ â”‚ +            ]
â”‚ â”‚ +        else:
â”‚ â”‚ +            return outputs
â”‚ â”‚  
â”‚ â”‚ -        correct = 0
â”‚ â”‚ -        incorrect = 0
â”‚ â”‚ -        similar = 0
â”‚ â”‚ -        correct_text = {}
â”‚ â”‚ -        incorrect_text = {}
â”‚ â”‚ -        similar_text = {}
â”‚ â”‚ -        predicted_answers = []
â”‚ â”‚ -        true_answers = []
â”‚ â”‚ -
â”‚ â”‚ -        for q_id, answer in truth_dict.items():
â”‚ â”‚ -            predicted_answers.append(predictions[q_id])
â”‚ â”‚ -            true_answers.append(answer)
â”‚ â”‚ -            if predictions[q_id].strip() == answer.strip():
â”‚ â”‚ -                correct += 1
â”‚ â”‚ -                correct_text[q_id] = answer
â”‚ â”‚ -            elif (
â”‚ â”‚ -                    predictions[q_id].strip() in answer.strip()
â”‚ â”‚ -                    or answer.strip() in predictions[q_id].strip()
â”‚ â”‚ -            ):
â”‚ â”‚ -                similar += 1
â”‚ â”‚ -                similar_text[q_id] = {
â”‚ â”‚ -                    "truth": answer,
â”‚ â”‚ -                    "predicted": predictions[q_id],
â”‚ â”‚ -                    "question": questions_dict[q_id],
â”‚ â”‚ -                }
â”‚ â”‚ -            else:
â”‚ â”‚ -                incorrect += 1
â”‚ â”‚ -                incorrect_text[q_id] = {
â”‚ â”‚ -                    "truth": answer,
â”‚ â”‚ -                    "predicted": predictions[q_id],
â”‚ â”‚ -                    "question": questions_dict[q_id],
â”‚ â”‚ -                }
â”‚ â”‚ +    def _decode(self, output_id):
â”‚ â”‚ +        return self.tokenizer.decode(
â”‚ â”‚ +            output_id,
â”‚ â”‚ +            skip_special_tokens=self.args.skip_special_tokens,
â”‚ â”‚ +            clean_up_tokenization_spaces=True,
â”‚ â”‚ +        )
â”‚ â”‚  
â”‚ â”‚ -        extra_metrics = {}
â”‚ â”‚ -        for metric, func in kwargs.items():
â”‚ â”‚ -            extra_metrics[metric] = func(true_answers, predicted_answers)
â”‚ â”‚ +    def compute_metrics(self, labels, preds, **kwargs):
â”‚ â”‚ +        """
â”‚ â”‚ +        Computes the evaluation metrics for the model predictions.
â”‚ â”‚  
â”‚ â”‚ -        result = {
â”‚ â”‚ -            "correct": correct,
â”‚ â”‚ -            "similar": similar,
â”‚ â”‚ -            "incorrect": incorrect,
â”‚ â”‚ -            **extra_metrics,
â”‚ â”‚ -        }
â”‚ â”‚ +        Args:
â”‚ â”‚ +            labels: List of target sequences
â”‚ â”‚ +            preds: List of model generated outputs
â”‚ â”‚ +            **kwargs: Custom metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).
â”‚ â”‚ +                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs
â”‚ â”‚ +                        will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.
â”‚ â”‚  
â”‚ â”‚ -        texts = {
â”‚ â”‚ -            "correct_text": correct_text,
â”‚ â”‚ -            "similar_text": similar_text,
â”‚ â”‚ -            "incorrect_text": incorrect_text,
â”‚ â”‚ -        }
â”‚ â”‚ +        Returns:
â”‚ â”‚ +            result: Dictionary containing evaluation results.
â”‚ â”‚ +        """  # noqa: ignore flake8"
â”‚ â”‚ +        assert len(labels) == len(preds)
â”‚ â”‚ +
â”‚ â”‚ +        results = {}
â”‚ â”‚ +        for metric, func in kwargs.items():
â”‚ â”‚ +            results[metric] = func(labels, preds)
â”‚ â”‚  
â”‚ â”‚ -        return result, texts
â”‚ â”‚ +        return results
â”‚ â”‚  
â”‚ â”‚      def _move_model_to_device(self):
â”‚ â”‚          self.model.to(self.device)
â”‚ â”‚  
â”‚ â”‚ -    def _get_last_metrics(self, metric_values):
â”‚ â”‚ -        return {metric: values[-1] for metric, values in metric_values.items()}
â”‚ â”‚ -
â”‚ â”‚      def _get_inputs_dict(self, batch):
â”‚ â”‚          if self.args.use_hf_datasets:
â”‚ â”‚ -            inputs = {key: value.to(self.device) for key, value in batch.items()}
â”‚ â”‚ -
â”‚ â”‚ -            if self.args.model_type in [
â”‚ â”‚ -                "xlm",
â”‚ â”‚ -                "roberta",
â”‚ â”‚ -                "distilbert",
â”‚ â”‚ -                "camembert",
â”‚ â”‚ -                "electra",
â”‚ â”‚ -                "xlmroberta",
â”‚ â”‚ -                "bart",
â”‚ â”‚ -            ]:
â”‚ â”‚ -                del inputs["token_type_ids"]
â”‚ â”‚ -            if self.args.model_type not in ["xlnet", "xlm"]:
â”‚ â”‚ -                del inputs["cls_index"]
â”‚ â”‚ -                del inputs["p_mask"]
â”‚ â”‚ +            inputs = {**batch, "labels": batch["input_ids"]}
â”‚ â”‚  
â”‚ â”‚ -            return inputs
â”‚ â”‚ +            return {key: value.to(self.device) for key, value in inputs.items()}
â”‚ â”‚          else:
â”‚ â”‚              batch = tuple(t.to(self.device) for t in batch)
â”‚ â”‚ +
â”‚ â”‚ +            input_ids = batch[0]
â”‚ â”‚ +            attention_mask = batch[1]
â”‚ â”‚ +            labels = batch[2]
â”‚ â”‚ +            labels[labels == self.tokenizer.pad_token_id] = -100
â”‚ â”‚ +
â”‚ â”‚              inputs = {
â”‚ â”‚ -                "input_ids": batch[0],
â”‚ â”‚ -                "attention_mask": batch[1],
â”‚ â”‚ -                "token_type_ids": batch[2],
â”‚ â”‚ -                "start_positions": batch[3],
â”‚ â”‚ -                "end_positions": batch[4],
â”‚ â”‚ +                "input_ids": input_ids,
â”‚ â”‚ +                "attention_mask": attention_mask,
â”‚ â”‚ +                "labels": labels,
â”‚ â”‚              }
â”‚ â”‚  
â”‚ â”‚ -            if self.args.model_type in [
â”‚ â”‚ -                "xlm",
â”‚ â”‚ -                "roberta",
â”‚ â”‚ -                "distilbert",
â”‚ â”‚ -                "camembert",
â”‚ â”‚ -                "electra",
â”‚ â”‚ -                "xlmroberta",
â”‚ â”‚ -                "bart",
â”‚ â”‚ -            ]:
â”‚ â”‚ -                del inputs["token_type_ids"]
â”‚ â”‚ +            return inputs
â”‚ â”‚  
â”‚ â”‚ -            if self.args.model_type in ["xlnet", "xlm"]:
â”‚ â”‚ -                inputs.update({"cls_index": batch[5], "p_mask": batch[6]})
â”‚ â”‚ +    def load_and_cache_examples(
â”‚ â”‚ +            self, data, evaluate=False, no_cache=False, verbose=True, silent=False
â”‚ â”‚ +    ):
â”‚ â”‚ +        """
â”‚ â”‚ +        Creates a T5Dataset from data.
â”‚ â”‚  
â”‚ â”‚ -            return inputs
â”‚ â”‚ +        Utility function for train() and eval() methods. Not intended to be used directly.
â”‚ â”‚ +        """
â”‚ â”‚ +
â”‚ â”‚ +        tokenizer = self.tokenizer
â”‚ â”‚ +        args = self.args
â”‚ â”‚ +
â”‚ â”‚ +        if not no_cache:
â”‚ â”‚ +            no_cache = args.no_cache
â”‚ â”‚ +
â”‚ â”‚ +        if not no_cache:
â”‚ â”‚ +            os.makedirs(self.args.cache_dir, exist_ok=True)
â”‚ â”‚ +
â”‚ â”‚ +        mode = "dev" if evaluate else "train"
â”‚ â”‚ +
â”‚ â”‚ +        if self.args.use_hf_datasets:
â”‚ â”‚ +            dataset = load_hf_dataset(data, tokenizer, self.args)
â”‚ â”‚ +            return dataset
â”‚ â”‚ +        elif args.dataset_class:
â”‚ â”‚ +            CustomDataset = args.dataset_class
â”‚ â”‚ +            return CustomDataset(tokenizer, args, data, mode)
â”‚ â”‚ +        else:
â”‚ â”‚ +            return T5Dataset(
â”‚ â”‚ +                tokenizer,
â”‚ â”‚ +                self.args,
â”‚ â”‚ +                data,
â”‚ â”‚ +                mode,
â”‚ â”‚ +            )
â”‚ â”‚  
â”‚ â”‚      def _create_training_progress_scores(self, **kwargs):
â”‚ â”‚          extra_metrics = {key: [] for key in kwargs}
â”‚ â”‚          training_progress_scores = {
â”‚ â”‚              "global_step": [],
â”‚ â”‚ -            "correct": [],
â”‚ â”‚ -            "similar": [],
â”‚ â”‚ -            "incorrect": [],
â”‚ â”‚ -            "train_loss": [],
â”‚ â”‚              "eval_loss": [],
â”‚ â”‚ +            "train_loss": [],
â”‚ â”‚              **extra_metrics,
â”‚ â”‚          }
â”‚ â”‚  
â”‚ â”‚          return training_progress_scores
â”‚ â”‚  
â”‚ â”‚ +    def _get_last_metrics(self, metric_values):
â”‚ â”‚ +        return {metric: values[-1] for metric, values in metric_values.items()}
â”‚ â”‚ +
â”‚ â”‚      def save_model(
â”‚ â”‚              self, output_dir=None, optimizer=None, scheduler=None, model=None, results=None
â”‚ â”‚      ):
â”‚ â”‚          if not output_dir:
â”‚ â”‚              output_dir = self.args.output_dir
â”‚ â”‚          os.makedirs(output_dir, exist_ok=True)
â”‚ â”‚  
â”‚ â”‚ @@ -1553,13 +1211,13 @@
â”‚ â”‚                      writer.write("{} = {}\n".format(key, str(results[key])))
â”‚ â”‚  
â”‚ â”‚      def save_model_args(self, output_dir):
â”‚ â”‚          os.makedirs(output_dir, exist_ok=True)
â”‚ â”‚          self.args.save(output_dir)
â”‚ â”‚  
â”‚ â”‚      def _load_model_args(self, input_dir):
â”‚ â”‚ -        args = QuestionAnsweringArgs()
â”‚ â”‚ +        args = T5Args()
â”‚ â”‚          args.load(input_dir)
â”‚ â”‚          return args
â”‚ â”‚  
â”‚ â”‚      def get_named_parameters(self):
â”‚ â”‚          return [n for n, p in self.model.named_parameters()]
â”‚ â”‚   --- textgen-0.1.7/textgen/seq2seq/bart_seq2seq_model.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/seq2seq/bart_seq2seq_model.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/seq2seq/bart_seq2seq_utils.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/seq2seq/bart_seq2seq_utils.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/seq2seq/conv_seq2seq_model.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/seq2seq/conv_seq2seq_model.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/seq2seq/data_reader.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/seq2seq/data_reader.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/seq2seq/seq2seq_model.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/seq2seq/seq2seq_model.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/seq2seq/seq2seq_trainer.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/seq2seq/seq2seq_trainer.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/t5/copyt5_model.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/t5/copyt5_model.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/t5/copyt5_utils.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/t5/copyt5_utils.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/t5/t5_utils.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/t5/t5_utils.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/unsup_generation/tgls_model.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/unsup_generation/tgls_model.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen/unsup_generation/tgls_util.py
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen/unsup_generation/tgls_util.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- textgen-0.1.7/textgen.egg-info/PKG-INFO
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen.egg-info/PKG-INFO
â”‚ â”‚â”„ Files 3% similar despite different names
â”‚ â”‚ @@ -1,10 +1,10 @@
â”‚ â”‚  Metadata-Version: 2.1
â”‚ â”‚  Name: textgen
â”‚ â”‚ -Version: 0.1.7
â”‚ â”‚ +Version: 0.1.8
â”‚ â”‚  Summary: Text Generation Model
â”‚ â”‚  Home-page: https://github.com/shibing624/textgen
â”‚ â”‚  Author: XuMing
â”‚ â”‚  Author-email: xuming624@qq.com
â”‚ â”‚  License: Apache 2.0
â”‚ â”‚  Description: [![PyPI version](https://badge.fury.io/py/textgen.svg)](https://badge.fury.io/py/textgen)
â”‚ â”‚          [![Downloads](https://pepy.tech/badge/textgen)](https://pepy.tech/project/textgen)
â”‚ â”‚ @@ -19,56 +19,62 @@
â”‚ â”‚          
â”‚ â”‚          ğŸŒˆ Implementation of Text Generation models.
â”‚ â”‚          
â”‚ â”‚          **textgen**å®ç°äº†å¤šç§æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼šUDAã€GPT2ã€Seq2Seqã€BARTã€T5ã€SongNetç­‰æ¨¡å‹ï¼Œå¼€ç®±å³ç”¨ã€‚
â”‚ â”‚          
â”‚ â”‚          **Guide**
â”‚ â”‚          
â”‚ â”‚ -        - [Question](#Question)
â”‚ â”‚ -        - [Solution](#Solution)
â”‚ â”‚          - [Feature](#Feature)
â”‚ â”‚          - [Install](#install)
â”‚ â”‚          - [Usage](#usage)
â”‚ â”‚          - [Contact](#Contact)
â”‚ â”‚          - [Reference](#reference)
â”‚ â”‚          
â”‚ â”‚ -        # Question
â”‚ â”‚ -        
â”‚ â”‚ -        æ–‡æœ¬ç”Ÿæˆï¼Œæ–‡æœ¬æ•°æ®å¢å¼ºæ€ä¹ˆåšï¼Ÿ
â”‚ â”‚ -        
â”‚ â”‚ -        # Solution
â”‚ â”‚ -        ## æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
â”‚ â”‚ +        # Feature
â”‚ â”‚ +        ## æ–‡æœ¬ç”Ÿæˆ
â”‚ â”‚          
â”‚ â”‚ -        1. Seq2Seqã€ConvSeq2Seqã€BART
â”‚ â”‚ -        2. GPT2ã€SongNet
â”‚ â”‚ -        3. T5ã€CopyT5
â”‚ â”‚ +        1. seq2seq: Seq2Seqã€ConvSeq2Seqã€BART
â”‚ â”‚ +        2. language_modeling: GPT2ã€SongNet
â”‚ â”‚ +        3. t5: T5ã€CopyT5
â”‚ â”‚ +        4. question_answering: BERTã€XLNet
â”‚ â”‚ +        5. chatglm: ChatGLM
â”‚ â”‚          
â”‚ â”‚          ## æ–‡æœ¬æ‰©å¢
â”‚ â”‚          ### è¯ç²’åº¦æ‰©å¢
â”‚ â”‚          1. UDAï¼Œéæ ¸å¿ƒè¯æ›¿æ¢
â”‚ â”‚          2. EDAï¼Œç®€å•æ•°æ®å¢å¼ºæŠ€æœ¯ï¼šç›¸ä¼¼è¯ã€åŒä¹‰è¯æ›¿æ¢ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢
â”‚ â”‚          
â”‚ â”‚          ### å¥ç²’åº¦æ‰©å¢
â”‚ â”‚          1. å›è¯‘ï¼ˆBT, Back Translateï¼‰ï¼šä¸­æ–‡-è‹±æ–‡-ä¸­æ–‡
â”‚ â”‚          2. GPT2æ¨¡å‹ç»­å†™ï¼šçŸ­æ–‡æœ¬->é•¿æ–‡æœ¬
â”‚ â”‚          3. BARTæ‘˜è¦æ¨¡å‹ï¼šé•¿æ–‡æœ¬->çŸ­æ–‡æœ¬
â”‚ â”‚          4. TGLSï¼šæ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
â”‚ â”‚          
â”‚ â”‚ -        
â”‚ â”‚ -        # Feature
â”‚ â”‚ -        
â”‚ â”‚ +        ## åŠŸèƒ½åˆ—è¡¨
â”‚ â”‚          - [UDA(éæ ¸å¿ƒè¯æ›¿æ¢)/EDA](textgen/augment/word_level_augment.py)ï¼šæœ¬é¡¹ç›®å‚è€ƒGoogleçš„UDA(éæ ¸å¿ƒè¯æ›¿æ¢)ç®—æ³•å’ŒEDAç®—æ³•ï¼ŒåŸºäºTF-IDFå°†å¥å­ä¸­éƒ¨åˆ†ä¸é‡è¦è¯æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼Œéšæœºè¯æ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ç­‰æ–¹æ³•ï¼Œäº§ç”Ÿæ–°çš„æ–‡æœ¬ï¼Œå®ç°äº†æ–‡æœ¬æ‰©å¢
â”‚ â”‚          - [BT(å›è¯‘)](textgen/augment/sentence_level_augment.py)ï¼šæœ¬é¡¹ç›®åŸºäºç™¾åº¦ç¿»è¯‘APIå®ç°äº†å›è¯‘åŠŸèƒ½ï¼Œå…ˆæŠŠä¸­æ–‡å¥å­ç¿»è¯‘ä¸ºè‹±æ–‡ï¼Œå†æŠŠè‹±æ–‡ç¿»è¯‘ä¸ºæ–°çš„ä¸­æ–‡
â”‚ â”‚          - [Seq2Seq](textgen/seq2seq)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†Seq2Seqã€ConvSeq2Seqã€BARTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€æ‘˜è¦ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
â”‚ â”‚          - [T5](textgen/t5)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†T5å’ŒCopyT5æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡æœ¬ç¿»è¯‘ã€å¯¹è¯ç”Ÿæˆã€å¯¹è”ç”Ÿæˆã€æ–‡æ¡ˆæ’°å†™ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
â”‚ â”‚          - [GPT2](textgen/language_modeling)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†GTP2æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºæ–‡ç« ç”Ÿæˆã€å¯¹è”ç”Ÿæˆç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
â”‚ â”‚          - [SongNet](textgen/language_modeling/songnet_model.py)ï¼šæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†SongNetæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¯ä»¥ç”¨äºè§„èŒƒæ ¼å¼çš„è¯—è¯ã€æ­Œè¯ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
â”‚ â”‚          - [TGLS](textgen/unsup_generation)ï¼šæœ¬é¡¹ç›®å®ç°äº†[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)æ— ç›‘ç£ç›¸ä¼¼æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯ä¸€ç§â€œå…ˆæœç´¢åå­¦ä¹ â€çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åå¤è¿­ä»£å­¦ä¹ å€™é€‰é›†ï¼Œæœ€ç»ˆæ¨¡å‹èƒ½ç”Ÿæˆç±»ä¼¼å€™é€‰é›†çš„é«˜è´¨é‡ç›¸ä¼¼æ–‡æœ¬
â”‚ â”‚          
â”‚ â”‚          
â”‚ â”‚ +        ## Release Models
â”‚ â”‚ +        releaseåŸºäº`textgen`è®­ç»ƒçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å·²ç»releaseåˆ°HuggingFace modelsï¼ŒæŒ‡å®šæ¨¡å‹åç§°`textgen`ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
â”‚ â”‚ +        
â”‚ â”‚ +        |Model|Arch|Intro|Training|Inference|
â”‚ â”‚ +        |:-- |:--- |:--- |:--- |:--- |
â”‚ â”‚ +        |[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
â”‚ â”‚ +        |[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
â”‚ â”‚ +        |[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
â”‚ â”‚ +        |[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
â”‚ â”‚ +        |[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
â”‚ â”‚ +        
â”‚ â”‚ +        
â”‚ â”‚          # Demo
â”‚ â”‚          
â”‚ â”‚          HuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate
â”‚ â”‚          
â”‚ â”‚          ![](docs/hf.png)
â”‚ â”‚          
â”‚ â”‚          run example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:
â”‚ â”‚ @@ -260,22 +266,14 @@
â”‚ â”‚          output:
â”‚ â”‚          ```shell
â”‚ â”‚          inputs: ['ä»€ä¹ˆæ˜¯ai', 'ä½ æ˜¯ä»€ä¹ˆç±»å‹çš„è®¡ç®—æœº', 'ä½ çŸ¥é“çƒ­åŠ›å­¦å—']
â”‚ â”‚          outputs: ['äººå·¥æ™ºèƒ½æœ‰ä¸¤ä¸ªå¹¿ä¹‰çš„å®šä¹‰,ä»»ä½•æ‹Ÿäººçš„æœºæ¢°,å¦‚åœ¨å¡é›·å°”capeks', 'æˆ‘çš„ç¨‹åºè¿è¡Œåœ¨Python,æ‰€ä»¥æˆ‘åœ¨ä»»ä½•ç”µè„‘ä¸Šå·¥ä½œ!', 'ä»€ä¹ˆæ˜¯çƒ­åŠ›å­¦']
â”‚ â”‚          ```
â”‚ â”‚          
â”‚ â”‚          
â”‚ â”‚ -        ### T5 æ¨¡å‹åº”ç”¨
â”‚ â”‚ -        
â”‚ â”‚ -        releaseåŸºäºT5çš„fine-tunedåçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å…¨éƒ¨releaseåˆ°HuggingFace modelsï¼Œ`textgen`å¯è‡ªåŠ¨ä¸‹è½½ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
â”‚ â”‚ -        
â”‚ â”‚ -        |Model|Arch|Intro|Training|Inference|
â”‚ â”‚ -        |:-- |:--- |:--- |:--- |:--- |
â”‚ â”‚ -        |[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|ä¸­æ–‡NLPå¤šä»»åŠ¡Promptæ¨¡å‹|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|
â”‚ â”‚ -        |[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tunedä¸­æ–‡å¯¹è”åçš„æ¨¡å‹|[å¯¹è”ç”Ÿæˆæ¨¡å‹è°ƒç ”](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|
â”‚ â”‚          
â”‚ â”‚          
â”‚ â”‚          ## GPT2 æ¨¡å‹
â”‚ â”‚          
â”‚ â”‚          ### ä¸­æ–‡GPT2 - æ–‡ç« ç”Ÿæˆ
â”‚ â”‚          
â”‚ â”‚          ä½¿ç”¨ä¸­æ–‡æ•°æ®é›†ï¼ˆæ®µè½æ ¼å¼ï¼Œ`\n`é—´éš”ï¼‰ï¼Œè®­ç»ƒGPT2æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè¯—æ­Œç”Ÿæˆã€æ–‡ç« ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
â”‚ â”‚ @@ -300,23 +298,14 @@
â”‚ â”‚          ## SongNet æ¨¡å‹
â”‚ â”‚          
â”‚ â”‚          æ ¼å¼æ§åˆ¶çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œpaperè§[SongNet: Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022)ï¼Œ
â”‚ â”‚          é€‚ç”¨äºå¼ºéŸµå¾‹æ ¼å¼è¦æ±‚çš„è¯—æ­Œã€å¯¹è”ã€æ­Œè¯ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
â”‚ â”‚          
â”‚ â”‚          example: [examples/language_generation/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)
â”‚ â”‚          
â”‚ â”‚ -        ### SongNet æ¨¡å‹åº”ç”¨
â”‚ â”‚ -        
â”‚ â”‚ -        releaseåŸºäºSongNetçš„ä¸­æ–‡æ¨¡å‹ï¼Œæ¨¡å‹å…¨éƒ¨releaseåˆ°HuggingFace modelsï¼Œ`textgen`å¯è‡ªåŠ¨ä¸‹è½½ï¼Œå¯ç›´æ¥ä½¿ç”¨ã€‚
â”‚ â”‚ -        
â”‚ â”‚ -        |Model|Arch|Intro|Training|Inference|
â”‚ â”‚ -        |:-- |:--- |:--- |:--- |:--- |
â”‚ â”‚ -        |[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNeté¢„è®­ç»ƒæ¨¡å‹|-|-|
â”‚ â”‚ -        |[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tunedå®‹è¯åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|
â”‚ â”‚ -        |[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tunedå¯¹è”åçš„æ¨¡å‹|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|
â”‚ â”‚          
â”‚ â”‚          
â”‚ â”‚          ## Keyword Text Augmentation(EDA/UDA)
â”‚ â”‚          
â”‚ â”‚          example: [examples/text_augmentation_demo.py](examples/text_augmentation_demo.py)
â”‚ â”‚          
â”‚ â”‚          ```python
â”‚ â”‚ @@ -434,14 +423,28 @@
â”‚ â”‚          - Issue(å»ºè®®)
â”‚ â”‚            ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)
â”‚ â”‚          - é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
â”‚ â”‚          - å¾®ä¿¡æˆ‘ï¼š åŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸å-NLP* è¿›NLPäº¤æµç¾¤ã€‚
â”‚ â”‚          
â”‚ â”‚          <img src="docs/wechat.jpeg" width="200" />
â”‚ â”‚          
â”‚ â”‚ +        
â”‚ â”‚ +        # Citation
â”‚ â”‚ +        
â”‚ â”‚ +        å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†textgenï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š
â”‚ â”‚ +        
â”‚ â”‚ +        ```latex
â”‚ â”‚ +        @misc{textgen,
â”‚ â”‚ +          title={textgen: Text Generation Tool},
â”‚ â”‚ +          author={Xu Ming},
â”‚ â”‚ +          year={2021},
â”‚ â”‚ +          howpublished={\url{https://github.com/shibing624/textgen}},
â”‚ â”‚ +        }
â”‚ â”‚ +        ```
â”‚ â”‚ +        
â”‚ â”‚          # License
â”‚ â”‚          
â”‚ â”‚          æˆæƒåè®®ä¸º [The Apache License 2.0](/LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ textgençš„é“¾æ¥å’Œæˆæƒåè®®ã€‚
â”‚ â”‚          
â”‚ â”‚          # Contribute
â”‚ â”‚          
â”‚ â”‚          é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š
â”‚ â”‚   --- textgen-0.1.7/textgen.egg-info/SOURCES.txt
â”‚ â”œâ”€â”€ +++ textgen-0.1.8/textgen.egg-info/SOURCES.txt
â”‚ â”‚â”„ Files 8% similar despite different names
â”‚ â”‚ @@ -11,14 +11,17 @@
â”‚ â”‚  textgen/augment/__init__.py
â”‚ â”‚  textgen/augment/sentence_level_augment.py
â”‚ â”‚  textgen/augment/text_augment.py
â”‚ â”‚  textgen/augment/tokenizer.py
â”‚ â”‚  textgen/augment/translate_api.py
â”‚ â”‚  textgen/augment/word_level_augment.py
â”‚ â”‚  textgen/augment/word_vocab.py
â”‚ â”‚ +textgen/chatglm/__init__.py
â”‚ â”‚ +textgen/chatglm/chatglm_model.py
â”‚ â”‚ +textgen/chatglm/chatglm_utils.py
â”‚ â”‚  textgen/config/__init__.py
â”‚ â”‚  textgen/config/global_args.py
â”‚ â”‚  textgen/config/model_args.py
â”‚ â”‚  textgen/custom_models/__init__.py
â”‚ â”‚  textgen/custom_models/models.py
â”‚ â”‚  textgen/data/HowNetPOSWord.txt
â”‚ â”‚  textgen/data/stopwords.txt
â”‚ â”‚ @@ -26,17 +29,14 @@
â”‚ â”‚  textgen/language_generation/language_generation_model.py
â”‚ â”‚  textgen/language_generation/language_generation_utils.py
â”‚ â”‚  textgen/language_modeling/__init__.py
â”‚ â”‚  textgen/language_modeling/language_modeling_model.py
â”‚ â”‚  textgen/language_modeling/language_modeling_utils.py
â”‚ â”‚  textgen/language_modeling/songnet_model.py
â”‚ â”‚  textgen/language_modeling/songnet_utils.py
â”‚ â”‚ -textgen/question_answering/__init__.py
â”‚ â”‚ -textgen/question_answering/question_answering_model.py
â”‚ â”‚ -textgen/question_answering/question_answering_utils.py
â”‚ â”‚  textgen/seq2seq/__init__.py
â”‚ â”‚  textgen/seq2seq/bart_seq2seq_model.py
â”‚ â”‚  textgen/seq2seq/bart_seq2seq_utils.py
â”‚ â”‚  textgen/seq2seq/conv_seq2seq_model.py
â”‚ â”‚  textgen/seq2seq/data_reader.py
â”‚ â”‚  textgen/seq2seq/seq2seq_model.py
â”‚ â”‚  textgen/seq2seq/seq2seq_trainer.py
