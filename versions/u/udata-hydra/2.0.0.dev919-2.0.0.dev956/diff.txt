--- tmp/udata_hydra-2.0.0.dev919.tar.gz
+++ tmp/udata_hydra-2.0.0.dev956.tar.gz
â”œâ”€â”€ filetype from file(1)
â”‚ @@ -1 +1 @@
â”‚ -gzip compressed data, was "udata_hydra-2.0.0.dev919.tar", max compression
â”‚ +gzip compressed data, was "udata_hydra-2.0.0.dev956.tar", max compression
â”‚   --- udata_hydra-2.0.0.dev919.tar
â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956.tar
â”‚ â”œâ”€â”€ file list
â”‚ â”‚ @@ -1,35 +1,39 @@
â”‚ â”‚ --rw-r--r--   0        0        0     8815 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/README.md
â”‚ â”‚ --rw-r--r--   0        0        0     1266 2023-02-01 16:42:03.727985 udata_hydra-2.0.0.dev919/pyproject.toml
â”‚ â”‚ --rw-r--r--   0        0        0     1145 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/__init__.py
â”‚ â”‚ --rw-r--r--   0        0        0        0 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/analysis/__init__.py
â”‚ â”‚ --rw-r--r--   0        0        0     9005 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/analysis/csv.py
â”‚ â”‚ --rw-r--r--   0        0        0     9530 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/analysis/resource.py
â”‚ â”‚ --rw-r--r--   0        0        0    10921 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/app.py
â”‚ â”‚ --rw-r--r--   0        0        0     9058 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/cli.py
â”‚ â”‚ --rw-r--r--   0        0        0     1436 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/config_default.toml
â”‚ â”‚ --rw-r--r--   0        0        0     1222 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/context.py
â”‚ â”‚ --rw-r--r--   0        0        0    12800 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/crawl.py
â”‚ â”‚ --rw-r--r--   0        0        0      875 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/logger.py
â”‚ â”‚ --rw-r--r--   0        0        0     2125 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/migrations/__init__.py
â”‚ â”‚ --rw-r--r--   0        0        0      224 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/migrations/csv/20221205_initial_up_rev1.sql
â”‚ â”‚ --rw-r--r--   0        0        0       63 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/migrations/csv/20230130_drop_migrations.sql
â”‚ â”‚ --rw-r--r--   0        0        0      756 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/migrations/main/20221205_initial_up_rev1.sql
â”‚ â”‚ --rw-r--r--   0        0        0       96 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/migrations/main/20221206_rev1_up_rev2.sql
â”‚ â”‚ --rw-r--r--   0        0        0       84 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/migrations/main/20221206_rev2_up_rev3.sql
â”‚ â”‚ --rw-r--r--   0        0        0       82 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/migrations/main/20221208_rev3_up_rev4.sql
â”‚ â”‚ --rw-r--r--   0        0        0      120 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/migrations/main/20221208_rev4_up_rev5.sql
â”‚ â”‚ --rw-r--r--   0        0        0      240 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/migrations/main/20230119_rev5_up_rev6.sql
â”‚ â”‚ --rw-r--r--   0        0        0      156 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/migrations/main/20230121_rev6_up_rev7.sql
â”‚ â”‚ --rw-r--r--   0        0        0      124 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/migrations/main/20230121_rev7_up_rev8.sql
â”‚ â”‚ --rw-r--r--   0        0        0       63 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/migrations/main/20230130_drop_migrations.sql
â”‚ â”‚ --rw-r--r--   0        0        0        0 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/utils/__init__.py
â”‚ â”‚ --rw-r--r--   0        0        0      342 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/utils/csv.py
â”‚ â”‚ --rw-r--r--   0        0        0     3030 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/utils/db.py
â”‚ â”‚ --rw-r--r--   0        0        0     1527 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/utils/file.py
â”‚ â”‚ --rw-r--r--   0        0        0     1174 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/utils/http.py
â”‚ â”‚ --rw-r--r--   0        0        0      236 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/utils/json.py
â”‚ â”‚ --rw-r--r--   0        0        0     1884 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/utils/minio.py
â”‚ â”‚ --rw-r--r--   0        0        0      482 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/utils/queue.py
â”‚ â”‚ --rw-r--r--   0        0        0      161 2023-02-01 16:40:32.000000 udata_hydra-2.0.0.dev919/udata_hydra/worker.py
â”‚ â”‚ --rw-r--r--   0        0        0    10697 1970-01-01 00:00:00.000000 udata_hydra-2.0.0.dev919/setup.py
â”‚ â”‚ --rw-r--r--   0        0        0    10222 1970-01-01 00:00:00.000000 udata_hydra-2.0.0.dev919/PKG-INFO
â”‚ â”‚ +-rw-r--r--   0        0        0     8815 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/README.md
â”‚ â”‚ +-rw-r--r--   0        0        0     1288 2023-02-11 09:03:37.414664 udata_hydra-2.0.0.dev956/pyproject.toml
â”‚ â”‚ +-rw-r--r--   0        0        0     1145 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/__init__.py
â”‚ â”‚ +-rw-r--r--   0        0        0        0 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/analysis/__init__.py
â”‚ â”‚ +-rw-r--r--   0        0        0    10167 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/analysis/csv.py
â”‚ â”‚ +-rw-r--r--   0        0        0      144 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/analysis/errors.py
â”‚ â”‚ +-rw-r--r--   0        0        0      694 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/analysis/helpers.py
â”‚ â”‚ +-rw-r--r--   0        0        0     9394 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/analysis/resource.py
â”‚ â”‚ +-rw-r--r--   0        0        0    10921 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/app.py
â”‚ â”‚ +-rw-r--r--   0        0        0     9198 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/cli.py
â”‚ â”‚ +-rw-r--r--   0        0        0     1436 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/config_default.toml
â”‚ â”‚ +-rw-r--r--   0        0        0     1222 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/context.py
â”‚ â”‚ +-rw-r--r--   0        0        0    12819 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/crawl.py
â”‚ â”‚ +-rw-r--r--   0        0        0      875 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/logger.py
â”‚ â”‚ +-rw-r--r--   0        0        0     2125 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/__init__.py
â”‚ â”‚ +-rw-r--r--   0        0        0      224 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/csv/20221205_initial_up_rev1.sql
â”‚ â”‚ +-rw-r--r--   0        0        0       63 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/csv/20230130_drop_migrations.sql
â”‚ â”‚ +-rw-r--r--   0        0        0      108 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/csv/20230206_datetime_aware.sql
â”‚ â”‚ +-rw-r--r--   0        0        0      756 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/main/20221205_initial_up_rev1.sql
â”‚ â”‚ +-rw-r--r--   0        0        0       96 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/main/20221206_rev1_up_rev2.sql
â”‚ â”‚ +-rw-r--r--   0        0        0       84 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/main/20221206_rev2_up_rev3.sql
â”‚ â”‚ +-rw-r--r--   0        0        0       82 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/main/20221208_rev3_up_rev4.sql
â”‚ â”‚ +-rw-r--r--   0        0        0      120 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/main/20221208_rev4_up_rev5.sql
â”‚ â”‚ +-rw-r--r--   0        0        0      240 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/main/20230119_rev5_up_rev6.sql
â”‚ â”‚ +-rw-r--r--   0        0        0      156 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/main/20230121_rev6_up_rev7.sql
â”‚ â”‚ +-rw-r--r--   0        0        0      124 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/main/20230121_rev7_up_rev8.sql
â”‚ â”‚ +-rw-r--r--   0        0        0       63 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/main/20230130_drop_migrations.sql
â”‚ â”‚ +-rw-r--r--   0        0        0      343 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/migrations/main/20230206_datetime_aware.sql
â”‚ â”‚ +-rw-r--r--   0        0        0        0 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/utils/__init__.py
â”‚ â”‚ +-rw-r--r--   0        0        0      342 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/utils/csv.py
â”‚ â”‚ +-rw-r--r--   0        0        0     3030 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/utils/db.py
â”‚ â”‚ +-rw-r--r--   0        0        0     1527 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/utils/file.py
â”‚ â”‚ +-rw-r--r--   0        0        0      955 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/utils/http.py
â”‚ â”‚ +-rw-r--r--   0        0        0      236 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/utils/json.py
â”‚ â”‚ +-rw-r--r--   0        0        0     1884 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/utils/minio.py
â”‚ â”‚ +-rw-r--r--   0        0        0      482 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/utils/queue.py
â”‚ â”‚ +-rw-r--r--   0        0        0      161 2023-02-11 09:01:58.000000 udata_hydra-2.0.0.dev956/udata_hydra/worker.py
â”‚ â”‚ +-rw-r--r--   0        0        0    10726 1970-01-01 00:00:00.000000 udata_hydra-2.0.0.dev956/setup.py
â”‚ â”‚ +-rw-r--r--   0        0        0    10265 1970-01-01 00:00:00.000000 udata_hydra-2.0.0.dev956/PKG-INFO
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/README.md
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/README.md
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/pyproject.toml
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/pyproject.toml
â”‚ â”‚â”„ Files 8% similar despite different names
â”‚ â”‚ @@ -1,10 +1,10 @@
â”‚ â”‚  [tool.poetry]
â”‚ â”‚  name = "udata-hydra"
â”‚ â”‚ -version = "2.0.0.dev919"
â”‚ â”‚ +version = "2.0.0.dev956"
â”‚ â”‚  description = "Async crawler and parsing service for data.gouv.fr"
â”‚ â”‚  authors = ["Opendata Team <opendatateam@data.gouv.fr>"]
â”‚ â”‚  license = "MIT"
â”‚ â”‚  readme = "README.md"
â”‚ â”‚  
â”‚ â”‚  [tool.poetry.dependencies]
â”‚ â”‚  python = "^3.9"
â”‚ â”‚ @@ -13,26 +13,27 @@
â”‚ â”‚  aiohttp = "^3.8.1"
â”‚ â”‚  asyncpg = "^0.27.0"
â”‚ â”‚  boto3 = "^1.21.21"
â”‚ â”‚  humanfriendly = "^10.0"
â”‚ â”‚  marshmallow = "^3.14.1"
â”‚ â”‚  minicli = "^0.5.0"
â”‚ â”‚  progressist = "^0.1.0"
â”‚ â”‚ -python-dateutil = "^2.8.2"
â”‚ â”‚  python-magic = "^0.4.25 "
â”‚ â”‚  redis = "^4.1.4"
â”‚ â”‚  sentry-sdk = "^1.11.1"
â”‚ â”‚  aiocontextvars = "^0.2.2"
â”‚ â”‚  coloredlogs = "^15.0.1"
â”‚ â”‚  rq = "^1.11.1"
â”‚ â”‚  toml = "^0.10.2"
â”‚ â”‚ -csv-detective = "^0.4.6"
â”‚ â”‚  str2float = "^0.0.9"
â”‚ â”‚  str2bool = "^1.1"
â”‚ â”‚  sqlalchemy = "^1.4.46"
â”‚ â”‚ +dateparser = "^1.1.7"
â”‚ â”‚ +python-dateutil = "^2.8.2"
â”‚ â”‚ +csv-detective = "^0.4.7"
â”‚ â”‚  
â”‚ â”‚  [tool.poetry.group.dev.dependencies]
â”‚ â”‚  flake8-quotes = "^3.3.1"
â”‚ â”‚  flake8 = "^4.0.1"
â”‚ â”‚  gunicorn = "^20.1.0"
â”‚ â”‚  aiohttp-devtools = "^1.0.post0"
â”‚ â”‚  aioresponses = "^0.7.3"
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/__init__.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/__init__.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/analysis/csv.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/analysis/csv.py
â”‚ â”‚â”„ Files 15% similar despite different names
â”‚ â”‚ @@ -8,25 +8,32 @@
â”‚ â”‚  from datetime import datetime
â”‚ â”‚  from typing import Any
â”‚ â”‚  
â”‚ â”‚  import sentry_sdk
â”‚ â”‚  
â”‚ â”‚  from csv_detective.explore_csv import routine as csv_detective_routine
â”‚ â”‚  from progressist import ProgressBar
â”‚ â”‚ -from sqlalchemy import MetaData, Table, Column, BigInteger, String, Float, Boolean, Integer
â”‚ â”‚ +from sqlalchemy import (
â”‚ â”‚ +    MetaData, Table, Column,
â”‚ â”‚ +    BigInteger, String, Float, Boolean, Integer, JSON, Date, DateTime,
â”‚ â”‚ +)
â”‚ â”‚  from sqlalchemy.dialects.postgresql import asyncpg
â”‚ â”‚  from sqlalchemy.schema import CreateTable
â”‚ â”‚  from str2bool import str2bool
â”‚ â”‚  from str2float import str2float
â”‚ â”‚  
â”‚ â”‚  from udata_hydra import context, config
â”‚ â”‚ +from udata_hydra.analysis import helpers
â”‚ â”‚ +from udata_hydra.analysis.errors import ParseException
â”‚ â”‚ +from udata_hydra.utils import queue
â”‚ â”‚  from udata_hydra.utils.db import (
â”‚ â”‚      get_check, insert_csv_analysis, compute_insert_query,
â”‚ â”‚      update_csv_analysis, get_csv_analysis,
â”‚ â”‚  )
â”‚ â”‚ +from udata_hydra.utils.http import send
â”‚ â”‚  from udata_hydra.utils.file import download_resource
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚  log = logging.getLogger("udata-hydra")
â”‚ â”‚  
â”‚ â”‚  # Increase CSV field size limit to maximum possible
â”‚ â”‚  # https://stackoverflow.com/a/15063941
â”‚ â”‚ @@ -39,63 +46,84 @@
â”‚ â”‚          field_size_limit = int(field_size_limit / 10)
â”‚ â”‚  
â”‚ â”‚  PYTHON_TYPE_TO_PG = {
â”‚ â”‚      "string": String,
â”‚ â”‚      "float": Float,
â”‚ â”‚      "int": BigInteger,
â”‚ â”‚      "bool": Boolean,
â”‚ â”‚ +    "json": JSON,
â”‚ â”‚ +    "date": Date,
â”‚ â”‚ +    "datetime": DateTime,
â”‚ â”‚  }
â”‚ â”‚  
â”‚ â”‚  PYTHON_TYPE_TO_PY = {
â”‚ â”‚      "string": str,
â”‚ â”‚      "float": float,
â”‚ â”‚      "int": int,
â”‚ â”‚      "bool": bool,
â”‚ â”‚ +    "json": helpers.to_json,
â”‚ â”‚ +    "date": helpers.to_date,
â”‚ â”‚ +    "datetime": helpers.to_datetime,
â”‚ â”‚  }
â”‚ â”‚  
â”‚ â”‚  RESERVED_COLS = ("__id", "tableoid", "xmin", "cmin", "xmax", "cmax", "ctid")
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚ +async def notify_udata(ca_id):
â”‚ â”‚ +    """Notify udata of the result of a parsing"""
â”‚ â”‚ +    analysis = await get_csv_analysis(ca_id)
â”‚ â”‚ +    resource_id = analysis["resource_id"]
â”‚ â”‚ +    db = await context.pool()
â”‚ â”‚ +    record = await db.fetchrow("SELECT dataset_id FROM catalog WHERE resource_id = $1", resource_id)
â”‚ â”‚ +    if record:
â”‚ â”‚ +        payload = {
â”‚ â”‚ +            "resource_id": resource_id,
â”‚ â”‚ +            "dataset_id": record["dataset_id"],
â”‚ â”‚ +            "document": {
â”‚ â”‚ +                "analysis:parsing:table": analysis["parsing_table"],
â”‚ â”‚ +                "analysis:parsing:error": analysis["parsing_error"],
â”‚ â”‚ +                "analysis:parsing:created_at": analysis["created_at"].isoformat(),
â”‚ â”‚ +            }
â”‚ â”‚ +        }
â”‚ â”‚ +        queue.enqueue(send, _priority="high", **payload)
â”‚ â”‚ +
â”‚ â”‚ +
â”‚ â”‚  async def analyse_csv(check_id: int = None, url: str = None, file_path: str = None, debug_insert: bool = False) -> None:
â”‚ â”‚      """Launch csv analysis from a check or an URL (debug), using previsously downloaded file at file_path if any"""
â”‚ â”‚      if not config.CSV_ANALYSIS_ENABLED:
â”‚ â”‚          log.debug("CSV_ANALYSIS_ENABLED turned off, skipping.")
â”‚ â”‚          return
â”‚ â”‚  
â”‚ â”‚      assert any(_ is not None for _ in (check_id, url))
â”‚ â”‚      check = await get_check(check_id) if check_id is not None else {}
â”‚ â”‚      url = check.get("url") or url
â”‚ â”‚  
â”‚ â”‚      headers = json.loads(check.get("headers") or "{}")
â”‚ â”‚      tmp_file = open(file_path, "rb") if file_path else await download_resource(url, headers)
â”‚ â”‚ +    table_name = hashlib.md5(url.encode("utf-8")).hexdigest()
â”‚ â”‚  
â”‚ â”‚      try:
â”‚ â”‚ -        try:
â”‚ â”‚ -            inspection_error = None
â”‚ â”‚ -            csv_inspection = await perform_csv_inspection(tmp_file.name)
â”‚ â”‚ -        except Exception as e:
â”‚ â”‚ -            inspection_error = e
â”‚ â”‚          ca_id = await insert_csv_analysis({
â”‚ â”‚              "resource_id": check.get("resource_id"),
â”‚ â”‚              "url": url,
â”‚ â”‚              "check_id": check_id,
â”‚ â”‚          })
â”‚ â”‚ -        if inspection_error:
â”‚ â”‚ -            await handle_parse_exception(inspection_error, "csv_detective", analysis_id=ca_id)
â”‚ â”‚ -            return
â”‚ â”‚ -        table_name = hashlib.md5(url.encode("utf-8")).hexdigest()
â”‚ â”‚ -        await csv_to_db(tmp_file.name, csv_inspection, table_name, debug_insert=debug_insert, analysis_id=ca_id)
â”‚ â”‚ +        csv_inspection = await perform_csv_inspection(tmp_file.name)
â”‚ â”‚ +        await csv_to_db(tmp_file.name, csv_inspection, table_name, debug_insert=debug_insert)
â”‚ â”‚          await update_csv_analysis(ca_id, {
â”‚ â”‚              "parsing_table": table_name,
â”‚ â”‚              "parsing_date": datetime.utcnow(),
â”‚ â”‚          })
â”‚ â”‚          await csv_to_db_index(table_name, csv_inspection, check)
â”‚ â”‚ +    except ParseException as e:
â”‚ â”‚ +        await handle_parse_exception(e, ca_id, table_name)
â”‚ â”‚      finally:
â”‚ â”‚          tmp_file.close()
â”‚ â”‚          os.remove(tmp_file.name)
â”‚ â”‚ +        await notify_udata(ca_id)
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚  def generate_dialect(inspection: dict) -> stdcsv.Dialect:
â”‚ â”‚      class CustomDialect(stdcsv.unix_dialect):
â”‚ â”‚          # TODO: it would be nice to have more info from csvdetective to feed the dialect
â”‚ â”‚          # in the meantime we might want to sniff the file a bit
â”‚ â”‚          delimiter = inspection["separator"]
â”‚ â”‚ @@ -129,28 +157,23 @@
â”‚ â”‚      compiled = CreateTable(table).compile(dialect=asyncpg.dialect())
â”‚ â”‚      # compiled query will want to write "%% mon pourcent" VARCHAR but will fail when querying "% mon pourcent"
â”‚ â”‚      # also, "% mon pourcent" works well in pg as a column
â”‚ â”‚      # TODO: dirty hack, maybe find an alternative
â”‚ â”‚      return compiled.string.replace("%%", "%")
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚ -async def csv_to_db(
â”‚ â”‚ -    file_path: str, inspection: dict, table_name: str,
â”‚ â”‚ -    debug_insert: bool = False, analysis_id: int = None,
â”‚ â”‚ -):
â”‚ â”‚ +async def csv_to_db(file_path: str, inspection: dict, table_name: str, debug_insert: bool = False):
â”‚ â”‚      """
â”‚ â”‚      Convert a csv file to database table using inspection data. It should (re)create one table:
â”‚ â”‚      - `table_name` with data from `file_path`
â”‚ â”‚  
â”‚ â”‚      :file_path: CSV file path to convert
â”‚ â”‚      :inspection: CSV detective report
â”‚ â”‚      :table_name: used to create tables
â”‚ â”‚      :debug_insert: insert record one by one instead of using postgresql COPY
â”‚ â”‚ -    # TODO: we might want to catch the error(s) a step above and get rid of this param
â”‚ â”‚ -    :analysis_id: used for reporting errors to the analysis object that lauched conversion
â”‚ â”‚      """
â”‚ â”‚      log.debug(f"Converting from CSV to db for {table_name}")
â”‚ â”‚      dialect = generate_dialect(inspection)
â”‚ â”‚      columns = inspection["columns"]
â”‚ â”‚      # build a `column_name: type` mapping and explicitely rename reserved column names
â”‚ â”‚      columns = {
â”‚ â”‚          f"{c}__hydra_renamed" if c.lower() in RESERVED_COLS else c: v["python_type"]
â”‚ â”‚ @@ -176,15 +199,15 @@
â”‚ â”‚          )
â”‚ â”‚          # this use postgresql COPY from an iterator, it's fast but might be difficult to debug
â”‚ â”‚          if not debug_insert:
â”‚ â”‚              # NB: also see copy_to_table for a file source
â”‚ â”‚              try:
â”‚ â”‚                  await db.copy_records_to_table(table_name, records=records, columns=columns.keys())
â”‚ â”‚              except Exception as e:  # I know what I'm doing, pinky swear
â”‚ â”‚ -                await handle_parse_exception(e, "copy_records_to_table", analysis_id)
â”‚ â”‚ +                raise ParseException("copy_records_to_table") from e
â”‚ â”‚          # this inserts rows from iterator one by one, slow but useful for debugging
â”‚ â”‚          else:
â”‚ â”‚              bar = ProgressBar(total=inspection["total_lines"])
â”‚ â”‚              for r in bar.iter(records):
â”‚ â”‚                  data = {k: v for k, v in zip(columns.keys(), r)}
â”‚ â”‚                  # NB: possible sql injection here, but should not be used in prod
â”‚ â”‚                  q = compute_insert_query(data, table_name, returning="__id")
â”‚ â”‚ @@ -196,35 +219,42 @@
â”‚ â”‚      db = await context.pool("csv")
â”‚ â”‚      q = "INSERT INTO tables_index(parsing_table, csv_detective, resource_id, url) VALUES($1, $2, $3, $4)"
â”‚ â”‚      await db.execute(q, table_name, json.dumps(inspection), check.get("resource_id"), check.get("url"))
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚  async def perform_csv_inspection(file_path):
â”‚ â”‚      """Launch csv-detective against given file"""
â”‚ â”‚ -    return csv_detective_routine(file_path)
â”‚ â”‚ +    try:
â”‚ â”‚ +        return csv_detective_routine(file_path)
â”‚ â”‚ +    except Exception as e:
â”‚ â”‚ +        raise ParseException("csv_detective") from e
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚  async def delete_table(table_name: str):
â”‚ â”‚      db = await context.pool("csv")
â”‚ â”‚      await db.execute(f'DROP TABLE IF EXISTS "{table_name}"')
â”‚ â”‚      await db.execute("DELETE FROM tables_index WHERE parsing_table = $1", table_name)
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚ -async def handle_parse_exception(e: Exception, step: str, analysis_id: int) -> None:
â”‚ â”‚ -    """Specific parsing_error handling. Enriches sentry w/ context if available,
â”‚ â”‚ -       and store error if in a check context"""
â”‚ â”‚ +async def handle_parse_exception(e: Exception, analysis_id: int, table_name: str) -> None:
â”‚ â”‚ +    """Specific ParsingError handling. Enriches sentry w/ context if available,
â”‚ â”‚ +       and store error if in a check context. Also cleanup :table_name: if needed."""
â”‚ â”‚ +    db = await context.pool("csv")
â”‚ â”‚ +    await db.execute(f'DROP TABLE IF EXISTS "{table_name}"')
â”‚ â”‚      if analysis_id:
â”‚ â”‚          if config.SENTRY_DSN:
â”‚ â”‚              analysis = await get_csv_analysis(analysis_id)
â”‚ â”‚              url = analysis["url"]
â”‚ â”‚              with sentry_sdk.push_scope() as scope:
â”‚ â”‚                  scope.set_extra("analysis_id", analysis_id)
â”‚ â”‚                  scope.set_extra("csv_url", url)
â”‚ â”‚                  scope.set_extra("resource_id", analysis["resource_id"])
â”‚ â”‚                  event_id = sentry_sdk.capture_exception(e)
â”‚ â”‚ -        err = f"{step}:sentry:{event_id}" if config.SENTRY_DSN else f"{step}:{str(e)}"
â”‚ â”‚ +        # e.__cause__ let us access the "inherited" error of ParseException (raise e from cause)
â”‚ â”‚ +        # it's called explicit exception chaining and it's very cool, look it up (PEP 3134)!
â”‚ â”‚ +        err = f"{e.step}:sentry:{event_id}" if config.SENTRY_DSN else f"{e.step}:{str(e.__cause__)}"
â”‚ â”‚          q = "UPDATE csv_analysis SET parsing_error = $1 WHERE id = $2"
â”‚ â”‚          db = await context.pool()
â”‚ â”‚          await db.execute(q, err, analysis_id)
â”‚ â”‚          log.error("Parsing error", exc_info=e)
â”‚ â”‚ -        return
â”‚ â”‚ -    raise e
â”‚ â”‚ +    else:
â”‚ â”‚ +        raise e
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/analysis/resource.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/analysis/resource.py
â”‚ â”‚â”„ Files 2% similar despite different names
â”‚ â”‚ @@ -3,15 +3,15 @@
â”‚ â”‚  import os
â”‚ â”‚  
â”‚ â”‚  from datetime import datetime
â”‚ â”‚  from typing import Union
â”‚ â”‚  
â”‚ â”‚  import magic
â”‚ â”‚  
â”‚ â”‚ -from dateutil.parser import parse as date_parser, ParserError
â”‚ â”‚ +from dateparser import parse as date_parser
â”‚ â”‚  
â”‚ â”‚  from udata_hydra import context
â”‚ â”‚  from udata_hydra.utils import queue
â”‚ â”‚  from udata_hydra.analysis.csv import analyse_csv
â”‚ â”‚  from udata_hydra.utils.csv import detect_csv_from_headers
â”‚ â”‚  from udata_hydra.utils.db import update_check, get_check
â”‚ â”‚  from udata_hydra.utils.file import compute_checksum_from_file, download_resource
â”‚ â”‚ @@ -194,23 +194,20 @@
â”‚ â”‚      async with pool.acquire() as connection:
â”‚ â”‚          data = await connection.fetchrow(q, value)
â”‚ â”‚      if not data:
â”‚ â”‚          return
â”‚ â”‚  
â”‚ â”‚      # last modified header check
â”‚ â”‚      if data["last_modified"]:
â”‚ â”‚ -        try:
â”‚ â”‚ -            # this is GMT so we should be able to safely ignore tz info
â”‚ â”‚ -            last_modified_date = date_parser(data["last_modified"], ignoretz=True).isoformat()
â”‚ â”‚ +        last_modified_date = date_parser(data["last_modified"])
â”‚ â”‚ +        if last_modified_date:
â”‚ â”‚              return {
â”‚ â”‚ -                "analysis:last-modified-at": last_modified_date,
â”‚ â”‚ +                "analysis:last-modified-at": last_modified_date.isoformat(),
â”‚ â”‚                  "analysis:last-modified-detection": "last-modified-header",
â”‚ â”‚              }
â”‚ â”‚ -        except ParserError:
â”‚ â”‚ -            pass
â”‚ â”‚  
â”‚ â”‚      # switch to content-length comparison
â”‚ â”‚      if not data["content_length"]:
â”‚ â”‚          return
â”‚ â”‚      q = """
â”‚ â”‚      SELECT
â”‚ â”‚          created_at,
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/app.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/app.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/cli.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/cli.py
â”‚ â”‚â”„ Files 3% similar despite different names
â”‚ â”‚ @@ -1,11 +1,11 @@
â”‚ â”‚  import csv
â”‚ â”‚  import os
â”‚ â”‚  
â”‚ â”‚ -from datetime import datetime
â”‚ â”‚ +from datetime import datetime, timezone
â”‚ â”‚  from pathlib import Path
â”‚ â”‚  from tempfile import NamedTemporaryFile
â”‚ â”‚  
â”‚ â”‚  import aiohttp
â”‚ â”‚  import asyncpg
â”‚ â”‚  
â”‚ â”‚  from humanfriendly import parse_size
â”‚ â”‚ @@ -69,15 +69,17 @@
â”‚ â”‚                      )
â”‚ â”‚                      VALUES ($1, $2, $3, $4, FALSE, FALSE)
â”‚ â”‚                      ON CONFLICT (dataset_id, resource_id, url) DO UPDATE SET deleted = FALSE
â”‚ â”‚                  """,
â”‚ â”‚                      row["dataset.id"],
â”‚ â”‚                      row["id"],
â”‚ â”‚                      row["url"],
â”‚ â”‚ -                    datetime.fromisoformat(row["harvest.modified_at"]) if row["harvest.modified_at"] else None,
â”‚ â”‚ +                    # force timezone info to UTC (catalog data should be in UTC)
â”‚ â”‚ +                    datetime.fromisoformat(row["harvest.modified_at"]).replace(tzinfo=timezone.utc)
â”‚ â”‚ +                    if row["harvest.modified_at"] else None,
â”‚ â”‚                  )
â”‚ â”‚          log.info("Catalog successfully upserted into DB.")
â”‚ â”‚      except Exception as e:
â”‚ â”‚          raise e
â”‚ â”‚      finally:
â”‚ â”‚          fd.close()
â”‚ â”‚          os.unlink(fd.name)
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/config_default.toml
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/config_default.toml
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/context.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/context.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/crawl.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/crawl.py
â”‚ â”‚â”„ Files 0% similar despite different names
â”‚ â”‚ @@ -1,11 +1,11 @@
â”‚ â”‚  import time
â”‚ â”‚  
â”‚ â”‚  from collections import defaultdict
â”‚ â”‚ -from datetime import datetime, timedelta
â”‚ â”‚ +from datetime import datetime, timedelta, timezone
â”‚ â”‚  from typing import Tuple
â”‚ â”‚  from urllib.parse import urlparse
â”‚ â”‚  
â”‚ â”‚  import aiohttp
â”‚ â”‚  import asyncio
â”‚ â”‚  
â”‚ â”‚  from humanfriendly import parse_timespan
â”‚ â”‚ @@ -100,15 +100,15 @@
â”‚ â”‚      return await insert_check(check_data), is_first_check
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚  async def is_backoff(domain) -> Tuple[bool, str]:
â”‚ â”‚      backoff = False, ""
â”‚ â”‚      no_backoff = [f"'{d}'" for d in config.NO_BACKOFF_DOMAINS]
â”‚ â”‚      no_backoff = f"({','.join(no_backoff)})"
â”‚ â”‚ -    since = datetime.utcnow() - timedelta(seconds=config.BACKOFF_PERIOD)
â”‚ â”‚ +    since = datetime.now(timezone.utc) - timedelta(seconds=config.BACKOFF_PERIOD)
â”‚ â”‚      pool = await context.pool()
â”‚ â”‚      async with pool.acquire() as connection:
â”‚ â”‚          # check if we trigger BACKOFF_NB_REQ for BACKOFF_PERIOD on this domain
â”‚ â”‚          res = await connection.fetchrow(
â”‚ â”‚              f"""
â”‚ â”‚              SELECT COUNT(*) FROM checks
â”‚ â”‚              WHERE domain = $1
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/logger.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/logger.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/migrations/__init__.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/migrations/__init__.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/migrations/main/20221205_initial_up_rev1.sql
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/migrations/main/20221205_initial_up_rev1.sql
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/utils/db.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/utils/db.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/utils/file.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/utils/file.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/utils/http.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/utils/http.py
â”‚ â”‚â”„ Files 14% similar despite different names
â”‚ â”‚ @@ -5,18 +5,14 @@
â”‚ â”‚  
â”‚ â”‚  from udata_hydra import config
â”‚ â”‚  
â”‚ â”‚  log = logging.getLogger("udata-hydra")
â”‚ â”‚  
â”‚ â”‚  
â”‚ â”‚  async def send(dataset_id: str, resource_id: str, document: dict) -> None:
â”‚ â”‚ -    # Extras in udata can't be None
â”‚ â”‚ -    # FIXME: we will need to send None values
â”‚ â”‚ -    # cf https://github.com/etalab/data.gouv.fr/issues/1001
â”‚ â”‚ -    document = {k: document[k] for k in document if document[k] is not None}
â”‚ â”‚      log.debug(f"Sending payload to udata {dataset_id}/{resource_id}: {json.dumps(document, indent=4)}")
â”‚ â”‚  
â”‚ â”‚      if not config.WEBHOOK_ENABLED:
â”‚ â”‚          log.debug("Webhook disabled, skipping send")
â”‚ â”‚          return
â”‚ â”‚  
â”‚ â”‚      if not config.UDATA_URI or not config.UDATA_URI_API_KEY:
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/udata_hydra/utils/minio.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/udata_hydra/utils/minio.py
â”‚ â”‚â”„ Files identical despite different names
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/setup.py
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/setup.py
â”‚ â”‚â”„ Files 1% similar despite different names
â”‚ â”‚ @@ -13,15 +13,16 @@
â”‚ â”‚  install_requires = \
â”‚ â”‚  ['aiocontextvars>=0.2.2,<0.3.0',
â”‚ â”‚   'aiohttp>=3.8.1,<4.0.0',
â”‚ â”‚   'asyncpg>=0.27.0,<0.28.0',
â”‚ â”‚   'boto3>=1.21.21,<2.0.0',
â”‚ â”‚   'cchardet>=2.1.7,<3.0.0',
â”‚ â”‚   'coloredlogs>=15.0.1,<16.0.0',
â”‚ â”‚ - 'csv-detective>=0.4.6,<0.5.0',
â”‚ â”‚ + 'csv-detective>=0.4.7,<0.5.0',
â”‚ â”‚ + 'dateparser>=1.1.7,<2.0.0',
â”‚ â”‚   'humanfriendly>=10.0,<11.0',
â”‚ â”‚   'marshmallow>=3.14.1,<4.0.0',
â”‚ â”‚   'minicli>=0.5.0,<0.6.0',
â”‚ â”‚   'pandas>=1.3.3,<2.0.0',
â”‚ â”‚   'progressist>=0.1.0,<0.2.0',
â”‚ â”‚   'python-dateutil>=2.8.2,<3.0.0',
â”‚ â”‚   'python-magic>=0.4.25,<0.5.0',
â”‚ â”‚ @@ -36,15 +37,15 @@
â”‚ â”‚  entry_points = \
â”‚ â”‚  {'console_scripts': ['udata-hydra = udata_hydra.cli:run',
â”‚ â”‚                       'udata-hydra-app = udata_hydra.app:run',
â”‚ â”‚                       'udata-hydra-crawl = udata_hydra.crawl:run']}
â”‚ â”‚  
â”‚ â”‚  setup_kwargs = {
â”‚ â”‚      'name': 'udata-hydra',
â”‚ â”‚ -    'version': '2.0.0.dev919',
â”‚ â”‚ +    'version': '2.0.0.dev956',
â”‚ â”‚      'description': 'Async crawler and parsing service for data.gouv.fr',
â”‚ â”‚      'long_description': '# udata-hydra ðŸ¦€\n\n`udata-hydra` is an async metadata crawler for [data.gouv.fr](https://www.data.gouv.fr).\n\nURLs are crawled via _aiohttp_, catalog and crawled metadata are stored in a _PostgreSQL_ database.\n\nSince it\'s called _hydra_, it also has mythical powers embedded:\n- analyse remote resource metadata over time to detect changes in the smartest way possible\n- if the remote resource is a CSV, convert it to a PostgreSQL table, ready for APIfication\n- send crawl and analysis info to a udata instance\n\n## CLI\n\n### Create database structure\n\nInstall udata-hydra dependencies and cli.\n`poetry install`\n\n`poetry run udata-hydra migrate`\n\n### Load (UPSERT) latest catalog version from data.gouv.fr\n\n`udata-hydra load-catalog`\n\n## Crawler\n\n`udata-hydra-crawl`\n\nIt will crawl (forever) the catalog according to config set in `config.py`.\n\n`BATCH_SIZE` URLs are queued at each loop run.\n\nThe crawler will start with URLs never checked and then proceed with URLs crawled before `SINCE` interval. It will then wait until something changes (catalog or time).\n\nThere\'s a by-domain backoff mecanism. The crawler will wait when, for a given domain in a given batch, `BACKOFF_NB_REQ` is exceeded in a period of `BACKOFF_PERIOD` seconds. It will retry until the backoff is lifted.\n\nIf an URL matches one of the `EXCLUDED_PATTERNS`, it will never be checked.\n\n## Worker\n\nA job queuing system is used to process long-running tasks. Launch the worker with the following command:\n\n`poetry run rq worker -c udata_hydra.worker`\n\nMonitor worker status:\n\n`poetry run rq info -c udata_hydra.worker --interval 1`\n\n## CSV conversion to database\n\nConverted CSV tables will be stored in the database specified via `config.DATABASE_URL_CSV`. For tests it\'s same database as for the catalog. Locally, `docker compose` will launch two distinct database containers.\n\n## API\n\n### Run\n\n```\npoetry install\npoetry run adev runserver udata_hydra/app.py\n```\n\n### Get latest check\n\nWorks with `?url={url}` and `?resource_id={resource_id}`.\n\n```\n$ curl -s "http://localhost:8000/api/checks/latest/?url=http://opendata-sig.saintdenis.re/datasets/661e19974bcc48849bbff7c9637c5c28_1.csv" | json_pp\n{\n   "status" : 200,\n   "catalog_id" : 64148,\n   "deleted" : false,\n   "error" : null,\n   "created_at" : "2021-02-06T12:19:08.203055",\n   "response_time" : 0.830198049545288,\n   "url" : "http://opendata-sig.saintdenis.re/datasets/661e19974bcc48849bbff7c9637c5c28_1.csv",\n   "domain" : "opendata-sig.saintdenis.re",\n   "timeout" : false,\n   "id" : 114750,\n   "dataset_id" : "5c34944606e3e73d4a551889",\n   "resource_id" : "b3678c59-5b35-43ad-9379-fce29e5b56fe",\n   "headers" : {\n      "content-disposition" : "attachment; filename=\\"xn--Dlimitation_des_cantons-bcc.csv\\"",\n      "server" : "openresty",\n      "x-amz-meta-cachetime" : "191",\n      "last-modified" : "Wed, 29 Apr 2020 02:19:04 GMT",\n      "content-encoding" : "gzip",\n      "content-type" : "text/csv",\n      "cache-control" : "must-revalidate",\n      "etag" : "\\"20415964703d9ccc4815d7126aa3a6d8\\"",\n      "content-length" : "207",\n      "date" : "Sat, 06 Feb 2021 12:19:08 GMT",\n      "x-amz-meta-contentlastmodified" : "2018-11-19T09:38:28.490Z",\n      "connection" : "keep-alive",\n      "vary" : "Accept-Encoding"\n   }\n}\n```\n\n### Get all checks for an URL or resource\n\nWorks with `?url={url}` and `?resource_id={resource_id}`.\n\n```\n$ curl -s "http://localhost:8000/api/checks/all/?url=http://www.drees.sante.gouv.fr/IMG/xls/er864.xls" | json_pp\n[\n   {\n      "domain" : "www.drees.sante.gouv.fr",\n      "dataset_id" : "53d6eadba3a72954d9dd62f5",\n      "timeout" : false,\n      "deleted" : false,\n      "response_time" : null,\n      "error" : "Cannot connect to host www.drees.sante.gouv.fr:443 ssl:True [SSLCertVerificationError: (1, \\"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for \'www.drees.sante.gouv.fr\'. (_ssl.c:1122)\\")]",\n      "catalog_id" : 232112,\n      "url" : "http://www.drees.sante.gouv.fr/IMG/xls/er864.xls",\n      "headers" : {},\n      "id" : 165107,\n      "created_at" : "2021-02-06T14:32:47.675854",\n      "resource_id" : "93dfd449-9d26-4bb0-a6a9-ee49b1b8a4d7",\n      "status" : null\n   },\n   {\n      "timeout" : false,\n      "deleted" : false,\n      "response_time" : null,\n      "error" : "Cannot connect to host www.drees.sante.gouv.fr:443 ssl:True [SSLCertVerificationError: (1, \\"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for \'www.drees.sante.gouv.fr\'. (_ssl.c:1122)\\")]",\n      "domain" : "www.drees.sante.gouv.fr",\n      "dataset_id" : "53d6eadba3a72954d9dd62f5",\n      "created_at" : "2020-12-24T17:06:58.158125",\n      "resource_id" : "93dfd449-9d26-4bb0-a6a9-ee49b1b8a4d7",\n      "status" : null,\n      "catalog_id" : 232112,\n      "url" : "http://www.drees.sante.gouv.fr/IMG/xls/er864.xls",\n      "headers" : {},\n      "id" : 65092\n   }\n]\n```\n\n### Get crawling status\n\n```\n$ curl -s "http://localhost:8000/api/status/crawler/" | json_pp\n{\n   "fresh_checks_percentage" : 0.4,\n   "pending_checks" : 142153,\n   "total" : 142687,\n   "fresh_checks" : 534,\n   "checks_percentage" : 0.4\n}\n```\n\n### Get worker status\n\n```\n$ curl -s "http://localhost:8000/api/status/worker/" | json_pp\n{\n   "queued" : {\n      "default" : 0,\n      "high" : 825,\n      "low" : 655\n   }\n}\n```\n\n### Get crawling stats\n\n```\n$ curl -s "http://localhost:8000/api/stats/" | json_pp\n{\n   "status" : [\n      {\n         "count" : 525,\n         "percentage" : 98.3,\n         "label" : "ok"\n      },\n      {\n         "label" : "error",\n         "percentage" : 1.3,\n         "count" : 7\n      },\n      {\n         "label" : "timeout",\n         "percentage" : 0.4,\n         "count" : 2\n      }\n   ],\n   "status_codes" : [\n      {\n         "code" : 200,\n         "count" : 413,\n         "percentage" : 78.7\n      },\n      {\n         "code" : 501,\n         "percentage" : 12.4,\n         "count" : 65\n      },\n      {\n         "percentage" : 6.1,\n         "count" : 32,\n         "code" : 404\n      },\n      {\n         "code" : 500,\n         "percentage" : 2.7,\n         "count" : 14\n      },\n      {\n         "code" : 502,\n         "count" : 1,\n         "percentage" : 0.2\n      }\n   ]\n}\n```\n\n## Using Webhook integration\n\n** Set the config values**\n\nCreate a `config.toml` where your service and commands are launched, or specify a path to a TOML file via the `HYDRA_SETTINGS` environment variable. `config.toml` or equivalent will override values from `udata_hydra/config_default.toml`, lookup there for values that can/need to be defined.\n\n```toml\nUDATA_URI = "https://dev.local:7000/api/2"\nUDATA_URI_API_KEY = "example.api.key"\nSENTRY_DSN = "https://{my-sentry-dsn}"\n```\n\nThe webhook integration sends HTTP messages to `udata` when resources are analyzed or checked to fill resources extras.\n\nRegarding analysis, there is a phase called "change detection". It will try to guess if a resource has been modified based on different criterions:\n- harvest modified date in catalog\n- content-length and last-modified headers\n- checksum comparison over time\n\nThe payload should look something like:\n\n```json\n{\n   "analysis:filesize": 91661,\n   "analysis:mime-type": "application/zip",\n   "analysis:checksum": "bef1de04601dedaf2d127418759b16915ba083be",\n   "analysis:last-modified-at": "2022-11-27T23:00:54.762000",\n   "analysis:last-modified-detection": "harvest-resource-metadata",\n}\n```\n\n## Development\n\n### docker-compose\n\nMultiple docker-compose files are provided:\n- a minimal `docker-compose.yml` with two PostgreSQL containers (one for catalog and metadata, the other for converted CSV to database)\n- `docker-compose.broker.yml` adds a Redis broker\n- `docker-compose.test.yml` launches a test DB, needed to run tests\n\nNB: you can launch compose from multiple files like this: `docker-compose -f docker-compose.yml -f docker-compose.test.yml up`\n\n### Logging & Debugging\n\nThe log level can be adjusted using the environment variable LOG_LEVEL.\nFor example, to set the log level to `DEBUG` when initializing the database, use `LOG_LEVEL="DEBUG" udata-hydra init_db `.\n\n### Writing a migration\n\n1. Add a file named `migrations/{YYYYMMDD}_{description}.sql` and write the SQL you need to perform migration.\n2. `udata-hydra migrate` will migrate the database as needeed.\n\n## Deployment\n\n3 services need to be deployed for the full stack to run:\n- worker\n- api / app\n- crawler\n\nRefer to each section to learn how to launch them. The only differences from dev to prod are:\n- use `HYDRA_SETTINGS` env var to point to your custom `config.toml`\n- use `HYDRA_APP_SOCKET_PATH` to configure where aiohttp should listen to a [reverse proxy connection (eg nginx)](https://docs.aiohttp.org/en/stable/deployment.html#nginx-configuration) and use `udata-hydra-app` to launch the app server\n',
â”‚ â”‚      'author': 'Opendata Team',
â”‚ â”‚      'author_email': 'opendatateam@data.gouv.fr',
â”‚ â”‚      'maintainer': 'None',
â”‚ â”‚      'maintainer_email': 'None',
â”‚ â”‚      'url': 'None',
â”‚ â”‚   --- udata_hydra-2.0.0.dev919/PKG-INFO
â”‚ â”œâ”€â”€ +++ udata_hydra-2.0.0.dev956/PKG-INFO
â”‚ â”‚â”„ Files 2% similar despite different names
â”‚ â”‚ @@ -1,10 +1,10 @@
â”‚ â”‚  Metadata-Version: 2.1
â”‚ â”‚  Name: udata-hydra
â”‚ â”‚ -Version: 2.0.0.dev919
â”‚ â”‚ +Version: 2.0.0.dev956
â”‚ â”‚  Summary: Async crawler and parsing service for data.gouv.fr
â”‚ â”‚  License: MIT
â”‚ â”‚  Author: Opendata Team
â”‚ â”‚  Author-email: opendatateam@data.gouv.fr
â”‚ â”‚  Requires-Python: >=3.9,<4.0
â”‚ â”‚  Classifier: License :: OSI Approved :: MIT License
â”‚ â”‚  Classifier: Programming Language :: Python :: 3
â”‚ â”‚ @@ -13,15 +13,16 @@
â”‚ â”‚  Classifier: Programming Language :: Python :: 3.11
â”‚ â”‚  Requires-Dist: aiocontextvars (>=0.2.2,<0.3.0)
â”‚ â”‚  Requires-Dist: aiohttp (>=3.8.1,<4.0.0)
â”‚ â”‚  Requires-Dist: asyncpg (>=0.27.0,<0.28.0)
â”‚ â”‚  Requires-Dist: boto3 (>=1.21.21,<2.0.0)
â”‚ â”‚  Requires-Dist: cchardet (>=2.1.7,<3.0.0)
â”‚ â”‚  Requires-Dist: coloredlogs (>=15.0.1,<16.0.0)
â”‚ â”‚ -Requires-Dist: csv-detective (>=0.4.6,<0.5.0)
â”‚ â”‚ +Requires-Dist: csv-detective (>=0.4.7,<0.5.0)
â”‚ â”‚ +Requires-Dist: dateparser (>=1.1.7,<2.0.0)
â”‚ â”‚  Requires-Dist: humanfriendly (>=10.0,<11.0)
â”‚ â”‚  Requires-Dist: marshmallow (>=3.14.1,<4.0.0)
â”‚ â”‚  Requires-Dist: minicli (>=0.5.0,<0.6.0)
â”‚ â”‚  Requires-Dist: pandas (>=1.3.3,<2.0.0)
â”‚ â”‚  Requires-Dist: progressist (>=0.1.0,<0.2.0)
â”‚ â”‚  Requires-Dist: python-dateutil (>=2.8.2,<3.0.0)
â”‚ â”‚  Requires-Dist: python-magic (>=0.4.25,<0.5.0)
